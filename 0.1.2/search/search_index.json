{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"kiara \u00b6 A research data management and orchestration engine. kiara user documentation: https://dharpa.org/kiara.documentation Code: https://github.com/DHARPA-Project/kiara Development documentation for this repo: https://dharpa.org/kiara Description \u00b6 Kiara is the data orchestration engine developed by the DHARPA project. It uses a modular approach to let users re-use tried and tested data orchestration pipelines, as well as create new ones from existing building blocks. It also helps you manage your research data, and augment it with automatically-, semi-automatically-, and manually- created metadata. Most of this is not yet implemented. Development \u00b6 Requirements \u00b6 Python (version >=3.6 -- some make targets only work for Python >=3.7, but kiara itself should work on 3.6) pip, virtualenv git make direnv (optional) Prepare development environment \u00b6 git clone https://github.com/DHARPA-Project/kiara.git cd kiara python3 -m venv .venv source .venv/bin/activate make init If you use direnv , you can alternatively do: git clone https://github.com/DHARPA-Project/kiara.git cd kiara cp .envrc.disabled .envrc direnv allow make init Note : you might want to adjust the Python version in .envrc (should not be necessary in most cases though) make targets \u00b6 init : init development project (install project & dev dependencies into virtualenv, as well as pre-commit git hook) update-modules : update default kiara modules package from git flake : run flake8 tests mypy : run mypy tests test : run unit tests docs : create static documentation pages (under build/site ) serve-docs : serve documentation pages (incl. auto-reload) for getting direct feedback when working on documentation clean : clean build directories For details (and other, minor targets), check the Makefile . Running tests \u00b6 > make test # or > make coverage Copyright & license \u00b6 This project is MPL v2.0 licensed, for the license text please check the LICENSE file in this repository. Copyright (c) 2021 DHARPA project","title":"Home"},{"location":"#kiara","text":"A research data management and orchestration engine. kiara user documentation: https://dharpa.org/kiara.documentation Code: https://github.com/DHARPA-Project/kiara Development documentation for this repo: https://dharpa.org/kiara","title":"kiara"},{"location":"#description","text":"Kiara is the data orchestration engine developed by the DHARPA project. It uses a modular approach to let users re-use tried and tested data orchestration pipelines, as well as create new ones from existing building blocks. It also helps you manage your research data, and augment it with automatically-, semi-automatically-, and manually- created metadata. Most of this is not yet implemented.","title":"Description"},{"location":"#development","text":"","title":"Development"},{"location":"#requirements","text":"Python (version >=3.6 -- some make targets only work for Python >=3.7, but kiara itself should work on 3.6) pip, virtualenv git make direnv (optional)","title":"Requirements"},{"location":"#prepare-development-environment","text":"git clone https://github.com/DHARPA-Project/kiara.git cd kiara python3 -m venv .venv source .venv/bin/activate make init If you use direnv , you can alternatively do: git clone https://github.com/DHARPA-Project/kiara.git cd kiara cp .envrc.disabled .envrc direnv allow make init Note : you might want to adjust the Python version in .envrc (should not be necessary in most cases though)","title":"Prepare development environment"},{"location":"#make-targets","text":"init : init development project (install project & dev dependencies into virtualenv, as well as pre-commit git hook) update-modules : update default kiara modules package from git flake : run flake8 tests mypy : run mypy tests test : run unit tests docs : create static documentation pages (under build/site ) serve-docs : serve documentation pages (incl. auto-reload) for getting direct feedback when working on documentation clean : clean build directories For details (and other, minor targets), check the Makefile .","title":"make targets"},{"location":"#running-tests","text":"> make test # or > make coverage","title":"Running tests"},{"location":"#copyright-license","text":"This project is MPL v2.0 licensed, for the license text please check the LICENSE file in this repository. Copyright (c) 2021 DHARPA project","title":"Copyright &amp; license"},{"location":"install/","text":"Installation \u00b6 Python package \u00b6 The python package is currently not available on pypi , so for now you have to install the package directly from the git repo. If you chooose this install method, I assume you know how to install Python packages manually, which is why I only show you an example way of getting kiara onto your machine: > python3 -m venv ~/.venvs/kiara > source ~/.venvs/kiara/bin/activate > pip install git+https://github.com/DHARPA-Project/kiara.git ... ... ... Successfully installed ... ... ... > kiara --help Usage: kiara [OPTIONS] COMMAND [ARGS]... ... ... In addition to the kiara package, you'll need a package containing modules, most likely kiara_modules.default : > pip install git+https://github.com/DHARPA-Project/kiara_modules.default.git","title":"Install"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#python-package","text":"The python package is currently not available on pypi , so for now you have to install the package directly from the git repo. If you chooose this install method, I assume you know how to install Python packages manually, which is why I only show you an example way of getting kiara onto your machine: > python3 -m venv ~/.venvs/kiara > source ~/.venvs/kiara/bin/activate > pip install git+https://github.com/DHARPA-Project/kiara.git ... ... ... Successfully installed ... ... ... > kiara --help Usage: kiara [OPTIONS] COMMAND [ARGS]... ... ... In addition to the kiara package, you'll need a package containing modules, most likely kiara_modules.default : > pip install git+https://github.com/DHARPA-Project/kiara_modules.default.git","title":"Python package"},{"location":"api_reference/kiara.config/","text":"kiara.config \u00b6 KiaraConfig pydantic-model \u00b6 data_store : str pydantic-field \u00b6 The path to the local kiara data store. default_processor : Union [ kiara . processing . synchronous . SynchronousProcessorConfig , kiara . processing . parallel . ThreadPoolProcessorConfig ] pydantic-field \u00b6 The configuration for the default processor to use. ignore_errors : bool pydantic-field \u00b6 If set, kiara will try to ignore most errors (that can be ignored). module_managers : List [ Union [ kiara . module_mgmt . python_classes . PythonModuleManagerConfig , kiara . module_mgmt . pipelines . PipelineModuleManagerConfig ]] pydantic-field \u00b6 The module managers to use in this kiara instance. yaml_config_settings_source ( settings ) \u00b6 A simple settings source that loads variables from a JSON file at the project's root. Here we happen to choose to use the env_file_encoding from Config when reading config.json Source code in kiara/config.py def yaml_config_settings_source ( settings : BaseSettings ) -> typing . Dict [ str , typing . Any ]: \"\"\" A simple settings source that loads variables from a JSON file at the project's root. Here we happen to choose to use the `env_file_encoding` from Config when reading `config.json` \"\"\" config_file = os . path . join ( kiara_app_dirs . user_config_dir , \"config.yaml\" ) if os . path . exists ( config_file ): data = get_data_from_file ( config_file ) return data else : return {}","title":"\u279c\u2007config"},{"location":"api_reference/kiara.config/#kiaraconfig","text":"","title":"kiara.config"},{"location":"api_reference/kiara.config/#kiara.config.KiaraConfig","text":"","title":"KiaraConfig"},{"location":"api_reference/kiara.config/#kiara.config.KiaraConfig.data_store","text":"The path to the local kiara data store.","title":"data_store"},{"location":"api_reference/kiara.config/#kiara.config.KiaraConfig.default_processor","text":"The configuration for the default processor to use.","title":"default_processor"},{"location":"api_reference/kiara.config/#kiara.config.KiaraConfig.ignore_errors","text":"If set, kiara will try to ignore most errors (that can be ignored).","title":"ignore_errors"},{"location":"api_reference/kiara.config/#kiara.config.KiaraConfig.module_managers","text":"The module managers to use in this kiara instance.","title":"module_managers"},{"location":"api_reference/kiara.config/#kiara.config.yaml_config_settings_source","text":"A simple settings source that loads variables from a JSON file at the project's root. Here we happen to choose to use the env_file_encoding from Config when reading config.json Source code in kiara/config.py def yaml_config_settings_source ( settings : BaseSettings ) -> typing . Dict [ str , typing . Any ]: \"\"\" A simple settings source that loads variables from a JSON file at the project's root. Here we happen to choose to use the `env_file_encoding` from Config when reading `config.json` \"\"\" config_file = os . path . join ( kiara_app_dirs . user_config_dir , \"config.yaml\" ) if os . path . exists ( config_file ): data = get_data_from_file ( config_file ) return data else : return {}","title":"yaml_config_settings_source()"},{"location":"api_reference/kiara.data/","text":"kiara.data \u00b6 Data and value related classes for Kiara .","title":"\u279c\u2007data"},{"location":"api_reference/kiara.data/#kiaradata","text":"Data and value related classes for Kiara .","title":"kiara.data"},{"location":"api_reference/kiara.data.registry/","text":"kiara.data.registry \u00b6 BaseDataRegistry \u00b6 Base class to extend if you want to write a kiara data registry. This outlines the main methods that must be available for an entity that holds some values and their data, as well as their associated aliases. In most cases users will interact with subclasses of the more fully featured DataRegistry class, but there are cases where it will make sense to sub-class this base class directly (like for example read-only 'archive' data registries). find_all_values_of_type ( self , value_type ) \u00b6 Find all values of a certain type. Source code in kiara/data/registry/__init__.py @deprecated ( reason = \"This will be removed soon in favor of a more generic way to query values.\" ) def find_all_values_of_type ( self , value_type : str ) -> typing . Dict [ str , Value ]: \"\"\"Find all values of a certain type.\"\"\" result = {} for value_id in self . value_ids : value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"No value registered for: { value_id } \" ) if value_obj . type_name == value_type : # type: ignore result [ value_id ] = value_obj return result get_versions_for_alias ( self , alias ) \u00b6 Return all available versions for the specified alias. Source code in kiara/data/registry/__init__.py def get_versions_for_alias ( self , alias : str ) -> typing . Iterable [ int ]: \"\"\"Return all available versions for the specified alias.\"\"\" if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . values . keys ()) DataRegistry \u00b6 register_alias ( self , value_or_schema , alias_name = None , callbacks = None ) \u00b6 Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Parameters: Name Type Description Default value_or_schema Union[kiara.data.values.Value, kiara.data.values.ValueSchema, str] a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) required preseed whether to add an empty/non-set value to the newly created slot required alias_name Optional[str] the alias name, will be auto-created if not provided None callbacks Optional[Iterable[kiara.data.registry.__init__.ValueSlotUpdateHandler]] a list of callbacks to trigger whenever the alias is updated None Source code in kiara/data/registry/__init__.py def register_alias ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], # preseed: bool = False, alias_name : typing . Optional [ str ] = None , callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> ValueSlot : \"\"\"Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Arguments: value_or_schema: a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) preseed: whether to add an empty/non-set value to the newly created slot alias_name: the alias name, will be auto-created if not provided callbacks: a list of callbacks to trigger whenever the alias is updated \"\"\" if alias_name is None : alias_name = str ( uuid . uuid4 ()) if not alias_name : raise Exception ( \"Empty alias name not allowed.\" ) match = bool ( re . match ( VALID_ALIAS_PATTERN , alias_name )) if not match : raise Exception ( f \"Invalid alias ' { alias_name } ': only alphanumeric characters, '-', and '_' allowed in alias name.\" ) # if the provided value_or_schema argument is a string, it must be a value_id (for now) if isinstance ( value_or_schema , str ): if value_or_schema not in self . value_ids : raise Exception ( f \"Can't register alias ' { alias_name } ', provided string ' { value_or_schema } ' not an existing value id.\" ) _value_or_schema = self . get_value_obj ( value_or_schema ) if _value_or_schema is None : raise Exception ( f \"No value registered for: { value_or_schema } \" ) value_or_schema = _value_or_schema if isinstance ( value_or_schema , Value ): _value_schema = value_or_schema . value_schema _value : typing . Optional [ Value ] = value_or_schema elif isinstance ( value_or_schema , ValueSchema ): _value_schema = value_or_schema _value = None else : raise TypeError ( f \"Invalid value type: { type ( value_or_schema ) } \" ) if alias_name in self . _get_available_aliases (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists.\" ) elif alias_name in self . _get_available_value_ids (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists as value id.\" ) # vs = ValueSlot.from_value(id=_id, value=value_or_schema) vs = self . _register_alias ( alias_name = alias_name , value_schema = _value_schema ) # self._value_slots[vs.id] = vs # if preseed: # _v = Value(value_schema=_value_schema, kiara=self._kiara, registry=self) # vs.add_value(_v) if callbacks : vs . register_callbacks ( * callbacks ) # self.register_callbacks(vs, *callbacks) if _value is not None : vs . add_value ( _value ) return vs ValueSlotUpdateHandler \u00b6 The call signature for callbacks that can be registered as value update handlers.","title":"\u279c\u2007data.registry"},{"location":"api_reference/kiara.data.registry/#kiaradataregistry","text":"","title":"kiara.data.registry"},{"location":"api_reference/kiara.data.registry/#kiara.data.registry.__init__.BaseDataRegistry","text":"Base class to extend if you want to write a kiara data registry. This outlines the main methods that must be available for an entity that holds some values and their data, as well as their associated aliases. In most cases users will interact with subclasses of the more fully featured DataRegistry class, but there are cases where it will make sense to sub-class this base class directly (like for example read-only 'archive' data registries).","title":"BaseDataRegistry"},{"location":"api_reference/kiara.data.registry/#kiara.data.registry.__init__.BaseDataRegistry.find_all_values_of_type","text":"Find all values of a certain type. Source code in kiara/data/registry/__init__.py @deprecated ( reason = \"This will be removed soon in favor of a more generic way to query values.\" ) def find_all_values_of_type ( self , value_type : str ) -> typing . Dict [ str , Value ]: \"\"\"Find all values of a certain type.\"\"\" result = {} for value_id in self . value_ids : value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"No value registered for: { value_id } \" ) if value_obj . type_name == value_type : # type: ignore result [ value_id ] = value_obj return result","title":"find_all_values_of_type()"},{"location":"api_reference/kiara.data.registry/#kiara.data.registry.__init__.BaseDataRegistry.get_versions_for_alias","text":"Return all available versions for the specified alias. Source code in kiara/data/registry/__init__.py def get_versions_for_alias ( self , alias : str ) -> typing . Iterable [ int ]: \"\"\"Return all available versions for the specified alias.\"\"\" if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . values . keys ())","title":"get_versions_for_alias()"},{"location":"api_reference/kiara.data.registry/#kiara.data.registry.__init__.DataRegistry","text":"","title":"DataRegistry"},{"location":"api_reference/kiara.data.registry/#kiara.data.registry.__init__.DataRegistry.register_alias","text":"Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Parameters: Name Type Description Default value_or_schema Union[kiara.data.values.Value, kiara.data.values.ValueSchema, str] a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) required preseed whether to add an empty/non-set value to the newly created slot required alias_name Optional[str] the alias name, will be auto-created if not provided None callbacks Optional[Iterable[kiara.data.registry.__init__.ValueSlotUpdateHandler]] a list of callbacks to trigger whenever the alias is updated None Source code in kiara/data/registry/__init__.py def register_alias ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], # preseed: bool = False, alias_name : typing . Optional [ str ] = None , callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> ValueSlot : \"\"\"Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Arguments: value_or_schema: a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) preseed: whether to add an empty/non-set value to the newly created slot alias_name: the alias name, will be auto-created if not provided callbacks: a list of callbacks to trigger whenever the alias is updated \"\"\" if alias_name is None : alias_name = str ( uuid . uuid4 ()) if not alias_name : raise Exception ( \"Empty alias name not allowed.\" ) match = bool ( re . match ( VALID_ALIAS_PATTERN , alias_name )) if not match : raise Exception ( f \"Invalid alias ' { alias_name } ': only alphanumeric characters, '-', and '_' allowed in alias name.\" ) # if the provided value_or_schema argument is a string, it must be a value_id (for now) if isinstance ( value_or_schema , str ): if value_or_schema not in self . value_ids : raise Exception ( f \"Can't register alias ' { alias_name } ', provided string ' { value_or_schema } ' not an existing value id.\" ) _value_or_schema = self . get_value_obj ( value_or_schema ) if _value_or_schema is None : raise Exception ( f \"No value registered for: { value_or_schema } \" ) value_or_schema = _value_or_schema if isinstance ( value_or_schema , Value ): _value_schema = value_or_schema . value_schema _value : typing . Optional [ Value ] = value_or_schema elif isinstance ( value_or_schema , ValueSchema ): _value_schema = value_or_schema _value = None else : raise TypeError ( f \"Invalid value type: { type ( value_or_schema ) } \" ) if alias_name in self . _get_available_aliases (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists.\" ) elif alias_name in self . _get_available_value_ids (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists as value id.\" ) # vs = ValueSlot.from_value(id=_id, value=value_or_schema) vs = self . _register_alias ( alias_name = alias_name , value_schema = _value_schema ) # self._value_slots[vs.id] = vs # if preseed: # _v = Value(value_schema=_value_schema, kiara=self._kiara, registry=self) # vs.add_value(_v) if callbacks : vs . register_callbacks ( * callbacks ) # self.register_callbacks(vs, *callbacks) if _value is not None : vs . add_value ( _value ) return vs","title":"register_alias()"},{"location":"api_reference/kiara.data.registry/#kiara.data.registry.__init__.ValueSlotUpdateHandler","text":"The call signature for callbacks that can be registered as value update handlers.","title":"ValueSlotUpdateHandler"},{"location":"api_reference/kiara.data.registry.store/","text":"kiara.data.registry.store \u00b6 SavedValueInfo pydantic-model \u00b6 load_config : LoadConfig pydantic-field required \u00b6 The configuration to load this value from disk (or however it is stored).","title":"\u279c\u2007data.registry.store"},{"location":"api_reference/kiara.data.registry.store/#kiaradataregistrystore","text":"","title":"kiara.data.registry.store"},{"location":"api_reference/kiara.data.registry.store/#kiara.data.registry.store.SavedValueInfo","text":"","title":"SavedValueInfo"},{"location":"api_reference/kiara.data.registry.store/#kiara.data.registry.store.SavedValueInfo.load_config","text":"The configuration to load this value from disk (or however it is stored).","title":"load_config"},{"location":"api_reference/kiara.data.types.core/","text":"kiara.data.types.core \u00b6 DeserializeConfigData \u00b6 A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = DeserializeConfig ( ** value ) return _value LoadConfigData \u00b6 A dictionary representing load config for a kiara value. The load config schema itself can be looked up via the wrapper class ' LoadConfig '. parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = LoadConfig ( ** value ) return _value ValueInfoData \u00b6 A dictionary representing a kiara ValueInfo object. parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueInfo ( ** value ) return _value ValueLineageData \u00b6 A dictionary representing a kiara ValueLineage. parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueLineage ( ** value ) return _value","title":"\u279c\u2007data.types.core"},{"location":"api_reference/kiara.data.types.core/#kiaradatatypescore","text":"","title":"kiara.data.types.core"},{"location":"api_reference/kiara.data.types.core/#kiara.data.types.core.DeserializeConfigData","text":"A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value.","title":"DeserializeConfigData"},{"location":"api_reference/kiara.data.types.core/#kiara.data.types.core.DeserializeConfigData.parse_value","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = DeserializeConfig ( ** value ) return _value","title":"parse_value()"},{"location":"api_reference/kiara.data.types.core/#kiara.data.types.core.LoadConfigData","text":"A dictionary representing load config for a kiara value. The load config schema itself can be looked up via the wrapper class ' LoadConfig '.","title":"LoadConfigData"},{"location":"api_reference/kiara.data.types.core/#kiara.data.types.core.LoadConfigData.parse_value","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = LoadConfig ( ** value ) return _value","title":"parse_value()"},{"location":"api_reference/kiara.data.types.core/#kiara.data.types.core.ValueInfoData","text":"A dictionary representing a kiara ValueInfo object.","title":"ValueInfoData"},{"location":"api_reference/kiara.data.types.core/#kiara.data.types.core.ValueInfoData.parse_value","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueInfo ( ** value ) return _value","title":"parse_value()"},{"location":"api_reference/kiara.data.types.core/#kiara.data.types.core.ValueLineageData","text":"A dictionary representing a kiara ValueLineage.","title":"ValueLineageData"},{"location":"api_reference/kiara.data.types.core/#kiara.data.types.core.ValueLineageData.parse_value","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueLineage ( ** value ) return _value","title":"parse_value()"},{"location":"api_reference/kiara.data.types/","text":"kiara.data.types \u00b6 This is the base module that contains everything data type-related in kiara . I'm still not 100% sure how to best implement the kiara type system, there are several ways it could be done, for example based on Python type-hints, using JSON-schema, Avro (which is my 2nd favourite option), as well as by implementing a custom type-class hierarchy. Which is what I have choosen to try first. For now, it looks like it'll work out, but there is a chance requirements I haven't forseen will crop up that could make this become ugly. Anyway, the way it works (for now) is that kiara comes with a set of often used types (the standard set of: scalars, list, dict, table & array, etc.) which each come with 2 functions that can serialize and deserialize values of that type in a persistant fashion -- which could be storing as a file on disk, or as a cell/row in a database. Those functions will most likley be kiara modules themselves, with even more restricted input/output type options. In addition, packages that contain modules can implement their own, custom types, if suitable ones are not available in core- kiara . Those can either be 'serialized/deserialized' into kiara -native types (which in turn will serialize them using their own serializing functions), or will have to implement custom serializing functionality (which will probably be discouraged, since this might not be trivial and there are quite a few things to consider). ValueType \u00b6 Base class that all kiara types must inherit from. kiara types have 3 main responsibilities: serialize into / deserialize from persistent state data validation metadata extraction Serializing being the arguably most important of those, because without most of the data management features of kiara would be impossible. Validation should not require any explanation. Metadata extraction is important, because that metadata will be available to other components of kiara (or frontends for it), without them having to request the actual data. That will hopefully make kiara very efficient in terms of memory management, as well as data transfer and I/O. Ideally, the actual data (bytes) will only be requested at the last possible moment. For example when a module needs the input data to do processing on it -- and even then it might be that it only requests a part of the data, say a single column of a table. Or when a frontend needs to display/visualize the data. calculate_value_hash ( value , hash_type ) classmethod \u00b6 Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. Source code in kiara/data/types/__init__.py @classmethod def calculate_value_hash ( cls , value : typing . Any , hash_type : str ) -> str : \"\"\"Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. \"\"\" raise Exception ( f \"Value type ' { cls . _value_type_name } ' does not support hash calculation.\" ) # type: ignore check_data ( data ) classmethod \u00b6 Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). Source code in kiara/data/types/__init__.py @classmethod def check_data ( cls , data : typing . Any ) -> typing . Optional [ \"ValueType\" ]: \"\"\"Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). \"\"\" return None get_type_hint ( self , context = 'python' ) \u00b6 Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. Source code in kiara/data/types/__init__.py def get_type_hint ( self , context : str = \"python\" ) -> typing . Optional [ typing . Type ]: \"\"\"Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. \"\"\" return None parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/__init__.py def parse_value ( self , value : typing . Any ) -> typing . Any : \"\"\"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Arguments: v: the value Returns: 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object \"\"\" return None ValueTypeConfigMetadata pydantic-model \u00b6 config_values : Dict [ str , kiara . metadata . ValueTypeAndDescription ] pydantic-field required \u00b6 The available configuration values. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this configuration. ValueTypeConfigSchema pydantic-model \u00b6 Base class that describes the configuration a ValueType class accepts. This is stored in the _config_cls class attribute in each ValueType class. By default, a ValueType is not configurable, unless the _config_cls class attribute points to a sub-class of this class. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/types/__init__.py def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/types/__init__.py def __hash__ ( self ): return hash ( self . config_hash ) get ( self , key ) \u00b6 Get the value for the specified configuation key. Source code in kiara/data/types/__init__.py def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) requires_config () classmethod \u00b6 Return whether this class can be used as-is, or requires configuration before an instance can be created. Source code in kiara/data/types/__init__.py @classmethod def requires_config ( cls ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : return True return False get_type_name ( obj ) \u00b6 Utility function to get a pretty string from the class of an object. Source code in kiara/data/types/__init__.py def get_type_name ( obj : typing . Any ): \"\"\"Utility function to get a pretty string from the class of an object.\"\"\" if obj . __class__ . __module__ == \"builtins\" : return obj . __class__ . __name__ else : return f \" { obj . __class__ . __module__ } . { obj . __class__ . __name__ } \"","title":"\u279c\u2007data.types"},{"location":"api_reference/kiara.data.types/#kiaradatatypes","text":"This is the base module that contains everything data type-related in kiara . I'm still not 100% sure how to best implement the kiara type system, there are several ways it could be done, for example based on Python type-hints, using JSON-schema, Avro (which is my 2nd favourite option), as well as by implementing a custom type-class hierarchy. Which is what I have choosen to try first. For now, it looks like it'll work out, but there is a chance requirements I haven't forseen will crop up that could make this become ugly. Anyway, the way it works (for now) is that kiara comes with a set of often used types (the standard set of: scalars, list, dict, table & array, etc.) which each come with 2 functions that can serialize and deserialize values of that type in a persistant fashion -- which could be storing as a file on disk, or as a cell/row in a database. Those functions will most likley be kiara modules themselves, with even more restricted input/output type options. In addition, packages that contain modules can implement their own, custom types, if suitable ones are not available in core- kiara . Those can either be 'serialized/deserialized' into kiara -native types (which in turn will serialize them using their own serializing functions), or will have to implement custom serializing functionality (which will probably be discouraged, since this might not be trivial and there are quite a few things to consider).","title":"kiara.data.types"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.ValueType","text":"Base class that all kiara types must inherit from. kiara types have 3 main responsibilities: serialize into / deserialize from persistent state data validation metadata extraction Serializing being the arguably most important of those, because without most of the data management features of kiara would be impossible. Validation should not require any explanation. Metadata extraction is important, because that metadata will be available to other components of kiara (or frontends for it), without them having to request the actual data. That will hopefully make kiara very efficient in terms of memory management, as well as data transfer and I/O. Ideally, the actual data (bytes) will only be requested at the last possible moment. For example when a module needs the input data to do processing on it -- and even then it might be that it only requests a part of the data, say a single column of a table. Or when a frontend needs to display/visualize the data.","title":"ValueType"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.ValueType.calculate_value_hash","text":"Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. Source code in kiara/data/types/__init__.py @classmethod def calculate_value_hash ( cls , value : typing . Any , hash_type : str ) -> str : \"\"\"Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. \"\"\" raise Exception ( f \"Value type ' { cls . _value_type_name } ' does not support hash calculation.\" ) # type: ignore","title":"calculate_value_hash()"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.ValueType.check_data","text":"Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). Source code in kiara/data/types/__init__.py @classmethod def check_data ( cls , data : typing . Any ) -> typing . Optional [ \"ValueType\" ]: \"\"\"Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). \"\"\" return None","title":"check_data()"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.ValueType.get_type_hint","text":"Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. Source code in kiara/data/types/__init__.py def get_type_hint ( self , context : str = \"python\" ) -> typing . Optional [ typing . Type ]: \"\"\"Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. \"\"\" return None","title":"get_type_hint()"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.ValueType.parse_value","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/__init__.py def parse_value ( self , value : typing . Any ) -> typing . Any : \"\"\"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Arguments: v: the value Returns: 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object \"\"\" return None","title":"parse_value()"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.ValueTypeConfigMetadata","text":"","title":"ValueTypeConfigMetadata"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.ValueTypeConfigMetadata.config_values","text":"The available configuration values.","title":"config_values"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.ValueTypeConfigMetadata.python_class","text":"The Python class for this configuration.","title":"python_class"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.ValueTypeConfigSchema","text":"Base class that describes the configuration a ValueType class accepts. This is stored in the _config_cls class attribute in each ValueType class. By default, a ValueType is not configurable, unless the _config_cls class attribute points to a sub-class of this class.","title":"ValueTypeConfigSchema"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.ValueTypeConfigSchema.__eq__","text":"Return self==value. Source code in kiara/data/types/__init__.py def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict ()","title":"__eq__()"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.ValueTypeConfigSchema.__hash__","text":"Return hash(self). Source code in kiara/data/types/__init__.py def __hash__ ( self ): return hash ( self . config_hash )","title":"__hash__()"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.ValueTypeConfigSchema.get","text":"Get the value for the specified configuation key. Source code in kiara/data/types/__init__.py def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key )","title":"get()"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.ValueTypeConfigSchema.requires_config","text":"Return whether this class can be used as-is, or requires configuration before an instance can be created. Source code in kiara/data/types/__init__.py @classmethod def requires_config ( cls ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : return True return False","title":"requires_config()"},{"location":"api_reference/kiara.data.types/#kiara.data.types.__init__.get_type_name","text":"Utility function to get a pretty string from the class of an object. Source code in kiara/data/types/__init__.py def get_type_name ( obj : typing . Any ): \"\"\"Utility function to get a pretty string from the class of an object.\"\"\" if obj . __class__ . __module__ == \"builtins\" : return obj . __class__ . __name__ else : return f \" { obj . __class__ . __module__ } . { obj . __class__ . __name__ } \"","title":"get_type_name()"},{"location":"api_reference/kiara.data.types.type_mgmt/","text":"kiara.data.types.type_mgmt \u00b6","title":"\u279c\u2007data.types.type_mgmt"},{"location":"api_reference/kiara.data.types.type_mgmt/#kiaradatatypestype_mgmt","text":"","title":"kiara.data.types.type_mgmt"},{"location":"api_reference/kiara.data.values/","text":"kiara.data.values \u00b6 A module that contains value-related classes for Kiara . A value in Kiara-speak is a pointer to actual data (aka 'bytes'). It contains metadata about that data (like whether it's valid/set, what type/schema it has, when it was last modified, ...), but it does not contain the data itself. The reason for that is that such data can be fairly large, and in a lot of cases it is not necessary for the code involved to have access to it, access to the metadata is enough. Each Value has a unique id, which can be used to retrieve the data (whole, or parts of it) from a DataRegistry . In addition, that id can be used to subscribe to change events for a value (published whenever the data that is associated with a value was changed). Value pydantic-model \u00b6 The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry. creation_date : datetime pydantic-field \u00b6 The time this value was created value happened. hashes : List [ kiara . data . values . __init__ . ValueHash ] pydantic-field \u00b6 Available hashes relating to the actual value data. This attribute is not populated by default, use the 'get_hashes()' method to request one or several hash items, afterwards those hashes will be reflected in this attribute. id : str pydantic-field required \u00b6 A unique id for this value. is_none : bool pydantic-field \u00b6 Whether the value is 'None'. is_set : bool pydantic-field \u00b6 Whether the value was set (in some way: user input, default, constant...). metadata : Dict [ str , Dict [ str , Any ]] pydantic-field \u00b6 Available metadata relating to the actual value data (size, no. of rows, etc. -- depending on data type). This attribute is not populated by default, use the 'get_metadata()' method to request one or several metadata items, afterwards those metadata items will be reflected in this attribute. type_obj property readonly \u00b6 Return the object that contains all the type information for this value. value_schema : ValueSchema pydantic-field required \u00b6 The schema of this value. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , Value ): return False return self . id == other . id __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash ( self . id ) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/data/values/__init__.py def __repr__ ( self ): return f \"Value(id= { self . id } , type= { self . type_name } , is_set= { self . is_set } , is_none= { self . is_none } \" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/data/values/__init__.py def __str__ ( self ): return self . __repr__ () get_hash ( self , hash_type ) \u00b6 Calculate the hash of a specified type for this value. Hashes are cached. Source code in kiara/data/values/__init__.py def get_hash ( self , hash_type : str ) -> ValueHash : \"\"\"Calculate the hash of a specified type for this value. Hashes are cached.\"\"\" hashes = self . get_hashes ( hash_type ) return list ( hashes )[ 0 ] get_metadata ( self , * metadata_keys , * , also_return_schema = False ) \u00b6 Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to True , the value will be a two-key map with the metadata under the metadata_item subkey, and the value schema under metadata_schema . If no metadata key is specified, all available metadata for this value type will be returned. Parameters: Name Type Description Default metadata_keys str the metadata keys to retrieve metadata (empty for all available metadata) () also_return_schema bool whether to also return the schema for each metadata item False Source code in kiara/data/values/__init__.py def get_metadata ( self , * metadata_keys : str , also_return_schema : bool = False ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: \"\"\"Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to `True`, the value will be a two-key map with the metadata under the ``metadata_item`` subkey, and the value schema under ``metadata_schema``. If no metadata key is specified, all available metadata for this value type will be returned. Arguments: metadata_keys: the metadata keys to retrieve metadata (empty for all available metadata) also_return_schema: whether to also return the schema for each metadata item \"\"\" if not metadata_keys : _metadata_keys = set ( self . _kiara . metadata_mgmt . get_metadata_keys_for_type ( self . type_name ) ) for key in self . metadata . keys (): _metadata_keys . add ( key ) else : _metadata_keys = set ( metadata_keys ) result = {} missing = set () for metadata_key in _metadata_keys : if metadata_key in self . metadata . keys (): if also_return_schema : result [ metadata_key ] = self . metadata [ metadata_key ] else : result [ metadata_key ] = self . metadata [ metadata_key ][ \"metadata_item\" ] else : missing . add ( metadata_key ) if not missing : return result _md = self . _kiara . metadata_mgmt . get_value_metadata ( self , * missing , also_return_schema = True ) for k , v in _md . items (): self . metadata [ k ] = v if also_return_schema : result [ k ] = v else : result [ k ] = v [ \"metadata_item\" ] return result item_is_valid ( self ) \u00b6 Check whether the current value is valid Source code in kiara/data/values/__init__.py def item_is_valid ( self ) -> bool : \"\"\"Check whether the current value is valid\"\"\" if self . value_schema . optional : return True else : return self . is_set and not self . is_none save ( self , aliases = None , register_missing_aliases = True ) \u00b6 Save this value, under the specified alias(es). Source code in kiara/data/values/__init__.py def save ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None , register_missing_aliases : bool = True , ) -> \"Value\" : \"\"\"Save this value, under the specified alias(es).\"\"\" # if self.get_value_lineage(): # for field_name, value in self.get_value_lineage().inputs.items(): # value_obj = self._registry.get_value_obj(value) # try: # value_obj.save() # except Exception as e: # print(e) value = self . _kiara . data_store . register_data ( self ) if aliases : self . _kiara . data_store . link_aliases ( value , * aliases , register_missing_aliases = register_missing_aliases ) return value ValueAlias pydantic-model \u00b6 repo_name : str pydantic-field \u00b6 The name of the data repo the value lives in. tag : str pydantic-field \u00b6 The tag for the alias. version : int pydantic-field \u00b6 The version of this alias. ValueHash pydantic-model \u00b6 hash : str pydantic-field required \u00b6 The value hash. hash_type : str pydantic-field required \u00b6 The value hash method. ValueInfo pydantic-model \u00b6 deserialize_config : DeserializeConfig pydantic-field \u00b6 The module config (incl. inputs) to deserialize the value. hashes : List [ kiara . data . values . __init__ . ValueHash ] pydantic-field \u00b6 All available hashes for this value. is_valid : bool pydantic-field required \u00b6 Whether the item is valid (in the context of its schema). lineage : ValueLineage pydantic-field \u00b6 Information about how the value was created. metadata : Dict [ str , Dict [ str , Any ]] pydantic-field required \u00b6 The metadata associated with this value. value_id : str pydantic-field required \u00b6 The value id. value_schema : ValueSchema pydantic-field required \u00b6 The value schema. ValueLineage pydantic-model \u00b6 Model containing the lineage of a value. inputs : Dict [ str , ValueInfo ] pydantic-field required \u00b6 The inputs that were used to create the value this refers to. output_name : str pydantic-field required \u00b6 The result field name for the value this refers to. value_index : Dict [ str , ValueInfo ] pydantic-field \u00b6 Index of all values that are associated with this value lineage. create_renderable ( self , ** config ) \u00b6 Create a renderable for this module configuration. Source code in kiara/data/values/__init__.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : all_ids = sorted ( find_all_ids_in_lineage ( self )) id_color_map = {} for idx , v_id in enumerate ( all_ids ): id_color_map [ v_id ] = COLOR_LIST [ idx % len ( COLOR_LIST )] show_ids = config . get ( \"include_ids\" , False ) tree = fill_lineage_tree ( self , include_ids = show_ids ) return tree ValueSchema pydantic-model \u00b6 The schema of a value. The schema contains the ValueType of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the type_config field. default : Any pydantic-field \u00b6 A default value. doc : str pydantic-field \u00b6 A description for the value of this input field. is_constant : bool pydantic-field \u00b6 Whether the value is a constant. optional : bool pydantic-field \u00b6 Whether this value is required (True), or whether 'None' value is allowed (False). type : str pydantic-field required \u00b6 The type of the value. type_config : Dict [ str , Any ] pydantic-field \u00b6 Configuration for the type, in case it's complex. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): if not isinstance ( other , ValueSchema ): return False return ( self . type , self . default ) == ( other . type , other . default ) __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash (( self . type , self . default )) ValueSlot pydantic-model \u00b6 id : str pydantic-field required \u00b6 The id for this slot. tags : Dict [ str , int ] pydantic-field \u00b6 The tags for this value slot (tag name as key, linked version as value. value_schema : ValueSchema pydantic-field required \u00b6 The schema for the values of this slot. values : Dict [ int , kiara . data . values . __init__ . Value ] pydantic-field \u00b6 The values of this slot, with versions as key. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): if not isinstance ( other , ValueSlot ): return False return self . id == other . id __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash ( self . id ) add_value ( self , value , trigger_callbacks = True , tags = None ) \u00b6 Add a value to this slot. Source code in kiara/data/values/__init__.py def add_value ( self , value : Value , trigger_callbacks : bool = True , tags : typing . Optional [ typing . Iterable [ str ]] = None , ) -> int : \"\"\"Add a value to this slot.\"\"\" if self . latest_version_nr != 0 and value . id == self . get_latest_value () . id : return self . latest_version_nr # TODO: check value data equality if self . value_schema . is_constant and self . values : if is_debug (): import traceback traceback . print_stack () raise Exception ( \"Can't add value: value slot marked as 'constant'.\" ) version = self . latest_version_nr + 1 assert version not in self . values . keys () self . values [ version ] = value if tags : for tag in tags : self . tags [ tag ] = version if trigger_callbacks : for cb in self . _callbacks . values (): cb . values_updated ( self ) return version","title":"\u279c\u2007data.values"},{"location":"api_reference/kiara.data.values/#kiaradatavalues","text":"A module that contains value-related classes for Kiara . A value in Kiara-speak is a pointer to actual data (aka 'bytes'). It contains metadata about that data (like whether it's valid/set, what type/schema it has, when it was last modified, ...), but it does not contain the data itself. The reason for that is that such data can be fairly large, and in a lot of cases it is not necessary for the code involved to have access to it, access to the metadata is enough. Each Value has a unique id, which can be used to retrieve the data (whole, or parts of it) from a DataRegistry . In addition, that id can be used to subscribe to change events for a value (published whenever the data that is associated with a value was changed).","title":"kiara.data.values"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value","text":"The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry.","title":"Value"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.creation_date","text":"The time this value was created value happened.","title":"creation_date"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.hashes","text":"Available hashes relating to the actual value data. This attribute is not populated by default, use the 'get_hashes()' method to request one or several hash items, afterwards those hashes will be reflected in this attribute.","title":"hashes"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.id","text":"A unique id for this value.","title":"id"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.is_none","text":"Whether the value is 'None'.","title":"is_none"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.is_set","text":"Whether the value was set (in some way: user input, default, constant...).","title":"is_set"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.metadata","text":"Available metadata relating to the actual value data (size, no. of rows, etc. -- depending on data type). This attribute is not populated by default, use the 'get_metadata()' method to request one or several metadata items, afterwards those metadata items will be reflected in this attribute.","title":"metadata"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.type_obj","text":"Return the object that contains all the type information for this value.","title":"type_obj"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.value_schema","text":"The schema of this value.","title":"value_schema"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.__eq__","text":"Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , Value ): return False return self . id == other . id","title":"__eq__()"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.__hash__","text":"Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash ( self . id )","title":"__hash__()"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.__repr__","text":"Return repr(self). Source code in kiara/data/values/__init__.py def __repr__ ( self ): return f \"Value(id= { self . id } , type= { self . type_name } , is_set= { self . is_set } , is_none= { self . is_none } \"","title":"__repr__()"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.__str__","text":"Return str(self). Source code in kiara/data/values/__init__.py def __str__ ( self ): return self . __repr__ ()","title":"__str__()"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.get_hash","text":"Calculate the hash of a specified type for this value. Hashes are cached. Source code in kiara/data/values/__init__.py def get_hash ( self , hash_type : str ) -> ValueHash : \"\"\"Calculate the hash of a specified type for this value. Hashes are cached.\"\"\" hashes = self . get_hashes ( hash_type ) return list ( hashes )[ 0 ]","title":"get_hash()"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.get_metadata","text":"Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to True , the value will be a two-key map with the metadata under the metadata_item subkey, and the value schema under metadata_schema . If no metadata key is specified, all available metadata for this value type will be returned. Parameters: Name Type Description Default metadata_keys str the metadata keys to retrieve metadata (empty for all available metadata) () also_return_schema bool whether to also return the schema for each metadata item False Source code in kiara/data/values/__init__.py def get_metadata ( self , * metadata_keys : str , also_return_schema : bool = False ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: \"\"\"Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to `True`, the value will be a two-key map with the metadata under the ``metadata_item`` subkey, and the value schema under ``metadata_schema``. If no metadata key is specified, all available metadata for this value type will be returned. Arguments: metadata_keys: the metadata keys to retrieve metadata (empty for all available metadata) also_return_schema: whether to also return the schema for each metadata item \"\"\" if not metadata_keys : _metadata_keys = set ( self . _kiara . metadata_mgmt . get_metadata_keys_for_type ( self . type_name ) ) for key in self . metadata . keys (): _metadata_keys . add ( key ) else : _metadata_keys = set ( metadata_keys ) result = {} missing = set () for metadata_key in _metadata_keys : if metadata_key in self . metadata . keys (): if also_return_schema : result [ metadata_key ] = self . metadata [ metadata_key ] else : result [ metadata_key ] = self . metadata [ metadata_key ][ \"metadata_item\" ] else : missing . add ( metadata_key ) if not missing : return result _md = self . _kiara . metadata_mgmt . get_value_metadata ( self , * missing , also_return_schema = True ) for k , v in _md . items (): self . metadata [ k ] = v if also_return_schema : result [ k ] = v else : result [ k ] = v [ \"metadata_item\" ] return result","title":"get_metadata()"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.item_is_valid","text":"Check whether the current value is valid Source code in kiara/data/values/__init__.py def item_is_valid ( self ) -> bool : \"\"\"Check whether the current value is valid\"\"\" if self . value_schema . optional : return True else : return self . is_set and not self . is_none","title":"item_is_valid()"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.Value.save","text":"Save this value, under the specified alias(es). Source code in kiara/data/values/__init__.py def save ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None , register_missing_aliases : bool = True , ) -> \"Value\" : \"\"\"Save this value, under the specified alias(es).\"\"\" # if self.get_value_lineage(): # for field_name, value in self.get_value_lineage().inputs.items(): # value_obj = self._registry.get_value_obj(value) # try: # value_obj.save() # except Exception as e: # print(e) value = self . _kiara . data_store . register_data ( self ) if aliases : self . _kiara . data_store . link_aliases ( value , * aliases , register_missing_aliases = register_missing_aliases ) return value","title":"save()"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueAlias","text":"","title":"ValueAlias"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueAlias.repo_name","text":"The name of the data repo the value lives in.","title":"repo_name"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueAlias.tag","text":"The tag for the alias.","title":"tag"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueAlias.version","text":"The version of this alias.","title":"version"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueHash","text":"","title":"ValueHash"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueHash.hash","text":"The value hash.","title":"hash"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueHash.hash_type","text":"The value hash method.","title":"hash_type"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueInfo","text":"","title":"ValueInfo"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueInfo.deserialize_config","text":"The module config (incl. inputs) to deserialize the value.","title":"deserialize_config"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueInfo.hashes","text":"All available hashes for this value.","title":"hashes"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueInfo.is_valid","text":"Whether the item is valid (in the context of its schema).","title":"is_valid"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueInfo.lineage","text":"Information about how the value was created.","title":"lineage"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueInfo.metadata","text":"The metadata associated with this value.","title":"metadata"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueInfo.value_id","text":"The value id.","title":"value_id"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueInfo.value_schema","text":"The value schema.","title":"value_schema"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueLineage","text":"Model containing the lineage of a value.","title":"ValueLineage"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueLineage.inputs","text":"The inputs that were used to create the value this refers to.","title":"inputs"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueLineage.output_name","text":"The result field name for the value this refers to.","title":"output_name"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueLineage.value_index","text":"Index of all values that are associated with this value lineage.","title":"value_index"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueLineage.create_renderable","text":"Create a renderable for this module configuration. Source code in kiara/data/values/__init__.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : all_ids = sorted ( find_all_ids_in_lineage ( self )) id_color_map = {} for idx , v_id in enumerate ( all_ids ): id_color_map [ v_id ] = COLOR_LIST [ idx % len ( COLOR_LIST )] show_ids = config . get ( \"include_ids\" , False ) tree = fill_lineage_tree ( self , include_ids = show_ids ) return tree","title":"create_renderable()"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSchema","text":"The schema of a value. The schema contains the ValueType of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the type_config field.","title":"ValueSchema"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSchema.default","text":"A default value.","title":"default"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSchema.doc","text":"A description for the value of this input field.","title":"doc"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSchema.is_constant","text":"Whether the value is a constant.","title":"is_constant"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSchema.optional","text":"Whether this value is required (True), or whether 'None' value is allowed (False).","title":"optional"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSchema.type","text":"The type of the value.","title":"type"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSchema.type_config","text":"Configuration for the type, in case it's complex.","title":"type_config"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSchema.__eq__","text":"Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): if not isinstance ( other , ValueSchema ): return False return ( self . type , self . default ) == ( other . type , other . default )","title":"__eq__()"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSchema.__hash__","text":"Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash (( self . type , self . default ))","title":"__hash__()"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSlot","text":"","title":"ValueSlot"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSlot.id","text":"The id for this slot.","title":"id"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSlot.tags","text":"The tags for this value slot (tag name as key, linked version as value.","title":"tags"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSlot.value_schema","text":"The schema for the values of this slot.","title":"value_schema"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSlot.values","text":"The values of this slot, with versions as key.","title":"values"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSlot.__eq__","text":"Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): if not isinstance ( other , ValueSlot ): return False return self . id == other . id","title":"__eq__()"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSlot.__hash__","text":"Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash ( self . id )","title":"__hash__()"},{"location":"api_reference/kiara.data.values/#kiara.data.values.__init__.ValueSlot.add_value","text":"Add a value to this slot. Source code in kiara/data/values/__init__.py def add_value ( self , value : Value , trigger_callbacks : bool = True , tags : typing . Optional [ typing . Iterable [ str ]] = None , ) -> int : \"\"\"Add a value to this slot.\"\"\" if self . latest_version_nr != 0 and value . id == self . get_latest_value () . id : return self . latest_version_nr # TODO: check value data equality if self . value_schema . is_constant and self . values : if is_debug (): import traceback traceback . print_stack () raise Exception ( \"Can't add value: value slot marked as 'constant'.\" ) version = self . latest_version_nr + 1 assert version not in self . values . keys () self . values [ version ] = value if tags : for tag in tags : self . tags [ tag ] = version if trigger_callbacks : for cb in self . _callbacks . values (): cb . values_updated ( self ) return version","title":"add_value()"},{"location":"api_reference/kiara.data.values.value_set/","text":"kiara.data.values.value_set \u00b6 SlottedValueSet \u00b6 __init__ ( self , items , read_only , check_for_sameness = False , title = None , kiara = None , registry = None ) special \u00b6 A ValueSet implementation that keeps a history of each fields value. Parameters: Name Type Description Default items Mapping[str, Union[ValueSlot, Value]] the value slots required read_only bool whether it is allowed to set new values to fields in this set required check_for_sameness bool whether a check should be performed that checks for equality of the new and old values, if equal, skip the update False title Optional[str] An optional title for this value set None kiara Optional[Kiara] the kiara context None registry Optional[kiara.data.registry.DataRegistry] the registry to use to register the values to None Source code in kiara/data/values/value_set.py def __init__ ( self , items : typing . Mapping [ str , typing . Union [ \"ValueSlot\" , \"Value\" ]], read_only : bool , check_for_sameness : bool = False , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ): \"\"\"A `ValueSet` implementation that keeps a history of each fields value. Arguments: items: the value slots read_only: whether it is allowed to set new values to fields in this set check_for_sameness: whether a check should be performed that checks for equality of the new and old values, if equal, skip the update title: An optional title for this value set kiara: the kiara context registry: the registry to use to register the values to \"\"\" if not items : raise ValueError ( \"Can't create ValueSet: no values provided\" ) self . _check_for_sameness : bool = check_for_sameness super () . __init__ ( read_only = read_only , title = title , kiara = kiara ) if registry is None : registry = self . _kiara . data_registry self . _registry : DataRegistry = registry _value_slots : typing . Dict [ str , ValueSlot ] = {} for item , value in items . items (): if value is None : raise Exception ( f \"Can't create value set, item ' { item } ' does not have a value (yet).\" ) if item . startswith ( \"_\" ): raise ValueError ( f \"Value name can't start with '_': { item } \" ) if item in INVALID_VALUE_NAMES : raise ValueError ( f \"Invalid value name ' { item } '.\" ) if isinstance ( value , Value ): slot = self . _registry . register_alias ( value ) elif isinstance ( value , ValueSlot ): slot = value else : raise TypeError ( f \"Invalid type: ' { type ( value ) } '\" ) _value_slots [ item ] = slot self . _value_slots : typing . Dict [ str , ValueSlot ] = _value_slots ValueSet \u00b6 get_value_data_for_fields ( self , * field_names , * , raise_exception_when_unset = False ) \u00b6 Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). Source code in kiara/data/values/value_set.py def get_value_data_for_fields ( self , * field_names : str , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). \"\"\" result : typing . Dict [ str , typing . Any ] = {} unset : typing . List [ str ] = [] for k in field_names : v = self . get_value_obj ( k ) if not v . is_set : if raise_exception_when_unset : unset . append ( k ) else : result [ k ] = None else : data = v . get_value_data () result [ k ] = data if unset : raise Exception ( f \"Can't get data for fields, one or several of the requested fields are not set yet: { ', ' . join ( unset ) } .\" ) return result","title":"\u279c\u2007data.values.value_set"},{"location":"api_reference/kiara.data.values.value_set/#kiaradatavaluesvalue_set","text":"","title":"kiara.data.values.value_set"},{"location":"api_reference/kiara.data.values.value_set/#kiara.data.values.value_set.SlottedValueSet","text":"","title":"SlottedValueSet"},{"location":"api_reference/kiara.data.values.value_set/#kiara.data.values.value_set.SlottedValueSet.__init__","text":"A ValueSet implementation that keeps a history of each fields value. Parameters: Name Type Description Default items Mapping[str, Union[ValueSlot, Value]] the value slots required read_only bool whether it is allowed to set new values to fields in this set required check_for_sameness bool whether a check should be performed that checks for equality of the new and old values, if equal, skip the update False title Optional[str] An optional title for this value set None kiara Optional[Kiara] the kiara context None registry Optional[kiara.data.registry.DataRegistry] the registry to use to register the values to None Source code in kiara/data/values/value_set.py def __init__ ( self , items : typing . Mapping [ str , typing . Union [ \"ValueSlot\" , \"Value\" ]], read_only : bool , check_for_sameness : bool = False , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ): \"\"\"A `ValueSet` implementation that keeps a history of each fields value. Arguments: items: the value slots read_only: whether it is allowed to set new values to fields in this set check_for_sameness: whether a check should be performed that checks for equality of the new and old values, if equal, skip the update title: An optional title for this value set kiara: the kiara context registry: the registry to use to register the values to \"\"\" if not items : raise ValueError ( \"Can't create ValueSet: no values provided\" ) self . _check_for_sameness : bool = check_for_sameness super () . __init__ ( read_only = read_only , title = title , kiara = kiara ) if registry is None : registry = self . _kiara . data_registry self . _registry : DataRegistry = registry _value_slots : typing . Dict [ str , ValueSlot ] = {} for item , value in items . items (): if value is None : raise Exception ( f \"Can't create value set, item ' { item } ' does not have a value (yet).\" ) if item . startswith ( \"_\" ): raise ValueError ( f \"Value name can't start with '_': { item } \" ) if item in INVALID_VALUE_NAMES : raise ValueError ( f \"Invalid value name ' { item } '.\" ) if isinstance ( value , Value ): slot = self . _registry . register_alias ( value ) elif isinstance ( value , ValueSlot ): slot = value else : raise TypeError ( f \"Invalid type: ' { type ( value ) } '\" ) _value_slots [ item ] = slot self . _value_slots : typing . Dict [ str , ValueSlot ] = _value_slots","title":"__init__()"},{"location":"api_reference/kiara.data.values.value_set/#kiara.data.values.value_set.ValueSet","text":"","title":"ValueSet"},{"location":"api_reference/kiara.data.values.value_set/#kiara.data.values.value_set.ValueSet.get_value_data_for_fields","text":"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). Source code in kiara/data/values/value_set.py def get_value_data_for_fields ( self , * field_names : str , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). \"\"\" result : typing . Dict [ str , typing . Any ] = {} unset : typing . List [ str ] = [] for k in field_names : v = self . get_value_obj ( k ) if not v . is_set : if raise_exception_when_unset : unset . append ( k ) else : result [ k ] = None else : data = v . get_value_data () result [ k ] = data if unset : raise Exception ( f \"Can't get data for fields, one or several of the requested fields are not set yet: { ', ' . join ( unset ) } .\" ) return result","title":"get_value_data_for_fields()"},{"location":"api_reference/kiara.defaults/","text":"kiara.defaults \u00b6 DEFAULT_EXCLUDE_DIRS \u00b6 List of directory names to exclude by default when walking a folder recursively. DEFAULT_EXCLUDE_FILES \u00b6 List of file names to exclude by default when reading folders. DEFAULT_PIPELINE_PARENT_ID \u00b6 Default parent id for pipeline objects that are not associated with a workflow. INVALID_VALUE_NAMES \u00b6 List of reserved names, inputs/outputs can't use those. KIARA_MODULE_BASE_FOLDER \u00b6 Marker to indicate the base folder for the kiara module. KIARA_RESOURCES_FOLDER \u00b6 Default resources folder for this package. MODULE_TYPE_KEY \u00b6 The key to specify the type of a module. MODULE_TYPE_NAME_KEY \u00b6 The string for the module type name in a module configuration dict. NO_HASH_MARKER \u00b6 Marker string to indicate no hash was calculated. NO_VALUE_ID_MARKER \u00b6 Marker string to indicate no value id exists. PIPELINE_PARENT_MARKER \u00b6 Marker string in the pipeline structure that indicates a parent pipeline element. STEP_ID_KEY \u00b6 The key to specify the step id. VALID_PIPELINE_FILE_EXTENSIONS \u00b6 File extensions a kiara pipeline/workflow file can have. SpecialValue \u00b6 An enumeration.","title":"\u279c\u2007defaults"},{"location":"api_reference/kiara.defaults/#kiaradefaults","text":"","title":"kiara.defaults"},{"location":"api_reference/kiara.defaults/#kiara.defaults.DEFAULT_EXCLUDE_DIRS","text":"List of directory names to exclude by default when walking a folder recursively.","title":"DEFAULT_EXCLUDE_DIRS"},{"location":"api_reference/kiara.defaults/#kiara.defaults.DEFAULT_EXCLUDE_FILES","text":"List of file names to exclude by default when reading folders.","title":"DEFAULT_EXCLUDE_FILES"},{"location":"api_reference/kiara.defaults/#kiara.defaults.DEFAULT_PIPELINE_PARENT_ID","text":"Default parent id for pipeline objects that are not associated with a workflow.","title":"DEFAULT_PIPELINE_PARENT_ID"},{"location":"api_reference/kiara.defaults/#kiara.defaults.INVALID_VALUE_NAMES","text":"List of reserved names, inputs/outputs can't use those.","title":"INVALID_VALUE_NAMES"},{"location":"api_reference/kiara.defaults/#kiara.defaults.KIARA_MODULE_BASE_FOLDER","text":"Marker to indicate the base folder for the kiara module.","title":"KIARA_MODULE_BASE_FOLDER"},{"location":"api_reference/kiara.defaults/#kiara.defaults.KIARA_RESOURCES_FOLDER","text":"Default resources folder for this package.","title":"KIARA_RESOURCES_FOLDER"},{"location":"api_reference/kiara.defaults/#kiara.defaults.MODULE_TYPE_KEY","text":"The key to specify the type of a module.","title":"MODULE_TYPE_KEY"},{"location":"api_reference/kiara.defaults/#kiara.defaults.MODULE_TYPE_NAME_KEY","text":"The string for the module type name in a module configuration dict.","title":"MODULE_TYPE_NAME_KEY"},{"location":"api_reference/kiara.defaults/#kiara.defaults.NO_HASH_MARKER","text":"Marker string to indicate no hash was calculated.","title":"NO_HASH_MARKER"},{"location":"api_reference/kiara.defaults/#kiara.defaults.NO_VALUE_ID_MARKER","text":"Marker string to indicate no value id exists.","title":"NO_VALUE_ID_MARKER"},{"location":"api_reference/kiara.defaults/#kiara.defaults.PIPELINE_PARENT_MARKER","text":"Marker string in the pipeline structure that indicates a parent pipeline element.","title":"PIPELINE_PARENT_MARKER"},{"location":"api_reference/kiara.defaults/#kiara.defaults.STEP_ID_KEY","text":"The key to specify the step id.","title":"STEP_ID_KEY"},{"location":"api_reference/kiara.defaults/#kiara.defaults.VALID_PIPELINE_FILE_EXTENSIONS","text":"File extensions a kiara pipeline/workflow file can have.","title":"VALID_PIPELINE_FILE_EXTENSIONS"},{"location":"api_reference/kiara.defaults/#kiara.defaults.SpecialValue","text":"An enumeration.","title":"SpecialValue"},{"location":"api_reference/kiara.doc.generate_api_doc/","text":"kiara.doc.generate_api_doc \u00b6 gen_pages_for_module ( module , prefix = 'api_reference' ) \u00b6 Generate pages for a set of modules (using the mkdocstring package. Source code in kiara/doc/generate_api_doc.py def gen_pages_for_module ( module : typing . Union [ str , ModuleType ], prefix : str = \"api_reference\" ): \"\"\"Generate pages for a set of modules (using the [mkdocstring](https://github.com/mkdocstrings/mkdocstrings) package.\"\"\" result = {} modules_info = get_source_tree ( module ) for module_name , path in modules_info . items (): page_name = module_name if page_name . endswith ( \"__init__\" ): page_name = page_name [ 0 : - 9 ] if page_name . endswith ( \"._frkl\" ): continue doc_path = f \" { prefix }{ os . path . sep }{ page_name } .md\" p = Path ( \"..\" , path [ \"abs_path\" ]) if not p . read_text () . strip (): continue main_module = path [ \"main_module\" ] if page_name == main_module : title = page_name else : title = page_name . replace ( f \" { main_module } .\" , \"\u279c\u2007\" ) result [ doc_path ] = { \"python_src\" : path , \"content\" : f \"--- \\n title: { title } \\n --- \\n # { page_name } \\n\\n ::: { module_name } \" , } return result get_source_tree ( module ) \u00b6 Find all python source files for a module. Source code in kiara/doc/generate_api_doc.py def get_source_tree ( module : typing . Union [ str , ModuleType ]): \"\"\"Find all python source files for a module.\"\"\" if isinstance ( module , str ): module = importlib . import_module ( module ) if not isinstance ( module , ModuleType ): raise TypeError ( f \"Invalid type ' { type ( module ) } ', input needs to be a string or module.\" ) module_root = os . path . dirname ( module . __file__ ) module_name = module . __name__ src = {} for path in Path ( module_root ) . glob ( \"**/*.py\" ): rel = os . path . relpath ( path , module_root ) mod_name = f \" { module_name } . { rel [ 0 : - 3 ] . replace ( os . path . sep , '.' ) } \" rel_path = f \" { module_name }{ os . path . sep }{ rel } \" src [ mod_name ] = { \"rel_path\" : rel_path , \"abs_path\" : path , \"main_module\" : module_name , } return src","title":"\u279c\u2007doc.generate_api_doc"},{"location":"api_reference/kiara.doc.generate_api_doc/#kiaradocgenerate_api_doc","text":"","title":"kiara.doc.generate_api_doc"},{"location":"api_reference/kiara.doc.generate_api_doc/#kiara.doc.generate_api_doc.gen_pages_for_module","text":"Generate pages for a set of modules (using the mkdocstring package. Source code in kiara/doc/generate_api_doc.py def gen_pages_for_module ( module : typing . Union [ str , ModuleType ], prefix : str = \"api_reference\" ): \"\"\"Generate pages for a set of modules (using the [mkdocstring](https://github.com/mkdocstrings/mkdocstrings) package.\"\"\" result = {} modules_info = get_source_tree ( module ) for module_name , path in modules_info . items (): page_name = module_name if page_name . endswith ( \"__init__\" ): page_name = page_name [ 0 : - 9 ] if page_name . endswith ( \"._frkl\" ): continue doc_path = f \" { prefix }{ os . path . sep }{ page_name } .md\" p = Path ( \"..\" , path [ \"abs_path\" ]) if not p . read_text () . strip (): continue main_module = path [ \"main_module\" ] if page_name == main_module : title = page_name else : title = page_name . replace ( f \" { main_module } .\" , \"\u279c\u2007\" ) result [ doc_path ] = { \"python_src\" : path , \"content\" : f \"--- \\n title: { title } \\n --- \\n # { page_name } \\n\\n ::: { module_name } \" , } return result","title":"gen_pages_for_module()"},{"location":"api_reference/kiara.doc.generate_api_doc/#kiara.doc.generate_api_doc.get_source_tree","text":"Find all python source files for a module. Source code in kiara/doc/generate_api_doc.py def get_source_tree ( module : typing . Union [ str , ModuleType ]): \"\"\"Find all python source files for a module.\"\"\" if isinstance ( module , str ): module = importlib . import_module ( module ) if not isinstance ( module , ModuleType ): raise TypeError ( f \"Invalid type ' { type ( module ) } ', input needs to be a string or module.\" ) module_root = os . path . dirname ( module . __file__ ) module_name = module . __name__ src = {} for path in Path ( module_root ) . glob ( \"**/*.py\" ): rel = os . path . relpath ( path , module_root ) mod_name = f \" { module_name } . { rel [ 0 : - 3 ] . replace ( os . path . sep , '.' ) } \" rel_path = f \" { module_name }{ os . path . sep }{ rel } \" src [ mod_name ] = { \"rel_path\" : rel_path , \"abs_path\" : path , \"main_module\" : module_name , } return src","title":"get_source_tree()"},{"location":"api_reference/kiara.doc/","text":"kiara.doc \u00b6 Main module for code that helps with documentation auto-generation in supported projects. FrklDocumentationPlugin \u00b6 mkdocs plugin to render API documentation for a project. To add to a project, add this to the 'plugins' section of a mkdocs config file: - frkl-docgen : main_module : \"module_name\" This will add an API reference navigation item to your page navigation, with auto-generated entries for every Python module in your package.","title":"\u279c\u2007doc"},{"location":"api_reference/kiara.doc/#kiaradoc","text":"Main module for code that helps with documentation auto-generation in supported projects.","title":"kiara.doc"},{"location":"api_reference/kiara.doc/#kiara.doc.__init__.FrklDocumentationPlugin","text":"mkdocs plugin to render API documentation for a project. To add to a project, add this to the 'plugins' section of a mkdocs config file: - frkl-docgen : main_module : \"module_name\" This will add an API reference navigation item to your page navigation, with auto-generated entries for every Python module in your package.","title":"FrklDocumentationPlugin"},{"location":"api_reference/kiara.doc.mkdocs_macros_cli/","text":"kiara.doc.mkdocs_macros_cli \u00b6 define_env ( env ) \u00b6 Helper macros for Python project documentation. Currently, those macros are available (check the source code for more details): cli \u00b6 Execute a command on the command-line, capture the output and return it to be used in a documentation page. inline_file_as_codeblock \u00b6 Read an external file, and return its content as a markdown code block. Source code in kiara/doc/mkdocs_macros_cli.py def define_env ( env ): \"\"\" Helper macros for Python project documentation. Currently, those macros are available (check the source code for more details): ## ``cli`` Execute a command on the command-line, capture the output and return it to be used in a documentation page. ## ``inline_file_as_codeblock`` Read an external file, and return its content as a markdown code block. \"\"\" # env.variables[\"baz\"] = \"John Doe\" @env . macro def cli ( * command , print_command : bool = True , code_block : bool = True , split_command_and_output : bool = True , max_height : Optional [ int ] = None , cache_key : Optional [ str ] = None , extra_env : Optional [ Dict [ str , str ]] = None , fake_command : Optional [ str ] = None , ): \"\"\"Execute the provided command, save the output and return it to be used in documentation pages.\"\"\" hashes = DeepHash ( command ) hash_str = hashes [ command ] hashes_env = DeepHash ( extra_env ) hashes_env_str = hashes_env [ extra_env ] hash_str = hash_str + \"_\" + hashes_env_str if cache_key : hash_str = hash_str + \"_\" + cache_key cache_file : Path = Path ( os . path . join ( CACHE_DIR , str ( hash_str ))) _run_env = dict ( os_env_vars ) if extra_env : _run_env . update ( extra_env ) if cache_file . is_file (): stdout = cache_file . read_text () else : try : print ( f \"RUNNING: { ' ' . join ( command ) } \" ) result = subprocess . check_output ( command , env = _run_env ) stdout = result . decode () cache_file . write_text ( stdout ) except subprocess . CalledProcessError as e : stdout = f \"Error: { e } \\n\\n Stdout: { e . stdout } \\n\\n Stderr: { e . stderr } \" print ( \"stdout:\" ) print ( e . stdout ) print ( \"stderr:\" ) print ( e . stderr ) if os . getenv ( \"FAIL_DOC_BUILD_ON_ERROR\" ) == \"true\" : sys . exit ( 1 ) if fake_command : command_str = fake_command else : command_str = \" \" . join ( command ) if split_command_and_output and print_command : _c = f \" \\n ``` console \\n { command_str } \\n ``` \\n \" _output = \"``` console \\n \" + stdout + \" \\n ``` \\n \" if max_height is not None and max_height > 0 : _output = f \"<div style='max-height: { max_height } px;overflow:auto'> \\n { _output } \\n </div>\" _stdout = _c + _output else : if print_command : _stdout = f \"> { command_str } \\n { stdout } \" if code_block : _stdout = \"``` console \\n \" + _stdout + \" \\n ``` \\n \" if max_height is not None and max_height > 0 : _stdout = f \"<div style='max-height: { max_height } px;overflow:auto'> \\n { _stdout } \\n </div>\" return _stdout @env . macro def inline_file_as_codeblock ( path , format : str = \"\" ): \"\"\"Import external file and return its content as a markdown code block.\"\"\" f = Path ( path ) return f \"``` { format } \\n { f . read_text () } \\n ```\"","title":"\u279c\u2007doc.mkdocs_macros_cli"},{"location":"api_reference/kiara.doc.mkdocs_macros_cli/#kiaradocmkdocs_macros_cli","text":"","title":"kiara.doc.mkdocs_macros_cli"},{"location":"api_reference/kiara.doc.mkdocs_macros_cli/#kiara.doc.mkdocs_macros_cli.define_env","text":"Helper macros for Python project documentation. Currently, those macros are available (check the source code for more details):","title":"define_env()"},{"location":"api_reference/kiara.doc.mkdocs_macros_kiara/","text":"kiara.doc.mkdocs_macros_kiara \u00b6 define_env ( env ) \u00b6 This is the hook for defining variables, macros and filters variables: the dictionary that contains the environment variables macro: a decorator function, to declare a macro. Source code in kiara/doc/mkdocs_macros_kiara.py def define_env ( env ): \"\"\" This is the hook for defining variables, macros and filters - variables: the dictionary that contains the environment variables - macro: a decorator function, to declare a macro. \"\"\" # env.variables[\"baz\"] = \"John Doe\" @env . macro def get_schema_for_model ( model_class : typing . Union [ str , typing . Type [ BaseModel ]]): if isinstance ( model_class , str ): _class : typing . Type [ BaseModel ] = locate ( model_class ) # type: ignore else : _class = model_class schema_json = _class . schema_json ( indent = 2 ) return schema_json @env . macro def get_src_of_object ( obj : typing . Union [ str , typing . Any ]): try : if isinstance ( obj , str ): _obj : typing . Type [ BaseModel ] = locate ( obj ) # type: ignore else : _obj = obj src = inspect . getsource ( _obj ) return src except Exception as e : return f \"Can't render object source: { str ( e ) } \" @env . macro def get_pipeline_config ( pipeline_name : str ): pmm = PipelineModuleManager () desc = pmm . pipeline_descs [ pipeline_name ][ \"data\" ] desc_str = yaml . dump ( desc ) return desc_str @env . macro def get_module_info ( module_type : str ): try : m_cls = Kiara . instance () . get_module_class ( module_type ) info = KiaraModuleTypeMetadata . from_module_class ( m_cls ) from rich.console import Console console = Console ( record = True ) console . print ( info ) html = console . export_text () return html except Exception as e : return f \"Can't render module info: { str ( e ) } \" @env . macro def get_module_list_for_package ( package_name : str , include_core_modules : bool = True , include_pipelines : bool = True , ): modules = kiara_obj . module_mgmt . find_modules_for_package ( package_name , include_core_modules = include_core_modules , include_pipelines = include_pipelines , ) result = [] for name , info in modules . items (): type_md = info . get_type_metadata () result . append ( f \" - [`` { name } ``]( { type_md . context . get_url_for_reference ( 'module_doc' ) } ): { type_md . documentation . description } \" ) print ( result ) return \" \\n \" . join ( result ) @env . macro def get_value_types_for_package ( package_name : str ): value_types = kiara_obj . type_mgmt . find_value_types_for_package ( package_name ) result = [] for name , info in value_types . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result ) @env . macro def get_metadata_schemas_for_package ( package_name : str ): metadata_schemas = kiara_obj . metadata_mgmt . find_all_schemas_for_package ( package_name ) result = [] for name , info in metadata_schemas . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result ) on_post_build ( env ) \u00b6 Post-build actions Source code in kiara/doc/mkdocs_macros_kiara.py def on_post_build ( env ): \"Post-build actions\" site_dir = env . conf [ \"site_dir\" ] for category , classes in KIARA_MODEL_CLASSES . items (): for cls in classes : schema_json = cls . schema_json ( indent = 2 ) file_path = os . path . join ( site_dir , \"development\" , \"entities\" , category , f \" { cls . __name__ } .json\" ) os . makedirs ( os . path . dirname ( file_path ), exist_ok = True ) with open ( file_path , \"w\" ) as f : f . write ( schema_json )","title":"\u279c\u2007doc.mkdocs_macros_kiara"},{"location":"api_reference/kiara.doc.mkdocs_macros_kiara/#kiaradocmkdocs_macros_kiara","text":"","title":"kiara.doc.mkdocs_macros_kiara"},{"location":"api_reference/kiara.doc.mkdocs_macros_kiara/#kiara.doc.mkdocs_macros_kiara.define_env","text":"This is the hook for defining variables, macros and filters variables: the dictionary that contains the environment variables macro: a decorator function, to declare a macro. Source code in kiara/doc/mkdocs_macros_kiara.py def define_env ( env ): \"\"\" This is the hook for defining variables, macros and filters - variables: the dictionary that contains the environment variables - macro: a decorator function, to declare a macro. \"\"\" # env.variables[\"baz\"] = \"John Doe\" @env . macro def get_schema_for_model ( model_class : typing . Union [ str , typing . Type [ BaseModel ]]): if isinstance ( model_class , str ): _class : typing . Type [ BaseModel ] = locate ( model_class ) # type: ignore else : _class = model_class schema_json = _class . schema_json ( indent = 2 ) return schema_json @env . macro def get_src_of_object ( obj : typing . Union [ str , typing . Any ]): try : if isinstance ( obj , str ): _obj : typing . Type [ BaseModel ] = locate ( obj ) # type: ignore else : _obj = obj src = inspect . getsource ( _obj ) return src except Exception as e : return f \"Can't render object source: { str ( e ) } \" @env . macro def get_pipeline_config ( pipeline_name : str ): pmm = PipelineModuleManager () desc = pmm . pipeline_descs [ pipeline_name ][ \"data\" ] desc_str = yaml . dump ( desc ) return desc_str @env . macro def get_module_info ( module_type : str ): try : m_cls = Kiara . instance () . get_module_class ( module_type ) info = KiaraModuleTypeMetadata . from_module_class ( m_cls ) from rich.console import Console console = Console ( record = True ) console . print ( info ) html = console . export_text () return html except Exception as e : return f \"Can't render module info: { str ( e ) } \" @env . macro def get_module_list_for_package ( package_name : str , include_core_modules : bool = True , include_pipelines : bool = True , ): modules = kiara_obj . module_mgmt . find_modules_for_package ( package_name , include_core_modules = include_core_modules , include_pipelines = include_pipelines , ) result = [] for name , info in modules . items (): type_md = info . get_type_metadata () result . append ( f \" - [`` { name } ``]( { type_md . context . get_url_for_reference ( 'module_doc' ) } ): { type_md . documentation . description } \" ) print ( result ) return \" \\n \" . join ( result ) @env . macro def get_value_types_for_package ( package_name : str ): value_types = kiara_obj . type_mgmt . find_value_types_for_package ( package_name ) result = [] for name , info in value_types . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result ) @env . macro def get_metadata_schemas_for_package ( package_name : str ): metadata_schemas = kiara_obj . metadata_mgmt . find_all_schemas_for_package ( package_name ) result = [] for name , info in metadata_schemas . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result )","title":"define_env()"},{"location":"api_reference/kiara.doc.mkdocs_macros_kiara/#kiara.doc.mkdocs_macros_kiara.on_post_build","text":"Post-build actions Source code in kiara/doc/mkdocs_macros_kiara.py def on_post_build ( env ): \"Post-build actions\" site_dir = env . conf [ \"site_dir\" ] for category , classes in KIARA_MODEL_CLASSES . items (): for cls in classes : schema_json = cls . schema_json ( indent = 2 ) file_path = os . path . join ( site_dir , \"development\" , \"entities\" , category , f \" { cls . __name__ } .json\" ) os . makedirs ( os . path . dirname ( file_path ), exist_ok = True ) with open ( file_path , \"w\" ) as f : f . write ( schema_json )","title":"on_post_build()"},{"location":"api_reference/kiara.events/","text":"kiara.events \u00b6 PipelineInputEvent pydantic-model \u00b6 Event that gets fired when one or several inputs for the pipeline itself have changed. updated_pipeline_inputs : List [ str ] pydantic-field required \u00b6 list of pipeline input names that where changed PipelineOutputEvent pydantic-model \u00b6 Event that gets fired when one or several outputs for the pipeline itself have changed. updated_pipeline_outputs : List [ str ] pydantic-field required \u00b6 list of pipeline output names that where changed StepEvent pydantic-model \u00b6 __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/events.py def __repr__ ( self ): d = self . dict () d . pop ( \"pipeline_id\" ) return f \" { self . __class__ . __name__ } (pipeline_id= { self . pipeline_id } data= { d } \" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/events.py def __str__ ( self ): return self . __repr__ () StepInputEvent pydantic-model \u00b6 Event that gets fired when one or several inputs for steps within a pipeline have changed. newly_stale_steps : List [ str ] property readonly \u00b6 Convenience method to display the steps that have been rendered 'stale' by this event. updated_step_inputs : Dict [ str , List [ str ]] pydantic-field required \u00b6 steps (keys) with updated inputs which need re-processing (value is list of updated input names) StepOutputEvent pydantic-model \u00b6 Event that gets fired when one or several outputs for steps within a pipeline have changed. updated_step_outputs : Dict [ str , List [ str ]] pydantic-field required \u00b6 steps (keys) that finished processing of one, several or all outputs (values are list of 'finished' output fields)","title":"\u279c\u2007events"},{"location":"api_reference/kiara.events/#kiaraevents","text":"","title":"kiara.events"},{"location":"api_reference/kiara.events/#kiara.events.PipelineInputEvent","text":"Event that gets fired when one or several inputs for the pipeline itself have changed.","title":"PipelineInputEvent"},{"location":"api_reference/kiara.events/#kiara.events.PipelineInputEvent.updated_pipeline_inputs","text":"list of pipeline input names that where changed","title":"updated_pipeline_inputs"},{"location":"api_reference/kiara.events/#kiara.events.PipelineOutputEvent","text":"Event that gets fired when one or several outputs for the pipeline itself have changed.","title":"PipelineOutputEvent"},{"location":"api_reference/kiara.events/#kiara.events.PipelineOutputEvent.updated_pipeline_outputs","text":"list of pipeline output names that where changed","title":"updated_pipeline_outputs"},{"location":"api_reference/kiara.events/#kiara.events.StepEvent","text":"","title":"StepEvent"},{"location":"api_reference/kiara.events/#kiara.events.StepEvent.__repr__","text":"Return repr(self). Source code in kiara/events.py def __repr__ ( self ): d = self . dict () d . pop ( \"pipeline_id\" ) return f \" { self . __class__ . __name__ } (pipeline_id= { self . pipeline_id } data= { d } \"","title":"__repr__()"},{"location":"api_reference/kiara.events/#kiara.events.StepEvent.__str__","text":"Return str(self). Source code in kiara/events.py def __str__ ( self ): return self . __repr__ ()","title":"__str__()"},{"location":"api_reference/kiara.events/#kiara.events.StepInputEvent","text":"Event that gets fired when one or several inputs for steps within a pipeline have changed.","title":"StepInputEvent"},{"location":"api_reference/kiara.events/#kiara.events.StepInputEvent.newly_stale_steps","text":"Convenience method to display the steps that have been rendered 'stale' by this event.","title":"newly_stale_steps"},{"location":"api_reference/kiara.events/#kiara.events.StepInputEvent.updated_step_inputs","text":"steps (keys) with updated inputs which need re-processing (value is list of updated input names)","title":"updated_step_inputs"},{"location":"api_reference/kiara.events/#kiara.events.StepOutputEvent","text":"Event that gets fired when one or several outputs for steps within a pipeline have changed.","title":"StepOutputEvent"},{"location":"api_reference/kiara.events/#kiara.events.StepOutputEvent.updated_step_outputs","text":"steps (keys) that finished processing of one, several or all outputs (values are list of 'finished' output fields)","title":"updated_step_outputs"},{"location":"api_reference/kiara.examples.example_controller/","text":"kiara.examples.example_controller \u00b6 ExampleController \u00b6 pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/examples/example_controller.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): print ( f \"Pipeline inputs changed: { event . updated_pipeline_inputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/examples/example_controller.py def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): print ( f \"Pipeline outputs changed: { event . updated_pipeline_outputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/examples/example_controller.py def step_inputs_changed ( self , event : StepInputEvent ): print ( \"Step inputs changed, new values:\" ) for step_id , input_names in event . updated_step_inputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in input_names : new_value = self . get_step_inputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) step_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Parameters: Name Type Description Default event StepOutputEvent the step output event required Source code in kiara/examples/example_controller.py def step_outputs_changed ( self , event : StepOutputEvent ): print ( \"Step outputs changed, new values:\" ) for step_id , output_names in event . updated_step_outputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in output_names : new_value = self . get_step_outputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" )","title":"\u279c\u2007examples.example_controller"},{"location":"api_reference/kiara.examples.example_controller/#kiaraexamplesexample_controller","text":"","title":"kiara.examples.example_controller"},{"location":"api_reference/kiara.examples.example_controller/#kiara.examples.example_controller.ExampleController","text":"","title":"ExampleController"},{"location":"api_reference/kiara.examples.example_controller/#kiara.examples.example_controller.ExampleController.pipeline_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/examples/example_controller.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): print ( f \"Pipeline inputs changed: { event . updated_pipeline_inputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" )","title":"pipeline_inputs_changed()"},{"location":"api_reference/kiara.examples.example_controller/#kiara.examples.example_controller.ExampleController.pipeline_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/examples/example_controller.py def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): print ( f \"Pipeline outputs changed: { event . updated_pipeline_outputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" )","title":"pipeline_outputs_changed()"},{"location":"api_reference/kiara.examples.example_controller/#kiara.examples.example_controller.ExampleController.step_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/examples/example_controller.py def step_inputs_changed ( self , event : StepInputEvent ): print ( \"Step inputs changed, new values:\" ) for step_id , input_names in event . updated_step_inputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in input_names : new_value = self . get_step_inputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" )","title":"step_inputs_changed()"},{"location":"api_reference/kiara.examples.example_controller/#kiara.examples.example_controller.ExampleController.step_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Parameters: Name Type Description Default event StepOutputEvent the step output event required Source code in kiara/examples/example_controller.py def step_outputs_changed ( self , event : StepOutputEvent ): print ( \"Step outputs changed, new values:\" ) for step_id , output_names in event . updated_step_outputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in output_names : new_value = self . get_step_outputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" )","title":"step_outputs_changed()"},{"location":"api_reference/kiara.exceptions/","text":"kiara.exceptions \u00b6","title":"\u279c\u2007exceptions"},{"location":"api_reference/kiara.exceptions/#kiaraexceptions","text":"","title":"kiara.exceptions"},{"location":"api_reference/kiara.info.kiara/","text":"kiara.info.kiara \u00b6 KiaraInfo pydantic-model \u00b6 modules : ModulesGroup pydantic-field required \u00b6 Information about the available modules. operations : OperationsGroupInfo pydantic-field required \u00b6 Information about supported operations in this kiara context. ModulesGroup pydantic-model \u00b6 module_types : ModuleTypesGroupInfo pydantic-field required \u00b6 The available module types. pipelines : PipelineTypesGroupInfo pydantic-field required \u00b6 The available pipelines.","title":"\u279c\u2007info.kiara"},{"location":"api_reference/kiara.info.kiara/#kiarainfokiara","text":"","title":"kiara.info.kiara"},{"location":"api_reference/kiara.info.kiara/#kiara.info.kiara.KiaraInfo","text":"","title":"KiaraInfo"},{"location":"api_reference/kiara.info.kiara/#kiara.info.kiara.KiaraInfo.modules","text":"Information about the available modules.","title":"modules"},{"location":"api_reference/kiara.info.kiara/#kiara.info.kiara.KiaraInfo.operations","text":"Information about supported operations in this kiara context.","title":"operations"},{"location":"api_reference/kiara.info.kiara/#kiara.info.kiara.ModulesGroup","text":"","title":"ModulesGroup"},{"location":"api_reference/kiara.info.kiara/#kiara.info.kiara.ModulesGroup.module_types","text":"The available module types.","title":"module_types"},{"location":"api_reference/kiara.info.kiara/#kiara.info.kiara.ModulesGroup.pipelines","text":"The available pipelines.","title":"pipelines"},{"location":"api_reference/kiara.info/","text":"kiara.info \u00b6","title":"\u279c\u2007info"},{"location":"api_reference/kiara.info/#kiarainfo","text":"","title":"kiara.info"},{"location":"api_reference/kiara.info.metadata/","text":"kiara.info.metadata \u00b6","title":"\u279c\u2007info.metadata"},{"location":"api_reference/kiara.info.metadata/#kiarainfometadata","text":"","title":"kiara.info.metadata"},{"location":"api_reference/kiara.info.modules/","text":"kiara.info.modules \u00b6 ModuleTypesGroupInfo pydantic-model \u00b6 create_renderable_from_module_info_map ( module_types , ignore_pipeline_modules = False , ignore_non_pipeline_modules = False , ** config ) classmethod \u00b6 Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description Source code in kiara/info/modules.py @classmethod def create_renderable_from_module_info_map ( cls , module_types : typing . Mapping [ str , KiaraModuleTypeMetadata ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if ignore_pipeline_modules : module_types = { k : v for k , v in module_types . items () if not v . is_pipeline } elif ignore_non_pipeline_modules : module_types = { k : v for k , v in module_types . items () if v . is_pipeline } show_lines = False table = Table ( show_header = False , box = box . SIMPLE , show_lines = show_lines ) table . add_column ( \"name\" , style = \"b\" ) table . add_column ( \"desc\" , style = \"i\" ) for name , details in module_types . items (): if config . get ( \"include_full_doc\" , False ): table . add_row ( name , details . documentation . full_doc ) else : table . add_row ( name , details . documentation . description ) return table create_renderable_from_module_type_map ( module_types , ignore_pipeline_modules = False , ignore_non_pipeline_modules = False , ** config ) classmethod \u00b6 Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description Source code in kiara/info/modules.py @classmethod def create_renderable_from_module_type_map ( cls , module_types : typing . Mapping [ str , typing . Type [ \"KiaraModule\" ]], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" return cls . create_renderable_from_module_info_map ( { k : KiaraModuleTypeMetadata . from_module_class ( v ) for k , v in module_types . items () }, ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config )","title":"\u279c\u2007info.modules"},{"location":"api_reference/kiara.info.modules/#kiarainfomodules","text":"","title":"kiara.info.modules"},{"location":"api_reference/kiara.info.modules/#kiara.info.modules.ModuleTypesGroupInfo","text":"","title":"ModuleTypesGroupInfo"},{"location":"api_reference/kiara.info.modules/#kiara.info.modules.ModuleTypesGroupInfo.create_renderable_from_module_info_map","text":"Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description Source code in kiara/info/modules.py @classmethod def create_renderable_from_module_info_map ( cls , module_types : typing . Mapping [ str , KiaraModuleTypeMetadata ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if ignore_pipeline_modules : module_types = { k : v for k , v in module_types . items () if not v . is_pipeline } elif ignore_non_pipeline_modules : module_types = { k : v for k , v in module_types . items () if v . is_pipeline } show_lines = False table = Table ( show_header = False , box = box . SIMPLE , show_lines = show_lines ) table . add_column ( \"name\" , style = \"b\" ) table . add_column ( \"desc\" , style = \"i\" ) for name , details in module_types . items (): if config . get ( \"include_full_doc\" , False ): table . add_row ( name , details . documentation . full_doc ) else : table . add_row ( name , details . documentation . description ) return table","title":"create_renderable_from_module_info_map()"},{"location":"api_reference/kiara.info.modules/#kiara.info.modules.ModuleTypesGroupInfo.create_renderable_from_module_type_map","text":"Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description Source code in kiara/info/modules.py @classmethod def create_renderable_from_module_type_map ( cls , module_types : typing . Mapping [ str , typing . Type [ \"KiaraModule\" ]], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" return cls . create_renderable_from_module_info_map ( { k : KiaraModuleTypeMetadata . from_module_class ( v ) for k , v in module_types . items () }, ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config )","title":"create_renderable_from_module_type_map()"},{"location":"api_reference/kiara.info.operations/","text":"kiara.info.operations \u00b6 OperationsGroupInfo pydantic-model \u00b6 operation_configs : Dict [ str , kiara . operations . Operation ] pydantic-field required \u00b6 The available operation ids and module_configs. operation_types : Dict [ str , kiara . info . operations . OperationsInfo ] pydantic-field required \u00b6 The available operation types and their details. OperationsInfo pydantic-model \u00b6 info : OperationsMetadata pydantic-field required \u00b6 Details about the type of operations contained in this collection. operation_configs : Dict [ str , kiara . operations . Operation ] pydantic-field required \u00b6 All available operation ids and their configurations.","title":"\u279c\u2007info.operations"},{"location":"api_reference/kiara.info.operations/#kiarainfooperations","text":"","title":"kiara.info.operations"},{"location":"api_reference/kiara.info.operations/#kiara.info.operations.OperationsGroupInfo","text":"","title":"OperationsGroupInfo"},{"location":"api_reference/kiara.info.operations/#kiara.info.operations.OperationsGroupInfo.operation_configs","text":"The available operation ids and module_configs.","title":"operation_configs"},{"location":"api_reference/kiara.info.operations/#kiara.info.operations.OperationsGroupInfo.operation_types","text":"The available operation types and their details.","title":"operation_types"},{"location":"api_reference/kiara.info.operations/#kiara.info.operations.OperationsInfo","text":"","title":"OperationsInfo"},{"location":"api_reference/kiara.info.operations/#kiara.info.operations.OperationsInfo.info","text":"Details about the type of operations contained in this collection.","title":"info"},{"location":"api_reference/kiara.info.operations/#kiara.info.operations.OperationsInfo.operation_configs","text":"All available operation ids and their configurations.","title":"operation_configs"},{"location":"api_reference/kiara.info.pipelines/","text":"kiara.info.pipelines \u00b6 PipelineModuleInfo pydantic-model \u00b6 structure : PipelineStructureDesc pydantic-field required \u00b6 The pipeline structure. PipelineState pydantic-model \u00b6 Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the dict or json methods to convert this object into a generic data structure. pipeline_inputs : PipelineValuesInfo pydantic-field required \u00b6 The current (externally facing) input values of this pipeline. pipeline_outputs : PipelineValuesInfo pydantic-field required \u00b6 The current (externally facing) output values of this pipeline. status : StepStatus pydantic-field required \u00b6 The current overal status of the pipeline. step_inputs : Dict [ str , kiara . pipeline . PipelineValuesInfo ] pydantic-field required \u00b6 The current (internal) input values of each step of this pipeline. step_outputs : Dict [ str , kiara . pipeline . PipelineValuesInfo ] pydantic-field required \u00b6 The current (internal) output values of each step of this pipeline. step_states : Dict [ str , kiara . pipeline . StepStatus ] pydantic-field required \u00b6 The status of each step. structure : PipelineStructureDesc pydantic-field required \u00b6 The structure (interconnections of modules/steps) of the pipeline. PipelineStructureDesc pydantic-model \u00b6 Outlines the internal structure of a Pipeline . pipeline_input_connections : Dict [ str , List [ str ]] pydantic-field required \u00b6 The connections of this pipelines input fields. One input field can be connected to one or several step input fields. pipeline_inputs : Dict [ str , kiara . pipeline . values . PipelineInputRef ] pydantic-field required \u00b6 The pipeline inputs. pipeline_output_connections : Dict [ str , str ] pydantic-field required \u00b6 The connections of this pipelines output fields. Each pipeline output is connected to exactly one step output field. pipeline_outputs : Dict [ str , kiara . pipeline . values . PipelineOutputRef ] pydantic-field required \u00b6 The pipeline outputs. processing_stages : List [ List [ str ]] pydantic-field required \u00b6 The order in which this pipeline has to be processed (basically the dependencies of each step on other steps, if any). steps : Dict [ str , kiara . pipeline . config . StepDesc ] pydantic-field required \u00b6 The steps contained in this pipeline, with the 'step_id' as key. StepsInfo pydantic-model \u00b6 processing_stages : List [ List [ str ]] pydantic-field required \u00b6 The stages in which the steps are processed. steps : Dict [ str , kiara . pipeline . config . StepDesc ] pydantic-field required \u00b6 A list of step details.","title":"\u279c\u2007info.pipelines"},{"location":"api_reference/kiara.info.pipelines/#kiarainfopipelines","text":"","title":"kiara.info.pipelines"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineModuleInfo","text":"","title":"PipelineModuleInfo"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineModuleInfo.structure","text":"The pipeline structure.","title":"structure"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineState","text":"Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the dict or json methods to convert this object into a generic data structure.","title":"PipelineState"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineState.pipeline_inputs","text":"The current (externally facing) input values of this pipeline.","title":"pipeline_inputs"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineState.pipeline_outputs","text":"The current (externally facing) output values of this pipeline.","title":"pipeline_outputs"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineState.status","text":"The current overal status of the pipeline.","title":"status"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineState.step_inputs","text":"The current (internal) input values of each step of this pipeline.","title":"step_inputs"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineState.step_outputs","text":"The current (internal) output values of each step of this pipeline.","title":"step_outputs"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineState.step_states","text":"The status of each step.","title":"step_states"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineState.structure","text":"The structure (interconnections of modules/steps) of the pipeline.","title":"structure"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineStructureDesc","text":"Outlines the internal structure of a Pipeline .","title":"PipelineStructureDesc"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineStructureDesc.pipeline_input_connections","text":"The connections of this pipelines input fields. One input field can be connected to one or several step input fields.","title":"pipeline_input_connections"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineStructureDesc.pipeline_inputs","text":"The pipeline inputs.","title":"pipeline_inputs"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineStructureDesc.pipeline_output_connections","text":"The connections of this pipelines output fields. Each pipeline output is connected to exactly one step output field.","title":"pipeline_output_connections"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineStructureDesc.pipeline_outputs","text":"The pipeline outputs.","title":"pipeline_outputs"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineStructureDesc.processing_stages","text":"The order in which this pipeline has to be processed (basically the dependencies of each step on other steps, if any).","title":"processing_stages"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.PipelineStructureDesc.steps","text":"The steps contained in this pipeline, with the 'step_id' as key.","title":"steps"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.StepsInfo","text":"","title":"StepsInfo"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.StepsInfo.processing_stages","text":"The stages in which the steps are processed.","title":"processing_stages"},{"location":"api_reference/kiara.info.pipelines/#kiara.info.pipelines.StepsInfo.steps","text":"A list of step details.","title":"steps"},{"location":"api_reference/kiara.info.types/","text":"kiara.info.types \u00b6 ValueTypeInfo pydantic-model \u00b6 config : ValueTypeConfigMetadata pydantic-field required \u00b6 Details on how this value type can be configured. context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type (description, tags, labels, references, ...). documentation : DocumentationMetadataModel pydantic-field required \u00b6 The documentation for this value type. metadata_types : Dict [ str , kiara . metadata . core_models . MetadataModelMetadata ] pydantic-field required \u00b6 The available metadata types for this value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about authorship for the value type. python_class : PythonClassMetadata pydantic-field required \u00b6 Information about the Python class for this value type. type_name : str pydantic-field required \u00b6 The name under which the type is registered.","title":"\u279c\u2007info.types"},{"location":"api_reference/kiara.info.types/#kiarainfotypes","text":"","title":"kiara.info.types"},{"location":"api_reference/kiara.info.types/#kiara.info.types.ValueTypeInfo","text":"","title":"ValueTypeInfo"},{"location":"api_reference/kiara.info.types/#kiara.info.types.ValueTypeInfo.config","text":"Details on how this value type can be configured.","title":"config"},{"location":"api_reference/kiara.info.types/#kiara.info.types.ValueTypeInfo.context","text":"Generic properties of this value type (description, tags, labels, references, ...).","title":"context"},{"location":"api_reference/kiara.info.types/#kiara.info.types.ValueTypeInfo.documentation","text":"The documentation for this value type.","title":"documentation"},{"location":"api_reference/kiara.info.types/#kiara.info.types.ValueTypeInfo.metadata_types","text":"The available metadata types for this value type.","title":"metadata_types"},{"location":"api_reference/kiara.info.types/#kiara.info.types.ValueTypeInfo.origin","text":"Information about authorship for the value type.","title":"origin"},{"location":"api_reference/kiara.info.types/#kiara.info.types.ValueTypeInfo.python_class","text":"Information about the Python class for this value type.","title":"python_class"},{"location":"api_reference/kiara.info.types/#kiara.info.types.ValueTypeInfo.type_name","text":"The name under which the type is registered.","title":"type_name"},{"location":"api_reference/kiara.interfaces.cli.data.commands/","text":"kiara.interfaces.cli.data.commands \u00b6 Data-related sub-commands for the cli.","title":"\u279c\u2007interfaces.cli.data.commands"},{"location":"api_reference/kiara.interfaces.cli.data.commands/#kiarainterfacesclidatacommands","text":"Data-related sub-commands for the cli.","title":"kiara.interfaces.cli.data.commands"},{"location":"api_reference/kiara.interfaces.cli.dev.commands/","text":"kiara.interfaces.cli.dev.commands \u00b6","title":"\u279c\u2007interfaces.cli.dev.commands"},{"location":"api_reference/kiara.interfaces.cli.dev.commands/#kiarainterfacesclidevcommands","text":"","title":"kiara.interfaces.cli.dev.commands"},{"location":"api_reference/kiara.interfaces.cli.explain/","text":"kiara.interfaces.cli.explain \u00b6 The 'run' subcommand for the cli. KiaraEntityMatches pydantic-model \u00b6 module_types : Dict [ str , kiara . metadata . module_models . KiaraModuleTypeMetadata ] pydantic-field \u00b6 Matching module types. operation_types : Dict [ str , kiara . metadata . operation_models . OperationsMetadata ] pydantic-field \u00b6 Matching operation types. operations : Dict [ str , kiara . operations . Operation ] pydantic-field \u00b6 Matching operations. values : Dict [ str , kiara . data . values . ValueInfo ] pydantic-field \u00b6 Matching values.","title":"\u279c\u2007interfaces.cli.explain"},{"location":"api_reference/kiara.interfaces.cli.explain/#kiarainterfacescliexplain","text":"The 'run' subcommand for the cli.","title":"kiara.interfaces.cli.explain"},{"location":"api_reference/kiara.interfaces.cli.explain/#kiara.interfaces.cli.explain.KiaraEntityMatches","text":"","title":"KiaraEntityMatches"},{"location":"api_reference/kiara.interfaces.cli.explain/#kiara.interfaces.cli.explain.KiaraEntityMatches.module_types","text":"Matching module types.","title":"module_types"},{"location":"api_reference/kiara.interfaces.cli.explain/#kiara.interfaces.cli.explain.KiaraEntityMatches.operation_types","text":"Matching operation types.","title":"operation_types"},{"location":"api_reference/kiara.interfaces.cli.explain/#kiara.interfaces.cli.explain.KiaraEntityMatches.operations","text":"Matching operations.","title":"operations"},{"location":"api_reference/kiara.interfaces.cli.explain/#kiara.interfaces.cli.explain.KiaraEntityMatches.values","text":"Matching values.","title":"values"},{"location":"api_reference/kiara.interfaces.cli.info.commands/","text":"kiara.interfaces.cli.info.commands \u00b6","title":"\u279c\u2007interfaces.cli.info.commands"},{"location":"api_reference/kiara.interfaces.cli.info.commands/#kiarainterfacescliinfocommands","text":"","title":"kiara.interfaces.cli.info.commands"},{"location":"api_reference/kiara.interfaces.cli/","text":"kiara.interfaces.cli \u00b6 A command-line interface for Kiara .","title":"\u279c\u2007interfaces.cli"},{"location":"api_reference/kiara.interfaces.cli/#kiarainterfacescli","text":"A command-line interface for Kiara .","title":"kiara.interfaces.cli"},{"location":"api_reference/kiara.interfaces.cli.metadata.commands/","text":"kiara.interfaces.cli.metadata.commands \u00b6","title":"\u279c\u2007interfaces.cli.metadata.commands"},{"location":"api_reference/kiara.interfaces.cli.metadata.commands/#kiarainterfacesclimetadatacommands","text":"","title":"kiara.interfaces.cli.metadata.commands"},{"location":"api_reference/kiara.interfaces.cli.metadata/","text":"kiara.interfaces.cli.metadata \u00b6 Metadata-related sub-commands for the cli.","title":"\u279c\u2007interfaces.cli.metadata"},{"location":"api_reference/kiara.interfaces.cli.metadata/#kiarainterfacesclimetadata","text":"Metadata-related sub-commands for the cli.","title":"kiara.interfaces.cli.metadata"},{"location":"api_reference/kiara.interfaces.cli.module.commands/","text":"kiara.interfaces.cli.module.commands \u00b6 Module related subcommands for the cli.","title":"\u279c\u2007interfaces.cli.module.commands"},{"location":"api_reference/kiara.interfaces.cli.module.commands/#kiarainterfacesclimodulecommands","text":"Module related subcommands for the cli.","title":"kiara.interfaces.cli.module.commands"},{"location":"api_reference/kiara.interfaces.cli.operation.commands/","text":"kiara.interfaces.cli.operation.commands \u00b6","title":"\u279c\u2007interfaces.cli.operation.commands"},{"location":"api_reference/kiara.interfaces.cli.operation.commands/#kiarainterfacesclioperationcommands","text":"","title":"kiara.interfaces.cli.operation.commands"},{"location":"api_reference/kiara.interfaces.cli.pipeline.commands/","text":"kiara.interfaces.cli.pipeline.commands \u00b6 Pipeline-related subcommands for the cli.","title":"\u279c\u2007interfaces.cli.pipeline.commands"},{"location":"api_reference/kiara.interfaces.cli.pipeline.commands/#kiarainterfacesclipipelinecommands","text":"Pipeline-related subcommands for the cli.","title":"kiara.interfaces.cli.pipeline.commands"},{"location":"api_reference/kiara.interfaces.cli.run/","text":"kiara.interfaces.cli.run \u00b6 The 'run' subcommand for the cli.","title":"\u279c\u2007interfaces.cli.run"},{"location":"api_reference/kiara.interfaces.cli.run/#kiarainterfacesclirun","text":"The 'run' subcommand for the cli.","title":"kiara.interfaces.cli.run"},{"location":"api_reference/kiara.interfaces.cli.service.commands/","text":"kiara.interfaces.cli.service.commands \u00b6","title":"\u279c\u2007interfaces.cli.service.commands"},{"location":"api_reference/kiara.interfaces.cli.service.commands/#kiarainterfacescliservicecommands","text":"","title":"kiara.interfaces.cli.service.commands"},{"location":"api_reference/kiara.interfaces.cli.type.command/","text":"kiara.interfaces.cli.type.command \u00b6 Type-related subcommands for the cli.","title":"\u279c\u2007interfaces.cli.type.command"},{"location":"api_reference/kiara.interfaces.cli.type.command/#kiarainterfacesclitypecommand","text":"Type-related subcommands for the cli.","title":"kiara.interfaces.cli.type.command"},{"location":"api_reference/kiara.interfaces.cli.utils/","text":"kiara.interfaces.cli.utils \u00b6 Shared utilties to be used by cli sub-commands.","title":"\u279c\u2007interfaces.cli.utils"},{"location":"api_reference/kiara.interfaces.cli.utils/#kiarainterfacescliutils","text":"Shared utilties to be used by cli sub-commands.","title":"kiara.interfaces.cli.utils"},{"location":"api_reference/kiara.interfaces/","text":"kiara.interfaces \u00b6 Implementation of interfaces for Kiara . get_console () \u00b6 Get a global Console instance. Returns: Type Description Console Console: A console instance. Source code in kiara/interfaces/__init__.py def get_console () -> Console : \"\"\"Get a global Console instance. Returns: Console: A console instance. \"\"\" global _console if _console is None or True : console_width = os . environ . get ( \"CONSOLE_WIDTH\" , None ) width = None if console_width : try : width = int ( console_width ) except Exception : pass _console = Console ( width = width ) return _console","title":"\u279c\u2007interfaces"},{"location":"api_reference/kiara.interfaces/#kiarainterfaces","text":"Implementation of interfaces for Kiara .","title":"kiara.interfaces"},{"location":"api_reference/kiara.interfaces/#kiara.interfaces.__init__.get_console","text":"Get a global Console instance. Returns: Type Description Console Console: A console instance. Source code in kiara/interfaces/__init__.py def get_console () -> Console : \"\"\"Get a global Console instance. Returns: Console: A console instance. \"\"\" global _console if _console is None or True : console_width = os . environ . get ( \"CONSOLE_WIDTH\" , None ) width = None if console_width : try : width = int ( console_width ) except Exception : pass _console = Console ( width = width ) return _console","title":"get_console()"},{"location":"api_reference/kiara.interfaces.python_api.controller/","text":"kiara.interfaces.python_api.controller \u00b6 ApiController \u00b6 A pipeline controller for pipelines generated by the kiara Python API. ensure_step ( self , step_id ) \u00b6 Ensure a step has valid outputs. Returns: Type Description bool 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs Source code in kiara/interfaces/python_api/controller.py def ensure_step ( self , step_id : str ) -> bool : \"\"\"Ensure a step has valid outputs. Returns: 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs \"\"\" if step_id not in self . pipeline . step_ids : raise Exception ( f \"No step with id: { step_id } \" ) outputs = self . get_step_outputs ( step_id = step_id ) if outputs . items_are_valid (): return True if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids : typing . List [ str ] = [] finished = False if step_id in stage : finished = True job_id = self . _process ( step_id = step_id ) if job_id is None : return True if not job_id : return False job_ids . append ( job_id ) # type: ignore else : for s_id in stage : job_id = self . _process ( step_id = s_id ) if job_id is None : continue elif job_id is False : return False job_ids . append ( job_id ) # type: ignore self . _processor . wait_for ( * job_ids ) if finished : break finally : self . _is_running = False return True pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/interfaces/python_api/controller.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): if self . _is_running : raise NotImplementedError () pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/interfaces/python_api/controller.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/interfaces/python_api/controller.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : # from kiara.utils.output import rich_print # rich_print(self.pipeline.get_current_state()) raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/interfaces/python_api/controller.py def step_inputs_changed ( self , event : StepInputEvent ): for step_id in event . updated_step_inputs . keys (): # outputs = self.get_step_outputs(step_id) raise NotImplementedError () # outputs.invalidate()","title":"\u279c\u2007interfaces.python_api.controller"},{"location":"api_reference/kiara.interfaces.python_api.controller/#kiarainterfacespython_apicontroller","text":"","title":"kiara.interfaces.python_api.controller"},{"location":"api_reference/kiara.interfaces.python_api.controller/#kiara.interfaces.python_api.controller.ApiController","text":"A pipeline controller for pipelines generated by the kiara Python API.","title":"ApiController"},{"location":"api_reference/kiara.interfaces.python_api.controller/#kiara.interfaces.python_api.controller.ApiController.ensure_step","text":"Ensure a step has valid outputs. Returns: Type Description bool 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs Source code in kiara/interfaces/python_api/controller.py def ensure_step ( self , step_id : str ) -> bool : \"\"\"Ensure a step has valid outputs. Returns: 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs \"\"\" if step_id not in self . pipeline . step_ids : raise Exception ( f \"No step with id: { step_id } \" ) outputs = self . get_step_outputs ( step_id = step_id ) if outputs . items_are_valid (): return True if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids : typing . List [ str ] = [] finished = False if step_id in stage : finished = True job_id = self . _process ( step_id = step_id ) if job_id is None : return True if not job_id : return False job_ids . append ( job_id ) # type: ignore else : for s_id in stage : job_id = self . _process ( step_id = s_id ) if job_id is None : continue elif job_id is False : return False job_ids . append ( job_id ) # type: ignore self . _processor . wait_for ( * job_ids ) if finished : break finally : self . _is_running = False return True","title":"ensure_step()"},{"location":"api_reference/kiara.interfaces.python_api.controller/#kiara.interfaces.python_api.controller.ApiController.pipeline_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/interfaces/python_api/controller.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): if self . _is_running : raise NotImplementedError ()","title":"pipeline_inputs_changed()"},{"location":"api_reference/kiara.interfaces.python_api.controller/#kiara.interfaces.python_api.controller.ApiController.pipeline_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/interfaces/python_api/controller.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False","title":"pipeline_outputs_changed()"},{"location":"api_reference/kiara.interfaces.python_api.controller/#kiara.interfaces.python_api.controller.ApiController.process_pipeline","text":"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/interfaces/python_api/controller.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : # from kiara.utils.output import rich_print # rich_print(self.pipeline.get_current_state()) raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False","title":"process_pipeline()"},{"location":"api_reference/kiara.interfaces.python_api.controller/#kiara.interfaces.python_api.controller.ApiController.step_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/interfaces/python_api/controller.py def step_inputs_changed ( self , event : StepInputEvent ): for step_id in event . updated_step_inputs . keys (): # outputs = self.get_step_outputs(step_id) raise NotImplementedError () # outputs.invalidate()","title":"step_inputs_changed()"},{"location":"api_reference/kiara.interfaces.python_api/","text":"kiara.interfaces.python_api \u00b6 A Python API for creating workflow sessions and dynamic pipelines in kiara . StepInfo pydantic-model \u00b6 module_metadata : KiaraModuleInstanceMetadata pydantic-field required \u00b6 The module metadata. step_id : str pydantic-field required \u00b6 The step id.","title":"\u279c\u2007interfaces.python_api"},{"location":"api_reference/kiara.interfaces.python_api/#kiarainterfacespython_api","text":"A Python API for creating workflow sessions and dynamic pipelines in kiara .","title":"kiara.interfaces.python_api"},{"location":"api_reference/kiara.interfaces.python_api/#kiara.interfaces.python_api.__init__.StepInfo","text":"","title":"StepInfo"},{"location":"api_reference/kiara.interfaces.python_api/#kiara.interfaces.python_api.__init__.StepInfo.module_metadata","text":"The module metadata.","title":"module_metadata"},{"location":"api_reference/kiara.interfaces.python_api/#kiara.interfaces.python_api.__init__.StepInfo.step_id","text":"The step id.","title":"step_id"},{"location":"api_reference/kiara.interfaces.rest_api/","text":"kiara.interfaces.rest_api \u00b6 ModuleRunResponse pydantic-model \u00b6 output_schema : Dict [ str , kiara . data . values . ValueSchema ] pydantic-field required \u00b6 The output schema. outputs : Dict [ str , Any ] pydantic-field required \u00b6 The module output.","title":"\u279c\u2007interfaces.rest_api"},{"location":"api_reference/kiara.interfaces.rest_api/#kiarainterfacesrest_api","text":"","title":"kiara.interfaces.rest_api"},{"location":"api_reference/kiara.interfaces.rest_api/#kiara.interfaces.rest_api.__init__.ModuleRunResponse","text":"","title":"ModuleRunResponse"},{"location":"api_reference/kiara.interfaces.rest_api/#kiara.interfaces.rest_api.__init__.ModuleRunResponse.output_schema","text":"The output schema.","title":"output_schema"},{"location":"api_reference/kiara.interfaces.rest_api/#kiara.interfaces.rest_api.__init__.ModuleRunResponse.outputs","text":"The module output.","title":"outputs"},{"location":"api_reference/kiara.kiara/","text":"kiara.kiara \u00b6 Main module. Kiara \u00b6 The core context of a kiara session. The Kiara object holds all information related to the current environment the user does works in. This includes: available modules, operations & pipelines available value types available metadata schemas available data items available controller and processor types misc. configuration options It's possible to use kiara without ever manually touching the 'Kiara' class, by default all relevant classes and functions will use a default instance of this class (available via the Kiara.instance() method. The Kiara class is highly dependent on the Python environment it lives in, because it auto-discovers available sub-classes of its building blocks (modules, value types, etc.). So, you can't assume that, for example, a pipeline you create will work the same way (or at all) in a different environment. kiara will always be able to tell you all the details of this environment, though, and it will attach those details to things like data, so there is always a record of how something was created, and in which environment. available_module_types : List [ str ] property readonly \u00b6 Return the names of all available modules available_non_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. available_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. config : KiaraConfig property readonly \u00b6 The configuration of this kiara environment. default_processor : ModuleProcessor property readonly \u00b6 The default module processor that will be used in this environment, unless otherwise specified. create_module ( self , module_type , module_config = None , id = None , parent_id = None ) \u00b6 Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Parameters: Name Type Description Default module_type str the module type- or operation-id required module_config Optional[Mapping[str, Any]] the module instance configuration (must be empty in case of the module_type being an operation id None id Optional[str] the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline None parent_id Optional[str] a reference to the pipeline that contains this module (if applicable) None Returns: Type Description KiaraModule The instantiated module object. Source code in kiara/kiara.py def create_module ( self , module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Arguments: module_type: the module type- or operation-id module_config: the module instance configuration (must be empty in case of the module_type being an operation id id: the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline parent_id: a reference to the pipeline that contains this module (if applicable) Returns: The instantiated module object. \"\"\" if module_type == \"pipeline\" : from kiara import PipelineModule module_cls = PipelineModule module = module_cls ( id = id , parent_id = parent_id , module_config = module_config , kiara = self ) return module elif ( module_type not in self . available_module_types and module_type in self . operation_mgmt . operation_ids ): op = self . operation_mgmt . get_operation ( module_type ) assert op is not None module_type = op . module_type op_config = op . module_config if not module_config : _module_config : typing . Optional [ typing . Mapping [ str , typing . Any ] ] = op_config else : _module_config = dict ( op_config ) _module_config . update ( module_config ) else : _module_config = module_config return self . _module_mgr . create_module ( kiara = self , id = id , module_type = module_type , module_config = _module_config , parent_id = parent_id , ) instance () classmethod \u00b6 The default kiara context. In most cases, it's recommended you create and manage your own, though. Source code in kiara/kiara.py @classmethod def instance ( cls ): \"\"\"The default *kiara* context. In most cases, it's recommended you create and manage your own, though.\"\"\" if cls . _instance is None : cls . _instance = Kiara () return cls . _instance explain ( item ) \u00b6 Pretty print information about an item on the terminal. Source code in kiara/kiara.py def explain ( item : typing . Any ): \"\"\"Pretty print information about an item on the terminal.\"\"\" if isinstance ( item , type ): from kiara.module import KiaraModule if issubclass ( item , KiaraModule ): item = KiaraModuleTypeMetadata . from_module_class ( module_cls = item ) elif isinstance ( item , Value ): item . get_metadata () console = get_console () console . print ( item )","title":"\u279c\u2007kiara"},{"location":"api_reference/kiara.kiara/#kiarakiara","text":"Main module.","title":"kiara.kiara"},{"location":"api_reference/kiara.kiara/#kiara.kiara.Kiara","text":"The core context of a kiara session. The Kiara object holds all information related to the current environment the user does works in. This includes: available modules, operations & pipelines available value types available metadata schemas available data items available controller and processor types misc. configuration options It's possible to use kiara without ever manually touching the 'Kiara' class, by default all relevant classes and functions will use a default instance of this class (available via the Kiara.instance() method. The Kiara class is highly dependent on the Python environment it lives in, because it auto-discovers available sub-classes of its building blocks (modules, value types, etc.). So, you can't assume that, for example, a pipeline you create will work the same way (or at all) in a different environment. kiara will always be able to tell you all the details of this environment, though, and it will attach those details to things like data, so there is always a record of how something was created, and in which environment.","title":"Kiara"},{"location":"api_reference/kiara.kiara/#kiara.kiara.Kiara.available_module_types","text":"Return the names of all available modules","title":"available_module_types"},{"location":"api_reference/kiara.kiara/#kiara.kiara.Kiara.available_non_pipeline_module_types","text":"Return the names of all available pipeline-type modules.","title":"available_non_pipeline_module_types"},{"location":"api_reference/kiara.kiara/#kiara.kiara.Kiara.available_pipeline_module_types","text":"Return the names of all available pipeline-type modules.","title":"available_pipeline_module_types"},{"location":"api_reference/kiara.kiara/#kiara.kiara.Kiara.config","text":"The configuration of this kiara environment.","title":"config"},{"location":"api_reference/kiara.kiara/#kiara.kiara.Kiara.default_processor","text":"The default module processor that will be used in this environment, unless otherwise specified.","title":"default_processor"},{"location":"api_reference/kiara.kiara/#kiara.kiara.Kiara.create_module","text":"Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Parameters: Name Type Description Default module_type str the module type- or operation-id required module_config Optional[Mapping[str, Any]] the module instance configuration (must be empty in case of the module_type being an operation id None id Optional[str] the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline None parent_id Optional[str] a reference to the pipeline that contains this module (if applicable) None Returns: Type Description KiaraModule The instantiated module object. Source code in kiara/kiara.py def create_module ( self , module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Arguments: module_type: the module type- or operation-id module_config: the module instance configuration (must be empty in case of the module_type being an operation id id: the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline parent_id: a reference to the pipeline that contains this module (if applicable) Returns: The instantiated module object. \"\"\" if module_type == \"pipeline\" : from kiara import PipelineModule module_cls = PipelineModule module = module_cls ( id = id , parent_id = parent_id , module_config = module_config , kiara = self ) return module elif ( module_type not in self . available_module_types and module_type in self . operation_mgmt . operation_ids ): op = self . operation_mgmt . get_operation ( module_type ) assert op is not None module_type = op . module_type op_config = op . module_config if not module_config : _module_config : typing . Optional [ typing . Mapping [ str , typing . Any ] ] = op_config else : _module_config = dict ( op_config ) _module_config . update ( module_config ) else : _module_config = module_config return self . _module_mgr . create_module ( kiara = self , id = id , module_type = module_type , module_config = _module_config , parent_id = parent_id , )","title":"create_module()"},{"location":"api_reference/kiara.kiara/#kiara.kiara.Kiara.instance","text":"The default kiara context. In most cases, it's recommended you create and manage your own, though. Source code in kiara/kiara.py @classmethod def instance ( cls ): \"\"\"The default *kiara* context. In most cases, it's recommended you create and manage your own, though.\"\"\" if cls . _instance is None : cls . _instance = Kiara () return cls . _instance","title":"instance()"},{"location":"api_reference/kiara.kiara/#kiara.kiara.explain","text":"Pretty print information about an item on the terminal. Source code in kiara/kiara.py def explain ( item : typing . Any ): \"\"\"Pretty print information about an item on the terminal.\"\"\" if isinstance ( item , type ): from kiara.module import KiaraModule if issubclass ( item , KiaraModule ): item = KiaraModuleTypeMetadata . from_module_class ( module_cls = item ) elif isinstance ( item , Value ): item . get_metadata () console = get_console () console . print ( item )","title":"explain()"},{"location":"api_reference/kiara/","text":"kiara \u00b6 __author__ special \u00b6 The author of this package. __email__ special \u00b6 Email address of the author. get_version () \u00b6 Return the current version of Kiara . Source code in kiara/__init__.py def get_version () -> str : \"\"\"Return the current version of *Kiara*.\"\"\" from pkg_resources import DistributionNotFound , get_distribution try : # Change here if project is renamed and does not equal the package name dist_name = __name__ __version__ = get_distribution ( dist_name ) . version except DistributionNotFound : try : version_file = os . path . join ( os . path . dirname ( __file__ ), \"version.txt\" ) if os . path . exists ( version_file ): with open ( version_file , encoding = \"utf-8\" ) as vf : __version__ = vf . read () else : __version__ = \"unknown\" except ( Exception ): pass if __version__ is None : __version__ = \"unknown\" return __version__","title":"kiara"},{"location":"api_reference/kiara/#kiara","text":"","title":"kiara"},{"location":"api_reference/kiara/#kiara.__init__.__author__","text":"The author of this package.","title":"__author__"},{"location":"api_reference/kiara/#kiara.__init__.__email__","text":"Email address of the author.","title":"__email__"},{"location":"api_reference/kiara/#kiara.__init__.get_version","text":"Return the current version of Kiara . Source code in kiara/__init__.py def get_version () -> str : \"\"\"Return the current version of *Kiara*.\"\"\" from pkg_resources import DistributionNotFound , get_distribution try : # Change here if project is renamed and does not equal the package name dist_name = __name__ __version__ = get_distribution ( dist_name ) . version except DistributionNotFound : try : version_file = os . path . join ( os . path . dirname ( __file__ ), \"version.txt\" ) if os . path . exists ( version_file ): with open ( version_file , encoding = \"utf-8\" ) as vf : __version__ = vf . read () else : __version__ = \"unknown\" except ( Exception ): pass if __version__ is None : __version__ = \"unknown\" return __version__","title":"get_version()"},{"location":"api_reference/kiara.metadata.core_models/","text":"kiara.metadata.core_models \u00b6 AuthorModel pydantic-model \u00b6 email : EmailStr pydantic-field \u00b6 The email address of the author name : str pydantic-field required \u00b6 The full name of the author. ContextMetadataModel pydantic-model \u00b6 labels : Dict [ str , str ] pydantic-field \u00b6 A list of labels for the item. references : Dict [ str , kiara . metadata . core_models . LinkModel ] pydantic-field \u00b6 References for the item. tags : Set [ str ] pydantic-field \u00b6 A list of tags for the item. DocumentationMetadataModel pydantic-model \u00b6 description : str pydantic-field \u00b6 Short description of the item. doc : str pydantic-field \u00b6 Detailed documentation of the item (in markdown). LinkModel pydantic-model \u00b6 desc : str pydantic-field \u00b6 A short description of the link content. url : AnyUrl pydantic-field required \u00b6 The url. MetadataModelMetadata pydantic-model \u00b6 context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type. documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about the creator of this value type. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this value type. type_name : str pydantic-field required \u00b6 The registered name for this value type. OriginMetadataModel pydantic-model \u00b6 authors : List [ kiara . metadata . core_models . AuthorModel ] pydantic-field \u00b6 The authors/creators of this item. PythonClassMetadata pydantic-model \u00b6 Python class and module information. class_name : str pydantic-field required \u00b6 The name of the Python class. full_name : str pydantic-field required \u00b6 The full class namespace. module_name : str pydantic-field required \u00b6 The name of the Python module this class lives in.","title":"\u279c\u2007metadata.core_models"},{"location":"api_reference/kiara.metadata.core_models/#kiarametadatacore_models","text":"","title":"kiara.metadata.core_models"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.AuthorModel","text":"","title":"AuthorModel"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.AuthorModel.email","text":"The email address of the author","title":"email"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.AuthorModel.name","text":"The full name of the author.","title":"name"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.ContextMetadataModel","text":"","title":"ContextMetadataModel"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.ContextMetadataModel.labels","text":"A list of labels for the item.","title":"labels"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.ContextMetadataModel.references","text":"References for the item.","title":"references"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.ContextMetadataModel.tags","text":"A list of tags for the item.","title":"tags"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.DocumentationMetadataModel","text":"","title":"DocumentationMetadataModel"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.DocumentationMetadataModel.description","text":"Short description of the item.","title":"description"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.DocumentationMetadataModel.doc","text":"Detailed documentation of the item (in markdown).","title":"doc"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.LinkModel","text":"","title":"LinkModel"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.LinkModel.desc","text":"A short description of the link content.","title":"desc"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.LinkModel.url","text":"The url.","title":"url"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.MetadataModelMetadata","text":"","title":"MetadataModelMetadata"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.MetadataModelMetadata.context","text":"Generic properties of this value type.","title":"context"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.MetadataModelMetadata.documentation","text":"Documentation for the value type.","title":"documentation"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.MetadataModelMetadata.origin","text":"Information about the creator of this value type.","title":"origin"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.MetadataModelMetadata.python_class","text":"The Python class for this value type.","title":"python_class"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.MetadataModelMetadata.type_name","text":"The registered name for this value type.","title":"type_name"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.OriginMetadataModel","text":"","title":"OriginMetadataModel"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.OriginMetadataModel.authors","text":"The authors/creators of this item.","title":"authors"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.PythonClassMetadata","text":"Python class and module information.","title":"PythonClassMetadata"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.PythonClassMetadata.class_name","text":"The name of the Python class.","title":"class_name"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.PythonClassMetadata.full_name","text":"The full class namespace.","title":"full_name"},{"location":"api_reference/kiara.metadata.core_models/#kiara.metadata.core_models.PythonClassMetadata.module_name","text":"The name of the Python module this class lives in.","title":"module_name"},{"location":"api_reference/kiara.metadata.data/","text":"kiara.metadata.data \u00b6 DeserializeConfig pydantic-model \u00b6 input : Any pydantic-field \u00b6 The inputs to use when running this module. output_name : str pydantic-field required \u00b6 The name of the output field for the value. serialization_type : str pydantic-field required \u00b6 The serialization type. LoadConfig pydantic-model \u00b6 base_path_input_name : str pydantic-field \u00b6 The name of the input that stores the base_path where the value is saved. inputs : Dict [ str , Any ] pydantic-field \u00b6 The inputs to use when running this module. output_name : str pydantic-field required \u00b6 The name of the output field for the value. value_id : str pydantic-field required \u00b6 The id of the value. SaveConfig pydantic-model \u00b6 inputs : Dict [ str , Any ] pydantic-field \u00b6 The inputs to use when running this module. load_config_output : str pydantic-field required \u00b6 The output name that will contain the load config output value.","title":"\u279c\u2007metadata.data"},{"location":"api_reference/kiara.metadata.data/#kiarametadatadata","text":"","title":"kiara.metadata.data"},{"location":"api_reference/kiara.metadata.data/#kiara.metadata.data.DeserializeConfig","text":"","title":"DeserializeConfig"},{"location":"api_reference/kiara.metadata.data/#kiara.metadata.data.DeserializeConfig.input","text":"The inputs to use when running this module.","title":"input"},{"location":"api_reference/kiara.metadata.data/#kiara.metadata.data.DeserializeConfig.output_name","text":"The name of the output field for the value.","title":"output_name"},{"location":"api_reference/kiara.metadata.data/#kiara.metadata.data.DeserializeConfig.serialization_type","text":"The serialization type.","title":"serialization_type"},{"location":"api_reference/kiara.metadata.data/#kiara.metadata.data.LoadConfig","text":"","title":"LoadConfig"},{"location":"api_reference/kiara.metadata.data/#kiara.metadata.data.LoadConfig.base_path_input_name","text":"The name of the input that stores the base_path where the value is saved.","title":"base_path_input_name"},{"location":"api_reference/kiara.metadata.data/#kiara.metadata.data.LoadConfig.inputs","text":"The inputs to use when running this module.","title":"inputs"},{"location":"api_reference/kiara.metadata.data/#kiara.metadata.data.LoadConfig.output_name","text":"The name of the output field for the value.","title":"output_name"},{"location":"api_reference/kiara.metadata.data/#kiara.metadata.data.LoadConfig.value_id","text":"The id of the value.","title":"value_id"},{"location":"api_reference/kiara.metadata.data/#kiara.metadata.data.SaveConfig","text":"","title":"SaveConfig"},{"location":"api_reference/kiara.metadata.data/#kiara.metadata.data.SaveConfig.inputs","text":"The inputs to use when running this module.","title":"inputs"},{"location":"api_reference/kiara.metadata.data/#kiara.metadata.data.SaveConfig.load_config_output","text":"The output name that will contain the load config output value.","title":"load_config_output"},{"location":"api_reference/kiara.metadata/","text":"kiara.metadata \u00b6 MetadataModel pydantic-model \u00b6 Base class for classes that represent value metadata in kiara. get_type_metadata () classmethod \u00b6 Return all metadata associated with this module type. Source code in kiara/metadata/__init__.py @classmethod def get_type_metadata ( cls ) -> \"MetadataModelMetadata\" : \"\"\"Return all metadata associated with this module type.\"\"\" from kiara.metadata.core_models import MetadataModelMetadata return MetadataModelMetadata . from_model_class ( model_cls = cls ) ValueTypeAndDescription pydantic-model \u00b6 description : str pydantic-field required \u00b6 The description for the value. type : str pydantic-field required \u00b6 The value type.","title":"\u279c\u2007metadata"},{"location":"api_reference/kiara.metadata/#kiarametadata","text":"","title":"kiara.metadata"},{"location":"api_reference/kiara.metadata/#kiara.metadata.__init__.MetadataModel","text":"Base class for classes that represent value metadata in kiara.","title":"MetadataModel"},{"location":"api_reference/kiara.metadata/#kiara.metadata.__init__.MetadataModel.get_type_metadata","text":"Return all metadata associated with this module type. Source code in kiara/metadata/__init__.py @classmethod def get_type_metadata ( cls ) -> \"MetadataModelMetadata\" : \"\"\"Return all metadata associated with this module type.\"\"\" from kiara.metadata.core_models import MetadataModelMetadata return MetadataModelMetadata . from_model_class ( model_cls = cls )","title":"get_type_metadata()"},{"location":"api_reference/kiara.metadata/#kiara.metadata.__init__.ValueTypeAndDescription","text":"","title":"ValueTypeAndDescription"},{"location":"api_reference/kiara.metadata/#kiara.metadata.__init__.ValueTypeAndDescription.description","text":"The description for the value.","title":"description"},{"location":"api_reference/kiara.metadata/#kiara.metadata.__init__.ValueTypeAndDescription.type","text":"The value type.","title":"type"},{"location":"api_reference/kiara.metadata.mgmt/","text":"kiara.metadata.mgmt \u00b6","title":"\u279c\u2007metadata.mgmt"},{"location":"api_reference/kiara.metadata.mgmt/#kiarametadatamgmt","text":"","title":"kiara.metadata.mgmt"},{"location":"api_reference/kiara.metadata.module_models/","text":"kiara.metadata.module_models \u00b6 KiaraModuleConfigMetadata pydantic-model \u00b6 config_values : Dict [ str , kiara . metadata . ValueTypeAndDescription ] pydantic-field required \u00b6 The available configuration values. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this configuration. KiaraModuleInstanceMetadata pydantic-model \u00b6 config : Dict [ str , Any ] pydantic-field required \u00b6 Configuration that was used to create this module instance. inputs_schema : Dict [ str , kiara . data . values . ValueSchema ] pydantic-field required \u00b6 The schema for the module inputs. outputs_schema : Dict [ str , kiara . data . values . ValueSchema ] pydantic-field required \u00b6 The schema for the module outputs. type_metadata : KiaraModuleTypeMetadata pydantic-field required \u00b6 Metadata for the module type of this instance. KiaraModuleTypeMetadata pydantic-model \u00b6 config : KiaraModuleConfigMetadata pydantic-field required \u00b6 Details on how this module type can be configured. context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this module (description, tags, labels, references, ...). documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the module. is_pipeline : bool pydantic-field required \u00b6 Whether the module type is a pipeline, or a core module. origin : OriginMetadataModel pydantic-field required \u00b6 Information about authorship for the module type. pipeline_config : PipelineConfig pydantic-field \u00b6 If this module is a pipeline, this field contains the pipeline configuration. process_src : str pydantic-field required \u00b6 The source code of the process method of the module. python_class : PythonClassMetadata pydantic-field required \u00b6 Information about the Python class for this module type. type_id : str pydantic-field required \u00b6 The full type id. type_name : str pydantic-field required \u00b6 The registered name for this module type.","title":"\u279c\u2007metadata.module_models"},{"location":"api_reference/kiara.metadata.module_models/#kiarametadatamodule_models","text":"","title":"kiara.metadata.module_models"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleConfigMetadata","text":"","title":"KiaraModuleConfigMetadata"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleConfigMetadata.config_values","text":"The available configuration values.","title":"config_values"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleConfigMetadata.python_class","text":"The Python class for this configuration.","title":"python_class"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleInstanceMetadata","text":"","title":"KiaraModuleInstanceMetadata"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleInstanceMetadata.config","text":"Configuration that was used to create this module instance.","title":"config"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleInstanceMetadata.inputs_schema","text":"The schema for the module inputs.","title":"inputs_schema"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleInstanceMetadata.outputs_schema","text":"The schema for the module outputs.","title":"outputs_schema"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleInstanceMetadata.type_metadata","text":"Metadata for the module type of this instance.","title":"type_metadata"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata","text":"","title":"KiaraModuleTypeMetadata"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.config","text":"Details on how this module type can be configured.","title":"config"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.context","text":"Generic properties of this module (description, tags, labels, references, ...).","title":"context"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.documentation","text":"Documentation for the module.","title":"documentation"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.is_pipeline","text":"Whether the module type is a pipeline, or a core module.","title":"is_pipeline"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.origin","text":"Information about authorship for the module type.","title":"origin"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.pipeline_config","text":"If this module is a pipeline, this field contains the pipeline configuration.","title":"pipeline_config"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.process_src","text":"The source code of the process method of the module.","title":"process_src"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.python_class","text":"Information about the Python class for this module type.","title":"python_class"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.type_id","text":"The full type id.","title":"type_id"},{"location":"api_reference/kiara.metadata.module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.type_name","text":"The registered name for this module type.","title":"type_name"},{"location":"api_reference/kiara.metadata.operation_models/","text":"kiara.metadata.operation_models \u00b6 OperationsMetadata pydantic-model \u00b6 context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type. documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about the creator of this value type. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this value type. type_name : str pydantic-field required \u00b6 The registered name for this value type.","title":"\u279c\u2007metadata.operation_models"},{"location":"api_reference/kiara.metadata.operation_models/#kiarametadataoperation_models","text":"","title":"kiara.metadata.operation_models"},{"location":"api_reference/kiara.metadata.operation_models/#kiara.metadata.operation_models.OperationsMetadata","text":"","title":"OperationsMetadata"},{"location":"api_reference/kiara.metadata.operation_models/#kiara.metadata.operation_models.OperationsMetadata.context","text":"Generic properties of this value type.","title":"context"},{"location":"api_reference/kiara.metadata.operation_models/#kiara.metadata.operation_models.OperationsMetadata.documentation","text":"Documentation for the value type.","title":"documentation"},{"location":"api_reference/kiara.metadata.operation_models/#kiara.metadata.operation_models.OperationsMetadata.origin","text":"Information about the creator of this value type.","title":"origin"},{"location":"api_reference/kiara.metadata.operation_models/#kiara.metadata.operation_models.OperationsMetadata.python_class","text":"The Python class for this value type.","title":"python_class"},{"location":"api_reference/kiara.metadata.operation_models/#kiara.metadata.operation_models.OperationsMetadata.type_name","text":"The registered name for this value type.","title":"type_name"},{"location":"api_reference/kiara.metadata.type_models/","text":"kiara.metadata.type_models \u00b6 ValueTypeMetadata pydantic-model \u00b6 context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type. documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about the creator of this value type. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this value type. type_name : str pydantic-field required \u00b6 The registered name for this value type.","title":"\u279c\u2007metadata.type_models"},{"location":"api_reference/kiara.metadata.type_models/#kiarametadatatype_models","text":"","title":"kiara.metadata.type_models"},{"location":"api_reference/kiara.metadata.type_models/#kiara.metadata.type_models.ValueTypeMetadata","text":"","title":"ValueTypeMetadata"},{"location":"api_reference/kiara.metadata.type_models/#kiara.metadata.type_models.ValueTypeMetadata.context","text":"Generic properties of this value type.","title":"context"},{"location":"api_reference/kiara.metadata.type_models/#kiara.metadata.type_models.ValueTypeMetadata.documentation","text":"Documentation for the value type.","title":"documentation"},{"location":"api_reference/kiara.metadata.type_models/#kiara.metadata.type_models.ValueTypeMetadata.origin","text":"Information about the creator of this value type.","title":"origin"},{"location":"api_reference/kiara.metadata.type_models/#kiara.metadata.type_models.ValueTypeMetadata.python_class","text":"The Python class for this value type.","title":"python_class"},{"location":"api_reference/kiara.metadata.type_models/#kiara.metadata.type_models.ValueTypeMetadata.type_name","text":"The registered name for this value type.","title":"type_name"},{"location":"api_reference/kiara.module/","text":"kiara.module \u00b6 KiaraModule \u00b6 The base class that every custom module in Kiara needs to inherit from. The core of every KiaraModule is a process method, which should be a 'pure', idempotent function that creates one or several output values from the given input(s), and its purpose is to transfor a set of inputs into a set of outputs. Every module can be configured. The module configuration schema can differ, but every one such configuration needs to subclass the ModuleTypeConfigSchema class and set as the value to the _config_cls attribute of the module class. This is useful, because it allows for some modules to serve a much larger variety of use-cases than non-configurable modules would be, which would mean more code duplication because of very simlilar, but slightly different module types. Each module class (type) has a unique -- within a kiara context -- module type id which can be accessed via the _module_type_id class attribute. Examples: A simple example would be an 'addition' module, with a and b configured as inputs, and z as the output field name. An implementing class would look something like this: TODO Parameters: Name Type Description Default id str the id for this module (needs to be unique within a pipeline) required parent_id Optional[str] the id of the parent, in case this module is part of a pipeline required module_config Any the configuation for this module required metadata Mapping[str, Any] metadata for this module (not implemented yet) required config : ~ KIARA_CONFIG property readonly \u00b6 Retrieve the configuration object for this module. Returns: Type Description ~KIARA_CONFIG the module-class-specific config object full_id : str property readonly \u00b6 The full id for this module. id : str property readonly \u00b6 The id of this module. This is only unique within a pipeline. info : KiaraModuleInstanceMetadata property readonly \u00b6 Return an info wrapper class for this module. input_names : Iterable [ str ] property readonly \u00b6 A list of input field names for this module. input_schemas : Mapping [ str , kiara . data . values . ValueSchema ] property readonly \u00b6 The input schema for this module. module_instance_doc : str property readonly \u00b6 Return documentation for this instance of the module. If not overwritten, will return this class' method doc() . module_instance_hash : int property readonly \u00b6 Return this modules 'module_hash'. If two module instances module_instance_hash values are the same, it is guaranteed that their process methods will return the same output, given the same inputs (except if that processing step uses randomness). It can also be assumed that the two instances have the same input and output fields, with the same schemas. Note This implementation is preliminary, since it's not yet 100% clear to me how much that will be needed, and in which situations. Also, module versioning needs to be implemented before this can work reliably. Also, for now it is assumed that a module configuration is not changed once set, this also might change in the future Returns: Type Description int this modules 'module_instance_hash' output_names : Iterable [ str ] property readonly \u00b6 A list of output field names for this module. output_schemas : Mapping [ str , kiara . data . values . ValueSchema ] property readonly \u00b6 The output schema for this module. parent_id : Optional [ str ] property readonly \u00b6 The id of the parent of this module (if part of a pipeline). create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/module.py @abstractmethod def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } \"\"\" create_instance ( module_type = None , module_config = None , kiara = None ) classmethod \u00b6 Create an instance of a kiara module. This class method is overloaded in a way that you can either provide the module_type argument, in which case the relevant sub-class will be queried from the kiara context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Parameters: Name Type Description Default module_type Optional[str] must be None if called on the KiaraModule base class, otherwise the module or operation id None module_config Optional[Mapping[str, Any]] the configuration of the module instance None kiara Optional[Kiara] the kiara context None Source code in kiara/module.py @classmethod def create_instance ( cls , module_type : typing . Optional [ str ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"KiaraModule\" : \"\"\"Create an instance of a *kiara* module. This class method is overloaded in a way that you can either provide the `module_type` argument, in which case the relevant sub-class will be queried from the *kiara* context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Arguments: module_type: must be None if called on the ``KiaraModule`` base class, otherwise the module or operation id module_config: the configuration of the module instance kiara: the *kiara* context \"\"\" if cls == KiaraModule : if not module_type : raise Exception ( \"This method must be either called on a subclass of KiaraModule, not KiaraModule itself, or it needs the 'module_type' argument specified.\" ) else : if module_type : raise Exception ( \"This method must be either called without the 'module_type' argument specified, or on a subclass of the KiaraModule class, but not both.\" ) if cls == KiaraModule : assert module_type is not None module_conf = ModuleConfig . create_module_config ( config = module_type , module_config = module_config , kiara = kiara ) else : module_conf = ModuleConfig . create_module_config ( config = cls , module_config = module_config , kiara = kiara ) return module_conf . create_module ( kiara = kiara ) create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/module.py @abstractmethod def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } \"\"\" get_config_value ( self , key ) \u00b6 Retrieve the value for a specific configuration option. Parameters: Name Type Description Default key str the config key required Returns: Type Description Any the value for the provided key Source code in kiara/module.py def get_config_value ( self , key : str ) -> typing . Any : \"\"\"Retrieve the value for a specific configuration option. Arguments: key: the config key Returns: the value for the provided key \"\"\" return self . config . get ( key ) get_type_metadata () classmethod \u00b6 Return all metadata associated with this module type. Source code in kiara/module.py @classmethod def get_type_metadata ( cls ) -> KiaraModuleTypeMetadata : \"\"\"Return all metadata associated with this module type.\"\"\" return KiaraModuleTypeMetadata . from_module_class ( cls ) is_pipeline () classmethod \u00b6 Check whether this module type is a pipeline, or not. Source code in kiara/module.py @classmethod def is_pipeline ( cls ) -> bool : \"\"\"Check whether this module type is a pipeline, or not.\"\"\" return False process_step ( self , inputs , outputs , job_log ) \u00b6 Kick off processing for a specific set of input/outputs. This method calls the implemented process method of the inheriting class, as well as wrapping input/output-data related functionality. Parameters: Name Type Description Default inputs ValueSet the input value set required outputs ValueSet the output value set required Source code in kiara/module.py def process_step ( self , inputs : ValueSet , outputs : ValueSet , job_log : JobLog ) -> None : \"\"\"Kick off processing for a specific set of input/outputs. This method calls the implemented [process][kiara.module.KiaraModule.process] method of the inheriting class, as well as wrapping input/output-data related functionality. Arguments: inputs: the input value set outputs: the output value set \"\"\" signature = inspect . signature ( self . process ) # type: ignore if \"job_log\" not in signature . parameters . keys (): try : self . process ( inputs = inputs , outputs = outputs ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e else : try : self . process ( inputs = inputs , outputs = outputs , job_log = job_log ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/module.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: \"\"\"Retrieve a collection of profiles (pre-set module configs) for this *kiara* module type. This is used to automatically create generally useful operations (incl. their ids). \"\"\" run ( self , _attach_lineage = True , ** inputs ) \u00b6 Execute the module with the provided inputs directly. Parameters: Name Type Description Default inputs Any a map of the input values (as described by the input schema {} Returns: Type Description ValueSet a map of the output values (as described by the output schema) Source code in kiara/module.py def run ( self , _attach_lineage : bool = True , ** inputs : typing . Any ) -> ValueSet : \"\"\"Execute the module with the provided inputs directly. Arguments: inputs: a map of the input values (as described by the input schema Returns: a map of the output values (as described by the output schema) \"\"\" resolved_inputs = self . create_full_inputs ( ** inputs ) # TODO: introduce a 'temp' value set implementation and use that here input_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . full_input_schemas , read_only = True , initial_values = resolved_inputs , title = f \"module_inputs_ { self . id } \" , ) if not input_value_set . items_are_valid (): raise Exception ( f \"Can't process module ' { self . _module_type_name } ': Inputs not valid.\" # type: ignore ) output_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = False , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore default_value = SpecialValue . NOT_SET , ) self . process ( inputs = input_value_set , outputs = output_value_set ) # type: ignore result_outputs : typing . MutableMapping [ str , Value ] = {} if _attach_lineage : input_infos = { k : v . get_info () for k , v in resolved_inputs . items ()} for field_name , output in output_value_set . items (): value_lineage = ValueLineage . from_module_and_inputs ( module = self , output_name = field_name , inputs = input_infos ) # value_lineage = None output_val = self . _kiara . data_registry . register_data ( value_data = output , value_lineage = value_lineage ) result_outputs [ field_name ] = output_val else : result_outputs = output_value_set result_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = True , initial_values = result_outputs , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore ) return result_set # result = output_value_set.get_all_value_objects() # return output_value_set # return ValueSetImpl(items=result, read_only=True) StepInputs \u00b6 Wrapper class to hold a set of inputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Parameters: Name Type Description Default inputs ValueSet the input values of a pipeline step required get_all_field_names ( self ) \u00b6 All field names included in this ValueSet. Source code in kiara/module.py def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _inputs . keys () StepOutputs \u00b6 Wrapper class to hold a set of outputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Internally, this class stores two sets of its values: the 'actual', up-to-date values, and the referenced (original) ones that were used when creating an object of this class. It's not a good idea to keep both synced all the time, because that could potentially involve unnecessary data transfer and I/O. Also, in some cases a developer might want to avoid events that could be triggered by a changed value. Both value sets can be synced manually using the 'sync()' method. Parameters: Name Type Description Default outputs ValueSet the output values of a pipeline step required get_all_field_names ( self ) \u00b6 All field names included in this ValueSet. Source code in kiara/module.py def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _outputs . get_all_field_names () sync ( self ) \u00b6 Sync this value sets 'shadow' values with the ones a user would retrieve. Source code in kiara/module.py def sync ( self ): \"\"\"Sync this value sets 'shadow' values with the ones a user would retrieve.\"\"\" self . _outputs . set_values ( ** self . _outputs_staging ) # type: ignore self . _outputs_staging . clear () # type: ignore","title":"\u279c\u2007module"},{"location":"api_reference/kiara.module/#kiaramodule","text":"","title":"kiara.module"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule","text":"The base class that every custom module in Kiara needs to inherit from. The core of every KiaraModule is a process method, which should be a 'pure', idempotent function that creates one or several output values from the given input(s), and its purpose is to transfor a set of inputs into a set of outputs. Every module can be configured. The module configuration schema can differ, but every one such configuration needs to subclass the ModuleTypeConfigSchema class and set as the value to the _config_cls attribute of the module class. This is useful, because it allows for some modules to serve a much larger variety of use-cases than non-configurable modules would be, which would mean more code duplication because of very simlilar, but slightly different module types. Each module class (type) has a unique -- within a kiara context -- module type id which can be accessed via the _module_type_id class attribute. Examples: A simple example would be an 'addition' module, with a and b configured as inputs, and z as the output field name. An implementing class would look something like this: TODO Parameters: Name Type Description Default id str the id for this module (needs to be unique within a pipeline) required parent_id Optional[str] the id of the parent, in case this module is part of a pipeline required module_config Any the configuation for this module required metadata Mapping[str, Any] metadata for this module (not implemented yet) required","title":"KiaraModule"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.config","text":"Retrieve the configuration object for this module. Returns: Type Description ~KIARA_CONFIG the module-class-specific config object","title":"config"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.full_id","text":"The full id for this module.","title":"full_id"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.id","text":"The id of this module. This is only unique within a pipeline.","title":"id"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.info","text":"Return an info wrapper class for this module.","title":"info"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.input_names","text":"A list of input field names for this module.","title":"input_names"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.input_schemas","text":"The input schema for this module.","title":"input_schemas"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.module_instance_doc","text":"Return documentation for this instance of the module. If not overwritten, will return this class' method doc() .","title":"module_instance_doc"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.module_instance_hash","text":"Return this modules 'module_hash'. If two module instances module_instance_hash values are the same, it is guaranteed that their process methods will return the same output, given the same inputs (except if that processing step uses randomness). It can also be assumed that the two instances have the same input and output fields, with the same schemas. Note This implementation is preliminary, since it's not yet 100% clear to me how much that will be needed, and in which situations. Also, module versioning needs to be implemented before this can work reliably. Also, for now it is assumed that a module configuration is not changed once set, this also might change in the future Returns: Type Description int this modules 'module_instance_hash'","title":"module_instance_hash"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.output_names","text":"A list of output field names for this module.","title":"output_names"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.output_schemas","text":"The output schema for this module.","title":"output_schemas"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.parent_id","text":"The id of the parent of this module (if part of a pipeline).","title":"parent_id"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/module.py @abstractmethod def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } \"\"\"","title":"create_input_schema()"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.create_instance","text":"Create an instance of a kiara module. This class method is overloaded in a way that you can either provide the module_type argument, in which case the relevant sub-class will be queried from the kiara context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Parameters: Name Type Description Default module_type Optional[str] must be None if called on the KiaraModule base class, otherwise the module or operation id None module_config Optional[Mapping[str, Any]] the configuration of the module instance None kiara Optional[Kiara] the kiara context None Source code in kiara/module.py @classmethod def create_instance ( cls , module_type : typing . Optional [ str ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"KiaraModule\" : \"\"\"Create an instance of a *kiara* module. This class method is overloaded in a way that you can either provide the `module_type` argument, in which case the relevant sub-class will be queried from the *kiara* context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Arguments: module_type: must be None if called on the ``KiaraModule`` base class, otherwise the module or operation id module_config: the configuration of the module instance kiara: the *kiara* context \"\"\" if cls == KiaraModule : if not module_type : raise Exception ( \"This method must be either called on a subclass of KiaraModule, not KiaraModule itself, or it needs the 'module_type' argument specified.\" ) else : if module_type : raise Exception ( \"This method must be either called without the 'module_type' argument specified, or on a subclass of the KiaraModule class, but not both.\" ) if cls == KiaraModule : assert module_type is not None module_conf = ModuleConfig . create_module_config ( config = module_type , module_config = module_config , kiara = kiara ) else : module_conf = ModuleConfig . create_module_config ( config = cls , module_config = module_config , kiara = kiara ) return module_conf . create_module ( kiara = kiara )","title":"create_instance()"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/module.py @abstractmethod def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } \"\"\"","title":"create_output_schema()"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.get_config_value","text":"Retrieve the value for a specific configuration option. Parameters: Name Type Description Default key str the config key required Returns: Type Description Any the value for the provided key Source code in kiara/module.py def get_config_value ( self , key : str ) -> typing . Any : \"\"\"Retrieve the value for a specific configuration option. Arguments: key: the config key Returns: the value for the provided key \"\"\" return self . config . get ( key )","title":"get_config_value()"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.get_type_metadata","text":"Return all metadata associated with this module type. Source code in kiara/module.py @classmethod def get_type_metadata ( cls ) -> KiaraModuleTypeMetadata : \"\"\"Return all metadata associated with this module type.\"\"\" return KiaraModuleTypeMetadata . from_module_class ( cls )","title":"get_type_metadata()"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.is_pipeline","text":"Check whether this module type is a pipeline, or not. Source code in kiara/module.py @classmethod def is_pipeline ( cls ) -> bool : \"\"\"Check whether this module type is a pipeline, or not.\"\"\" return False","title":"is_pipeline()"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.process_step","text":"Kick off processing for a specific set of input/outputs. This method calls the implemented process method of the inheriting class, as well as wrapping input/output-data related functionality. Parameters: Name Type Description Default inputs ValueSet the input value set required outputs ValueSet the output value set required Source code in kiara/module.py def process_step ( self , inputs : ValueSet , outputs : ValueSet , job_log : JobLog ) -> None : \"\"\"Kick off processing for a specific set of input/outputs. This method calls the implemented [process][kiara.module.KiaraModule.process] method of the inheriting class, as well as wrapping input/output-data related functionality. Arguments: inputs: the input value set outputs: the output value set \"\"\" signature = inspect . signature ( self . process ) # type: ignore if \"job_log\" not in signature . parameters . keys (): try : self . process ( inputs = inputs , outputs = outputs ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e else : try : self . process ( inputs = inputs , outputs = outputs , job_log = job_log ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e","title":"process_step()"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/module.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: \"\"\"Retrieve a collection of profiles (pre-set module configs) for this *kiara* module type. This is used to automatically create generally useful operations (incl. their ids). \"\"\"","title":"retrieve_module_profiles()"},{"location":"api_reference/kiara.module/#kiara.module.KiaraModule.run","text":"Execute the module with the provided inputs directly. Parameters: Name Type Description Default inputs Any a map of the input values (as described by the input schema {} Returns: Type Description ValueSet a map of the output values (as described by the output schema) Source code in kiara/module.py def run ( self , _attach_lineage : bool = True , ** inputs : typing . Any ) -> ValueSet : \"\"\"Execute the module with the provided inputs directly. Arguments: inputs: a map of the input values (as described by the input schema Returns: a map of the output values (as described by the output schema) \"\"\" resolved_inputs = self . create_full_inputs ( ** inputs ) # TODO: introduce a 'temp' value set implementation and use that here input_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . full_input_schemas , read_only = True , initial_values = resolved_inputs , title = f \"module_inputs_ { self . id } \" , ) if not input_value_set . items_are_valid (): raise Exception ( f \"Can't process module ' { self . _module_type_name } ': Inputs not valid.\" # type: ignore ) output_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = False , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore default_value = SpecialValue . NOT_SET , ) self . process ( inputs = input_value_set , outputs = output_value_set ) # type: ignore result_outputs : typing . MutableMapping [ str , Value ] = {} if _attach_lineage : input_infos = { k : v . get_info () for k , v in resolved_inputs . items ()} for field_name , output in output_value_set . items (): value_lineage = ValueLineage . from_module_and_inputs ( module = self , output_name = field_name , inputs = input_infos ) # value_lineage = None output_val = self . _kiara . data_registry . register_data ( value_data = output , value_lineage = value_lineage ) result_outputs [ field_name ] = output_val else : result_outputs = output_value_set result_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = True , initial_values = result_outputs , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore ) return result_set # result = output_value_set.get_all_value_objects() # return output_value_set # return ValueSetImpl(items=result, read_only=True)","title":"run()"},{"location":"api_reference/kiara.module/#kiara.module.StepInputs","text":"Wrapper class to hold a set of inputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Parameters: Name Type Description Default inputs ValueSet the input values of a pipeline step required","title":"StepInputs"},{"location":"api_reference/kiara.module/#kiara.module.StepInputs.get_all_field_names","text":"All field names included in this ValueSet. Source code in kiara/module.py def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _inputs . keys ()","title":"get_all_field_names()"},{"location":"api_reference/kiara.module/#kiara.module.StepOutputs","text":"Wrapper class to hold a set of outputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Internally, this class stores two sets of its values: the 'actual', up-to-date values, and the referenced (original) ones that were used when creating an object of this class. It's not a good idea to keep both synced all the time, because that could potentially involve unnecessary data transfer and I/O. Also, in some cases a developer might want to avoid events that could be triggered by a changed value. Both value sets can be synced manually using the 'sync()' method. Parameters: Name Type Description Default outputs ValueSet the output values of a pipeline step required","title":"StepOutputs"},{"location":"api_reference/kiara.module/#kiara.module.StepOutputs.get_all_field_names","text":"All field names included in this ValueSet. Source code in kiara/module.py def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _outputs . get_all_field_names ()","title":"get_all_field_names()"},{"location":"api_reference/kiara.module/#kiara.module.StepOutputs.sync","text":"Sync this value sets 'shadow' values with the ones a user would retrieve. Source code in kiara/module.py def sync ( self ): \"\"\"Sync this value sets 'shadow' values with the ones a user would retrieve.\"\"\" self . _outputs . set_values ( ** self . _outputs_staging ) # type: ignore self . _outputs_staging . clear () # type: ignore","title":"sync()"},{"location":"api_reference/kiara.module_config/","text":"kiara.module_config \u00b6 Module-related configuration models for the Kiara package. ModuleConfig pydantic-model \u00b6 A class to hold the type and configuration for a module instance. doc : DocumentationMetadataModel pydantic-field \u00b6 Documentation for this operation. module_config : Dict [ str , Any ] pydantic-field \u00b6 The configuration for the module. module_type : str pydantic-field required \u00b6 The module type. create_module ( self , kiara = None , module_id = None ) \u00b6 Create a module instance from this configuration. Source code in kiara/module_config.py def create_module ( self , kiara : typing . Optional [ \"Kiara\" ] = None , module_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance from this configuration.\"\"\" if module_id and not isinstance ( module_id , str ): raise TypeError ( f \"Invalid type, module_id must be a string, not: { type ( module_id ) } \" ) if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if self . _module is None : self . _module = kiara . create_module ( id = module_id , module_type = self . module_type , module_config = self . module_config , ) return self . _module create_renderable ( self , ** config ) \u00b6 Create a renderable for this module configuration. Source code in kiara/module_config.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a renderable for this module configuration.\"\"\" conf = Syntax ( self . json ( exclude_none = True , indent = 2 ), \"json\" , background_color = \"default\" , ) return conf create_renderable_from_module_instance_configs ( configs , ** render_config ) classmethod \u00b6 Convenience method to create a renderable for this module configuration, to be printed to terminal. Source code in kiara/module_config.py @classmethod def create_renderable_from_module_instance_configs ( cls , configs : typing . Mapping [ str , \"ModuleConfig\" ], ** render_config : typing . Any , ): \"\"\"Convenience method to create a renderable for this module configuration, to be printed to terminal.\"\"\" table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" , style = \"i\" , no_wrap = True ) table . add_column ( \"Description\" ) if not render_config . get ( \"omit_config\" , False ): table . add_column ( \"Configuration\" ) for id , config in configs . items (): if config . doc : doc = config . doc . description else : doc = DEFAULT_NO_DESC_VALUE row : typing . List [ RenderableType ] = [ id , doc ] if not render_config . get ( \"omit_config\" , False ): conf = config . json ( exclude = { \"doc\" }, indent = 2 ) row . append ( Syntax ( conf , \"json\" , background_color = \"default\" )) table . add_row ( * row ) return table ModuleTypeConfigSchema pydantic-model \u00b6 Base class that describes the configuration a KiaraModule class accepts. This is stored in the _config_cls class attribute in each KiaraModule class. There are two config options every KiaraModule supports: constants , and defaults Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default values that override the schema defaults, and those can be overwritten by users. If both a constant and a default value is set for an input field, an error is thrown. constants : Dict [ str , Any ] pydantic-field \u00b6 Value constants for this module. defaults : Dict [ str , Any ] pydantic-field \u00b6 Value defaults for this module. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/module_config.py def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/module_config.py def __hash__ ( self ): return hash ( self . config_hash ) get ( self , key ) \u00b6 Get the value for the specified configuation key. Source code in kiara/module_config.py def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) requires_config () classmethod \u00b6 Return whether this class can be used as-is, or requires configuration before an instance can be created. Source code in kiara/module_config.py @classmethod def requires_config ( cls ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : return True return False parse_and_create_module_config ( config , module_config = None , kiara = None ) \u00b6 Utility method to create a ModuleConfig object from different input types. Parameters: Name Type Description Default config Union[ModuleConfig, Mapping, str, Type[KiaraModule], PipelineConfig] the 'main' configuation required module_config Optional[Mapping[str, Any]] if the 'main' configuration value is an (unconfigured) module type, this argument can contain the configuration for the module instance None kiara Optional[Kiara] the kiara context None Source code in kiara/module_config.py def parse_and_create_module_config ( config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str , typing . Type [ \"KiaraModule\" ], \"PipelineConfig\" , ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleConfig\" : \"\"\"Utility method to create a `ModuleConfig` object from different input types. Arguments: config: the 'main' configuation module_config: if the 'main' configuration value is an (unconfigured) module type, this argument can contain the configuration for the module instance kiara: the *kiara* context \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if isinstance ( config , type ): from kiara.module import KiaraModule if issubclass ( config , KiaraModule ): # this basically makes sure the class is augmented with the '_module_type_id` attribute if not hasattr ( config , \"_module_type_id\" ): match = False for mod_name in kiara . available_module_types : mod_cls = kiara . get_module_class ( mod_name ) if mod_cls == config : match = True break if not match : raise Exception ( f \"Class ' { config } ' not a valid kiara module.\" ) config = config . _module_type_id # type: ignore else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for module configuration.\" ) if isinstance ( config , typing . Mapping ): if module_config : raise NotImplementedError () module_instance_config : ModuleConfig = ModuleConfig ( ** config ) elif isinstance ( config , str ): if config == \"pipeline\" : if not module_config : raise Exception ( \"Can't create module from 'pipeline' module type without further configuration.\" ) elif isinstance ( module_config , typing . Mapping ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = module_config ) else : from kiara.pipeline.config import PipelineConfig if isinstance ( config , PipelineConfig ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = config . dict () ) else : raise Exception ( f \"Can't create module config, invalid type for 'module_config' parameter: { type ( module_config ) } \" ) elif config in kiara . available_module_types : if module_config : module_instance_config = ModuleConfig ( module_type = config , module_config = module_config ) else : module_instance_config = ModuleConfig ( module_type = config ) elif config in kiara . operation_mgmt . profiles . keys (): module_instance_config = kiara . operation_mgmt . profiles [ config ] elif os . path . isfile ( os . path . expanduser ( config )): path = os . path . expanduser ( config ) module_config_data = get_data_from_file ( path ) if module_config : raise NotImplementedError () if not isinstance ( module_config_data , typing . Mapping ): raise Exception ( f \"Invalid module/pipeline config, must be a mapping type: { module_config_data } \" ) if \"steps\" in module_config_data . keys (): module_type = \"pipeline\" module_instance_config = ModuleConfig ( module_type = module_type , module_config = module_config_data ) elif \"module_type\" in module_config_data . keys (): module_instance_config = ModuleConfig ( ** module_config_data ) else : raise Exception ( f \"Can't create module config from string ' { config } '. Value must be path to a file, or one of: { ', ' . join ( kiara . available_module_types ) } \" ) elif isinstance ( config , ModuleConfig ): module_instance_config = config if module_config : raise NotImplementedError () else : from kiara.pipeline.config import PipelineConfig if isinstance ( config , PipelineConfig ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = config . dict () ) else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for module configuration.\" ) return module_instance_config","title":"\u279c\u2007module_config"},{"location":"api_reference/kiara.module_config/#kiaramodule_config","text":"Module-related configuration models for the Kiara package.","title":"kiara.module_config"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleConfig","text":"A class to hold the type and configuration for a module instance.","title":"ModuleConfig"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleConfig.doc","text":"Documentation for this operation.","title":"doc"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleConfig.module_config","text":"The configuration for the module.","title":"module_config"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleConfig.module_type","text":"The module type.","title":"module_type"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleConfig.create_module","text":"Create a module instance from this configuration. Source code in kiara/module_config.py def create_module ( self , kiara : typing . Optional [ \"Kiara\" ] = None , module_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance from this configuration.\"\"\" if module_id and not isinstance ( module_id , str ): raise TypeError ( f \"Invalid type, module_id must be a string, not: { type ( module_id ) } \" ) if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if self . _module is None : self . _module = kiara . create_module ( id = module_id , module_type = self . module_type , module_config = self . module_config , ) return self . _module","title":"create_module()"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleConfig.create_renderable","text":"Create a renderable for this module configuration. Source code in kiara/module_config.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a renderable for this module configuration.\"\"\" conf = Syntax ( self . json ( exclude_none = True , indent = 2 ), \"json\" , background_color = \"default\" , ) return conf","title":"create_renderable()"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleConfig.create_renderable_from_module_instance_configs","text":"Convenience method to create a renderable for this module configuration, to be printed to terminal. Source code in kiara/module_config.py @classmethod def create_renderable_from_module_instance_configs ( cls , configs : typing . Mapping [ str , \"ModuleConfig\" ], ** render_config : typing . Any , ): \"\"\"Convenience method to create a renderable for this module configuration, to be printed to terminal.\"\"\" table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" , style = \"i\" , no_wrap = True ) table . add_column ( \"Description\" ) if not render_config . get ( \"omit_config\" , False ): table . add_column ( \"Configuration\" ) for id , config in configs . items (): if config . doc : doc = config . doc . description else : doc = DEFAULT_NO_DESC_VALUE row : typing . List [ RenderableType ] = [ id , doc ] if not render_config . get ( \"omit_config\" , False ): conf = config . json ( exclude = { \"doc\" }, indent = 2 ) row . append ( Syntax ( conf , \"json\" , background_color = \"default\" )) table . add_row ( * row ) return table","title":"create_renderable_from_module_instance_configs()"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleTypeConfigSchema","text":"Base class that describes the configuration a KiaraModule class accepts. This is stored in the _config_cls class attribute in each KiaraModule class. There are two config options every KiaraModule supports: constants , and defaults Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default values that override the schema defaults, and those can be overwritten by users. If both a constant and a default value is set for an input field, an error is thrown.","title":"ModuleTypeConfigSchema"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleTypeConfigSchema.constants","text":"Value constants for this module.","title":"constants"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleTypeConfigSchema.defaults","text":"Value defaults for this module.","title":"defaults"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleTypeConfigSchema.__eq__","text":"Return self==value. Source code in kiara/module_config.py def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict ()","title":"__eq__()"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleTypeConfigSchema.__hash__","text":"Return hash(self). Source code in kiara/module_config.py def __hash__ ( self ): return hash ( self . config_hash )","title":"__hash__()"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleTypeConfigSchema.get","text":"Get the value for the specified configuation key. Source code in kiara/module_config.py def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key )","title":"get()"},{"location":"api_reference/kiara.module_config/#kiara.module_config.ModuleTypeConfigSchema.requires_config","text":"Return whether this class can be used as-is, or requires configuration before an instance can be created. Source code in kiara/module_config.py @classmethod def requires_config ( cls ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : return True return False","title":"requires_config()"},{"location":"api_reference/kiara.module_config/#kiara.module_config.parse_and_create_module_config","text":"Utility method to create a ModuleConfig object from different input types. Parameters: Name Type Description Default config Union[ModuleConfig, Mapping, str, Type[KiaraModule], PipelineConfig] the 'main' configuation required module_config Optional[Mapping[str, Any]] if the 'main' configuration value is an (unconfigured) module type, this argument can contain the configuration for the module instance None kiara Optional[Kiara] the kiara context None Source code in kiara/module_config.py def parse_and_create_module_config ( config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str , typing . Type [ \"KiaraModule\" ], \"PipelineConfig\" , ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleConfig\" : \"\"\"Utility method to create a `ModuleConfig` object from different input types. Arguments: config: the 'main' configuation module_config: if the 'main' configuration value is an (unconfigured) module type, this argument can contain the configuration for the module instance kiara: the *kiara* context \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if isinstance ( config , type ): from kiara.module import KiaraModule if issubclass ( config , KiaraModule ): # this basically makes sure the class is augmented with the '_module_type_id` attribute if not hasattr ( config , \"_module_type_id\" ): match = False for mod_name in kiara . available_module_types : mod_cls = kiara . get_module_class ( mod_name ) if mod_cls == config : match = True break if not match : raise Exception ( f \"Class ' { config } ' not a valid kiara module.\" ) config = config . _module_type_id # type: ignore else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for module configuration.\" ) if isinstance ( config , typing . Mapping ): if module_config : raise NotImplementedError () module_instance_config : ModuleConfig = ModuleConfig ( ** config ) elif isinstance ( config , str ): if config == \"pipeline\" : if not module_config : raise Exception ( \"Can't create module from 'pipeline' module type without further configuration.\" ) elif isinstance ( module_config , typing . Mapping ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = module_config ) else : from kiara.pipeline.config import PipelineConfig if isinstance ( config , PipelineConfig ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = config . dict () ) else : raise Exception ( f \"Can't create module config, invalid type for 'module_config' parameter: { type ( module_config ) } \" ) elif config in kiara . available_module_types : if module_config : module_instance_config = ModuleConfig ( module_type = config , module_config = module_config ) else : module_instance_config = ModuleConfig ( module_type = config ) elif config in kiara . operation_mgmt . profiles . keys (): module_instance_config = kiara . operation_mgmt . profiles [ config ] elif os . path . isfile ( os . path . expanduser ( config )): path = os . path . expanduser ( config ) module_config_data = get_data_from_file ( path ) if module_config : raise NotImplementedError () if not isinstance ( module_config_data , typing . Mapping ): raise Exception ( f \"Invalid module/pipeline config, must be a mapping type: { module_config_data } \" ) if \"steps\" in module_config_data . keys (): module_type = \"pipeline\" module_instance_config = ModuleConfig ( module_type = module_type , module_config = module_config_data ) elif \"module_type\" in module_config_data . keys (): module_instance_config = ModuleConfig ( ** module_config_data ) else : raise Exception ( f \"Can't create module config from string ' { config } '. Value must be path to a file, or one of: { ', ' . join ( kiara . available_module_types ) } \" ) elif isinstance ( config , ModuleConfig ): module_instance_config = config if module_config : raise NotImplementedError () else : from kiara.pipeline.config import PipelineConfig if isinstance ( config , PipelineConfig ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = config . dict () ) else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for module configuration.\" ) return module_instance_config","title":"parse_and_create_module_config()"},{"location":"api_reference/kiara.module_mgmt/","text":"kiara.module_mgmt \u00b6 Base module for code that handles the import and management of KiaraModule sub-classes.","title":"\u279c\u2007module_mgmt"},{"location":"api_reference/kiara.module_mgmt/#kiaramodule_mgmt","text":"Base module for code that handles the import and management of KiaraModule sub-classes.","title":"kiara.module_mgmt"},{"location":"api_reference/kiara.module_mgmt.merged/","text":"kiara.module_mgmt.merged \u00b6 MergedModuleManager \u00b6 available_module_types : List [ str ] property readonly \u00b6 Return the names of all available modules available_non_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. available_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. MergedModuleManagerConfig pydantic-model \u00b6 folders : List [ str ] pydantic-field \u00b6 A list of folders that contain pipeline descriptions.","title":"\u279c\u2007module_mgmt.merged"},{"location":"api_reference/kiara.module_mgmt.merged/#kiaramodule_mgmtmerged","text":"","title":"kiara.module_mgmt.merged"},{"location":"api_reference/kiara.module_mgmt.merged/#kiara.module_mgmt.merged.MergedModuleManager","text":"","title":"MergedModuleManager"},{"location":"api_reference/kiara.module_mgmt.merged/#kiara.module_mgmt.merged.MergedModuleManager.available_module_types","text":"Return the names of all available modules","title":"available_module_types"},{"location":"api_reference/kiara.module_mgmt.merged/#kiara.module_mgmt.merged.MergedModuleManager.available_non_pipeline_module_types","text":"Return the names of all available pipeline-type modules.","title":"available_non_pipeline_module_types"},{"location":"api_reference/kiara.module_mgmt.merged/#kiara.module_mgmt.merged.MergedModuleManager.available_pipeline_module_types","text":"Return the names of all available pipeline-type modules.","title":"available_pipeline_module_types"},{"location":"api_reference/kiara.module_mgmt.merged/#kiara.module_mgmt.merged.MergedModuleManagerConfig","text":"","title":"MergedModuleManagerConfig"},{"location":"api_reference/kiara.module_mgmt.merged/#kiara.module_mgmt.merged.MergedModuleManagerConfig.folders","text":"A list of folders that contain pipeline descriptions.","title":"folders"},{"location":"api_reference/kiara.module_mgmt.pipelines/","text":"kiara.module_mgmt.pipelines \u00b6 PipelineModuleManager \u00b6 add_pipelines_path ( self , namespace , path , base_module ) \u00b6 Add a pipeline description file or folder containing some to this manager. Parameters: Name Type Description Default namespace str the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted required path Union[str, pathlib.Path] the path to a pipeline description file, or folder which contains some required Returns: Type Description Iterable[str] a list of module type names that were added Source code in kiara/module_mgmt/pipelines.py def add_pipelines_path ( self , namespace : str , path : typing . Union [ str , Path ], base_module : typing . Optional [ str ], ) -> typing . Iterable [ str ]: \"\"\"Add a pipeline description file or folder containing some to this manager. Arguments: namespace: the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted path: the path to a pipeline description file, or folder which contains some Returns: a list of module type names that were added \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) elif isinstance ( path , typing . Iterable ): raise TypeError ( f \"Invalid type for path: { path } \" ) if not path . exists (): log . warning ( f \"Can't add pipeline path ' { path } ': path does not exist\" ) return [] elif path . is_dir (): files : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} for root , dirnames , filenames in os . walk ( path , topdown = True ): dirnames [:] = [ d for d in dirnames if d not in DEFAULT_EXCLUDE_DIRS ] for filename in [ f for f in filenames if os . path . isfile ( os . path . join ( root , f )) and any ( f . endswith ( ext ) for ext in VALID_PIPELINE_FILE_EXTENSIONS ) ]: try : full_path = os . path . join ( root , filename ) name , data = get_pipeline_details_from_path ( path = full_path , base_module = base_module ) data = check_doc_sidecar ( full_path , data ) rel_path = os . path . relpath ( os . path . dirname ( full_path ), path ) if not rel_path or rel_path == \".\" : ns_name = name else : _rel_path = rel_path . replace ( os . path . sep , \".\" ) ns_name = f \" { _rel_path } . { name } \" if ns_name in files . keys (): raise Exception ( f \"Duplicate workflow name in namespace ' { namespace } ': { ns_name } \" ) files [ ns_name ] = data except Exception as e : log . warning ( f \"Ignoring invalid pipeline file ' { full_path } ': { e } \" ) elif path . is_file (): name , data = get_pipeline_details_from_path ( path = path , base_module = base_module ) data = check_doc_sidecar ( path , data ) files = { name : data } result = {} for k , v in files . items (): if namespace . startswith ( \"_\" ): tokens = namespace . split ( \".\" ) if len ( tokens ) == 1 : _namespace = \"\" else : _namespace = \".\" . join ( tokens [ 1 :]) else : _namespace = namespace if not _namespace : full_name = k else : full_name = f \" { _namespace } . { k } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Duplicate workflow name: { name } \" ) result [ full_name ] = v self . _pipeline_descs . update ( result ) return result . keys () register_pipeline ( self , data , module_type_name = None , namespace = None ) \u00b6 Register a pipeline description to the pipeline pool. Parameters: Name Type Description Default data Union[str, pathlib.Path, Mapping[str, Any]] the pipeline data (a dict, or a path to a file) required module_type_name Optional[str] the type name this pipeline should be registered as None Source code in kiara/module_mgmt/pipelines.py def register_pipeline ( self , data : typing . Union [ str , Path , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , ) -> str : \"\"\"Register a pipeline description to the pipeline pool. Arguments: data: the pipeline data (a dict, or a path to a file) module_type_name: the type name this pipeline should be registered as \"\"\" # TODO: verify that there is no conflict with module_type_name if isinstance ( data , str ): data = Path ( os . path . expanduser ( data )) if isinstance ( data , Path ): _name , _data = get_pipeline_details_from_path ( data ) _data = check_doc_sidecar ( data , _data ) if module_type_name : _name = module_type_name elif isinstance ( data , typing . Mapping ): _data = dict ( data ) if module_type_name : _data [ MODULE_TYPE_NAME_KEY ] = module_type_name _name = _data . get ( MODULE_TYPE_NAME_KEY , None ) if not _name : raise Exception ( f \"Can't register pipeline, no module type name available: { data } \" ) _data = { \"data\" : _data , \"source\" : data , \"source_type\" : \"dict\" } else : raise Exception ( f \"Can't register pipeline, must be dict-like data, not { type ( data ) } \" ) if not namespace : full_name = _name else : full_name = f \" { namespace } . { _name } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Can't register pipeline: duplicate workflow name ' { _name } '\" ) self . _pipeline_descs [ full_name ] = _data return full_name PipelineModuleManagerConfig pydantic-model \u00b6 folders : List [ str ] pydantic-field \u00b6 A list of folders that contain pipeline descriptions.","title":"\u279c\u2007module_mgmt.pipelines"},{"location":"api_reference/kiara.module_mgmt.pipelines/#kiaramodule_mgmtpipelines","text":"","title":"kiara.module_mgmt.pipelines"},{"location":"api_reference/kiara.module_mgmt.pipelines/#kiara.module_mgmt.pipelines.PipelineModuleManager","text":"","title":"PipelineModuleManager"},{"location":"api_reference/kiara.module_mgmt.pipelines/#kiara.module_mgmt.pipelines.PipelineModuleManager.add_pipelines_path","text":"Add a pipeline description file or folder containing some to this manager. Parameters: Name Type Description Default namespace str the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted required path Union[str, pathlib.Path] the path to a pipeline description file, or folder which contains some required Returns: Type Description Iterable[str] a list of module type names that were added Source code in kiara/module_mgmt/pipelines.py def add_pipelines_path ( self , namespace : str , path : typing . Union [ str , Path ], base_module : typing . Optional [ str ], ) -> typing . Iterable [ str ]: \"\"\"Add a pipeline description file or folder containing some to this manager. Arguments: namespace: the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted path: the path to a pipeline description file, or folder which contains some Returns: a list of module type names that were added \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) elif isinstance ( path , typing . Iterable ): raise TypeError ( f \"Invalid type for path: { path } \" ) if not path . exists (): log . warning ( f \"Can't add pipeline path ' { path } ': path does not exist\" ) return [] elif path . is_dir (): files : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} for root , dirnames , filenames in os . walk ( path , topdown = True ): dirnames [:] = [ d for d in dirnames if d not in DEFAULT_EXCLUDE_DIRS ] for filename in [ f for f in filenames if os . path . isfile ( os . path . join ( root , f )) and any ( f . endswith ( ext ) for ext in VALID_PIPELINE_FILE_EXTENSIONS ) ]: try : full_path = os . path . join ( root , filename ) name , data = get_pipeline_details_from_path ( path = full_path , base_module = base_module ) data = check_doc_sidecar ( full_path , data ) rel_path = os . path . relpath ( os . path . dirname ( full_path ), path ) if not rel_path or rel_path == \".\" : ns_name = name else : _rel_path = rel_path . replace ( os . path . sep , \".\" ) ns_name = f \" { _rel_path } . { name } \" if ns_name in files . keys (): raise Exception ( f \"Duplicate workflow name in namespace ' { namespace } ': { ns_name } \" ) files [ ns_name ] = data except Exception as e : log . warning ( f \"Ignoring invalid pipeline file ' { full_path } ': { e } \" ) elif path . is_file (): name , data = get_pipeline_details_from_path ( path = path , base_module = base_module ) data = check_doc_sidecar ( path , data ) files = { name : data } result = {} for k , v in files . items (): if namespace . startswith ( \"_\" ): tokens = namespace . split ( \".\" ) if len ( tokens ) == 1 : _namespace = \"\" else : _namespace = \".\" . join ( tokens [ 1 :]) else : _namespace = namespace if not _namespace : full_name = k else : full_name = f \" { _namespace } . { k } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Duplicate workflow name: { name } \" ) result [ full_name ] = v self . _pipeline_descs . update ( result ) return result . keys ()","title":"add_pipelines_path()"},{"location":"api_reference/kiara.module_mgmt.pipelines/#kiara.module_mgmt.pipelines.PipelineModuleManager.register_pipeline","text":"Register a pipeline description to the pipeline pool. Parameters: Name Type Description Default data Union[str, pathlib.Path, Mapping[str, Any]] the pipeline data (a dict, or a path to a file) required module_type_name Optional[str] the type name this pipeline should be registered as None Source code in kiara/module_mgmt/pipelines.py def register_pipeline ( self , data : typing . Union [ str , Path , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , ) -> str : \"\"\"Register a pipeline description to the pipeline pool. Arguments: data: the pipeline data (a dict, or a path to a file) module_type_name: the type name this pipeline should be registered as \"\"\" # TODO: verify that there is no conflict with module_type_name if isinstance ( data , str ): data = Path ( os . path . expanduser ( data )) if isinstance ( data , Path ): _name , _data = get_pipeline_details_from_path ( data ) _data = check_doc_sidecar ( data , _data ) if module_type_name : _name = module_type_name elif isinstance ( data , typing . Mapping ): _data = dict ( data ) if module_type_name : _data [ MODULE_TYPE_NAME_KEY ] = module_type_name _name = _data . get ( MODULE_TYPE_NAME_KEY , None ) if not _name : raise Exception ( f \"Can't register pipeline, no module type name available: { data } \" ) _data = { \"data\" : _data , \"source\" : data , \"source_type\" : \"dict\" } else : raise Exception ( f \"Can't register pipeline, must be dict-like data, not { type ( data ) } \" ) if not namespace : full_name = _name else : full_name = f \" { namespace } . { _name } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Can't register pipeline: duplicate workflow name ' { _name } '\" ) self . _pipeline_descs [ full_name ] = _data return full_name","title":"register_pipeline()"},{"location":"api_reference/kiara.module_mgmt.pipelines/#kiara.module_mgmt.pipelines.PipelineModuleManagerConfig","text":"","title":"PipelineModuleManagerConfig"},{"location":"api_reference/kiara.module_mgmt.pipelines/#kiara.module_mgmt.pipelines.PipelineModuleManagerConfig.folders","text":"A list of folders that contain pipeline descriptions.","title":"folders"},{"location":"api_reference/kiara.module_mgmt.python_classes/","text":"kiara.module_mgmt.python_classes \u00b6 PythonModuleManagerConfig pydantic-model \u00b6 module_classes : Dict [ str , Type [ KiaraModule ]] pydantic-field required \u00b6 The module classes this manager should hold.","title":"\u279c\u2007module_mgmt.python_classes"},{"location":"api_reference/kiara.module_mgmt.python_classes/#kiaramodule_mgmtpython_classes","text":"","title":"kiara.module_mgmt.python_classes"},{"location":"api_reference/kiara.module_mgmt.python_classes/#kiara.module_mgmt.python_classes.PythonModuleManagerConfig","text":"","title":"PythonModuleManagerConfig"},{"location":"api_reference/kiara.module_mgmt.python_classes/#kiara.module_mgmt.python_classes.PythonModuleManagerConfig.module_classes","text":"The module classes this manager should hold.","title":"module_classes"},{"location":"api_reference/kiara.modules/","text":"kiara.modules \u00b6 Base module under which the 'official' KiaraModule implementations live.","title":"\u279c\u2007modules"},{"location":"api_reference/kiara.modules/#kiaramodules","text":"Base module under which the 'official' KiaraModule implementations live.","title":"kiara.modules"},{"location":"api_reference/kiara.modules.metadata/","text":"kiara.modules.metadata \u00b6 ExtractPythonClass \u00b6 Extract metadata about the Python type of a value.","title":"\u279c\u2007modules.metadata"},{"location":"api_reference/kiara.modules.metadata/#kiaramodulesmetadata","text":"","title":"kiara.modules.metadata"},{"location":"api_reference/kiara.modules.metadata/#kiara.modules.metadata.ExtractPythonClass","text":"Extract metadata about the Python type of a value.","title":"ExtractPythonClass"},{"location":"api_reference/kiara.modules.pipelines/","text":"kiara.modules.pipelines \u00b6 Base module that holds PipelineModule classes that are auto-generated from pipeline descriptions in the pipelines folder.","title":"\u279c\u2007modules.pipelines"},{"location":"api_reference/kiara.modules.pipelines/#kiaramodulespipelines","text":"Base module that holds PipelineModule classes that are auto-generated from pipeline descriptions in the pipelines folder.","title":"kiara.modules.pipelines"},{"location":"api_reference/kiara.modules.save_value/","text":"kiara.modules.save_value \u00b6 LoadValueModule \u00b6 Load a value from the kiara data store. create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/save_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ \"ValueSchema\" , typing . Mapping [ str , typing . Any ]] ]: return { \"value_id\" : { \"type\" : \"string\" , \"doc\" : \"The id or alias of the saved value you want to load.\" , } } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/save_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ \"ValueSchema\" , typing . Mapping [ str , typing . Any ]] ]: return { \"value_item\" : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : \"The loaded value.\" , } } SaveValueModule \u00b6 Save a value into the kiara data store. create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/save_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ \"ValueSchema\" , typing . Mapping [ str , typing . Any ]] ]: return { \"value_item\" : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : \"The value to save.\" , }, \"aliases\" : { \"type\" : \"list\" , \"doc\" : \"A list of aliases to link to the saved value id.\" , \"optional\" : True , }, } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/save_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ \"ValueSchema\" , typing . Mapping [ str , typing . Any ]] ]: return { \"value_id\" : { \"type\" : \"string\" , \"doc\" : f \"The id of the saved { self . get_config_value ( 'value_type' ) } data-item.\" , } } SaveValueModuleConfig pydantic-model \u00b6 value_type : str pydantic-field required \u00b6 The type of the value to save.","title":"\u279c\u2007modules.save_value"},{"location":"api_reference/kiara.modules.save_value/#kiaramodulessave_value","text":"","title":"kiara.modules.save_value"},{"location":"api_reference/kiara.modules.save_value/#kiara.modules.save_value.LoadValueModule","text":"Load a value from the kiara data store.","title":"LoadValueModule"},{"location":"api_reference/kiara.modules.save_value/#kiara.modules.save_value.LoadValueModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/save_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ \"ValueSchema\" , typing . Mapping [ str , typing . Any ]] ]: return { \"value_id\" : { \"type\" : \"string\" , \"doc\" : \"The id or alias of the saved value you want to load.\" , } }","title":"create_input_schema()"},{"location":"api_reference/kiara.modules.save_value/#kiara.modules.save_value.LoadValueModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/save_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ \"ValueSchema\" , typing . Mapping [ str , typing . Any ]] ]: return { \"value_item\" : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : \"The loaded value.\" , } }","title":"create_output_schema()"},{"location":"api_reference/kiara.modules.save_value/#kiara.modules.save_value.SaveValueModule","text":"Save a value into the kiara data store.","title":"SaveValueModule"},{"location":"api_reference/kiara.modules.save_value/#kiara.modules.save_value.SaveValueModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/save_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ \"ValueSchema\" , typing . Mapping [ str , typing . Any ]] ]: return { \"value_item\" : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : \"The value to save.\" , }, \"aliases\" : { \"type\" : \"list\" , \"doc\" : \"A list of aliases to link to the saved value id.\" , \"optional\" : True , }, }","title":"create_input_schema()"},{"location":"api_reference/kiara.modules.save_value/#kiara.modules.save_value.SaveValueModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/save_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ \"ValueSchema\" , typing . Mapping [ str , typing . Any ]] ]: return { \"value_id\" : { \"type\" : \"string\" , \"doc\" : f \"The id of the saved { self . get_config_value ( 'value_type' ) } data-item.\" , } }","title":"create_output_schema()"},{"location":"api_reference/kiara.modules.save_value/#kiara.modules.save_value.SaveValueModuleConfig","text":"","title":"SaveValueModuleConfig"},{"location":"api_reference/kiara.modules.save_value/#kiara.modules.save_value.SaveValueModuleConfig.value_type","text":"The type of the value to save.","title":"value_type"},{"location":"api_reference/kiara.modules.type_conversion/","text":"kiara.modules.type_conversion \u00b6 OldTypeConversionModule \u00b6 create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/type_conversion.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"source_value\" : { \"type\" : self . source_type , \"doc\" : f \"A value of type ' { self . source_type } '.\" , }, \"config\" : { \"type\" : \"dict\" , \"doc\" : \"The configuration for the transformation.\" , \"optional\" : True , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/type_conversion.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"target_value\" : { \"type\" : self . target_type , \"doc\" : f \"A value of type ' { self . target_type } '.\" , } } return outputs TypeConversionModuleConfig pydantic-model \u00b6 source_type : str pydantic-field required \u00b6 The source type. target_type : str pydantic-field required \u00b6 The target type.","title":"\u279c\u2007modules.type_conversion"},{"location":"api_reference/kiara.modules.type_conversion/#kiaramodulestype_conversion","text":"","title":"kiara.modules.type_conversion"},{"location":"api_reference/kiara.modules.type_conversion/#kiara.modules.type_conversion.OldTypeConversionModule","text":"","title":"OldTypeConversionModule"},{"location":"api_reference/kiara.modules.type_conversion/#kiara.modules.type_conversion.OldTypeConversionModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/type_conversion.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"source_value\" : { \"type\" : self . source_type , \"doc\" : f \"A value of type ' { self . source_type } '.\" , }, \"config\" : { \"type\" : \"dict\" , \"doc\" : \"The configuration for the transformation.\" , \"optional\" : True , }, } return inputs","title":"create_input_schema()"},{"location":"api_reference/kiara.modules.type_conversion/#kiara.modules.type_conversion.OldTypeConversionModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/type_conversion.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"target_value\" : { \"type\" : self . target_type , \"doc\" : f \"A value of type ' { self . target_type } '.\" , } } return outputs","title":"create_output_schema()"},{"location":"api_reference/kiara.modules.type_conversion/#kiara.modules.type_conversion.TypeConversionModuleConfig","text":"","title":"TypeConversionModuleConfig"},{"location":"api_reference/kiara.modules.type_conversion/#kiara.modules.type_conversion.TypeConversionModuleConfig.source_type","text":"The source type.","title":"source_type"},{"location":"api_reference/kiara.modules.type_conversion/#kiara.modules.type_conversion.TypeConversionModuleConfig.target_type","text":"The target type.","title":"target_type"},{"location":"api_reference/kiara.operations.calculate_hash/","text":"kiara.operations.calculate_hash \u00b6 CalculateHashOperationType \u00b6 Calculate a hash for a dataset. CalculateValueHashesConfig pydantic-model \u00b6 hash_type : str pydantic-field required \u00b6 The hash type. value_type : str pydantic-field required \u00b6 The type of the value to calculate the hash for. CalculateValueHashModule \u00b6 Calculate the hash of a value. create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/calculate_hash.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"value_item\" : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , } } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/calculate_hash.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"hash\" : { \"type\" : \"string\" , \"doc\" : \"The hash string.\" }} retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/calculate_hash.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type in kiara . type_mgmt . value_types . items (): hash_types = value_type . get_supported_hash_types () for ht in hash_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"hash_type\" : ht }, \"doc\" : f \"Calculate ' { ht } ' hash for value type ' { value_type_name } '.\" , } all_metadata_profiles [ f \" { value_type_name } .calculate_hash. { ht } \" ] = op_config return all_metadata_profiles","title":"\u279c\u2007operations.calculate_hash"},{"location":"api_reference/kiara.operations.calculate_hash/#kiaraoperationscalculate_hash","text":"","title":"kiara.operations.calculate_hash"},{"location":"api_reference/kiara.operations.calculate_hash/#kiara.operations.calculate_hash.CalculateHashOperationType","text":"Calculate a hash for a dataset.","title":"CalculateHashOperationType"},{"location":"api_reference/kiara.operations.calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashesConfig","text":"","title":"CalculateValueHashesConfig"},{"location":"api_reference/kiara.operations.calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashesConfig.hash_type","text":"The hash type.","title":"hash_type"},{"location":"api_reference/kiara.operations.calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashesConfig.value_type","text":"The type of the value to calculate the hash for.","title":"value_type"},{"location":"api_reference/kiara.operations.calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashModule","text":"Calculate the hash of a value.","title":"CalculateValueHashModule"},{"location":"api_reference/kiara.operations.calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/calculate_hash.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"value_item\" : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , } }","title":"create_input_schema()"},{"location":"api_reference/kiara.operations.calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/calculate_hash.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"hash\" : { \"type\" : \"string\" , \"doc\" : \"The hash string.\" }}","title":"create_output_schema()"},{"location":"api_reference/kiara.operations.calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/calculate_hash.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type in kiara . type_mgmt . value_types . items (): hash_types = value_type . get_supported_hash_types () for ht in hash_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"hash_type\" : ht }, \"doc\" : f \"Calculate ' { ht } ' hash for value type ' { value_type_name } '.\" , } all_metadata_profiles [ f \" { value_type_name } .calculate_hash. { ht } \" ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"api_reference/kiara.operations.data_import/","text":"kiara.operations.data_import \u00b6 DataImportModule \u00b6 create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_import.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Dict [ str , typing . Any ] = { \"source\" : { \"type\" : self . get_config_value ( \"source_type\" ), \"doc\" : f \"A { self . get_config_value ( 'source_profile' ) } ' { self . get_config_value ( 'source_type' ) } ' value.\" , }, } allow_save = self . get_config_value ( \"allow_save_input\" ) save_default = self . get_config_value ( \"save_default\" ) if allow_save : inputs [ \"save\" ] = { \"type\" : \"boolean\" , \"doc\" : \"Whether to save the imported value, or not.\" , \"default\" : save_default , } allow_aliases : typing . Optional [ bool ] = self . get_config_value ( \"allow_aliases_input\" ) if allow_aliases is None : allow_aliases = allow_save if allow_aliases and not allow_save and not save_default : raise Exception ( \"Invalid module configuration: allowing aliases input does not make sense if save is disabled.\" ) if allow_aliases : default_aliases = self . get_config_value ( \"aliases_default\" ) inputs [ \"aliases\" ] = { \"type\" : \"list\" , \"doc\" : \"A list of aliases to use when storing the value (only applicable if 'save' is set).\" , \"default\" : default_aliases , } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_import.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"value_item\" : { \"type\" : self . get_target_value_type (), \"doc\" : f \"The imported { self . get_target_value_type () } value.\" , }, } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/data_import.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_target_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data import operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"import_from__\" ): continue tokens = attr [ 13 :] . rsplit ( \"__\" , maxsplit = 1 ) if len ( tokens ) != 2 : log_message ( f \"Can't determine source name and type from string in module { cls . _module_type_id } , ignoring method: { attr } \" # type: ignore ) source_profile , source_type = tokens op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_profile\" : source_profile , \"source_type\" : source_type , }, \"doc\" : f \"Import data of type ' { sup_type } ' from a { source_profile } { source_type } and save it to the kiara data store.\" , } all_metadata_profiles [ f \" { sup_type } .import_from. { source_profile } . { source_type } \" ] = op_config return all_metadata_profiles DataImportModuleConfig pydantic-model \u00b6 aliases_default : List [ str ] pydantic-field \u00b6 Default value for aliases. allow_aliases_input : bool pydantic-field \u00b6 Allow the user to choose aliases for the saved value. allow_save_input : bool pydantic-field \u00b6 Allow the user to choose whether to save the imported item or not. save_default : bool pydantic-field \u00b6 The default of the 'save' input if not specified by the user. source_profile : str pydantic-field required \u00b6 The name of the source profile. Used to distinguish different input categories for the same input type. source_type : str pydantic-field required \u00b6 The type of the source to import from. FileBundleImportModule \u00b6 Import a file, optionally saving it to the data store. FileBundleImportOperationType \u00b6 Save a value into a data store. get_import_operations ( self ) \u00b6 Return all available import operataions for a value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. Source code in kiara/operations/data_import.py def get_import_operations ( self ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operataions for a value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , Operation ]] = {} for op_config in self . operation_configs . values (): source_type = op_config . module_config [ \"source_type\" ] source_profile = op_config . module_config [ \"source_profile\" ] result . setdefault ( source_type , {})[ source_profile ] = op_config return result FileImportModule \u00b6 Import a file, optionally saving it to the data store. FileImportOperationType \u00b6 Save a value into a data store. get_import_operations ( self ) \u00b6 Return all available import operataions for a value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. Source code in kiara/operations/data_import.py def get_import_operations ( self ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operataions for a value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , Operation ]] = {} for op_config in self . operation_configs . values (): source_type = op_config . module_config [ \"source_type\" ] source_profile = op_config . module_config [ \"source_profile\" ] result . setdefault ( source_type , {})[ source_profile ] = op_config return result","title":"\u279c\u2007operations.data_import"},{"location":"api_reference/kiara.operations.data_import/#kiaraoperationsdata_import","text":"","title":"kiara.operations.data_import"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModule","text":"","title":"DataImportModule"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_import.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Dict [ str , typing . Any ] = { \"source\" : { \"type\" : self . get_config_value ( \"source_type\" ), \"doc\" : f \"A { self . get_config_value ( 'source_profile' ) } ' { self . get_config_value ( 'source_type' ) } ' value.\" , }, } allow_save = self . get_config_value ( \"allow_save_input\" ) save_default = self . get_config_value ( \"save_default\" ) if allow_save : inputs [ \"save\" ] = { \"type\" : \"boolean\" , \"doc\" : \"Whether to save the imported value, or not.\" , \"default\" : save_default , } allow_aliases : typing . Optional [ bool ] = self . get_config_value ( \"allow_aliases_input\" ) if allow_aliases is None : allow_aliases = allow_save if allow_aliases and not allow_save and not save_default : raise Exception ( \"Invalid module configuration: allowing aliases input does not make sense if save is disabled.\" ) if allow_aliases : default_aliases = self . get_config_value ( \"aliases_default\" ) inputs [ \"aliases\" ] = { \"type\" : \"list\" , \"doc\" : \"A list of aliases to use when storing the value (only applicable if 'save' is set).\" , \"default\" : default_aliases , } return inputs","title":"create_input_schema()"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_import.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"value_item\" : { \"type\" : self . get_target_value_type (), \"doc\" : f \"The imported { self . get_target_value_type () } value.\" , }, } return outputs","title":"create_output_schema()"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/data_import.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_target_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data import operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"import_from__\" ): continue tokens = attr [ 13 :] . rsplit ( \"__\" , maxsplit = 1 ) if len ( tokens ) != 2 : log_message ( f \"Can't determine source name and type from string in module { cls . _module_type_id } , ignoring method: { attr } \" # type: ignore ) source_profile , source_type = tokens op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_profile\" : source_profile , \"source_type\" : source_type , }, \"doc\" : f \"Import data of type ' { sup_type } ' from a { source_profile } { source_type } and save it to the kiara data store.\" , } all_metadata_profiles [ f \" { sup_type } .import_from. { source_profile } . { source_type } \" ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModuleConfig","text":"","title":"DataImportModuleConfig"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModuleConfig.aliases_default","text":"Default value for aliases.","title":"aliases_default"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModuleConfig.allow_aliases_input","text":"Allow the user to choose aliases for the saved value.","title":"allow_aliases_input"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModuleConfig.allow_save_input","text":"Allow the user to choose whether to save the imported item or not.","title":"allow_save_input"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModuleConfig.save_default","text":"The default of the 'save' input if not specified by the user.","title":"save_default"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModuleConfig.source_profile","text":"The name of the source profile. Used to distinguish different input categories for the same input type.","title":"source_profile"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModuleConfig.source_type","text":"The type of the source to import from.","title":"source_type"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.FileBundleImportModule","text":"Import a file, optionally saving it to the data store.","title":"FileBundleImportModule"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.FileBundleImportOperationType","text":"Save a value into a data store.","title":"FileBundleImportOperationType"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.FileBundleImportOperationType.get_import_operations","text":"Return all available import operataions for a value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. Source code in kiara/operations/data_import.py def get_import_operations ( self ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operataions for a value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , Operation ]] = {} for op_config in self . operation_configs . values (): source_type = op_config . module_config [ \"source_type\" ] source_profile = op_config . module_config [ \"source_profile\" ] result . setdefault ( source_type , {})[ source_profile ] = op_config return result","title":"get_import_operations()"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.FileImportModule","text":"Import a file, optionally saving it to the data store.","title":"FileImportModule"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.FileImportOperationType","text":"Save a value into a data store.","title":"FileImportOperationType"},{"location":"api_reference/kiara.operations.data_import/#kiara.operations.data_import.FileImportOperationType.get_import_operations","text":"Return all available import operataions for a value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. Source code in kiara/operations/data_import.py def get_import_operations ( self ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operataions for a value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , Operation ]] = {} for op_config in self . operation_configs . values (): source_type = op_config . module_config [ \"source_type\" ] source_profile = op_config . module_config [ \"source_profile\" ] result . setdefault ( source_type , {})[ source_profile ] = op_config return result","title":"get_import_operations()"},{"location":"api_reference/kiara.operations.extract_metadata/","text":"kiara.operations.extract_metadata \u00b6 ExtractMetadataModule \u00b6 Base class to use when writing a module to extract metadata from a file. It's possible to use any arbitrary kiara module for this purpose, but sub-classing this makes it easier. create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/extract_metadata.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs = { \"value_item\" : { \"type\" : self . value_type , \"doc\" : f \"A value of type ' { self . value_type } '\" , \"optional\" : False , } } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/extract_metadata.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"metadata_item\" : { \"type\" : \"dict\" , \"doc\" : \"The metadata for the provided value.\" , }, \"metadata_item_schema\" : { \"type\" : \"string\" , \"doc\" : \"The (json) schema for the metadata.\" , }, } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/extract_metadata.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_types : typing . Iterable = cls . get_supported_value_types () if \"*\" in value_types : value_types = kiara . type_mgmt . value_type_names metadata_key = cls . get_metadata_key () for value_type in value_types : if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring metadata-extract operation (metadata key: { metadata_key } ) for type ' { value_type } ': type not available\" ) continue op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type }, \"doc\" : f \"Extract ' { metadata_key } ' for value of type ' { value_type } '.\" , } all_metadata_profiles [ f \" { value_type } .extract_metadata. { metadata_key } \" ] = op_config return all_metadata_profiles ExtractMetadataOperationType \u00b6 Extract metadata from a dataset. MetadataModuleConfig pydantic-model \u00b6 value_type : str pydantic-field required \u00b6 The data type this module will be used for.","title":"\u279c\u2007operations.extract_metadata"},{"location":"api_reference/kiara.operations.extract_metadata/#kiaraoperationsextract_metadata","text":"","title":"kiara.operations.extract_metadata"},{"location":"api_reference/kiara.operations.extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataModule","text":"Base class to use when writing a module to extract metadata from a file. It's possible to use any arbitrary kiara module for this purpose, but sub-classing this makes it easier.","title":"ExtractMetadataModule"},{"location":"api_reference/kiara.operations.extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/extract_metadata.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs = { \"value_item\" : { \"type\" : self . value_type , \"doc\" : f \"A value of type ' { self . value_type } '\" , \"optional\" : False , } } return inputs","title":"create_input_schema()"},{"location":"api_reference/kiara.operations.extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/extract_metadata.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"metadata_item\" : { \"type\" : \"dict\" , \"doc\" : \"The metadata for the provided value.\" , }, \"metadata_item_schema\" : { \"type\" : \"string\" , \"doc\" : \"The (json) schema for the metadata.\" , }, } return outputs","title":"create_output_schema()"},{"location":"api_reference/kiara.operations.extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/extract_metadata.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_types : typing . Iterable = cls . get_supported_value_types () if \"*\" in value_types : value_types = kiara . type_mgmt . value_type_names metadata_key = cls . get_metadata_key () for value_type in value_types : if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring metadata-extract operation (metadata key: { metadata_key } ) for type ' { value_type } ': type not available\" ) continue op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type }, \"doc\" : f \"Extract ' { metadata_key } ' for value of type ' { value_type } '.\" , } all_metadata_profiles [ f \" { value_type } .extract_metadata. { metadata_key } \" ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"api_reference/kiara.operations.extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataOperationType","text":"Extract metadata from a dataset.","title":"ExtractMetadataOperationType"},{"location":"api_reference/kiara.operations.extract_metadata/#kiara.operations.extract_metadata.MetadataModuleConfig","text":"","title":"MetadataModuleConfig"},{"location":"api_reference/kiara.operations.extract_metadata/#kiara.operations.extract_metadata.MetadataModuleConfig.value_type","text":"The data type this module will be used for.","title":"value_type"},{"location":"api_reference/kiara.operations/","text":"kiara.operations \u00b6 Operation pydantic-model \u00b6 id : str pydantic-field required \u00b6 The operation id. create_renderable ( self , ** config ) \u00b6 Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code Source code in kiara/operations/__init__.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code \"\"\" include_full_doc = config . get ( \"include_full_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . doc : if include_full_doc : table . add_row ( \"Documentation\" , self . doc . full_doc ) else : table . add_row ( \"Description\" , self . doc . description ) module_type_md = self . module . get_type_metadata () table . add_row ( \"Module type\" , self . module_type ) conf = Syntax ( json . dumps ( self . module_config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Module config\" , conf ) constants = self . module_config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . module . input_schemas , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . module . output_schemas , ) table . add_row ( \"Outputs\" , outputs_table ) m_md_o = module_type_md . origin . create_renderable () m_md_c = module_type_md . context . create_renderable () m_md = RenderGroup ( m_md_o , m_md_c ) table . add_row ( \"Module metadata\" , m_md ) if config . get ( \"include_src\" , False ): table . add_row ( \"Source code\" , module_type_md . process_src ) return table","title":"\u279c\u2007operations"},{"location":"api_reference/kiara.operations/#kiaraoperations","text":"","title":"kiara.operations"},{"location":"api_reference/kiara.operations/#kiara.operations.__init__.Operation","text":"","title":"Operation"},{"location":"api_reference/kiara.operations/#kiara.operations.__init__.Operation.id","text":"The operation id.","title":"id"},{"location":"api_reference/kiara.operations/#kiara.operations.__init__.Operation.create_renderable","text":"Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code Source code in kiara/operations/__init__.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code \"\"\" include_full_doc = config . get ( \"include_full_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . doc : if include_full_doc : table . add_row ( \"Documentation\" , self . doc . full_doc ) else : table . add_row ( \"Description\" , self . doc . description ) module_type_md = self . module . get_type_metadata () table . add_row ( \"Module type\" , self . module_type ) conf = Syntax ( json . dumps ( self . module_config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Module config\" , conf ) constants = self . module_config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . module . input_schemas , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . module . output_schemas , ) table . add_row ( \"Outputs\" , outputs_table ) m_md_o = module_type_md . origin . create_renderable () m_md_c = module_type_md . context . create_renderable () m_md = RenderGroup ( m_md_o , m_md_c ) table . add_row ( \"Module metadata\" , m_md ) if config . get ( \"include_src\" , False ): table . add_row ( \"Source code\" , module_type_md . process_src ) return table","title":"create_renderable()"},{"location":"api_reference/kiara.operations.merge_values/","text":"kiara.operations.merge_values \u00b6 ValueMergeModule \u00b6 create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/merge_values.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_schema_dicts : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . get_config_value ( \"input_schemas\" ) if not input_schema_dicts : raise Exception ( \"No input schemas provided.\" ) input_schemas : typing . Dict [ str , ValueSchema ] = {} for k , v in input_schema_dicts . items (): input_schemas [ k ] = ValueSchema ( ** v ) return input_schemas create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/merge_values.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"merged_value\" : { \"type\" : self . get_config_value ( \"output_type\" )}} ValueMergeModuleConfig pydantic-model \u00b6 input_schemas : Dict [ str , Mapping [ str , Any ]] pydantic-field required \u00b6 The schemas for all of the expected inputs. output_type : str pydantic-field required \u00b6 The result type of the merged value.","title":"\u279c\u2007operations.merge_values"},{"location":"api_reference/kiara.operations.merge_values/#kiaraoperationsmerge_values","text":"","title":"kiara.operations.merge_values"},{"location":"api_reference/kiara.operations.merge_values/#kiara.operations.merge_values.ValueMergeModule","text":"","title":"ValueMergeModule"},{"location":"api_reference/kiara.operations.merge_values/#kiara.operations.merge_values.ValueMergeModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/merge_values.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_schema_dicts : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . get_config_value ( \"input_schemas\" ) if not input_schema_dicts : raise Exception ( \"No input schemas provided.\" ) input_schemas : typing . Dict [ str , ValueSchema ] = {} for k , v in input_schema_dicts . items (): input_schemas [ k ] = ValueSchema ( ** v ) return input_schemas","title":"create_input_schema()"},{"location":"api_reference/kiara.operations.merge_values/#kiara.operations.merge_values.ValueMergeModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/merge_values.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"merged_value\" : { \"type\" : self . get_config_value ( \"output_type\" )}}","title":"create_output_schema()"},{"location":"api_reference/kiara.operations.merge_values/#kiara.operations.merge_values.ValueMergeModuleConfig","text":"","title":"ValueMergeModuleConfig"},{"location":"api_reference/kiara.operations.merge_values/#kiara.operations.merge_values.ValueMergeModuleConfig.input_schemas","text":"The schemas for all of the expected inputs.","title":"input_schemas"},{"location":"api_reference/kiara.operations.merge_values/#kiara.operations.merge_values.ValueMergeModuleConfig.output_type","text":"The result type of the merged value.","title":"output_type"},{"location":"api_reference/kiara.operations.pretty_print/","text":"kiara.operations.pretty_print \u00b6 PrettyPrintModuleConfig pydantic-model \u00b6 target_type : str pydantic-field \u00b6 The target to print the value to. value_type : str pydantic-field required \u00b6 The type of the value to save. PrettyPrintValueModule \u00b6 create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/pretty_print.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"value_item\" : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , }, \"print_config\" : { \"type\" : \"dict\" , \"doc\" : \"Optional print configuration.\" , \"optional\" : True , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/pretty_print.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"printed\" : { \"type\" : self . get_config_value ( \"target_type\" ), \"doc\" : f \"The printed value as ' { self . get_config_value ( 'target_type' ) } '.\" , } } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/pretty_print.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type_cls in kiara . type_mgmt . value_types . items (): for attr in dir ( value_type_cls ): if not attr . startswith ( \"pretty_print_as_\" ): continue target_type = attr [ 16 :] if target_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Pretty print target type ' { target_type } ' for source type ' { value_type_name } ' not valid, ignoring.\" ) continue op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"target_type\" : target_type , }, \"doc\" : f \"Pretty print a value of type ' { value_type_name } ' as ' { target_type } '.\" , } all_metadata_profiles [ f \" { value_type_name } .pretty_print_as. { target_type } \" ] = op_config return all_metadata_profiles","title":"\u279c\u2007operations.pretty_print"},{"location":"api_reference/kiara.operations.pretty_print/#kiaraoperationspretty_print","text":"","title":"kiara.operations.pretty_print"},{"location":"api_reference/kiara.operations.pretty_print/#kiara.operations.pretty_print.PrettyPrintModuleConfig","text":"","title":"PrettyPrintModuleConfig"},{"location":"api_reference/kiara.operations.pretty_print/#kiara.operations.pretty_print.PrettyPrintModuleConfig.target_type","text":"The target to print the value to.","title":"target_type"},{"location":"api_reference/kiara.operations.pretty_print/#kiara.operations.pretty_print.PrettyPrintModuleConfig.value_type","text":"The type of the value to save.","title":"value_type"},{"location":"api_reference/kiara.operations.pretty_print/#kiara.operations.pretty_print.PrettyPrintValueModule","text":"","title":"PrettyPrintValueModule"},{"location":"api_reference/kiara.operations.pretty_print/#kiara.operations.pretty_print.PrettyPrintValueModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/pretty_print.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"value_item\" : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , }, \"print_config\" : { \"type\" : \"dict\" , \"doc\" : \"Optional print configuration.\" , \"optional\" : True , }, } return inputs","title":"create_input_schema()"},{"location":"api_reference/kiara.operations.pretty_print/#kiara.operations.pretty_print.PrettyPrintValueModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/pretty_print.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"printed\" : { \"type\" : self . get_config_value ( \"target_type\" ), \"doc\" : f \"The printed value as ' { self . get_config_value ( 'target_type' ) } '.\" , } } return outputs","title":"create_output_schema()"},{"location":"api_reference/kiara.operations.pretty_print/#kiara.operations.pretty_print.PrettyPrintValueModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/pretty_print.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type_cls in kiara . type_mgmt . value_types . items (): for attr in dir ( value_type_cls ): if not attr . startswith ( \"pretty_print_as_\" ): continue target_type = attr [ 16 :] if target_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Pretty print target type ' { target_type } ' for source type ' { value_type_name } ' not valid, ignoring.\" ) continue op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"target_type\" : target_type , }, \"doc\" : f \"Pretty print a value of type ' { value_type_name } ' as ' { target_type } '.\" , } all_metadata_profiles [ f \" { value_type_name } .pretty_print_as. { target_type } \" ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"api_reference/kiara.operations.save_value/","text":"kiara.operations.save_value \u00b6 SaveOperationType \u00b6 Save a value into a data store. SaveValueModuleConfig pydantic-model \u00b6 value_type : str pydantic-field required \u00b6 The type of the value to save. SaveValueTypeModule \u00b6 Save a specific value type. This is used internally. create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/save_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"value_id\" : { \"type\" : \"string\" , \"doc\" : \"The id to use when saving the value.\" , }, \"value_item\" : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The base path to save the value to.\" , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/save_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"load_config\" : { \"type\" : \"load_config\" , \"doc\" : \"The configuration to use with kiara to load the saved value.\" , } } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/save_value.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for sup_type in cls . get_supported_value_types (): if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring save operation for type ' { sup_type } ': type not available\" ) op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : sup_type }, \"doc\" : f \"Save value of type ' { sup_type } '.\" , } all_metadata_profiles [ f \" { sup_type } .save\" ] = op_config return all_metadata_profiles save_value ( self , value , base_path ) \u00b6 Save the value, and return the load config needed to load it again. Source code in kiara/operations/save_value.py @abc . abstractmethod def save_value ( self , value : Value , base_path : str ) -> typing . Dict [ str , typing . Any ]: \"\"\"Save the value, and return the load config needed to load it again.\"\"\"","title":"\u279c\u2007operations.save_value"},{"location":"api_reference/kiara.operations.save_value/#kiaraoperationssave_value","text":"","title":"kiara.operations.save_value"},{"location":"api_reference/kiara.operations.save_value/#kiara.operations.save_value.SaveOperationType","text":"Save a value into a data store.","title":"SaveOperationType"},{"location":"api_reference/kiara.operations.save_value/#kiara.operations.save_value.SaveValueModuleConfig","text":"","title":"SaveValueModuleConfig"},{"location":"api_reference/kiara.operations.save_value/#kiara.operations.save_value.SaveValueModuleConfig.value_type","text":"The type of the value to save.","title":"value_type"},{"location":"api_reference/kiara.operations.save_value/#kiara.operations.save_value.SaveValueTypeModule","text":"Save a specific value type. This is used internally.","title":"SaveValueTypeModule"},{"location":"api_reference/kiara.operations.save_value/#kiara.operations.save_value.SaveValueTypeModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/save_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"value_id\" : { \"type\" : \"string\" , \"doc\" : \"The id to use when saving the value.\" , }, \"value_item\" : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The base path to save the value to.\" , }, } return inputs","title":"create_input_schema()"},{"location":"api_reference/kiara.operations.save_value/#kiara.operations.save_value.SaveValueTypeModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/save_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"load_config\" : { \"type\" : \"load_config\" , \"doc\" : \"The configuration to use with kiara to load the saved value.\" , } } return outputs","title":"create_output_schema()"},{"location":"api_reference/kiara.operations.save_value/#kiara.operations.save_value.SaveValueTypeModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/save_value.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for sup_type in cls . get_supported_value_types (): if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring save operation for type ' { sup_type } ': type not available\" ) op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : sup_type }, \"doc\" : f \"Save value of type ' { sup_type } '.\" , } all_metadata_profiles [ f \" { sup_type } .save\" ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"api_reference/kiara.operations.save_value/#kiara.operations.save_value.SaveValueTypeModule.save_value","text":"Save the value, and return the load config needed to load it again. Source code in kiara/operations/save_value.py @abc . abstractmethod def save_value ( self , value : Value , base_path : str ) -> typing . Dict [ str , typing . Any ]: \"\"\"Save the value, and return the load config needed to load it again.\"\"\"","title":"save_value()"},{"location":"api_reference/kiara.operations.serialize/","text":"kiara.operations.serialize \u00b6 SerializeValueModule \u00b6 create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/serialize.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) # serialization_type: str = self.get_config_value(\"serialization_type\") return { \"value_item\" : { \"type\" : value_type , \"doc\" : f \"The ' { value_type } ' value to be serialized.\" , } } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/serialize.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) return { \"value_info\" : { \"type\" : \"value_info\" , \"doc\" : \"Information about the (original) serialized value (can be used to re-constitute the value, incl. its original id).\" , }, \"deserialize_config\" : { \"type\" : \"deserialize_config\" , \"doc\" : f \"The config to use to deserialize the value of type ' { value_type } '.\" , }, } retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/serialize.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring serialization operation for source type ' { value_type } ': type not available\" ) return {} for serialization_type in cls . get_supported_serialization_types (): mod_conf = { \"value_type\" : value_type , \"serialization_type\" : serialization_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Serialize value of type ' { value_type } ' to ' { serialization_type } '.\" , } key = f \" { value_type } .serialize_to. { serialization_type } \" if key in all_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_profiles [ key ] = op_config return all_profiles SerializeValueModuleConfig pydantic-model \u00b6 serialization_type : str pydantic-field required \u00b6 The type of the converted value. value_type : str pydantic-field required \u00b6 The type of the source value. SerializeValueOperationType \u00b6 get_operations_for_value_type ( self , value_type ) \u00b6 Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. Source code in kiara/operations/serialize.py def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operation_configs . items (): source_type = op . module_config [ \"value_type\" ] if source_type == value_type : target_type = op . module_config [ \"serialization_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to serialize ' { source_type } ' to { target_type } \" ) result [ target_type ] = op return result","title":"\u279c\u2007operations.serialize"},{"location":"api_reference/kiara.operations.serialize/#kiaraoperationsserialize","text":"","title":"kiara.operations.serialize"},{"location":"api_reference/kiara.operations.serialize/#kiara.operations.serialize.SerializeValueModule","text":"","title":"SerializeValueModule"},{"location":"api_reference/kiara.operations.serialize/#kiara.operations.serialize.SerializeValueModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/serialize.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) # serialization_type: str = self.get_config_value(\"serialization_type\") return { \"value_item\" : { \"type\" : value_type , \"doc\" : f \"The ' { value_type } ' value to be serialized.\" , } }","title":"create_input_schema()"},{"location":"api_reference/kiara.operations.serialize/#kiara.operations.serialize.SerializeValueModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/serialize.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) return { \"value_info\" : { \"type\" : \"value_info\" , \"doc\" : \"Information about the (original) serialized value (can be used to re-constitute the value, incl. its original id).\" , }, \"deserialize_config\" : { \"type\" : \"deserialize_config\" , \"doc\" : f \"The config to use to deserialize the value of type ' { value_type } '.\" , }, }","title":"create_output_schema()"},{"location":"api_reference/kiara.operations.serialize/#kiara.operations.serialize.SerializeValueModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/serialize.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring serialization operation for source type ' { value_type } ': type not available\" ) return {} for serialization_type in cls . get_supported_serialization_types (): mod_conf = { \"value_type\" : value_type , \"serialization_type\" : serialization_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Serialize value of type ' { value_type } ' to ' { serialization_type } '.\" , } key = f \" { value_type } .serialize_to. { serialization_type } \" if key in all_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_profiles [ key ] = op_config return all_profiles","title":"retrieve_module_profiles()"},{"location":"api_reference/kiara.operations.serialize/#kiara.operations.serialize.SerializeValueModuleConfig","text":"","title":"SerializeValueModuleConfig"},{"location":"api_reference/kiara.operations.serialize/#kiara.operations.serialize.SerializeValueModuleConfig.serialization_type","text":"The type of the converted value.","title":"serialization_type"},{"location":"api_reference/kiara.operations.serialize/#kiara.operations.serialize.SerializeValueModuleConfig.value_type","text":"The type of the source value.","title":"value_type"},{"location":"api_reference/kiara.operations.serialize/#kiara.operations.serialize.SerializeValueOperationType","text":"","title":"SerializeValueOperationType"},{"location":"api_reference/kiara.operations.serialize/#kiara.operations.serialize.SerializeValueOperationType.get_operations_for_value_type","text":"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. Source code in kiara/operations/serialize.py def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operation_configs . items (): source_type = op . module_config [ \"value_type\" ] if source_type == value_type : target_type = op . module_config [ \"serialization_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to serialize ' { source_type } ' to { target_type } \" ) result [ target_type ] = op return result","title":"get_operations_for_value_type()"},{"location":"api_reference/kiara.operations.type_convert/","text":"kiara.operations.type_convert \u00b6 ConvertValueModule \u00b6 create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/type_convert.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: supported = self . get_supported_value_types () source_type = self . get_config_value ( \"source_type\" ) target_type = self . get_config_value ( \"target_type\" ) if source_type in supported : # means this is a 'from supported type' conversion if target_type not in self . get_target_value_types (): raise Exception ( f \"Can't create input schema for module ' { self . _module_type_id } ': converstion to target value type ' { target_type } ' not supported.\" # type: ignore ) return { \"value_item\" : { \"type\" : source_type , \"doc\" : f \"The ' { source_type } ' value to be converted.\" , } } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/type_convert.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: supported = self . get_supported_value_types () source_type = self . get_config_value ( \"source_type\" ) target_type = self . get_config_value ( \"target_type\" ) if target_type in supported : # means this is a 'to supported type' conversion if source_type not in self . get_source_value_types (): raise Exception ( f \"Can't create output schema for module ' { self . _module_type_id } ': conversion from source value type ' { source_type } ' not supported.\" # type: ignore ) return { \"value_item\" : { \"type\" : target_type , \"doc\" : f \"The converted ' { source_type } ' value as ' { target_type } '.\" , } } retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/type_convert.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_types : typing . Iterable [ str ] = cls . get_supported_value_types () if \"*\" in value_types : value_types = kiara . type_mgmt . value_type_names for value_type in value_types : if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring type convert operation for source type ' { value_type } ': type not available\" ) continue for target_type in cls . get_target_value_types (): if target_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring type convert operation for target type ' { target_type } ': type not available\" ) continue mod_conf = { \"source_type\" : value_type , \"target_type\" : target_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Convert value of type ' { value_type } ' to type ' { target_type } '.\" , } key = f \" { value_type } .convert_to. { target_type } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config # key = f\"{target_type}.convert_from.{value_type}\" # if key in all_metadata_profiles.keys(): # raise Exception(f\"Duplicate profile key: {key}\") # all_metadata_profiles[key] = op_config for source_type in cls . get_source_value_types (): mod_conf = { \"source_type\" : source_type , \"target_type\" : value_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Convert value of type ' { source_type } ' to type ' { value_type } '.\" , } # key = f\"{value_type}.convert_from.{target_type}\" # type: ignore # if key in all_metadata_profiles.keys(): # raise Exception(f\"Duplicate profile key: {key}\") # all_metadata_profiles[key] = op_config key = f \" { source_type } .convert_to. { value_type } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles ConvertValueOperationType \u00b6 get_operations_for_source_type ( self , value_type ) \u00b6 Find all operations that transform from the specified type. The result dict uses the target type of the conversion as key, and the operation itself as value. Source code in kiara/operations/type_convert.py def get_operations_for_source_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that transform from the specified type. The result dict uses the target type of the conversion as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operation_configs . items (): source_type = op . module_config [ \"source_type\" ] if source_type == value_type : target_type = op . module_config [ \"target_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to transform from ' { source_type } ' to { target_type } \" ) result [ target_type ] = op return result get_operations_for_target_type ( self , value_type ) \u00b6 Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. Source code in kiara/operations/type_convert.py def get_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operation_configs . items (): target_type = op . module_config [ \"target_type\" ] if target_type == value_type : source_type = op . module_config [ \"source_type\" ] if source_type in result . keys (): raise Exception ( f \"Multiple operations to transform from ' { source_type } ' to { target_type } \" ) result [ source_type ] = op return result TypeConversionModuleConfig pydantic-model \u00b6 source_type : str pydantic-field required \u00b6 The type of the source value. target_type : str pydantic-field required \u00b6 The type of the converted value.","title":"\u279c\u2007operations.type_convert"},{"location":"api_reference/kiara.operations.type_convert/#kiaraoperationstype_convert","text":"","title":"kiara.operations.type_convert"},{"location":"api_reference/kiara.operations.type_convert/#kiara.operations.type_convert.ConvertValueModule","text":"","title":"ConvertValueModule"},{"location":"api_reference/kiara.operations.type_convert/#kiara.operations.type_convert.ConvertValueModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/type_convert.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: supported = self . get_supported_value_types () source_type = self . get_config_value ( \"source_type\" ) target_type = self . get_config_value ( \"target_type\" ) if source_type in supported : # means this is a 'from supported type' conversion if target_type not in self . get_target_value_types (): raise Exception ( f \"Can't create input schema for module ' { self . _module_type_id } ': converstion to target value type ' { target_type } ' not supported.\" # type: ignore ) return { \"value_item\" : { \"type\" : source_type , \"doc\" : f \"The ' { source_type } ' value to be converted.\" , } }","title":"create_input_schema()"},{"location":"api_reference/kiara.operations.type_convert/#kiara.operations.type_convert.ConvertValueModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/type_convert.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: supported = self . get_supported_value_types () source_type = self . get_config_value ( \"source_type\" ) target_type = self . get_config_value ( \"target_type\" ) if target_type in supported : # means this is a 'to supported type' conversion if source_type not in self . get_source_value_types (): raise Exception ( f \"Can't create output schema for module ' { self . _module_type_id } ': conversion from source value type ' { source_type } ' not supported.\" # type: ignore ) return { \"value_item\" : { \"type\" : target_type , \"doc\" : f \"The converted ' { source_type } ' value as ' { target_type } '.\" , } }","title":"create_output_schema()"},{"location":"api_reference/kiara.operations.type_convert/#kiara.operations.type_convert.ConvertValueModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/type_convert.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_types : typing . Iterable [ str ] = cls . get_supported_value_types () if \"*\" in value_types : value_types = kiara . type_mgmt . value_type_names for value_type in value_types : if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring type convert operation for source type ' { value_type } ': type not available\" ) continue for target_type in cls . get_target_value_types (): if target_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring type convert operation for target type ' { target_type } ': type not available\" ) continue mod_conf = { \"source_type\" : value_type , \"target_type\" : target_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Convert value of type ' { value_type } ' to type ' { target_type } '.\" , } key = f \" { value_type } .convert_to. { target_type } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config # key = f\"{target_type}.convert_from.{value_type}\" # if key in all_metadata_profiles.keys(): # raise Exception(f\"Duplicate profile key: {key}\") # all_metadata_profiles[key] = op_config for source_type in cls . get_source_value_types (): mod_conf = { \"source_type\" : source_type , \"target_type\" : value_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Convert value of type ' { source_type } ' to type ' { value_type } '.\" , } # key = f\"{value_type}.convert_from.{target_type}\" # type: ignore # if key in all_metadata_profiles.keys(): # raise Exception(f\"Duplicate profile key: {key}\") # all_metadata_profiles[key] = op_config key = f \" { source_type } .convert_to. { value_type } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"api_reference/kiara.operations.type_convert/#kiara.operations.type_convert.ConvertValueOperationType","text":"","title":"ConvertValueOperationType"},{"location":"api_reference/kiara.operations.type_convert/#kiara.operations.type_convert.ConvertValueOperationType.get_operations_for_source_type","text":"Find all operations that transform from the specified type. The result dict uses the target type of the conversion as key, and the operation itself as value. Source code in kiara/operations/type_convert.py def get_operations_for_source_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that transform from the specified type. The result dict uses the target type of the conversion as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operation_configs . items (): source_type = op . module_config [ \"source_type\" ] if source_type == value_type : target_type = op . module_config [ \"target_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to transform from ' { source_type } ' to { target_type } \" ) result [ target_type ] = op return result","title":"get_operations_for_source_type()"},{"location":"api_reference/kiara.operations.type_convert/#kiara.operations.type_convert.ConvertValueOperationType.get_operations_for_target_type","text":"Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. Source code in kiara/operations/type_convert.py def get_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operation_configs . items (): target_type = op . module_config [ \"target_type\" ] if target_type == value_type : source_type = op . module_config [ \"source_type\" ] if source_type in result . keys (): raise Exception ( f \"Multiple operations to transform from ' { source_type } ' to { target_type } \" ) result [ source_type ] = op return result","title":"get_operations_for_target_type()"},{"location":"api_reference/kiara.operations.type_convert/#kiara.operations.type_convert.TypeConversionModuleConfig","text":"","title":"TypeConversionModuleConfig"},{"location":"api_reference/kiara.operations.type_convert/#kiara.operations.type_convert.TypeConversionModuleConfig.source_type","text":"The type of the source value.","title":"source_type"},{"location":"api_reference/kiara.operations.type_convert/#kiara.operations.type_convert.TypeConversionModuleConfig.target_type","text":"The type of the converted value.","title":"target_type"},{"location":"api_reference/kiara.pipeline.config/","text":"kiara.pipeline.config \u00b6 PipelineConfig pydantic-model \u00b6 A class to hold the configuration for a PipelineModule . If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify auto , in which case Kiara will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a nand logic gate (in Python): and_step = PipelineStepConfig ( module_type = \"and\" , step_id = \"and\" ) not_step = PipelineStepConfig ( module_type = \"not\" , step_id = \"not\" , input_links = { \"a\" : [ \"and.y\" ]} nand_p_conf = PipelineConfig ( doc = \"Returns 'False' if both inputs are 'True'.\" , steps = [ and_step , not_step ], input_aliases = { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, output_aliases = { \"not__y\" : \"y\" }} Or, the same thing in json: { \"module_type_name\" : \"nand\" , \"doc\" : \"Returns 'False' if both inputs are 'True'.\" , \"steps\" : [ { \"module_type\" : \"and\" , \"step_id\" : \"and\" }, { \"module_type\" : \"not\" , \"step_id\" : \"not\" , \"input_links\" : { \"a\" : \"and.y\" } } ], \"input_aliases\" : { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, \"output_aliases\" : { \"not__y\" : \"y\" } } context : Dict [ str , Any ] pydantic-field \u00b6 Metadata for this workflow. documentation : str pydantic-field \u00b6 Documentation about what the pipeline does. input_aliases : Union [ str , Dict [ str , str ]] pydantic-field \u00b6 A map of input aliases, with the calculated ( __ -- double underscore!) name as key, and a string (the resulting workflow input alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible. output_aliases : Union [ str , Dict [ str , str ]] pydantic-field \u00b6 A map of output aliases, with the calculated ( __ -- double underscore!) name as key, and a string (the resulting workflow output alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible. steps : List [ kiara . pipeline . config . PipelineStepConfig ] pydantic-field \u00b6 A list of steps/modules of this pipeline, and their connections. create_pipeline_config ( config , module_config = None , kiara = None ) classmethod \u00b6 Create a PipelineModule instance. The main 'config' argument here can be either: a string: in which case it needs to be (in that order): a module id an operation id a path to a local file a ModuleConfig object a dict (to create a ModuleInstsanceConfig from Parameters: Name Type Description Default kiara Optional[Kiara] the kiara context None config Union[kiara.module_config.ModuleConfig, Mapping[str, Any], str] the 'main' config object required module_config Optional[Mapping[str, Any]] in case the 'main' config object was a module id, this argument is used to instantiate the module None kiara Optional[Kiara] the kiara context (will use default context instance if not provided) None Source code in kiara/pipeline/config.py @classmethod def create_pipeline_config ( cls , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineConfig\" : \"\"\"Create a PipelineModule instance. The main 'config' argument here can be either: - a string: in which case it needs to be (in that order): - a module id - an operation id - a path to a local file - a [ModuleConfig][kiara.module_config.ModuleConfig] object - a dict (to create a `ModuleInstsanceConfig` from Arguments: kiara: the kiara context config: the 'main' config object module_config: in case the 'main' config object was a module id, this argument is used to instantiate the module kiara: the kiara context (will use default context instance if not provided) \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () module_config_obj = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) if not module_config_obj . module_type == \"pipeline\" : raise Exception ( f \"Not a valid pipeline configuration: { config } \" ) # TODO: this is a bit round-about, to create a module config first, but it probably doesn't matter pipeline_config_data = module_config_obj . module_config module : PipelineConfig = PipelineConfig ( ** pipeline_config_data ) return module PipelineStepConfig pydantic-model \u00b6 A class to hold the configuration of one module within a PipelineModule . input_links : Dict [ str , List [ kiara . pipeline . StepValueAddress ]] pydantic-field \u00b6 The map with the name of an input link as key, and the connected module output name(s) as value. step_id : str pydantic-field required \u00b6 The id of the step. PipelineStructureConfig pydantic-model \u00b6 parent_id : str pydantic-field required \u00b6 The id of the parent of this structure. StepDesc pydantic-model \u00b6 Details of a single PipelineStep (which lives within a Pipeline input_connections : Dict [ str , List [ str ]] pydantic-field required \u00b6 A map that explains what elements connect to this steps inputs. A connection could either be a Pipeline input (indicated by the __pipeline__ token), or another steps output. Examples: i n pu t _co nne c t io ns : { \"a\" : [ \"__pipeline__.a\" ], \"b\" : [ \"step_one.a\" ] } output_connections : Dict [ str , List [ str ]] pydantic-field required \u00b6 A map that explains what elemnts connect to this steps outputs. A connection could be either a Pipeline output, or another steps input. processing_stage : int pydantic-field required \u00b6 The processing stage of this step within a Pipeline. required : bool pydantic-field required \u00b6 Whether this step is always required, or potentially could be skipped in case some inputs are not available. step : PipelineStep pydantic-field required \u00b6 Attributes of the step itself.","title":"\u279c\u2007pipeline.config"},{"location":"api_reference/kiara.pipeline.config/#kiarapipelineconfig","text":"","title":"kiara.pipeline.config"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.PipelineConfig","text":"A class to hold the configuration for a PipelineModule . If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify auto , in which case Kiara will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a nand logic gate (in Python): and_step = PipelineStepConfig ( module_type = \"and\" , step_id = \"and\" ) not_step = PipelineStepConfig ( module_type = \"not\" , step_id = \"not\" , input_links = { \"a\" : [ \"and.y\" ]} nand_p_conf = PipelineConfig ( doc = \"Returns 'False' if both inputs are 'True'.\" , steps = [ and_step , not_step ], input_aliases = { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, output_aliases = { \"not__y\" : \"y\" }} Or, the same thing in json: { \"module_type_name\" : \"nand\" , \"doc\" : \"Returns 'False' if both inputs are 'True'.\" , \"steps\" : [ { \"module_type\" : \"and\" , \"step_id\" : \"and\" }, { \"module_type\" : \"not\" , \"step_id\" : \"not\" , \"input_links\" : { \"a\" : \"and.y\" } } ], \"input_aliases\" : { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, \"output_aliases\" : { \"not__y\" : \"y\" } }","title":"PipelineConfig"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.PipelineConfig.context","text":"Metadata for this workflow.","title":"context"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.PipelineConfig.documentation","text":"Documentation about what the pipeline does.","title":"documentation"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.PipelineConfig.input_aliases","text":"A map of input aliases, with the calculated ( __ -- double underscore!) name as key, and a string (the resulting workflow input alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.","title":"input_aliases"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.PipelineConfig.output_aliases","text":"A map of output aliases, with the calculated ( __ -- double underscore!) name as key, and a string (the resulting workflow output alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.","title":"output_aliases"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.PipelineConfig.steps","text":"A list of steps/modules of this pipeline, and their connections.","title":"steps"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.PipelineConfig.create_pipeline_config","text":"Create a PipelineModule instance. The main 'config' argument here can be either: a string: in which case it needs to be (in that order): a module id an operation id a path to a local file a ModuleConfig object a dict (to create a ModuleInstsanceConfig from Parameters: Name Type Description Default kiara Optional[Kiara] the kiara context None config Union[kiara.module_config.ModuleConfig, Mapping[str, Any], str] the 'main' config object required module_config Optional[Mapping[str, Any]] in case the 'main' config object was a module id, this argument is used to instantiate the module None kiara Optional[Kiara] the kiara context (will use default context instance if not provided) None Source code in kiara/pipeline/config.py @classmethod def create_pipeline_config ( cls , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineConfig\" : \"\"\"Create a PipelineModule instance. The main 'config' argument here can be either: - a string: in which case it needs to be (in that order): - a module id - an operation id - a path to a local file - a [ModuleConfig][kiara.module_config.ModuleConfig] object - a dict (to create a `ModuleInstsanceConfig` from Arguments: kiara: the kiara context config: the 'main' config object module_config: in case the 'main' config object was a module id, this argument is used to instantiate the module kiara: the kiara context (will use default context instance if not provided) \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () module_config_obj = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) if not module_config_obj . module_type == \"pipeline\" : raise Exception ( f \"Not a valid pipeline configuration: { config } \" ) # TODO: this is a bit round-about, to create a module config first, but it probably doesn't matter pipeline_config_data = module_config_obj . module_config module : PipelineConfig = PipelineConfig ( ** pipeline_config_data ) return module","title":"create_pipeline_config()"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.PipelineStepConfig","text":"A class to hold the configuration of one module within a PipelineModule .","title":"PipelineStepConfig"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.PipelineStepConfig.input_links","text":"The map with the name of an input link as key, and the connected module output name(s) as value.","title":"input_links"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.PipelineStepConfig.step_id","text":"The id of the step.","title":"step_id"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.PipelineStructureConfig","text":"","title":"PipelineStructureConfig"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.PipelineStructureConfig.parent_id","text":"The id of the parent of this structure.","title":"parent_id"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.StepDesc","text":"Details of a single PipelineStep (which lives within a Pipeline","title":"StepDesc"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.StepDesc.input_connections","text":"A map that explains what elements connect to this steps inputs. A connection could either be a Pipeline input (indicated by the __pipeline__ token), or another steps output. Examples: i n pu t _co nne c t io ns : { \"a\" : [ \"__pipeline__.a\" ], \"b\" : [ \"step_one.a\" ] }","title":"input_connections"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.StepDesc.output_connections","text":"A map that explains what elemnts connect to this steps outputs. A connection could be either a Pipeline output, or another steps input.","title":"output_connections"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.StepDesc.processing_stage","text":"The processing stage of this step within a Pipeline.","title":"processing_stage"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.StepDesc.required","text":"Whether this step is always required, or potentially could be skipped in case some inputs are not available.","title":"required"},{"location":"api_reference/kiara.pipeline.config/#kiara.pipeline.config.StepDesc.step","text":"Attributes of the step itself.","title":"step"},{"location":"api_reference/kiara.pipeline.controller.batch/","text":"kiara.pipeline.controller.batch \u00b6 BatchController \u00b6 A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline the pipeline to control required auto_process whether to automatically start processing the pipeline as soon as the input set is valid required pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_exc () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) # for j_id in job_ids: # job = self._processor.get_job_details(j_id) # assert job is not None # if job.error: # print(job.error) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return if self . _auto_process : self . process_pipeline () BatchControllerManual \u00b6 A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline the pipeline to control required auto_process whether to automatically start processing the pipeline as soon as the input set is valid required pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/pipeline/controller/batch.py def pipeline_inputs_changed ( self , event : \"PipelineInputEvent\" ): self . _finished_until = None pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return","title":"\u279c\u2007pipeline.controller.batch"},{"location":"api_reference/kiara.pipeline.controller.batch/#kiarapipelinecontrollerbatch","text":"","title":"kiara.pipeline.controller.batch"},{"location":"api_reference/kiara.pipeline.controller.batch/#kiara.pipeline.controller.batch.BatchController","text":"A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline the pipeline to control required auto_process whether to automatically start processing the pipeline as soon as the input set is valid required","title":"BatchController"},{"location":"api_reference/kiara.pipeline.controller.batch/#kiara.pipeline.controller.batch.BatchController.pipeline_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False","title":"pipeline_outputs_changed()"},{"location":"api_reference/kiara.pipeline.controller.batch/#kiara.pipeline.controller.batch.BatchController.process_pipeline","text":"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_exc () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) # for j_id in job_ids: # job = self._processor.get_job_details(j_id) # assert job is not None # if job.error: # print(job.error) finally : self . _is_running = False","title":"process_pipeline()"},{"location":"api_reference/kiara.pipeline.controller.batch/#kiara.pipeline.controller.batch.BatchController.step_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return if self . _auto_process : self . process_pipeline ()","title":"step_inputs_changed()"},{"location":"api_reference/kiara.pipeline.controller.batch/#kiara.pipeline.controller.batch.BatchControllerManual","text":"A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline the pipeline to control required auto_process whether to automatically start processing the pipeline as soon as the input set is valid required","title":"BatchControllerManual"},{"location":"api_reference/kiara.pipeline.controller.batch/#kiara.pipeline.controller.batch.BatchControllerManual.pipeline_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/pipeline/controller/batch.py def pipeline_inputs_changed ( self , event : \"PipelineInputEvent\" ): self . _finished_until = None","title":"pipeline_inputs_changed()"},{"location":"api_reference/kiara.pipeline.controller.batch/#kiara.pipeline.controller.batch.BatchControllerManual.pipeline_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False","title":"pipeline_outputs_changed()"},{"location":"api_reference/kiara.pipeline.controller.batch/#kiara.pipeline.controller.batch.BatchControllerManual.process_pipeline","text":"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False","title":"process_pipeline()"},{"location":"api_reference/kiara.pipeline.controller.batch/#kiara.pipeline.controller.batch.BatchControllerManual.step_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return","title":"step_inputs_changed()"},{"location":"api_reference/kiara.pipeline.controller/","text":"kiara.pipeline.controller \u00b6 PipelineController \u00b6 An object that controls how a Pipeline should react to events related to it's inputs/outputs. This is the base for the central controller class that needs to be implemented by a Kiara frontend. The default implementation that is used if no PipelineController is provided in a Pipeline constructor is the BatchController , which basically waits until all required inputs are set, and then processes all pipeline steps in one go (in the right order). The pipeline object to control can be set either in the constructor, or via the set_pipeline method. But only once, every subsequent attempt to set a pipeline will raise an Exception. If you want to implement your own controller, you have to override at least one of the (empty) event hook methods: pipeline_inputs_changed pipeline_outputs_changed step_inputs_changed step_outputs_changed Parameters: Name Type Description Default pipeline Pipeline the pipeline object to control required pipeline : Pipeline property readonly \u00b6 Return the pipeline this controller, well, ...controls... pipeline_inputs : ValueSet property writable \u00b6 Return the inputs object for this pipeline. pipeline_outputs : ValueSet property readonly \u00b6 Return the (current) pipeline outputs object for this pipeline. processing_stages : List [ List [ str ]] property readonly \u00b6 Return the processing stage order of the pipeline. Returns: Type Description List[List[str]] a list of lists of step ids can_be_processed ( self , step_id ) \u00b6 Check whether the step with the provided id is ready to be processed. Source code in kiara/pipeline/controller/__init__.py def can_be_processed ( self , step_id : str ) -> bool : \"\"\"Check whether the step with the provided id is ready to be processed.\"\"\" result = True step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): result = False break return result can_be_skipped ( self , step_id ) \u00b6 Check whether the processing of a step can be skipped. Source code in kiara/pipeline/controller/__init__.py def can_be_skipped ( self , step_id : str ) -> bool : \"\"\"Check whether the processing of a step can be skipped.\"\"\" result = True step = self . get_step ( step_id ) if step . required : result = self . can_be_processed ( step_id ) return result get_current_pipeline_state ( self ) \u00b6 Return a description of the current pipeline state. This methods creates a new PipelineState object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: Type Description PipelineState an object outlining the current pipeline state Source code in kiara/pipeline/controller/__init__.py def get_current_pipeline_state ( self ) -> \"PipelineState\" : \"\"\"Return a description of the current pipeline state. This methods creates a new [PipelineState][kiara.pipeline.pipeline.PipelineState] object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: an object outlining the current pipeline state \"\"\" return self . pipeline . get_current_state () get_step ( self , step_id ) \u00b6 Return the step object for the provided id. Parameters: Name Type Description Default step_id str the step id required Returns: Type Description PipelineStep the step object Source code in kiara/pipeline/controller/__init__.py def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the step object for the provided id. Arguments: step_id: the step id Returns: the step object \"\"\" return self . pipeline . get_step ( step_id ) get_step_input ( self , step_id , input_name ) \u00b6 Get the (current) input value for a specified step and input field name. Source code in kiara/pipeline/controller/__init__.py def get_step_input ( self , step_id : str , input_name : str ) -> Value : \"\"\"Get the (current) input value for a specified step and input field name.\"\"\" item = self . get_step_inputs ( step_id ) . get_value_obj ( input_name ) assert item is not None return item get_step_inputs ( self , step_id ) \u00b6 Return the inputs object for the pipeline. Source code in kiara/pipeline/controller/__init__.py def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the inputs object for the pipeline.\"\"\" return self . pipeline . get_step_inputs ( step_id ) get_step_output ( self , step_id , output_name ) \u00b6 Get the (current) output value for a specified step and output field name. Source code in kiara/pipeline/controller/__init__.py def get_step_output ( self , step_id : str , output_name : str ) -> Value : \"\"\"Get the (current) output value for a specified step and output field name.\"\"\" item = self . get_step_outputs ( step_id ) . get_value_obj ( output_name ) assert item is not None return item get_step_outputs ( self , step_id ) \u00b6 Return the outputs object for the pipeline. Source code in kiara/pipeline/controller/__init__.py def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the outputs object for the pipeline.\"\"\" return self . pipeline . get_step_outputs ( step_id ) pipeline_is_finished ( self ) \u00b6 Return whether the pipeline has been processed successfully. A True result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: Type Description bool whether the pipeline was processed successfully ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def pipeline_is_finished ( self ) -> bool : \"\"\"Return whether the pipeline has been processed successfully. A ``True`` result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: whether the pipeline was processed successfully (``True``) or not (``False``) \"\"\" return self . pipeline . outputs . items_are_valid () pipeline_is_ready ( self ) \u00b6 Return whether the pipeline is ready to be processed. A True result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: Type Description bool whether the pipeline can be processed as a whole ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def pipeline_is_ready ( self ) -> bool : \"\"\"Return whether the pipeline is ready to be processed. A ``True`` result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: whether the pipeline can be processed as a whole (``True``) or not (``False``) \"\"\" return self . pipeline . inputs . items_are_valid () process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/__init__.py def process_pipeline ( self ): \"\"\"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Returns: \"\"\" raise NotImplementedError () process_step ( self , step_id ) \u00b6 Kick off processing for the step with the provided id. Parameters: Name Type Description Default step_id str the id of the step that should be started required Source code in kiara/pipeline/controller/__init__.py def process_step ( self , step_id : str ) -> str : \"\"\"Kick off processing for the step with the provided id. Arguments: step_id: the id of the step that should be started \"\"\" step_inputs = self . get_step_inputs ( step_id ) # if the inputs are not valid, ignore this step if not step_inputs . items_are_valid (): raise Exception ( f \"Can't execute step ' { step_id } ': it does not have valid input set.\" ) # get the output 'holder' objects, which we'll need to pass to the module step_outputs = self . get_step_outputs ( step_id ) # get the module object that holds the code that will do the processing step = self . get_step ( step_id ) job_id = self . _processor . start ( pipeline_id = self . pipeline . id , pipeline_name = self . pipeline . title , step_id = step_id , module = step . module , inputs = step_inputs , outputs = step_outputs , ) self . _job_ids [ step_id ] = job_id return job_id set_pipeline ( self , pipeline ) \u00b6 Set the pipeline object for this controller. Only one pipeline can be set, once. Parameters: Name Type Description Default pipeline Pipeline the pipeline object required Source code in kiara/pipeline/controller/__init__.py def set_pipeline ( self , pipeline : \"Pipeline\" ): \"\"\"Set the pipeline object for this controller. Only one pipeline can be set, once. Arguments: pipeline: the pipeline object \"\"\" if self . _pipeline is not None : raise Exception ( \"Pipeline already set.\" ) self . _pipeline = pipeline set_pipeline_inputs ( self , ** inputs ) \u00b6 Set one, several or all inputs for this pipeline. Parameters: Name Type Description Default **inputs Any the input values to set {} Source code in kiara/pipeline/controller/__init__.py def set_pipeline_inputs ( self , ** inputs : typing . Any ): \"\"\"Set one, several or all inputs for this pipeline. Arguments: **inputs: the input values to set \"\"\" _inputs = self . _pipeline_input_hook ( ** inputs ) self . pipeline_inputs . set_values ( ** _inputs ) step_is_finished ( self , step_id ) \u00b6 Return whether the step with the provided id has been processed successfully. A True result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Parameters: Name Type Description Default step_id str the id of the step to check required Returns: Type Description bool whether the step result is valid ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def step_is_finished ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id has been processed successfully. A ``True`` result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Arguments: step_id: the id of the step to check Returns: whether the step result is valid (``True``) or not (``False``) \"\"\" return self . get_step_outputs ( step_id ) . items_are_valid () step_is_ready ( self , step_id ) \u00b6 Return whether the step with the provided id is ready to be processed. A True result means that all input fields are currently set with valid values. Parameters: Name Type Description Default step_id str the id of the step to check required Returns: Type Description bool whether the step is ready ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def step_is_ready ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id is ready to be processed. A ``True`` result means that all input fields are currently set with valid values. Arguments: step_id: the id of the step to check Returns: whether the step is ready (``True``) or not (``False``) \"\"\" return self . get_step_inputs ( step_id ) . items_are_valid ()","title":"\u279c\u2007pipeline.controller"},{"location":"api_reference/kiara.pipeline.controller/#kiarapipelinecontroller","text":"","title":"kiara.pipeline.controller"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController","text":"An object that controls how a Pipeline should react to events related to it's inputs/outputs. This is the base for the central controller class that needs to be implemented by a Kiara frontend. The default implementation that is used if no PipelineController is provided in a Pipeline constructor is the BatchController , which basically waits until all required inputs are set, and then processes all pipeline steps in one go (in the right order). The pipeline object to control can be set either in the constructor, or via the set_pipeline method. But only once, every subsequent attempt to set a pipeline will raise an Exception. If you want to implement your own controller, you have to override at least one of the (empty) event hook methods: pipeline_inputs_changed pipeline_outputs_changed step_inputs_changed step_outputs_changed Parameters: Name Type Description Default pipeline Pipeline the pipeline object to control required","title":"PipelineController"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.pipeline","text":"Return the pipeline this controller, well, ...controls...","title":"pipeline"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.pipeline_inputs","text":"Return the inputs object for this pipeline.","title":"pipeline_inputs"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.pipeline_outputs","text":"Return the (current) pipeline outputs object for this pipeline.","title":"pipeline_outputs"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.processing_stages","text":"Return the processing stage order of the pipeline. Returns: Type Description List[List[str]] a list of lists of step ids","title":"processing_stages"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.can_be_processed","text":"Check whether the step with the provided id is ready to be processed. Source code in kiara/pipeline/controller/__init__.py def can_be_processed ( self , step_id : str ) -> bool : \"\"\"Check whether the step with the provided id is ready to be processed.\"\"\" result = True step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): result = False break return result","title":"can_be_processed()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.can_be_skipped","text":"Check whether the processing of a step can be skipped. Source code in kiara/pipeline/controller/__init__.py def can_be_skipped ( self , step_id : str ) -> bool : \"\"\"Check whether the processing of a step can be skipped.\"\"\" result = True step = self . get_step ( step_id ) if step . required : result = self . can_be_processed ( step_id ) return result","title":"can_be_skipped()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.get_current_pipeline_state","text":"Return a description of the current pipeline state. This methods creates a new PipelineState object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: Type Description PipelineState an object outlining the current pipeline state Source code in kiara/pipeline/controller/__init__.py def get_current_pipeline_state ( self ) -> \"PipelineState\" : \"\"\"Return a description of the current pipeline state. This methods creates a new [PipelineState][kiara.pipeline.pipeline.PipelineState] object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: an object outlining the current pipeline state \"\"\" return self . pipeline . get_current_state ()","title":"get_current_pipeline_state()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.get_step","text":"Return the step object for the provided id. Parameters: Name Type Description Default step_id str the step id required Returns: Type Description PipelineStep the step object Source code in kiara/pipeline/controller/__init__.py def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the step object for the provided id. Arguments: step_id: the step id Returns: the step object \"\"\" return self . pipeline . get_step ( step_id )","title":"get_step()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.get_step_input","text":"Get the (current) input value for a specified step and input field name. Source code in kiara/pipeline/controller/__init__.py def get_step_input ( self , step_id : str , input_name : str ) -> Value : \"\"\"Get the (current) input value for a specified step and input field name.\"\"\" item = self . get_step_inputs ( step_id ) . get_value_obj ( input_name ) assert item is not None return item","title":"get_step_input()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.get_step_inputs","text":"Return the inputs object for the pipeline. Source code in kiara/pipeline/controller/__init__.py def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the inputs object for the pipeline.\"\"\" return self . pipeline . get_step_inputs ( step_id )","title":"get_step_inputs()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.get_step_output","text":"Get the (current) output value for a specified step and output field name. Source code in kiara/pipeline/controller/__init__.py def get_step_output ( self , step_id : str , output_name : str ) -> Value : \"\"\"Get the (current) output value for a specified step and output field name.\"\"\" item = self . get_step_outputs ( step_id ) . get_value_obj ( output_name ) assert item is not None return item","title":"get_step_output()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.get_step_outputs","text":"Return the outputs object for the pipeline. Source code in kiara/pipeline/controller/__init__.py def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the outputs object for the pipeline.\"\"\" return self . pipeline . get_step_outputs ( step_id )","title":"get_step_outputs()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.pipeline_is_finished","text":"Return whether the pipeline has been processed successfully. A True result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: Type Description bool whether the pipeline was processed successfully ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def pipeline_is_finished ( self ) -> bool : \"\"\"Return whether the pipeline has been processed successfully. A ``True`` result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: whether the pipeline was processed successfully (``True``) or not (``False``) \"\"\" return self . pipeline . outputs . items_are_valid ()","title":"pipeline_is_finished()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.pipeline_is_ready","text":"Return whether the pipeline is ready to be processed. A True result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: Type Description bool whether the pipeline can be processed as a whole ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def pipeline_is_ready ( self ) -> bool : \"\"\"Return whether the pipeline is ready to be processed. A ``True`` result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: whether the pipeline can be processed as a whole (``True``) or not (``False``) \"\"\" return self . pipeline . inputs . items_are_valid ()","title":"pipeline_is_ready()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.process_pipeline","text":"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/__init__.py def process_pipeline ( self ): \"\"\"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Returns: \"\"\" raise NotImplementedError ()","title":"process_pipeline()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.process_step","text":"Kick off processing for the step with the provided id. Parameters: Name Type Description Default step_id str the id of the step that should be started required Source code in kiara/pipeline/controller/__init__.py def process_step ( self , step_id : str ) -> str : \"\"\"Kick off processing for the step with the provided id. Arguments: step_id: the id of the step that should be started \"\"\" step_inputs = self . get_step_inputs ( step_id ) # if the inputs are not valid, ignore this step if not step_inputs . items_are_valid (): raise Exception ( f \"Can't execute step ' { step_id } ': it does not have valid input set.\" ) # get the output 'holder' objects, which we'll need to pass to the module step_outputs = self . get_step_outputs ( step_id ) # get the module object that holds the code that will do the processing step = self . get_step ( step_id ) job_id = self . _processor . start ( pipeline_id = self . pipeline . id , pipeline_name = self . pipeline . title , step_id = step_id , module = step . module , inputs = step_inputs , outputs = step_outputs , ) self . _job_ids [ step_id ] = job_id return job_id","title":"process_step()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.set_pipeline","text":"Set the pipeline object for this controller. Only one pipeline can be set, once. Parameters: Name Type Description Default pipeline Pipeline the pipeline object required Source code in kiara/pipeline/controller/__init__.py def set_pipeline ( self , pipeline : \"Pipeline\" ): \"\"\"Set the pipeline object for this controller. Only one pipeline can be set, once. Arguments: pipeline: the pipeline object \"\"\" if self . _pipeline is not None : raise Exception ( \"Pipeline already set.\" ) self . _pipeline = pipeline","title":"set_pipeline()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.set_pipeline_inputs","text":"Set one, several or all inputs for this pipeline. Parameters: Name Type Description Default **inputs Any the input values to set {} Source code in kiara/pipeline/controller/__init__.py def set_pipeline_inputs ( self , ** inputs : typing . Any ): \"\"\"Set one, several or all inputs for this pipeline. Arguments: **inputs: the input values to set \"\"\" _inputs = self . _pipeline_input_hook ( ** inputs ) self . pipeline_inputs . set_values ( ** _inputs )","title":"set_pipeline_inputs()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.step_is_finished","text":"Return whether the step with the provided id has been processed successfully. A True result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Parameters: Name Type Description Default step_id str the id of the step to check required Returns: Type Description bool whether the step result is valid ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def step_is_finished ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id has been processed successfully. A ``True`` result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Arguments: step_id: the id of the step to check Returns: whether the step result is valid (``True``) or not (``False``) \"\"\" return self . get_step_outputs ( step_id ) . items_are_valid ()","title":"step_is_finished()"},{"location":"api_reference/kiara.pipeline.controller/#kiara.pipeline.controller.__init__.PipelineController.step_is_ready","text":"Return whether the step with the provided id is ready to be processed. A True result means that all input fields are currently set with valid values. Parameters: Name Type Description Default step_id str the id of the step to check required Returns: Type Description bool whether the step is ready ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def step_is_ready ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id is ready to be processed. A ``True`` result means that all input fields are currently set with valid values. Arguments: step_id: the id of the step to check Returns: whether the step is ready (``True``) or not (``False``) \"\"\" return self . get_step_inputs ( step_id ) . items_are_valid ()","title":"step_is_ready()"},{"location":"api_reference/kiara.pipeline.listeners/","text":"kiara.pipeline.listeners \u00b6 PipelineListener \u00b6 pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/pipeline/listeners.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. !!! note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the ``step_inputs_changed`` method if you want to. This behaviour might change in the future. Arguments: event: the pipeline input event \"\"\" pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/listeners.py def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Arguments: event: the pipeline output event \"\"\" step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/listeners.py def step_inputs_changed ( self , event : StepInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Arguments: event: the step input event \"\"\" step_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Parameters: Name Type Description Default event StepOutputEvent the step output event required Source code in kiara/pipeline/listeners.py def step_outputs_changed ( self , event : StepOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Arguments: event: the step output event \"\"\"","title":"\u279c\u2007pipeline.listeners"},{"location":"api_reference/kiara.pipeline.listeners/#kiarapipelinelisteners","text":"","title":"kiara.pipeline.listeners"},{"location":"api_reference/kiara.pipeline.listeners/#kiara.pipeline.listeners.PipelineListener","text":"","title":"PipelineListener"},{"location":"api_reference/kiara.pipeline.listeners/#kiara.pipeline.listeners.PipelineListener.pipeline_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/pipeline/listeners.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. !!! note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the ``step_inputs_changed`` method if you want to. This behaviour might change in the future. Arguments: event: the pipeline input event \"\"\"","title":"pipeline_inputs_changed()"},{"location":"api_reference/kiara.pipeline.listeners/#kiara.pipeline.listeners.PipelineListener.pipeline_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/listeners.py def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Arguments: event: the pipeline output event \"\"\"","title":"pipeline_outputs_changed()"},{"location":"api_reference/kiara.pipeline.listeners/#kiara.pipeline.listeners.PipelineListener.step_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/listeners.py def step_inputs_changed ( self , event : StepInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Arguments: event: the step input event \"\"\"","title":"step_inputs_changed()"},{"location":"api_reference/kiara.pipeline.listeners/#kiara.pipeline.listeners.PipelineListener.step_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Parameters: Name Type Description Default event StepOutputEvent the step output event required Source code in kiara/pipeline/listeners.py def step_outputs_changed ( self , event : StepOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Arguments: event: the step output event \"\"\"","title":"step_outputs_changed()"},{"location":"api_reference/kiara.pipeline/","text":"kiara.pipeline \u00b6 PipelineValueInfo pydantic-model \u00b6 Convenience wrapper to make the PipelineState json/dict export prettier. id : str pydantic-field required \u00b6 A unique id for this value. is_set : bool pydantic-field required \u00b6 Whether the value is set. is_valid : bool pydantic-field \u00b6 Whether the value is set and valid. metadata : Dict [ str , Any ] pydantic-field \u00b6 Metadata relating to the actual data (size, no. of rows, etc. -- depending on data type). value_schema : ValueSchema pydantic-field required \u00b6 The schema of this value. PipelineValuesInfo pydantic-model \u00b6 Convenience wrapper to make the PipelineState json/dict export prettier. This is basically just a simplified version of the ValueSet class that is using pydantic, in order to make it easy to export to json. values : Dict [ str , kiara . pipeline . __init__ . PipelineValueInfo ] pydantic-field required \u00b6 Field names are keys, and the data as values. StepStatus \u00b6 Enum to describe the state of a workflow. StepValueAddress pydantic-model \u00b6 Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure. alias property readonly \u00b6 An alias string for this address (in the form [step_id].[value_name] ). step_id : str pydantic-field required \u00b6 The id of a step within a pipeline. sub_value : Dict [ str , Any ] pydantic-field \u00b6 A reference to a subitem of a value (e.g. column, list item) value_name : str pydantic-field required \u00b6 The name of the value (output name or pipeline input name). __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/pipeline/__init__.py def __eq__ ( self , other ): if not isinstance ( other , StepValueAddress ): return False return ( self . step_id , self . value_name , self . sub_value ) == ( other . step_id , other . value_name , other . sub_value , ) __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/pipeline/__init__.py def __hash__ ( self ): return hash (( self . step_id , self . value_name , self . sub_value )) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/pipeline/__init__.py def __repr__ ( self ): if self . sub_value : sub_value = f \" sub_value= { self . sub_value } \" else : sub_value = \"\" return f \"StepValueAddres(step_id= { self . step_id } , value_name= { self . value_name }{ sub_value } )\" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/pipeline/__init__.py def __str__ ( self ): return self . __repr__ ()","title":"\u279c\u2007pipeline"},{"location":"api_reference/kiara.pipeline/#kiarapipeline","text":"","title":"kiara.pipeline"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.PipelineValueInfo","text":"Convenience wrapper to make the PipelineState json/dict export prettier.","title":"PipelineValueInfo"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.PipelineValueInfo.id","text":"A unique id for this value.","title":"id"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.PipelineValueInfo.is_set","text":"Whether the value is set.","title":"is_set"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.PipelineValueInfo.is_valid","text":"Whether the value is set and valid.","title":"is_valid"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.PipelineValueInfo.metadata","text":"Metadata relating to the actual data (size, no. of rows, etc. -- depending on data type).","title":"metadata"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.PipelineValueInfo.value_schema","text":"The schema of this value.","title":"value_schema"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.PipelineValuesInfo","text":"Convenience wrapper to make the PipelineState json/dict export prettier. This is basically just a simplified version of the ValueSet class that is using pydantic, in order to make it easy to export to json.","title":"PipelineValuesInfo"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.PipelineValuesInfo.values","text":"Field names are keys, and the data as values.","title":"values"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.StepStatus","text":"Enum to describe the state of a workflow.","title":"StepStatus"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.StepValueAddress","text":"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.","title":"StepValueAddress"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.StepValueAddress.alias","text":"An alias string for this address (in the form [step_id].[value_name] ).","title":"alias"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.StepValueAddress.step_id","text":"The id of a step within a pipeline.","title":"step_id"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.StepValueAddress.sub_value","text":"A reference to a subitem of a value (e.g. column, list item)","title":"sub_value"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.StepValueAddress.value_name","text":"The name of the value (output name or pipeline input name).","title":"value_name"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.StepValueAddress.__eq__","text":"Return self==value. Source code in kiara/pipeline/__init__.py def __eq__ ( self , other ): if not isinstance ( other , StepValueAddress ): return False return ( self . step_id , self . value_name , self . sub_value ) == ( other . step_id , other . value_name , other . sub_value , )","title":"__eq__()"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.StepValueAddress.__hash__","text":"Return hash(self). Source code in kiara/pipeline/__init__.py def __hash__ ( self ): return hash (( self . step_id , self . value_name , self . sub_value ))","title":"__hash__()"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.StepValueAddress.__repr__","text":"Return repr(self). Source code in kiara/pipeline/__init__.py def __repr__ ( self ): if self . sub_value : sub_value = f \" sub_value= { self . sub_value } \" else : sub_value = \"\" return f \"StepValueAddres(step_id= { self . step_id } , value_name= { self . value_name }{ sub_value } )\"","title":"__repr__()"},{"location":"api_reference/kiara.pipeline/#kiara.pipeline.__init__.StepValueAddress.__str__","text":"Return str(self). Source code in kiara/pipeline/__init__.py def __str__ ( self ): return self . __repr__ ()","title":"__str__()"},{"location":"api_reference/kiara.pipeline.module/","text":"kiara.pipeline.module \u00b6 PipelineModule \u00b6 A KiaraModule that contains a collection of interconnected other modules. structure : PipelineStructure property readonly \u00b6 The PipelineStructure of this module. create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/pipeline/module.py def create_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_input_schema create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/pipeline/module.py def create_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_output_schema is_pipeline () classmethod \u00b6 Check whether this module type is a pipeline, or not. Source code in kiara/pipeline/module.py @classmethod def is_pipeline ( cls ) -> bool : return True","title":"\u279c\u2007pipeline.module"},{"location":"api_reference/kiara.pipeline.module/#kiarapipelinemodule","text":"","title":"kiara.pipeline.module"},{"location":"api_reference/kiara.pipeline.module/#kiara.pipeline.module.PipelineModule","text":"A KiaraModule that contains a collection of interconnected other modules.","title":"PipelineModule"},{"location":"api_reference/kiara.pipeline.module/#kiara.pipeline.module.PipelineModule.structure","text":"The PipelineStructure of this module.","title":"structure"},{"location":"api_reference/kiara.pipeline.module/#kiara.pipeline.module.PipelineModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc \": \"[a description of this input]\", \"optional ': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/pipeline/module.py def create_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_input_schema","title":"create_input_schema()"},{"location":"api_reference/kiara.pipeline.module/#kiara.pipeline.module.PipelineModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/pipeline/module.py def create_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_output_schema","title":"create_output_schema()"},{"location":"api_reference/kiara.pipeline.module/#kiara.pipeline.module.PipelineModule.is_pipeline","text":"Check whether this module type is a pipeline, or not. Source code in kiara/pipeline/module.py @classmethod def is_pipeline ( cls ) -> bool : return True","title":"is_pipeline()"},{"location":"api_reference/kiara.pipeline.pipeline/","text":"kiara.pipeline.pipeline \u00b6 Pipeline \u00b6 An instance of a PipelineStructure that holds state for all of the inputs/outputs of the steps within.","title":"\u279c\u2007pipeline.pipeline"},{"location":"api_reference/kiara.pipeline.pipeline/#kiarapipelinepipeline","text":"","title":"kiara.pipeline.pipeline"},{"location":"api_reference/kiara.pipeline.pipeline/#kiara.pipeline.pipeline.Pipeline","text":"An instance of a PipelineStructure that holds state for all of the inputs/outputs of the steps within.","title":"Pipeline"},{"location":"api_reference/kiara.pipeline.structure/","text":"kiara.pipeline.structure \u00b6 PipelineStep pydantic-model \u00b6 A step within a pipeline-structure, includes information about it's connection(s) and other metadata. input_links : Mapping [ str , List [ kiara . pipeline . StepValueAddress ]] pydantic-field \u00b6 The links that connect to inputs of the module. module_config : Mapping [ str , Any ] pydantic-field \u00b6 The module config. module_type : str pydantic-field required \u00b6 The module type. processing_stage : int pydantic-field \u00b6 The stage number this step is executed within the pipeline. required : bool pydantic-field \u00b6 Whether this step is required within the workflow. In some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/pipeline/structure.py def __eq__ ( self , other ): if not isinstance ( other , PipelineStep ): return False return self . _id == other . _id # # TODO: also check whether _kiara obj is equal? # eq = (self.step_id, self.parent_id, self.module, self.processing_stage,) == ( # other.step_id, # other.parent_id, # other.module, # other.processing_stage, # ) # # if not eq: # return False # # hs = DeepHash(self.input_links) # ho = DeepHash(other.input_links) # # return hs[self.input_links] == ho[other.input_links] __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/pipeline/structure.py def __hash__ ( self ): return hash ( self . _id ) # # TODO: also include _kiara obj? # # TODO: figure out whether that can be made to work without deephash # hs = DeepHash(self.input_links) # return hash( # ( # self.step_id, # self.parent_id, # self.module, # self.processing_stage, # hs[self.input_links], # ) # ) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/pipeline/structure.py def __repr__ ( self ): return f \" { self . __class__ . __name__ } (step_id= { self . step_id } module_type= { self . module_type } processing_stage= { self . processing_stage } )\" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/pipeline/structure.py def __str__ ( self ): return f \"step: { self . step_id } (module: { self . module_type } )\" PipelineStructure \u00b6 An object that holds one or several steps, and describes the connections between them.","title":"\u279c\u2007pipeline.structure"},{"location":"api_reference/kiara.pipeline.structure/#kiarapipelinestructure","text":"","title":"kiara.pipeline.structure"},{"location":"api_reference/kiara.pipeline.structure/#kiara.pipeline.structure.PipelineStep","text":"A step within a pipeline-structure, includes information about it's connection(s) and other metadata.","title":"PipelineStep"},{"location":"api_reference/kiara.pipeline.structure/#kiara.pipeline.structure.PipelineStep.input_links","text":"The links that connect to inputs of the module.","title":"input_links"},{"location":"api_reference/kiara.pipeline.structure/#kiara.pipeline.structure.PipelineStep.module_config","text":"The module config.","title":"module_config"},{"location":"api_reference/kiara.pipeline.structure/#kiara.pipeline.structure.PipelineStep.module_type","text":"The module type.","title":"module_type"},{"location":"api_reference/kiara.pipeline.structure/#kiara.pipeline.structure.PipelineStep.processing_stage","text":"The stage number this step is executed within the pipeline.","title":"processing_stage"},{"location":"api_reference/kiara.pipeline.structure/#kiara.pipeline.structure.PipelineStep.required","text":"Whether this step is required within the workflow. In some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required.","title":"required"},{"location":"api_reference/kiara.pipeline.structure/#kiara.pipeline.structure.PipelineStep.__eq__","text":"Return self==value. Source code in kiara/pipeline/structure.py def __eq__ ( self , other ): if not isinstance ( other , PipelineStep ): return False return self . _id == other . _id # # TODO: also check whether _kiara obj is equal? # eq = (self.step_id, self.parent_id, self.module, self.processing_stage,) == ( # other.step_id, # other.parent_id, # other.module, # other.processing_stage, # ) # # if not eq: # return False # # hs = DeepHash(self.input_links) # ho = DeepHash(other.input_links) # # return hs[self.input_links] == ho[other.input_links]","title":"__eq__()"},{"location":"api_reference/kiara.pipeline.structure/#kiara.pipeline.structure.PipelineStep.__hash__","text":"Return hash(self). Source code in kiara/pipeline/structure.py def __hash__ ( self ): return hash ( self . _id ) # # TODO: also include _kiara obj? # # TODO: figure out whether that can be made to work without deephash # hs = DeepHash(self.input_links) # return hash( # ( # self.step_id, # self.parent_id, # self.module, # self.processing_stage, # hs[self.input_links], # ) # )","title":"__hash__()"},{"location":"api_reference/kiara.pipeline.structure/#kiara.pipeline.structure.PipelineStep.__repr__","text":"Return repr(self). Source code in kiara/pipeline/structure.py def __repr__ ( self ): return f \" { self . __class__ . __name__ } (step_id= { self . step_id } module_type= { self . module_type } processing_stage= { self . processing_stage } )\"","title":"__repr__()"},{"location":"api_reference/kiara.pipeline.structure/#kiara.pipeline.structure.PipelineStep.__str__","text":"Return str(self). Source code in kiara/pipeline/structure.py def __str__ ( self ): return f \"step: { self . step_id } (module: { self . module_type } )\"","title":"__str__()"},{"location":"api_reference/kiara.pipeline.structure/#kiara.pipeline.structure.PipelineStructure","text":"An object that holds one or several steps, and describes the connections between them.","title":"PipelineStructure"},{"location":"api_reference/kiara.pipeline.utils/","text":"kiara.pipeline.utils \u00b6","title":"\u279c\u2007pipeline.utils"},{"location":"api_reference/kiara.pipeline.utils/#kiarapipelineutils","text":"","title":"kiara.pipeline.utils"},{"location":"api_reference/kiara.pipeline.values/","text":"kiara.pipeline.values \u00b6 PipelineInputRef pydantic-model \u00b6 An input to a pipeline. connected_inputs : List [ kiara . pipeline . StepValueAddress ] pydantic-field \u00b6 The step inputs that are connected to this pipeline input PipelineOutputRef pydantic-model \u00b6 An output to a pipeline. connected_output : StepValueAddress pydantic-field required \u00b6 Connected step outputs. StepInputRef pydantic-model \u00b6 An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both. connected_outputs : List [ kiara . pipeline . StepValueAddress ] pydantic-field \u00b6 A potential connected list of one or several module outputs. connected_pipeline_input : str pydantic-field \u00b6 A potential pipeline input. step_id : str pydantic-field required \u00b6 The step id. StepOutputRef pydantic-model \u00b6 An output to a step. connected_inputs : List [ kiara . pipeline . StepValueAddress ] pydantic-field \u00b6 The step inputs that are connected to this step output pipeline_output : str pydantic-field \u00b6 The connected pipeline output. step_id : str pydantic-field required \u00b6 The step id. ValueRef pydantic-model \u00b6 An object that holds information about the location of a value within a pipeline (or other structure). Basically, a ValueRef helps the containing object where in its structure the value belongs (for example so it can update dependent other values). A ValueRef object (obviously) does not contain the value itself. There are four different ValueRef type that are relevant for pipelines: kiara.pipeline.values.StepInputRef : an input to a step kiara.pipeline.values.StepOutputRef : an output of a step kiara.pipeline.values.PipelineInputRef : an input to a pipeline kiara.pipeline.values.PipelineOutputRef : an output for a pipeline Several ValueRef objects can target the same value, for example a step output and a connected step input would reference the same Value (in most cases).. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/pipeline/values.py def __eq__ ( self , other ): if not isinstance ( other , self . __class__ ): return False return self . _id == other . _id __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/pipeline/values.py def __hash__ ( self ): return hash ( self . _id ) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/pipeline/values.py def __repr__ ( self ): step_id = \"\" if hasattr ( self , \"step_id\" ): step_id = f \" step_id=' { self . step_id } '\" return f \" { self . __class__ . __name__ } (value_name=' { self . value_name } ' { step_id } )\" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/pipeline/values.py def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . value_name } ( { self . value_schema . type } )\"","title":"\u279c\u2007pipeline.values"},{"location":"api_reference/kiara.pipeline.values/#kiarapipelinevalues","text":"","title":"kiara.pipeline.values"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.PipelineInputRef","text":"An input to a pipeline.","title":"PipelineInputRef"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.PipelineInputRef.connected_inputs","text":"The step inputs that are connected to this pipeline input","title":"connected_inputs"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.PipelineOutputRef","text":"An output to a pipeline.","title":"PipelineOutputRef"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.PipelineOutputRef.connected_output","text":"Connected step outputs.","title":"connected_output"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.StepInputRef","text":"An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both.","title":"StepInputRef"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.StepInputRef.connected_outputs","text":"A potential connected list of one or several module outputs.","title":"connected_outputs"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.StepInputRef.connected_pipeline_input","text":"A potential pipeline input.","title":"connected_pipeline_input"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.StepInputRef.step_id","text":"The step id.","title":"step_id"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.StepOutputRef","text":"An output to a step.","title":"StepOutputRef"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.StepOutputRef.connected_inputs","text":"The step inputs that are connected to this step output","title":"connected_inputs"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.StepOutputRef.pipeline_output","text":"The connected pipeline output.","title":"pipeline_output"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.StepOutputRef.step_id","text":"The step id.","title":"step_id"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.ValueRef","text":"An object that holds information about the location of a value within a pipeline (or other structure). Basically, a ValueRef helps the containing object where in its structure the value belongs (for example so it can update dependent other values). A ValueRef object (obviously) does not contain the value itself. There are four different ValueRef type that are relevant for pipelines: kiara.pipeline.values.StepInputRef : an input to a step kiara.pipeline.values.StepOutputRef : an output of a step kiara.pipeline.values.PipelineInputRef : an input to a pipeline kiara.pipeline.values.PipelineOutputRef : an output for a pipeline Several ValueRef objects can target the same value, for example a step output and a connected step input would reference the same Value (in most cases)..","title":"ValueRef"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.ValueRef.__eq__","text":"Return self==value. Source code in kiara/pipeline/values.py def __eq__ ( self , other ): if not isinstance ( other , self . __class__ ): return False return self . _id == other . _id","title":"__eq__()"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.ValueRef.__hash__","text":"Return hash(self). Source code in kiara/pipeline/values.py def __hash__ ( self ): return hash ( self . _id )","title":"__hash__()"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.ValueRef.__repr__","text":"Return repr(self). Source code in kiara/pipeline/values.py def __repr__ ( self ): step_id = \"\" if hasattr ( self , \"step_id\" ): step_id = f \" step_id=' { self . step_id } '\" return f \" { self . __class__ . __name__ } (value_name=' { self . value_name } ' { step_id } )\"","title":"__repr__()"},{"location":"api_reference/kiara.pipeline.values/#kiara.pipeline.values.ValueRef.__str__","text":"Return str(self). Source code in kiara/pipeline/values.py def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . value_name } ( { self . value_schema . type } )\"","title":"__str__()"},{"location":"api_reference/kiara.processing/","text":"kiara.processing \u00b6 Job pydantic-model \u00b6 error : str pydantic-field \u00b6 Potential error message. finished : datetime pydantic-field \u00b6 When the job was finished. id : str pydantic-field required \u00b6 The id of the job. inputs : PipelineValuesInfo pydantic-field required \u00b6 The input values. job_log : JobLog pydantic-field \u00b6 Details about the job progress. module_config : Dict [ str , Any ] pydantic-field required \u00b6 The module configuration. module_doc : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the module that runs the job. module_type : str pydantic-field required \u00b6 The module type name. outputs : PipelineValuesInfo pydantic-field required \u00b6 The output values. pipeline_id : str pydantic-field required \u00b6 The id of the pipeline this jobs runs for. pipeline_name : str pydantic-field required \u00b6 The name/type of the pipeline. started : datetime pydantic-field \u00b6 When the job was started. status : JobStatus pydantic-field \u00b6 The current status of the job. step_id : str pydantic-field required \u00b6 The id of the step within the pipeline. submitted : datetime pydantic-field \u00b6 When the job was submitted. JobLog pydantic-model \u00b6 log : Dict [ int , kiara . processing . __init__ . LogMessage ] pydantic-field \u00b6 The logs for this job. percent_finished : int pydantic-field \u00b6 Describes how much of the job is finished. A negative number means the module does not support progress tracking. JobStatus \u00b6 An enumeration. LogMessage pydantic-model \u00b6 log_level : int pydantic-field required \u00b6 The log level. msg : str pydantic-field required \u00b6 The log message timestamp : datetime pydantic-field \u00b6 The time the message was logged.","title":"\u279c\u2007processing"},{"location":"api_reference/kiara.processing/#kiaraprocessing","text":"","title":"kiara.processing"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job","text":"","title":"Job"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.error","text":"Potential error message.","title":"error"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.finished","text":"When the job was finished.","title":"finished"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.id","text":"The id of the job.","title":"id"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.inputs","text":"The input values.","title":"inputs"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.job_log","text":"Details about the job progress.","title":"job_log"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.module_config","text":"The module configuration.","title":"module_config"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.module_doc","text":"Documentation for the module that runs the job.","title":"module_doc"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.module_type","text":"The module type name.","title":"module_type"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.outputs","text":"The output values.","title":"outputs"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.pipeline_id","text":"The id of the pipeline this jobs runs for.","title":"pipeline_id"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.pipeline_name","text":"The name/type of the pipeline.","title":"pipeline_name"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.started","text":"When the job was started.","title":"started"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.status","text":"The current status of the job.","title":"status"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.step_id","text":"The id of the step within the pipeline.","title":"step_id"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.Job.submitted","text":"When the job was submitted.","title":"submitted"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.JobLog","text":"","title":"JobLog"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.JobLog.log","text":"The logs for this job.","title":"log"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.JobLog.percent_finished","text":"Describes how much of the job is finished. A negative number means the module does not support progress tracking.","title":"percent_finished"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.JobStatus","text":"An enumeration.","title":"JobStatus"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.LogMessage","text":"","title":"LogMessage"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.LogMessage.log_level","text":"The log level.","title":"log_level"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.LogMessage.msg","text":"The log message","title":"msg"},{"location":"api_reference/kiara.processing/#kiara.processing.__init__.LogMessage.timestamp","text":"The time the message was logged.","title":"timestamp"},{"location":"api_reference/kiara.processing.parallel/","text":"kiara.processing.parallel \u00b6 ThreadPoolProcessorConfig pydantic-model \u00b6 max_workers : int pydantic-field \u00b6 The max mount of workers for the thread pool.","title":"\u279c\u2007processing.parallel"},{"location":"api_reference/kiara.processing.parallel/#kiaraprocessingparallel","text":"","title":"kiara.processing.parallel"},{"location":"api_reference/kiara.processing.parallel/#kiara.processing.parallel.ThreadPoolProcessorConfig","text":"","title":"ThreadPoolProcessorConfig"},{"location":"api_reference/kiara.processing.parallel/#kiara.processing.parallel.ThreadPoolProcessorConfig.max_workers","text":"The max mount of workers for the thread pool.","title":"max_workers"},{"location":"api_reference/kiara.processing.processor/","text":"kiara.processing.processor \u00b6 ModuleProcessor \u00b6 wait_for ( self , * job_ids , * , sync_outputs = True ) \u00b6 Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state. Source code in kiara/processing/processor.py def wait_for ( self , * job_ids : str , sync_outputs : bool = True ): \"\"\"Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state.\"\"\" self . _wait_for ( * job_ids ) if sync_outputs : self . sync_outputs ( * job_ids )","title":"\u279c\u2007processing.processor"},{"location":"api_reference/kiara.processing.processor/#kiaraprocessingprocessor","text":"","title":"kiara.processing.processor"},{"location":"api_reference/kiara.processing.processor/#kiara.processing.processor.ModuleProcessor","text":"","title":"ModuleProcessor"},{"location":"api_reference/kiara.processing.processor/#kiara.processing.processor.ModuleProcessor.wait_for","text":"Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state. Source code in kiara/processing/processor.py def wait_for ( self , * job_ids : str , sync_outputs : bool = True ): \"\"\"Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state.\"\"\" self . _wait_for ( * job_ids ) if sync_outputs : self . sync_outputs ( * job_ids )","title":"wait_for()"},{"location":"api_reference/kiara.processing.synchronous/","text":"kiara.processing.synchronous \u00b6","title":"\u279c\u2007processing.synchronous"},{"location":"api_reference/kiara.processing.synchronous/#kiaraprocessingsynchronous","text":"","title":"kiara.processing.synchronous"},{"location":"api_reference/kiara.rendering.pipeline/","text":"kiara.rendering.pipeline \u00b6","title":"\u279c\u2007rendering.pipeline"},{"location":"api_reference/kiara.rendering.pipeline/#kiararenderingpipeline","text":"","title":"kiara.rendering.pipeline"},{"location":"api_reference/kiara.rendering.workflow/","text":"kiara.rendering.workflow \u00b6","title":"\u279c\u2007rendering.workflow"},{"location":"api_reference/kiara.rendering.workflow/#kiararenderingworkflow","text":"","title":"kiara.rendering.workflow"},{"location":"api_reference/kiara.rendering.workflow.terminal/","text":"kiara.rendering.workflow.terminal \u00b6","title":"\u279c\u2007rendering.workflow.terminal"},{"location":"api_reference/kiara.rendering.workflow.terminal/#kiararenderingworkflowterminal","text":"","title":"kiara.rendering.workflow.terminal"},{"location":"api_reference/kiara.sessions/","text":"kiara.sessions \u00b6","title":"\u279c\u2007sessions"},{"location":"api_reference/kiara.sessions/#kiarasessions","text":"","title":"kiara.sessions"},{"location":"api_reference/kiara.utils.class_loading/","text":"kiara.utils.class_loading \u00b6 find_all_kiara_modules () \u00b6 Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_kiara_modules () -> typing . Dict [ str , typing . Type [ \"KiaraModule\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" from kiara.module import KiaraModule modules = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.modules\" , base_class = KiaraModule , # type: ignore set_id_attribute = \"_module_type_name\" , remove_namespace_tokens = [ \"core.\" ], ) result = {} # need to test this, since I couldn't add an abstract method to the KiaraModule class itself (mypy complained because it is potentially overloaded) for k , cls in modules . items (): if not hasattr ( cls , \"process\" ): msg = f \"Ignoring module class ' { cls } ': no 'process' method.\" if is_debug (): log . warning ( msg ) else : log . debug ( msg ) continue # TODO: check signature of process method if k . startswith ( \"_\" ): tokens = k . split ( \".\" ) if len ( tokens ) == 1 : k = k [ 1 :] else : k = \".\" . join ( tokens [ 1 :]) result [ k ] = cls return result find_all_metadata_schemas () \u00b6 Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_metadata_schemas () -> typing . Dict [ str , typing . Type [ \"MetadataModel\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" return load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.metadata_schemas\" , base_class = MetadataModel , set_id_attribute = \"_metadata_key\" , remove_namespace_tokens = [ \"core.\" ], ) find_all_value_types () \u00b6 Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_value_types () -> typing . Dict [ str , typing . Type [ \"ValueType\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" all_value_types = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.value_types\" , base_class = ValueType , set_id_attribute = \"_value_type_name\" , remove_namespace_tokens = True , ) invalid = [ x for x in all_value_types . keys () if \".\" in x ] if invalid : raise Exception ( f \"Invalid value type name(s), type names can't contain '.': { ', ' . join ( invalid ) } \" ) return all_value_types find_subclasses_under ( base_class , module , prefix = '' , remove_namespace_tokens = None , module_name_func = None ) \u00b6 Find all (non-abstract) subclasses of a base class that live under a module (recursively). Parameters: Name Type Description Default base_class Type[~SUBCLASS_TYPE] the parent class required module Union[str, module] the module to search required prefix Optional[str] a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used '' remove_namespace_tokens Optional[Iterable[str]] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None None Returns: Type Description Mapping[str, Type[~SUBCLASS_TYPE]] a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value Source code in kiara/utils/class_loading.py def find_subclasses_under ( base_class : typing . Type [ SUBCLASS_TYPE ], module : typing . Union [ str , ModuleType ], prefix : typing . Optional [ str ] = \"\" , remove_namespace_tokens : typing . Optional [ typing . Iterable [ str ]] = None , module_name_func : typing . Callable = None , ) -> typing . Mapping [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all (non-abstract) subclasses of a base class that live under a module (recursively). Arguments: base_class: the parent class module: the module to search prefix: a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None Returns: a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value \"\"\" if hasattr ( sys , \"frozen\" ): raise NotImplementedError ( \"Pyinstaller bundling not supported yet.\" ) if isinstance ( module , str ): module = importlib . import_module ( module ) _import_modules_recursively ( module ) subclasses : typing . Iterable [ typing . Type [ SUBCLASS_TYPE ]] = _get_all_subclasses ( base_class ) result = {} for sc in subclasses : if not sc . __module__ . startswith ( module . __name__ ): continue if inspect . isabstract ( sc ): if is_debug (): # import traceback # traceback.print_stack() log . warning ( f \"Ignoring abstract subclass: { sc } \" ) else : log . debug ( f \"Ignoring abstract subclass: { sc } \" ) continue if module_name_func is None : module_name_func = _get_subclass_name name = module_name_func ( sc ) path = sc . __module__ [ len ( module . __name__ ) + 1 :] # noqa if path : full_name = f \" { path } . { name } \" else : full_name = name if prefix is None : prefix = module . __name__ + \".\" if remove_namespace_tokens : for rnt in remove_namespace_tokens : if prefix . startswith ( rnt ): prefix = prefix [ 0 : - len ( rnt )] # noqa if prefix : full_name = f \" { prefix } . { full_name } \" result [ full_name ] = sc return result load_all_subclasses_for_entry_point ( entry_point_name , base_class , set_id_attribute = None , remove_namespace_tokens = None ) \u00b6 Find all subclasses of a base class via package entry points. Parameters: Name Type Description Default entry_point_name str the entry point name to query entries for required base_class Type[~SUBCLASS_TYPE] the base class to look for required set_id_attribute Optional[str] whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set None remove_namespace_tokens Union[Iterable[str], bool] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed None TODO Source code in kiara/utils/class_loading.py def load_all_subclasses_for_entry_point ( entry_point_name : str , base_class : typing . Type [ SUBCLASS_TYPE ], set_id_attribute : typing . Union [ None , str ] = None , remove_namespace_tokens : typing . Union [ typing . Iterable [ str ], bool , None ] = None , ) -> typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all subclasses of a base class via package entry points. Arguments: entry_point_name: the entry point name to query entries for base_class: the base class to look for set_id_attribute: whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed TODO \"\"\" log2 = logging . getLogger ( \"stevedore\" ) out_hdlr = logging . StreamHandler ( sys . stdout ) out_hdlr . setFormatter ( logging . Formatter ( f \" { entry_point_name } plugin search error -> %(message)s\" ) ) out_hdlr . setLevel ( logging . INFO ) log2 . addHandler ( out_hdlr ) log2 . setLevel ( logging . INFO ) log . debug ( f \"Finding { entry_point_name } items from search paths...\" ) mgr = ExtensionManager ( namespace = entry_point_name , invoke_on_load = False , propagate_map_exceptions = True , ) result_entrypoints : typing . Dict [ str , typing . Type ] = {} result_dynamic : typing . Dict [ str , typing . Type ] = {} for plugin in mgr : name = plugin . name if isinstance ( plugin . plugin , type ) and issubclass ( plugin . plugin , base_class ): ep = plugin . entry_point module_cls = ep . load () if set_id_attribute : if hasattr ( module_cls , set_id_attribute ): if not getattr ( module_cls , set_id_attribute ) == name : log . warning ( f \"Item id mismatch for type { entry_point_name } : { getattr ( module_cls , set_id_attribute ) } != { name } , entry point key takes precedence: { name } )\" ) setattr ( module_cls , set_id_attribute , name ) else : setattr ( module_cls , set_id_attribute , name ) result_entrypoints [ name ] = module_cls elif ( isinstance ( plugin . plugin , tuple ) and len ( plugin . plugin ) >= 1 and callable ( plugin . plugin [ 0 ]) ) or callable ( plugin . plugin ): modules = _callable_wrapper ( plugin . plugin ) for k , v in modules . items (): _name = f \" { name } . { k } \" if _name in result_dynamic . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { _name } \" ) result_dynamic [ _name ] = v else : raise Exception ( f \"Can't load subclasses for entry point { entry_point_name } and base class { base_class } : invalid plugin type { type ( plugin . plugin ) } \" ) for k , v in result_dynamic . items (): if k in result_entrypoints . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { k } \" ) result_entrypoints [ k ] = v result : typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]] = {} for k , v in result_entrypoints . items (): if remove_namespace_tokens : if remove_namespace_tokens is True : k = k . split ( \".\" )[ - 1 ] elif isinstance ( remove_namespace_tokens , typing . Iterable ): for rnt in remove_namespace_tokens : if k . startswith ( rnt ): k = k [ len ( rnt ) :] # noqa if k in result . keys (): raise Exception ( f \"Duplicate item name for base class { base_class } : { k } \" ) result [ k ] = v return result","title":"\u279c\u2007utils.class_loading"},{"location":"api_reference/kiara.utils.class_loading/#kiarautilsclass_loading","text":"","title":"kiara.utils.class_loading"},{"location":"api_reference/kiara.utils.class_loading/#kiara.utils.class_loading.find_all_kiara_modules","text":"Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_kiara_modules () -> typing . Dict [ str , typing . Type [ \"KiaraModule\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" from kiara.module import KiaraModule modules = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.modules\" , base_class = KiaraModule , # type: ignore set_id_attribute = \"_module_type_name\" , remove_namespace_tokens = [ \"core.\" ], ) result = {} # need to test this, since I couldn't add an abstract method to the KiaraModule class itself (mypy complained because it is potentially overloaded) for k , cls in modules . items (): if not hasattr ( cls , \"process\" ): msg = f \"Ignoring module class ' { cls } ': no 'process' method.\" if is_debug (): log . warning ( msg ) else : log . debug ( msg ) continue # TODO: check signature of process method if k . startswith ( \"_\" ): tokens = k . split ( \".\" ) if len ( tokens ) == 1 : k = k [ 1 :] else : k = \".\" . join ( tokens [ 1 :]) result [ k ] = cls return result","title":"find_all_kiara_modules()"},{"location":"api_reference/kiara.utils.class_loading/#kiara.utils.class_loading.find_all_metadata_schemas","text":"Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_metadata_schemas () -> typing . Dict [ str , typing . Type [ \"MetadataModel\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" return load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.metadata_schemas\" , base_class = MetadataModel , set_id_attribute = \"_metadata_key\" , remove_namespace_tokens = [ \"core.\" ], )","title":"find_all_metadata_schemas()"},{"location":"api_reference/kiara.utils.class_loading/#kiara.utils.class_loading.find_all_value_types","text":"Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_value_types () -> typing . Dict [ str , typing . Type [ \"ValueType\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" all_value_types = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.value_types\" , base_class = ValueType , set_id_attribute = \"_value_type_name\" , remove_namespace_tokens = True , ) invalid = [ x for x in all_value_types . keys () if \".\" in x ] if invalid : raise Exception ( f \"Invalid value type name(s), type names can't contain '.': { ', ' . join ( invalid ) } \" ) return all_value_types","title":"find_all_value_types()"},{"location":"api_reference/kiara.utils.class_loading/#kiara.utils.class_loading.find_subclasses_under","text":"Find all (non-abstract) subclasses of a base class that live under a module (recursively). Parameters: Name Type Description Default base_class Type[~SUBCLASS_TYPE] the parent class required module Union[str, module] the module to search required prefix Optional[str] a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used '' remove_namespace_tokens Optional[Iterable[str]] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None None Returns: Type Description Mapping[str, Type[~SUBCLASS_TYPE]] a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value Source code in kiara/utils/class_loading.py def find_subclasses_under ( base_class : typing . Type [ SUBCLASS_TYPE ], module : typing . Union [ str , ModuleType ], prefix : typing . Optional [ str ] = \"\" , remove_namespace_tokens : typing . Optional [ typing . Iterable [ str ]] = None , module_name_func : typing . Callable = None , ) -> typing . Mapping [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all (non-abstract) subclasses of a base class that live under a module (recursively). Arguments: base_class: the parent class module: the module to search prefix: a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None Returns: a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value \"\"\" if hasattr ( sys , \"frozen\" ): raise NotImplementedError ( \"Pyinstaller bundling not supported yet.\" ) if isinstance ( module , str ): module = importlib . import_module ( module ) _import_modules_recursively ( module ) subclasses : typing . Iterable [ typing . Type [ SUBCLASS_TYPE ]] = _get_all_subclasses ( base_class ) result = {} for sc in subclasses : if not sc . __module__ . startswith ( module . __name__ ): continue if inspect . isabstract ( sc ): if is_debug (): # import traceback # traceback.print_stack() log . warning ( f \"Ignoring abstract subclass: { sc } \" ) else : log . debug ( f \"Ignoring abstract subclass: { sc } \" ) continue if module_name_func is None : module_name_func = _get_subclass_name name = module_name_func ( sc ) path = sc . __module__ [ len ( module . __name__ ) + 1 :] # noqa if path : full_name = f \" { path } . { name } \" else : full_name = name if prefix is None : prefix = module . __name__ + \".\" if remove_namespace_tokens : for rnt in remove_namespace_tokens : if prefix . startswith ( rnt ): prefix = prefix [ 0 : - len ( rnt )] # noqa if prefix : full_name = f \" { prefix } . { full_name } \" result [ full_name ] = sc return result","title":"find_subclasses_under()"},{"location":"api_reference/kiara.utils.class_loading/#kiara.utils.class_loading.load_all_subclasses_for_entry_point","text":"Find all subclasses of a base class via package entry points. Parameters: Name Type Description Default entry_point_name str the entry point name to query entries for required base_class Type[~SUBCLASS_TYPE] the base class to look for required set_id_attribute Optional[str] whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set None remove_namespace_tokens Union[Iterable[str], bool] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed None TODO Source code in kiara/utils/class_loading.py def load_all_subclasses_for_entry_point ( entry_point_name : str , base_class : typing . Type [ SUBCLASS_TYPE ], set_id_attribute : typing . Union [ None , str ] = None , remove_namespace_tokens : typing . Union [ typing . Iterable [ str ], bool , None ] = None , ) -> typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all subclasses of a base class via package entry points. Arguments: entry_point_name: the entry point name to query entries for base_class: the base class to look for set_id_attribute: whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed TODO \"\"\" log2 = logging . getLogger ( \"stevedore\" ) out_hdlr = logging . StreamHandler ( sys . stdout ) out_hdlr . setFormatter ( logging . Formatter ( f \" { entry_point_name } plugin search error -> %(message)s\" ) ) out_hdlr . setLevel ( logging . INFO ) log2 . addHandler ( out_hdlr ) log2 . setLevel ( logging . INFO ) log . debug ( f \"Finding { entry_point_name } items from search paths...\" ) mgr = ExtensionManager ( namespace = entry_point_name , invoke_on_load = False , propagate_map_exceptions = True , ) result_entrypoints : typing . Dict [ str , typing . Type ] = {} result_dynamic : typing . Dict [ str , typing . Type ] = {} for plugin in mgr : name = plugin . name if isinstance ( plugin . plugin , type ) and issubclass ( plugin . plugin , base_class ): ep = plugin . entry_point module_cls = ep . load () if set_id_attribute : if hasattr ( module_cls , set_id_attribute ): if not getattr ( module_cls , set_id_attribute ) == name : log . warning ( f \"Item id mismatch for type { entry_point_name } : { getattr ( module_cls , set_id_attribute ) } != { name } , entry point key takes precedence: { name } )\" ) setattr ( module_cls , set_id_attribute , name ) else : setattr ( module_cls , set_id_attribute , name ) result_entrypoints [ name ] = module_cls elif ( isinstance ( plugin . plugin , tuple ) and len ( plugin . plugin ) >= 1 and callable ( plugin . plugin [ 0 ]) ) or callable ( plugin . plugin ): modules = _callable_wrapper ( plugin . plugin ) for k , v in modules . items (): _name = f \" { name } . { k } \" if _name in result_dynamic . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { _name } \" ) result_dynamic [ _name ] = v else : raise Exception ( f \"Can't load subclasses for entry point { entry_point_name } and base class { base_class } : invalid plugin type { type ( plugin . plugin ) } \" ) for k , v in result_dynamic . items (): if k in result_entrypoints . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { k } \" ) result_entrypoints [ k ] = v result : typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]] = {} for k , v in result_entrypoints . items (): if remove_namespace_tokens : if remove_namespace_tokens is True : k = k . split ( \".\" )[ - 1 ] elif isinstance ( remove_namespace_tokens , typing . Iterable ): for rnt in remove_namespace_tokens : if k . startswith ( rnt ): k = k [ len ( rnt ) :] # noqa if k in result . keys (): raise Exception ( f \"Duplicate item name for base class { base_class } : { k } \" ) result [ k ] = v return result","title":"load_all_subclasses_for_entry_point()"},{"location":"api_reference/kiara.utils.doc/","text":"kiara.utils.doc \u00b6","title":"\u279c\u2007utils.doc"},{"location":"api_reference/kiara.utils.doc/#kiarautilsdoc","text":"","title":"kiara.utils.doc"},{"location":"api_reference/kiara.utils.global_metadata/","text":"kiara.utils.global_metadata \u00b6","title":"\u279c\u2007utils.global_metadata"},{"location":"api_reference/kiara.utils.global_metadata/#kiarautilsglobal_metadata","text":"","title":"kiara.utils.global_metadata"},{"location":"api_reference/kiara.utils.jupyter/","text":"kiara.utils.jupyter \u00b6","title":"\u279c\u2007utils.jupyter"},{"location":"api_reference/kiara.utils.jupyter/#kiarautilsjupyter","text":"","title":"kiara.utils.jupyter"},{"location":"api_reference/kiara.utils/","text":"kiara.utils \u00b6 check_valid_field_names ( * field_names ) \u00b6 Check whether the provided field names are all valid. Returns: Type Description List[str] an iterable of strings with invalid field names Source code in kiara/utils/__init__.py def check_valid_field_names ( * field_names ) -> typing . List [ str ]: \"\"\"Check whether the provided field names are all valid. Returns: an iterable of strings with invalid field names \"\"\" return [ x for x in field_names if x in INVALID_VALUE_NAMES or x . startswith ( \"_\" )] get_auto_workflow_alias ( module_type , use_incremental_ids = False ) \u00b6 Return an id for a workflow obj of a provided module class. If 'use_incremental_ids' is set to True, a unique id is returned. Parameters: Name Type Description Default module_type str the name of the module type required use_incremental_ids bool whether to return a unique (incremental) id False Returns: Type Description str str: a module id Source code in kiara/utils/__init__.py def get_auto_workflow_alias ( module_type : str , use_incremental_ids : bool = False ) -> str : \"\"\"Return an id for a workflow obj of a provided module class. If 'use_incremental_ids' is set to True, a unique id is returned. Args: module_type (str): the name of the module type use_incremental_ids (bool): whether to return a unique (incremental) id Returns: str: a module id \"\"\" if not use_incremental_ids : return module_type nr = _AUTO_MODULE_ID . setdefault ( module_type , 0 ) _AUTO_MODULE_ID [ module_type ] = nr + 1 return f \" { module_type } _ { nr } \"","title":"\u279c\u2007utils"},{"location":"api_reference/kiara.utils/#kiarautils","text":"","title":"kiara.utils"},{"location":"api_reference/kiara.utils/#kiara.utils.__init__.check_valid_field_names","text":"Check whether the provided field names are all valid. Returns: Type Description List[str] an iterable of strings with invalid field names Source code in kiara/utils/__init__.py def check_valid_field_names ( * field_names ) -> typing . List [ str ]: \"\"\"Check whether the provided field names are all valid. Returns: an iterable of strings with invalid field names \"\"\" return [ x for x in field_names if x in INVALID_VALUE_NAMES or x . startswith ( \"_\" )]","title":"check_valid_field_names()"},{"location":"api_reference/kiara.utils/#kiara.utils.__init__.get_auto_workflow_alias","text":"Return an id for a workflow obj of a provided module class. If 'use_incremental_ids' is set to True, a unique id is returned. Parameters: Name Type Description Default module_type str the name of the module type required use_incremental_ids bool whether to return a unique (incremental) id False Returns: Type Description str str: a module id Source code in kiara/utils/__init__.py def get_auto_workflow_alias ( module_type : str , use_incremental_ids : bool = False ) -> str : \"\"\"Return an id for a workflow obj of a provided module class. If 'use_incremental_ids' is set to True, a unique id is returned. Args: module_type (str): the name of the module type use_incremental_ids (bool): whether to return a unique (incremental) id Returns: str: a module id \"\"\" if not use_incremental_ids : return module_type nr = _AUTO_MODULE_ID . setdefault ( module_type , 0 ) _AUTO_MODULE_ID [ module_type ] = nr + 1 return f \" { module_type } _ { nr } \"","title":"get_auto_workflow_alias()"},{"location":"api_reference/kiara.utils.modules/","text":"kiara.utils.modules \u00b6","title":"\u279c\u2007utils.modules"},{"location":"api_reference/kiara.utils.modules/#kiarautilsmodules","text":"","title":"kiara.utils.modules"},{"location":"api_reference/kiara.utils.output/","text":"kiara.utils.output \u00b6 OutputDetails pydantic-model \u00b6 config : Dict [ str , Any ] pydantic-field \u00b6 Output configuration. format : str pydantic-field required \u00b6 The output format. target : str pydantic-field required \u00b6 The output target.","title":"\u279c\u2007utils.output"},{"location":"api_reference/kiara.utils.output/#kiarautilsoutput","text":"","title":"kiara.utils.output"},{"location":"api_reference/kiara.utils.output/#kiara.utils.output.OutputDetails","text":"","title":"OutputDetails"},{"location":"api_reference/kiara.utils.output/#kiara.utils.output.OutputDetails.config","text":"Output configuration.","title":"config"},{"location":"api_reference/kiara.utils.output/#kiara.utils.output.OutputDetails.format","text":"The output format.","title":"format"},{"location":"api_reference/kiara.utils.output/#kiara.utils.output.OutputDetails.target","text":"The output target.","title":"target"},{"location":"api_reference/kiara.workflow.kiara_workflow/","text":"kiara.workflow.kiara_workflow \u00b6 KiaraWorkflow \u00b6 A thin wrapper class around a PipelineModule , mostly handling initialization from simplified configuration data.","title":"\u279c\u2007workflow.kiara_workflow"},{"location":"api_reference/kiara.workflow.kiara_workflow/#kiaraworkflowkiara_workflow","text":"","title":"kiara.workflow.kiara_workflow"},{"location":"api_reference/kiara.workflow.kiara_workflow/#kiara.workflow.kiara_workflow.KiaraWorkflow","text":"A thin wrapper class around a PipelineModule , mostly handling initialization from simplified configuration data.","title":"KiaraWorkflow"},{"location":"api_reference/kiara.workflow.wrapped/","text":"kiara.workflow.wrapped \u00b6","title":"\u279c\u2007workflow.wrapped"},{"location":"api_reference/kiara.workflow.wrapped/#kiaraworkflowwrapped","text":"","title":"kiara.workflow.wrapped"},{"location":"architecture/","text":"Architecture documents \u00b6 This section contains architecture-related documents for the Kiara project. Not all of those might be up-to-date, but they should help to understand certain design decisions, and why they were made.","title":"Overview"},{"location":"architecture/#architecture-documents","text":"This section contains architecture-related documents for the Kiara project. Not all of those might be up-to-date, but they should help to understand certain design decisions, and why they were made.","title":"Architecture documents"},{"location":"architecture/assumptions/","text":"Assumptions & considerations \u00b6 Core assumptions \u00b6 I consider the following assumptions a given. They are not fuelled by user stories, but are the 'minimal' requirements that emerged after initially presenting the 'open questions', and in other discussions with Sean and the team. If any of those assumptions are wrong, some of the conclusions below will have to be adjusted. our (only) target audience (for now) are digital historians (and maybe also other digital humanity researchers) who can't code themselves the most important outcome of our project is for our target audience to be able to execute workflows in order to explore, explain, transform or augment their data we want the creation of workflows to be as easy and frictionless as possible, although not at the expense of end-user usability we want our product to be used by all DH researchers around the word, independent of their affiliation(s) q- collaboration/sharing of data is not a priority, most of our target audience are either individuals, sometimes small teams (sharing of results and sharing of workflows are different issues, and not included in this assumption) Considerations around adoption \u00b6 One way to look at how to prioritize and implement some of our user stories is through the lens of ease-of-adoption: which characteristics make our application more likely to be adopted, by a larger group of researchers? Those ones are obvious (at least to me) -- in no particular order: ease of workflow use ease of file-management use ease of installation (if there is one involved) whether there is a login/account creation requirement how well it integrates and plays with tools researchers already use day to day provides relevant (to them) workflows the cheaper to use the better (free/monthly cost/pay-per-usage) stability / reliability performance (most importantly on the compute side, but also UI) how easy it is to create workflows, and what skills are necessary to do that (easier creation -> more workflows) whether and how easy it will be to share, re-use and adapt workflows (different to sharing data)","title":"Assumptions"},{"location":"architecture/assumptions/#assumptions-considerations","text":"","title":"Assumptions &amp; considerations"},{"location":"architecture/assumptions/#core-assumptions","text":"I consider the following assumptions a given. They are not fuelled by user stories, but are the 'minimal' requirements that emerged after initially presenting the 'open questions', and in other discussions with Sean and the team. If any of those assumptions are wrong, some of the conclusions below will have to be adjusted. our (only) target audience (for now) are digital historians (and maybe also other digital humanity researchers) who can't code themselves the most important outcome of our project is for our target audience to be able to execute workflows in order to explore, explain, transform or augment their data we want the creation of workflows to be as easy and frictionless as possible, although not at the expense of end-user usability we want our product to be used by all DH researchers around the word, independent of their affiliation(s) q- collaboration/sharing of data is not a priority, most of our target audience are either individuals, sometimes small teams (sharing of results and sharing of workflows are different issues, and not included in this assumption)","title":"Core assumptions"},{"location":"architecture/assumptions/#considerations-around-adoption","text":"One way to look at how to prioritize and implement some of our user stories is through the lens of ease-of-adoption: which characteristics make our application more likely to be adopted, by a larger group of researchers? Those ones are obvious (at least to me) -- in no particular order: ease of workflow use ease of file-management use ease of installation (if there is one involved) whether there is a login/account creation requirement how well it integrates and plays with tools researchers already use day to day provides relevant (to them) workflows the cheaper to use the better (free/monthly cost/pay-per-usage) stability / reliability performance (most importantly on the compute side, but also UI) how easy it is to create workflows, and what skills are necessary to do that (easier creation -> more workflows) whether and how easy it will be to share, re-use and adapt workflows (different to sharing data)","title":"Considerations around adoption"},{"location":"architecture/decisions/","text":"Decisions \u00b6 This page lists a few of the main decisions that were taken, what the considerations around them were, their impact, as well as why they were made. Supporting two sorts of modules: 'core', and 'pipeline' modules \u00b6 When starting to write code, we didn't have yet many examples of modules and how specific/broad they would be in their utility. I think we should have done a better job gathering those and coming up with a set of modules that would be sufficient for our first 10 or so workflows before writing any code, but alas, we didn't. One way I saw to lower the risk of us implementing us ourselves into a corner was to make our modules as flexible, re-usable and 'combinable' as possible. And the best way I could think of to do that was to have a very simple interface for each module (each module has a defined set of input/output fields, and one main function to transform inputs into outputs), and to allow several modules to be combined into a 'new' module that has those same characteristics/interface. Advantages: - easy to declaratively create re-usable modules with just json/yaml - easy to re-use modules/other pipelines for UI-specific subtasks (like data previews/querying) - in most cases, the higher-level backend code does not know about core- and pipeline- modules, since they can be treated the same Disadvantages: - the lower-level backend code needs to implement two different ways to assemble/create modules, depending on whether it's a core-module, or a pipeline Use of subclassing in general \u00b6 Across kiara , I'm using subclassing and inheritance in some instances, esp. important base classes are KiaraModule and PipelineController . I'm aware that this is considered bad practice in a lot of cases, and I have read my share of opinions and thoughts about the matter. In principle I agree, and I'm not 100% happy with every decision I made (or thought I had to made) in this area for kiara , but overall I decided to allow for some inheritance and class-based code sharing in the code, partly to speed up my implementation work, partly because I thought some of the disadvantages (like having to search base classes for some function definitions) are not as bad in a certain context than in others. I can totally see how others would disagree here, though, and there are a few things I would like to change/improve later on, if I find the time. One of the main advantages I get out of using inheritance is being able to automatically discover subclasses of a base class. This is done for multiple of those, like: KiaraModule ValueType MetadataModel Using auto-discovery in a Python virtualenv removes the need for workflow/module developers to understand Python packaging and entry_points. I've written a project template that sets up all the basics, and developers focus on creating new classes (basically plugins), with no extra registration work to be done. I hope this will aid adoption. And that I've managed to design those base classes well enough so that they are easy to use and understand, so that some of the main drawbacks of subclassing won't matter all that much. Requiring to subclass an abstract base class when creating a module \u00b6 The main class that uses a subclassing-strategy is KiaraModule . At it's heart, it's basically just a wrapper around a pure function, with some utility methods describing it's input and output. One reason I decided to not just create a decorator that wraps any function was the need to be able to describe the input the function takes, and the output it produces in a stricter way than would have been possible with just type hints. Another reason is that this way it is possible to add configuration to a module object, which should make module code much more flexible and re-usable, and developers do not have to implement separate modules for just slightly different use-cases. This design decision does not prevent to allow for more 'loose' implementations of a module, like the above mentioned function with a decorator. Those would be dynamically converted into a KiaraModule subclass/object, with potential downsides of not being able to version control it properly (or as easliy). The point is, though, that the default way of doing things will give us the best guarantees (and metadata). Advantages: - relatively easy to manage 'plugin-like' architecture, discovery of modules - being able to describe module input/output fields in detail - module versioning: by requiring the subclassing of a base class, and also having to add modules as entry_points, it will be possible describe exactly which version of the module was used in a workflow (as well as which version of the base class) Disadvantages: - more abstraction layers than strictly necessary - other, usual disadvantages associated with subclassing/inheritance Separating data from the Python objects that describe them / Data registry \u00b6 TBD Advantages: - efficiency, option to save on memory and IO - (hopefully) decrease of complexity for non trivial scenarios like multi-process or remote job execution Disadvantages: - extra level of abstraction - increase in complexity (at least for simple use-cases)","title":"Decisions"},{"location":"architecture/decisions/#decisions","text":"This page lists a few of the main decisions that were taken, what the considerations around them were, their impact, as well as why they were made.","title":"Decisions"},{"location":"architecture/decisions/#supporting-two-sorts-of-modules-core-and-pipeline-modules","text":"When starting to write code, we didn't have yet many examples of modules and how specific/broad they would be in their utility. I think we should have done a better job gathering those and coming up with a set of modules that would be sufficient for our first 10 or so workflows before writing any code, but alas, we didn't. One way I saw to lower the risk of us implementing us ourselves into a corner was to make our modules as flexible, re-usable and 'combinable' as possible. And the best way I could think of to do that was to have a very simple interface for each module (each module has a defined set of input/output fields, and one main function to transform inputs into outputs), and to allow several modules to be combined into a 'new' module that has those same characteristics/interface. Advantages: - easy to declaratively create re-usable modules with just json/yaml - easy to re-use modules/other pipelines for UI-specific subtasks (like data previews/querying) - in most cases, the higher-level backend code does not know about core- and pipeline- modules, since they can be treated the same Disadvantages: - the lower-level backend code needs to implement two different ways to assemble/create modules, depending on whether it's a core-module, or a pipeline","title":"Supporting two sorts of modules: 'core', and 'pipeline' modules"},{"location":"architecture/decisions/#use-of-subclassing-in-general","text":"Across kiara , I'm using subclassing and inheritance in some instances, esp. important base classes are KiaraModule and PipelineController . I'm aware that this is considered bad practice in a lot of cases, and I have read my share of opinions and thoughts about the matter. In principle I agree, and I'm not 100% happy with every decision I made (or thought I had to made) in this area for kiara , but overall I decided to allow for some inheritance and class-based code sharing in the code, partly to speed up my implementation work, partly because I thought some of the disadvantages (like having to search base classes for some function definitions) are not as bad in a certain context than in others. I can totally see how others would disagree here, though, and there are a few things I would like to change/improve later on, if I find the time. One of the main advantages I get out of using inheritance is being able to automatically discover subclasses of a base class. This is done for multiple of those, like: KiaraModule ValueType MetadataModel Using auto-discovery in a Python virtualenv removes the need for workflow/module developers to understand Python packaging and entry_points. I've written a project template that sets up all the basics, and developers focus on creating new classes (basically plugins), with no extra registration work to be done. I hope this will aid adoption. And that I've managed to design those base classes well enough so that they are easy to use and understand, so that some of the main drawbacks of subclassing won't matter all that much.","title":"Use of subclassing in general"},{"location":"architecture/decisions/#requiring-to-subclass-an-abstract-base-class-when-creating-a-module","text":"The main class that uses a subclassing-strategy is KiaraModule . At it's heart, it's basically just a wrapper around a pure function, with some utility methods describing it's input and output. One reason I decided to not just create a decorator that wraps any function was the need to be able to describe the input the function takes, and the output it produces in a stricter way than would have been possible with just type hints. Another reason is that this way it is possible to add configuration to a module object, which should make module code much more flexible and re-usable, and developers do not have to implement separate modules for just slightly different use-cases. This design decision does not prevent to allow for more 'loose' implementations of a module, like the above mentioned function with a decorator. Those would be dynamically converted into a KiaraModule subclass/object, with potential downsides of not being able to version control it properly (or as easliy). The point is, though, that the default way of doing things will give us the best guarantees (and metadata). Advantages: - relatively easy to manage 'plugin-like' architecture, discovery of modules - being able to describe module input/output fields in detail - module versioning: by requiring the subclassing of a base class, and also having to add modules as entry_points, it will be possible describe exactly which version of the module was used in a workflow (as well as which version of the base class) Disadvantages: - more abstraction layers than strictly necessary - other, usual disadvantages associated with subclassing/inheritance","title":"Requiring to subclass an abstract base class when creating a module"},{"location":"architecture/decisions/#separating-data-from-the-python-objects-that-describe-them-data-registry","text":"TBD Advantages: - efficiency, option to save on memory and IO - (hopefully) decrease of complexity for non trivial scenarios like multi-process or remote job execution Disadvantages: - extra level of abstraction - increase in complexity (at least for simple use-cases)","title":"Separating data from the Python objects that describe them / Data registry"},{"location":"architecture/metadata/","text":"Metadata \u00b6 Metadata is more important in research than in other fields. Metadata can be used to, among other things, track provenance of data, describe authorship, time of creation, location of creation, describing the 'shape' of data (schemas, etc.). In some cases it's not easy to determine what's data and what's metadata. Sometimes metadata becomes data (\"One persons metadata...\"). Handling metadata is difficult, and it regularly gets lost somewhere in the process. Creating metadata in the first place can be very time-consuming, I would wager that is more true in the digital humanities than in the harder sciences. With the growing popularity of the open data movement, people are getting more aware of the importance of metadata, and there is a growing infrastructure and services around all of this (DOIs, RDF, 'linked data', Dublin core, ...). None of it is easy or intuitive to use, but I guess that's just the nature of the beast. I think it is safe to say that whatever we come up with has to be able to create and handle metadata in some way or form, and personally, I think we should 'bake' metadata handling in from the beginning. Looking at the user-stories it's quite clear that this an important topic. How exactly that will look, I think there is some leeway, but all architecture proposals should at least include some indication on how this would be handled. Schema information \u00b6 One important piece of metadata is often schema information: what exactly is the shape of the data, how can I read it? In some cases this can be inferred from the data easily, sometimes it's even obvious. But often that is not the case at all, which makes things like creating generic data exploration tools very hard, if not impossible. We would have, if we choose to create and attach it, all that information available, always, which would mean it would be easy to create generic, peripheral tools like a generic data explorer. It will, of course, also make it easier to re-use such data in other workflows, because users would not have to explicitly specify what their data is; we could infer that from the attached schema. Workflow metadata \u00b6 One thing that is specific to our application is that we have full control over every part of the data-flow. So, we can attach metadata of all inputs and previous steps to each result (or intermediate result) along the way. Which is quite an unique opportunity; this is often not available at all, or has to be done manually by the researcher. There is a lot that can be done with such annotated (result-)data. For example, each data set can include pointers to all the original data that was involved in creating it (or it could even include that data itself), as well as a description of all the transformation steps it went through. This means that one could potentially create a full visual representation of what happened to the data since it was created, just by looking at the attached metadata. This is usually impossible, because there is never a sort of 'unbroken cold-chain' of metadata available. Of course, this would also help with reproducability and related issues. This possibility is something I'm particularly excited about, even though it does not directly appear in any of our user stories (so would not be a core requirement). But it's one of the things I would have liked to have available often in the past.","title":"Metadata"},{"location":"architecture/metadata/#metadata","text":"Metadata is more important in research than in other fields. Metadata can be used to, among other things, track provenance of data, describe authorship, time of creation, location of creation, describing the 'shape' of data (schemas, etc.). In some cases it's not easy to determine what's data and what's metadata. Sometimes metadata becomes data (\"One persons metadata...\"). Handling metadata is difficult, and it regularly gets lost somewhere in the process. Creating metadata in the first place can be very time-consuming, I would wager that is more true in the digital humanities than in the harder sciences. With the growing popularity of the open data movement, people are getting more aware of the importance of metadata, and there is a growing infrastructure and services around all of this (DOIs, RDF, 'linked data', Dublin core, ...). None of it is easy or intuitive to use, but I guess that's just the nature of the beast. I think it is safe to say that whatever we come up with has to be able to create and handle metadata in some way or form, and personally, I think we should 'bake' metadata handling in from the beginning. Looking at the user-stories it's quite clear that this an important topic. How exactly that will look, I think there is some leeway, but all architecture proposals should at least include some indication on how this would be handled.","title":"Metadata"},{"location":"architecture/metadata/#schema-information","text":"One important piece of metadata is often schema information: what exactly is the shape of the data, how can I read it? In some cases this can be inferred from the data easily, sometimes it's even obvious. But often that is not the case at all, which makes things like creating generic data exploration tools very hard, if not impossible. We would have, if we choose to create and attach it, all that information available, always, which would mean it would be easy to create generic, peripheral tools like a generic data explorer. It will, of course, also make it easier to re-use such data in other workflows, because users would not have to explicitly specify what their data is; we could infer that from the attached schema.","title":"Schema information"},{"location":"architecture/metadata/#workflow-metadata","text":"One thing that is specific to our application is that we have full control over every part of the data-flow. So, we can attach metadata of all inputs and previous steps to each result (or intermediate result) along the way. Which is quite an unique opportunity; this is often not available at all, or has to be done manually by the researcher. There is a lot that can be done with such annotated (result-)data. For example, each data set can include pointers to all the original data that was involved in creating it (or it could even include that data itself), as well as a description of all the transformation steps it went through. This means that one could potentially create a full visual representation of what happened to the data since it was created, just by looking at the attached metadata. This is usually impossible, because there is never a sort of 'unbroken cold-chain' of metadata available. Of course, this would also help with reproducability and related issues. This possibility is something I'm particularly excited about, even though it does not directly appear in any of our user stories (so would not be a core requirement). But it's one of the things I would have liked to have available often in the past.","title":"Workflow metadata"},{"location":"architecture/data/","text":"From looking at the user stories, and after listening to the interviews Lorella conducted and also considering my own personal experience in eResearch, I think its save to say that the central topic we are dealing with is data. Without data, none of the other topics (workflows, visualisation, metadata...) would even exist. Because of its central nature I want to lay out the different forms it comes in, and which characteristics of it are important in our context. What's data? \u00b6 Data is created from sources. Sources come in different forms (analog, digital) and can be anything from handwritten documents in an archive to a twitter feed. Photos, cave-paintings, what have you. I'm not websters dictionary, but I think one usable working definition of data could be a 'materialized source', in our context 'materialized source in digital form'. From here on out I'll assume we are talking about 'digital' data when I mention data. One thing I'll leave out in this discussion is what is usually called 'dirty data' in data engineering, although it is an important topic. Most of the issues there map fairly well to the structured/unstructured thing below. There are a few differences, but in the interest of clarity let's ignore those for now... Structured data / Data transformations \u00b6 Important for us is that data can come in two different formats: unstructured, and, who'd have guessed... structured. The same piece of data can be theoretically expressed in structured as well as unstructured form: the meaning to a researcher would be 100% the same, but the ways to handle, digest and operate with the data can differ, and in most scenarios adding structure opens up possibilities to work with the data that weren't there before. In my head I call those two forms 'useless', and 'useful' data, but researcher usually get a bit agitated when I do, so I have learned to not do that in public anymore. For researchers, the most (and arguably only) important feature of 'structure' is that it enables them to do more with the data they already possess. By means of computation. I think it's fair to say that only structured data can be used in a meaningful way in a computational context. With the exception that unstructured data is useful input to create structured data. One more thing to mention is that the line between structured and un-structured is sometimes hard to draw, and can depend entirely on context. \"One persons structured data is another persons unstructured data.\", something like that. In addition, in some instances unstructured data can be converted to structured data trivially, meaning without much effort or any user-interaction. I'd argue we can consider those sorts of datasets basically 'structured'. Example \u00b6 Lets use a simple example to illustrate all that: a digital image of a document . Depending on what you are interested in, such an image might already be structured data. For example it could contain geo-tags, and a timestamp, which are both digitally readable. If you want to visualize on a map where a document is from, you can do that instantly. Structured data, yay! Similarly, if you are interested in the color of the paper of the document (ok, I'm stretching my argument here as this seems fairly unlikely, but this is really just to illustrate...), you might get the color histogram of the image (which is trivial to extract, but needs some batch-computation), and for your purposes you would also consider the image file structured data. Now, if you are interested in the text content of the document, things get more interesting. You will have to jump through some hoops, and feed the image file to an OCR pipeline that will spit out a text file for example. The data itself would still be the same, but now computers can access not only some probably irrelevant metadata, but also the text content, which, in almost all cases, is where the 'soul' of the data is. It could be argued that 'just' a text file is not actually structured. I'd say that groups of ascii-characters that can be found in english-language dictionaries, separated by whitespaces and new-lines can be considered a structure, even if only barely. The new format certainly allows the researcher to interact with the data in other ways (e.g. full-text search). We can go further, and might be interested in characteristics of the text content (language, topics, etc.). This is where the actual magic happens, everything before that is just rote data preparation: turning unstructured (or 'other-ly' structured) data into (meaningful) structured data... On a technical level, those two parts (preparation/computation) of a research workflow might look (or be) the same, but I think there is a difference worth keeping in mind. If I don't forget I'll elaborate on that later. 'Big-ish' data \u00b6 I'm not talking about real 'Big data'-big data here, just largish files, or lots of them, or both. I don't think we'll encounter many use-cases where we have to move or analyze terabytes of data, but I wouldn't be surprised if we come across a few gigabytes worth of it every now and then. There are a few things we have to be prepared for, in those cases: transferring that sort of data is not trivial (esp. from home internet connections with limited upload bandwidth) -- and we will most likely have to be able to offer some sort of resumable-upload (and download) option (in case of a hosted solution) if we offer a hosted service, we will have to take into account and plan for this, so we don't run out of storage space (we might have to impose quotas, for example) computation-wise, we need to make sure we are prepared for large datasets and handle that in a smart way (if we load a huge dataset into memory, it can crash the machine where that is done) similarly, when we feed large datasets into a pipeline, we might not be able to just duplicate and edit the dataset like we could do for small amounts of data (too expensive, storage-wise) -- so we might need to have different strategies in place on how to execute a workflow, depending on file sizes (for example some sort of copy-on-write)","title":"Overview"},{"location":"architecture/data/#whats-data","text":"Data is created from sources. Sources come in different forms (analog, digital) and can be anything from handwritten documents in an archive to a twitter feed. Photos, cave-paintings, what have you. I'm not websters dictionary, but I think one usable working definition of data could be a 'materialized source', in our context 'materialized source in digital form'. From here on out I'll assume we are talking about 'digital' data when I mention data. One thing I'll leave out in this discussion is what is usually called 'dirty data' in data engineering, although it is an important topic. Most of the issues there map fairly well to the structured/unstructured thing below. There are a few differences, but in the interest of clarity let's ignore those for now...","title":"What's data?"},{"location":"architecture/data/#structured-data-data-transformations","text":"Important for us is that data can come in two different formats: unstructured, and, who'd have guessed... structured. The same piece of data can be theoretically expressed in structured as well as unstructured form: the meaning to a researcher would be 100% the same, but the ways to handle, digest and operate with the data can differ, and in most scenarios adding structure opens up possibilities to work with the data that weren't there before. In my head I call those two forms 'useless', and 'useful' data, but researcher usually get a bit agitated when I do, so I have learned to not do that in public anymore. For researchers, the most (and arguably only) important feature of 'structure' is that it enables them to do more with the data they already possess. By means of computation. I think it's fair to say that only structured data can be used in a meaningful way in a computational context. With the exception that unstructured data is useful input to create structured data. One more thing to mention is that the line between structured and un-structured is sometimes hard to draw, and can depend entirely on context. \"One persons structured data is another persons unstructured data.\", something like that. In addition, in some instances unstructured data can be converted to structured data trivially, meaning without much effort or any user-interaction. I'd argue we can consider those sorts of datasets basically 'structured'.","title":"Structured data / Data transformations"},{"location":"architecture/data/#example","text":"Lets use a simple example to illustrate all that: a digital image of a document . Depending on what you are interested in, such an image might already be structured data. For example it could contain geo-tags, and a timestamp, which are both digitally readable. If you want to visualize on a map where a document is from, you can do that instantly. Structured data, yay! Similarly, if you are interested in the color of the paper of the document (ok, I'm stretching my argument here as this seems fairly unlikely, but this is really just to illustrate...), you might get the color histogram of the image (which is trivial to extract, but needs some batch-computation), and for your purposes you would also consider the image file structured data. Now, if you are interested in the text content of the document, things get more interesting. You will have to jump through some hoops, and feed the image file to an OCR pipeline that will spit out a text file for example. The data itself would still be the same, but now computers can access not only some probably irrelevant metadata, but also the text content, which, in almost all cases, is where the 'soul' of the data is. It could be argued that 'just' a text file is not actually structured. I'd say that groups of ascii-characters that can be found in english-language dictionaries, separated by whitespaces and new-lines can be considered a structure, even if only barely. The new format certainly allows the researcher to interact with the data in other ways (e.g. full-text search). We can go further, and might be interested in characteristics of the text content (language, topics, etc.). This is where the actual magic happens, everything before that is just rote data preparation: turning unstructured (or 'other-ly' structured) data into (meaningful) structured data... On a technical level, those two parts (preparation/computation) of a research workflow might look (or be) the same, but I think there is a difference worth keeping in mind. If I don't forget I'll elaborate on that later.","title":"Example"},{"location":"architecture/data/#big-ish-data","text":"I'm not talking about real 'Big data'-big data here, just largish files, or lots of them, or both. I don't think we'll encounter many use-cases where we have to move or analyze terabytes of data, but I wouldn't be surprised if we come across a few gigabytes worth of it every now and then. There are a few things we have to be prepared for, in those cases: transferring that sort of data is not trivial (esp. from home internet connections with limited upload bandwidth) -- and we will most likely have to be able to offer some sort of resumable-upload (and download) option (in case of a hosted solution) if we offer a hosted service, we will have to take into account and plan for this, so we don't run out of storage space (we might have to impose quotas, for example) computation-wise, we need to make sure we are prepared for large datasets and handle that in a smart way (if we load a huge dataset into memory, it can crash the machine where that is done) similarly, when we feed large datasets into a pipeline, we might not be able to just duplicate and edit the dataset like we could do for small amounts of data (too expensive, storage-wise) -- so we might need to have different strategies in place on how to execute a workflow, depending on file sizes (for example some sort of copy-on-write)","title":"'Big-ish' data"},{"location":"architecture/data/data_centric_approach/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); A (parallel?) data centric approach for kiara/lumy \u00b6 - decision between Workflow creation and Workflow execution \u00b6","title":"Data centric approach"},{"location":"architecture/data/data_centric_approach/#a-parallel-data-centric-approach-for-kiaralumy","text":"","title":"A (parallel?) data centric approach for kiara/lumy"},{"location":"architecture/data/data_centric_approach/#-decision-between-workflow-creation-and-workflow-execution","text":"","title":"- decision between Workflow creation and Workflow execution"},{"location":"architecture/data/data_formats/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); from dharpa.benchmarking.data import clear_system_cache , MemoryRecorder , get_example_file from rich.jupyter import print This document is a primer on data formats and structures, and how and why those affect our project. I have no idea about how much of this is common knowledge, and how much is news to the majority. I have noticed a few common misconceptions and assumptions about some of the topics in here, so I figured it makes sense to try to get everyone on the same page. I've tried to keep this simple and short, so there are some things in here that are over-simplified bordering on incorrect. My educated guess is that in our project we will mostly be concerned about structured, tabular data, which is why I'll be focussing on that. I might add a companion document about 'binary-blob' data later on. Data serialization and storage \u00b6 data lives in memory or on disk lots of 0's and 1's -- binary format only 'decoding' gets you a useful representation 'text' is just an often used encoding format The natural habitat of (digital) data is computer memory or on disk. Data is always stored in binary form, and there is always some sort of decoding involved to make data usable in one way or another (with the exception of booleans maybe). Even when we talk about text files (seemingly implying that those are not binary since they are 'text'), we are dealing with binary data. It's just such a standard data encoding format that tools to decode that sort of data are available everywhere. Decoding text is by no means trivial, but luckily our tools have evolved so much by now -- and we have standards like utf-8 commonly available -- that we as users hardly ever come across decoding issues anymore. At least not to the extent we used to. It still happens, and I would imagine quite a bit more in the Digital Humanities than in your everyday 'business' use-case. So it helps to be aware of at least the basics involved in text encoding standards, and I would recommend anyone doing any sort of programming to read up on it. Tabular data (serialization) formats \u00b6 serialization/deserialization binary or not, here I come: avro, protobuf, pickle csv, json, yaml, xml 'structured formats' with schema, or without: avro, protobuf, thrift, flatbuffers, xml csv, json, yaml, messagepack, pickle zero-copy, memory-mapping? The (arguably) most important type of data we'll be dealing with is structured, tabular data. So it pays to think about how its digital form is represented in memory, and what issues are involved when trying to store, manipulate and transfer it. In our context, tabular data is always a 2 dimensional matrix (rows, columns), where each column has the same number of items, and each column contains items of the same data type (or union of data types). Tabular data can have a column that can be used as index, but that is optional. Each table can be described with a schema, which is basically a list of column names, and the data types associated with each of those columns. Serialization / deserialization \u00b6 Every programming language represents its basic data types differently in memory. That's especially true for the arguably most common data type: the string. Also, that in-memory representation is (almost) always different to the format of the (functially) same data when exported into a file on disk. This means that, if we want to export our data in our code, we need to do a step that is called 'serializing' (or 'marshalling'): we convert the data into a commonly accepted representation of a commonly accepted set of data-types. This serialization is usually expensive, computationally speaking. We want to avoid it, if at all possible, or at least always postpone it until the last possible moment, when we are sure the data won't change anymore, so we only have to do it once. The same goes for de-serializing data, just in the other direction: we only want to do it once, then keep it in memory in our native representation (if the size of the data allows it), so we don't have to read it again. Even if the content of a file is in the OS (page) cache (which would mean we don't actually have to read the file-content from disk) we'd still have to spend the cpu-cycles for de-serialization. So, big no-no, bad data-scientist! Format types \u00b6 For serialization, we have two basic options: text, and binary (let's just not nitpick and assume that this distinction makes sense). Text-based formats \u00b6 Serializing into a text-based format usually means taking all the elements our tabular data consists of, one by one, then serialize each element into its textual representation (like for example \"hello world\" for a string, 5 for an integer, true for a boolean in json), and then assembling one big 'meta'-string out of all those sub-elements. For csv, that might include a header-row, and adding delimiters like ',' in between the elements. For json it would be adding list (' [ ', ' ] ') or dictionary (' { ', ' } ') indicators, as well as deliminters and other elements as outlined in the JSON specification. I haven't done any research on it, but I'd imagine csv would be one of the oldest widely-used data storage formats. Csv is a text based tabular format, and it allows you to specify an optional header to describe column names. It allows for different deliminters between row cells (whole rows are delimited by the end-of-line special character). Other commonly used text-based formats are json, yaml, toml, xml. Those are not strictly tabular data formats, they can also contain just scalars, dictionaries, or lists (tabular data is always a list of dictionaries of the same shape). Binary formats \u00b6 Serializing into a binary format is usually very specific to the format itself, so there are not really any common tools to read more than one of them (like there are for text-based formats, where you could use any text editor and at least display the content in a meaningful way), and different formats have different priorites (like small size of the resulting blob, quick random read/write access, suitability for streaming, etc). Binary formats often have compression built-in, whereas text formats never have (but can be usually compressed well enough by external tools due to certain characteristics of encoded strings). Also, they usually are a lot easier on the cpu for serialization/deserialization purposes, since it's easier to optimize them for that scenario. Binary formats existed for a long time, but in recent years they are used more widely again. Decentralized software architecture (microservices) as well as 'big(-ish) data' played a huge part in that. Because, as it turns out that, while serializing a few items of data per seconds into json and back is not that much of a problem, doing the same thing for millions of large (or even small) chunks of data actually is. In some cases that serialization step can take more time than the actual computation that was done on the data. To counter that issue, people came up with formats like 'Avro', 'Thrift', 'ProtoBuf'. Pythons 'pickle' can also be considerd a binary serialization format. Schema \u00b6 Another useful way to separate data formats is to check whether they include a (native) schema that describes the value types of the data they hold, or not. If schema information is present, it can either be included in the resulting file, or be stored elsewhere. Having schema information for a dataset is highly desirable, because it tells us exactly what type of data we are dealing with (is it a string, integer, float? what precision?). Whereas most text-based data formats don't include a schema definition format, there are sometimes external efforts to remedy that (JSON-schema, for example). None of the text-based formats I can think of at the top of my head include the schema in a resulting file. This is important, because the complexity of tools that handle data increases if they need to worry about secondary, 'side-car' files for incoming data. Slight detour: csv \u00b6 Csv is bit special in that it can contain a 'header' row in the first line, which can be used to determine column names. Since this row is optional, it is not always present which of course complicates the import process. Because csv files are so common in data science, most of the tools we use include some csv-import method that more or less smartly determines the (text-)encoding of the file, whether it has a header row, as well as the schema of the included data. This method usually serializes the data into the appropriate internal representations of the column types after it is reasonably sure the inferred schema is correct-ish. Without a definite, (externally provided) schema it is not possible to guess this reliably in every case, so a developer should always assert the correct types are present after such an import. Streaming, zero-copy, memory-mapping \u00b6 One thing that everyone working semi-seriously in data science and handling big-ish data should be aware of is that in most OS'es you can read (or 'load') data in more ways than one. The 'one' way is usually something like: file = open ( 'dataset.csv' ) lines = file . read () # or file.readlines() When using Pandas, it'll probably take the form of: import pandas as pd pd . read_csv ( 'dataset.csv' ) Both of those read the whole file into memory. Which will be fine if the dataset is small or there is a need for it to be in memory in full. Depending on the situation, it might be wasteful, though. For example when calculating the mean for a column of integers. In that case it's a better strategy to read one line of the file, process the column we are interested in, eject the line from memory, then read the next line, only keeping the current total and the number of items we processed so far. That way we'll never allocate more memory than what is needed for a single line. We can even process datasets that are larger than the available memory of or our workstation. As a matter of fact, we could do even better if we would know the offset and length of the column we are interested in, in that case, we would only have to read the bytes that hold the integer value we need, and could ignore the other cells of a row completely. Again, this might or might be an issue depending on the size of the data in a row, but if we have a dataset with a lot of large columns, the I/O operations we would not have to do by only reading the exact data we need could improve the speed of processing considerably. Doing that doesn't really work for csv files, for example. Since there is no good way for us to know the exact offset of length of the column we are interested in. There are data formats that support that kind of operation though. Along with those fairly simple strategies to deal with data efficiently, there are more advanced ones that also deal with data and how it is handled in a system memory as well as on disk. For those of you who are interested, I would recommend looking up the terms 'memory-mapping', and 'zero-copy'. Some random benchmarks, to illustrate \u00b6 clear_system_cache () file_path = get_example_file () def count_lines ( path ): f = open ( path ) counter = 0 length = 0 # ignores '\\n' characters lines = f . readlines () for line in lines : counter = counter + 1 length = length + len ( line ) return { \"no_lines\" : counter , \"size\" : length } profile_count_lines = MemoryRecorder . profile_func ( \"Reading the whole file\" , \"This iterates through all lines in memory, keeping all of them in memory at the same time.\" , False , count_lines , file_path ) print ( profile_count_lines . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading the whole file \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This iterates through all lines in memory, keeping all of them in memory at the same \u2502 \u2502 time. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 count_lines \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 53.75 MB \u2502 \u2502 execution time \u2502 144 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def count_lines (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 f = open (path) \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 length = 0 # ignores '\\n' characters \u2502 \u2502 \u2502 \u2502 lines = f . readlines() \u2502 \u2502 \u2502 \u2502 for line in lines: \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 length = length + len (line) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return { \"no_lines\" : counter, \"size\" : length} \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f clear_system_cache () file_obj = open ( file_path , buffering = True ) def count_lines ( f ): counter = 0 length = 0 # ignores '\\n' characters for line in f : # when using open like we do here, it returns an iterator not a materialized list counter = counter + 1 length = length + len ( line ) return { \"no_lines\" : counter , \"size\" : length } profile_count_lines = MemoryRecorder . profile_func ( \"Reading the file line by line\" , \"This allocates only very little memory, since once a line is read and processed, it will be disregarded.\" , False , count_lines , file_obj ) file_obj . close () print ( profile_count_lines . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading the file line by line \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This allocates only very little memory, since once a line is read and processed, it will \u2502 \u2502 be disregarded. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 count_lines \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 0.0 MB \u2502 \u2502 execution time \u2502 115 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def count_lines (f): \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 length = 0 # ignores '\\n' characters \u2502 \u2502 \u2502 \u2502 for line in f: # when using open like we do here, it returns an iterator not a \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 length = length + len (line) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return { \"no_lines\" : counter, \"size\" : length} \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Structured (binary) data layout strategies \u00b6 Row-based: (most commonly used dbs): sqlite, Postgres, MySQL, ... Avro Column-based OLAP dbs: duckdb, ClickHouse, BigQuery, Snowflake ... pandas dataframe parquet, feather In order to use the more advanced operations on data I described earlier, the data formats we use need to support them. None of the simple formats like csv, json, yaml do. There is one category of applications that had to deal with those things for decades though: databases. So I think it pays to look a bit at how they handle storing data, and what kind of trade-offs they are making. Basically, a database is a system that lets you persist (mostly) structured data on disk, and gives you an easy, memory- and processing-efficient way to query and retrieve it back. To do that, they have different ways to persist data, add indexes, cache 'hot' data, and so on. As it turns out, there are 2 main ways data can be stored on disk for efficient retrieval: row-based, and column-based (I'm ignoring document/'nosql' databases here, since -- for almost all practical use-cases -- they are inferior to relational ones). Row-oriented databases \u00b6 The most common database type is 'row-oriented'. This means that data is stored in a way so that each row represents a continuous block of disk (or memory). Data is quick and easy to read (if you are interested in a subset of rows) and it is very easy and fast to add new rows/records. This fits the most common requirements businesses have for a database, since new items are added constantly, which is why most databases we encounter in the wild are row-based. Examples for such databases are: Postgres, sqlite, MySQL. Column-oriented databases \u00b6 Column-oriented databases have existed for a long time, but they are not as prevalent as their row-based cousins, and often ignored by developers who haven't been exposed to them and their advantages. Instead of storing data row-by-row, they store data column-by-column. This means that column-cells are layed out next to each other on disk, and different columns occupy different regions of the storage (not necessarily close to each other at all). The querying logic is quite different for this type of database, the main advantage is that a certain type of analytical query is really fast (speedups of 10x or even 100x are quite possible), also it is very easy to request whole columns from such a database without it having to access any irrelevant parts of the data. Compressing data is also easier with column-oriented databases, so usually those occupy less disk space than their row-based peers. The disadvantage of those databases is that it's harder and slower to add new data, so they are more common for situations where one either has a fixed dataset, or updates are rare, and come in big(er) batches. Also, certain types of queries are less suited for that layout, which makes it always a good idea to think about what you need out of your data before deciding on a database/database type. Row-based/column-based in data science \u00b6 How is this relevant? Well, because in data science we are dealing mostly with fixed datasets, and the queries we do on them are mostly analytical in a way that fits column-oriented data layouts; although exceptions from that rule are not uncommon. So it makes sense to depart from the 'common wisdom' of using a row-based approach. In fact, Numpy arrays and Pandas dataframes (which depend on them) are kept in memory using the column-based approach. This is important to know, because it helps us using and querying data correctly in our code. For example, it's not a good idea and very slow to add 'rows' to a Pandas dataframe. Instead, we should initialize the Dataframe with the whole dataset once at the beginning, and then only add columns to it (which is very fast), but no new rows if at all possible. One issue with Numpy/Pandas is that commonly a dataset is loaded into memory as a whole. There are ways around that (for example by processing a csv file in batches), but very often those are not used. In reality, it's probably not that big an issue in the DH field, since datasets seem to be smaller on average. But it is still a good idea to be as efficient as possible in this regard, esp. for our purpose, since we won't have any knowledge or guarantees in advance about the data we'll be handling (which might very well be bigger than the availble memory). Also, since we are building an interactive application, it makes a difference whether a query comes back within a second, or ten. More random benchmarks, this time with Pandas \u00b6 clear_system_cache () import pandas as pd def load_csv ( path ): counter = 0 df = pd . read_csv ( path ) for _ in df [ \"year_month\" ]: counter = counter + 1 return counter profile_read_csv = MemoryRecorder . profile_func ( \"Reading a csv file with Pandas as a whole.\" , \"This is ok, as long as the dataset is not too big.\" , False , load_csv , file_path ) print ( profile_read_csv . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading a csv file with Pandas as a whole. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This is ok, as long as the dataset is not too big. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 load_csv \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 63.625 MB \u2502 \u2502 execution time \u2502 342 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def load_csv (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 df = pd . read_csv(path) \u2502 \u2502 \u2502 \u2502 for _ in df[ \"year_month\" ]: \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return counter \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f clear_system_cache () import pandas as pd def load_csv ( path ): counter = 0 df = pd . read_csv ( path ) for index , row in df . iterrows (): counter = counter + 1 return counter profile_read_csv = MemoryRecorder . profile_func ( \"Reading a csv file with Pandas, iterating over rows\" , \"As one can see, this is very very slow, and not a good idea at all to do in Pandas.\" , False , load_csv , file_path ) print ( profile_read_csv . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading a csv file with Pandas, iterating over rows \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 As one can see, this is very very slow, and not a good idea at all to do in Pandas. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 load_csv \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 80.875 MB \u2502 \u2502 execution time \u2502 21704 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def load_csv (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 df = pd . read_csv(path) \u2502 \u2502 \u2502 \u2502 for index, row in df . iterrows(): \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return counter \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f def load_csv_in_chunks ( path ): counter = 0 chunksize = 1000 with pd . read_csv ( path , chunksize = chunksize ) as reader : for chunk_df in reader : for _ in chunk_df [ \"year_month\" ]: counter = counter + 1 return counter profile_read_csv = MemoryRecorder . profile_func ( \"Reading a csv file with Pandas, in chunks.\" , \"This is a good approach when dealing with a dataset that is large, and we don't need it except for a single operation on a single column. We can optimize the execution-time/memory-usage by adjusting the 'chunksize' value.\" , False , load_csv_in_chunks , file_path ) print ( profile_read_csv . report ) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading a csv file with Pandas, in chunks. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This is a good approach when dealing with a dataset that is large, and we don't need it \u2502 \u2502 except for a single operation on a single column. We can optimize the \u2502 \u2502 execution-time/memory-usage by adjusting the 'chunksize' value. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 load_csv_in_chunks \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 6.0390625 MB \u2502 \u2502 execution time \u2502 892 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def load_csv_in_chunks (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 chunksize =1000 \u2502 \u2502 \u2502 \u2502 with pd . read_csv(path, chunksize = chunksize) as reader: \u2502 \u2502 \u2502 \u2502 for chunk_df in reader: \u2502 \u2502 \u2502 \u2502 for _ in chunk_df[ \"year_month\" ]: \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return counter \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f","title":"Formats"},{"location":"architecture/data/data_formats/#data-serialization-and-storage","text":"data lives in memory or on disk lots of 0's and 1's -- binary format only 'decoding' gets you a useful representation 'text' is just an often used encoding format The natural habitat of (digital) data is computer memory or on disk. Data is always stored in binary form, and there is always some sort of decoding involved to make data usable in one way or another (with the exception of booleans maybe). Even when we talk about text files (seemingly implying that those are not binary since they are 'text'), we are dealing with binary data. It's just such a standard data encoding format that tools to decode that sort of data are available everywhere. Decoding text is by no means trivial, but luckily our tools have evolved so much by now -- and we have standards like utf-8 commonly available -- that we as users hardly ever come across decoding issues anymore. At least not to the extent we used to. It still happens, and I would imagine quite a bit more in the Digital Humanities than in your everyday 'business' use-case. So it helps to be aware of at least the basics involved in text encoding standards, and I would recommend anyone doing any sort of programming to read up on it.","title":"Data serialization and storage"},{"location":"architecture/data/data_formats/#tabular-data-serialization-formats","text":"serialization/deserialization binary or not, here I come: avro, protobuf, pickle csv, json, yaml, xml 'structured formats' with schema, or without: avro, protobuf, thrift, flatbuffers, xml csv, json, yaml, messagepack, pickle zero-copy, memory-mapping? The (arguably) most important type of data we'll be dealing with is structured, tabular data. So it pays to think about how its digital form is represented in memory, and what issues are involved when trying to store, manipulate and transfer it. In our context, tabular data is always a 2 dimensional matrix (rows, columns), where each column has the same number of items, and each column contains items of the same data type (or union of data types). Tabular data can have a column that can be used as index, but that is optional. Each table can be described with a schema, which is basically a list of column names, and the data types associated with each of those columns.","title":"Tabular data (serialization) formats"},{"location":"architecture/data/data_formats/#serialization-deserialization","text":"Every programming language represents its basic data types differently in memory. That's especially true for the arguably most common data type: the string. Also, that in-memory representation is (almost) always different to the format of the (functially) same data when exported into a file on disk. This means that, if we want to export our data in our code, we need to do a step that is called 'serializing' (or 'marshalling'): we convert the data into a commonly accepted representation of a commonly accepted set of data-types. This serialization is usually expensive, computationally speaking. We want to avoid it, if at all possible, or at least always postpone it until the last possible moment, when we are sure the data won't change anymore, so we only have to do it once. The same goes for de-serializing data, just in the other direction: we only want to do it once, then keep it in memory in our native representation (if the size of the data allows it), so we don't have to read it again. Even if the content of a file is in the OS (page) cache (which would mean we don't actually have to read the file-content from disk) we'd still have to spend the cpu-cycles for de-serialization. So, big no-no, bad data-scientist!","title":"Serialization / deserialization"},{"location":"architecture/data/data_formats/#format-types","text":"For serialization, we have two basic options: text, and binary (let's just not nitpick and assume that this distinction makes sense).","title":"Format types"},{"location":"architecture/data/data_formats/#schema","text":"Another useful way to separate data formats is to check whether they include a (native) schema that describes the value types of the data they hold, or not. If schema information is present, it can either be included in the resulting file, or be stored elsewhere. Having schema information for a dataset is highly desirable, because it tells us exactly what type of data we are dealing with (is it a string, integer, float? what precision?). Whereas most text-based data formats don't include a schema definition format, there are sometimes external efforts to remedy that (JSON-schema, for example). None of the text-based formats I can think of at the top of my head include the schema in a resulting file. This is important, because the complexity of tools that handle data increases if they need to worry about secondary, 'side-car' files for incoming data.","title":"Schema"},{"location":"architecture/data/data_formats/#streaming-zero-copy-memory-mapping","text":"One thing that everyone working semi-seriously in data science and handling big-ish data should be aware of is that in most OS'es you can read (or 'load') data in more ways than one. The 'one' way is usually something like: file = open ( 'dataset.csv' ) lines = file . read () # or file.readlines() When using Pandas, it'll probably take the form of: import pandas as pd pd . read_csv ( 'dataset.csv' ) Both of those read the whole file into memory. Which will be fine if the dataset is small or there is a need for it to be in memory in full. Depending on the situation, it might be wasteful, though. For example when calculating the mean for a column of integers. In that case it's a better strategy to read one line of the file, process the column we are interested in, eject the line from memory, then read the next line, only keeping the current total and the number of items we processed so far. That way we'll never allocate more memory than what is needed for a single line. We can even process datasets that are larger than the available memory of or our workstation. As a matter of fact, we could do even better if we would know the offset and length of the column we are interested in, in that case, we would only have to read the bytes that hold the integer value we need, and could ignore the other cells of a row completely. Again, this might or might be an issue depending on the size of the data in a row, but if we have a dataset with a lot of large columns, the I/O operations we would not have to do by only reading the exact data we need could improve the speed of processing considerably. Doing that doesn't really work for csv files, for example. Since there is no good way for us to know the exact offset of length of the column we are interested in. There are data formats that support that kind of operation though. Along with those fairly simple strategies to deal with data efficiently, there are more advanced ones that also deal with data and how it is handled in a system memory as well as on disk. For those of you who are interested, I would recommend looking up the terms 'memory-mapping', and 'zero-copy'.","title":"Streaming, zero-copy, memory-mapping"},{"location":"architecture/data/data_formats/#some-random-benchmarks-to-illustrate","text":"clear_system_cache () file_path = get_example_file () def count_lines ( path ): f = open ( path ) counter = 0 length = 0 # ignores '\\n' characters lines = f . readlines () for line in lines : counter = counter + 1 length = length + len ( line ) return { \"no_lines\" : counter , \"size\" : length } profile_count_lines = MemoryRecorder . profile_func ( \"Reading the whole file\" , \"This iterates through all lines in memory, keeping all of them in memory at the same time.\" , False , count_lines , file_path ) print ( profile_count_lines . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading the whole file \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This iterates through all lines in memory, keeping all of them in memory at the same \u2502 \u2502 time. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 count_lines \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 53.75 MB \u2502 \u2502 execution time \u2502 144 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def count_lines (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 f = open (path) \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 length = 0 # ignores '\\n' characters \u2502 \u2502 \u2502 \u2502 lines = f . readlines() \u2502 \u2502 \u2502 \u2502 for line in lines: \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 length = length + len (line) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return { \"no_lines\" : counter, \"size\" : length} \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f clear_system_cache () file_obj = open ( file_path , buffering = True ) def count_lines ( f ): counter = 0 length = 0 # ignores '\\n' characters for line in f : # when using open like we do here, it returns an iterator not a materialized list counter = counter + 1 length = length + len ( line ) return { \"no_lines\" : counter , \"size\" : length } profile_count_lines = MemoryRecorder . profile_func ( \"Reading the file line by line\" , \"This allocates only very little memory, since once a line is read and processed, it will be disregarded.\" , False , count_lines , file_obj ) file_obj . close () print ( profile_count_lines . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading the file line by line \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This allocates only very little memory, since once a line is read and processed, it will \u2502 \u2502 be disregarded. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 count_lines \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 0.0 MB \u2502 \u2502 execution time \u2502 115 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def count_lines (f): \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 length = 0 # ignores '\\n' characters \u2502 \u2502 \u2502 \u2502 for line in f: # when using open like we do here, it returns an iterator not a \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 length = length + len (line) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return { \"no_lines\" : counter, \"size\" : length} \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f","title":"Some random benchmarks, to illustrate"},{"location":"architecture/data/data_formats/#structured-binary-data-layout-strategies","text":"Row-based: (most commonly used dbs): sqlite, Postgres, MySQL, ... Avro Column-based OLAP dbs: duckdb, ClickHouse, BigQuery, Snowflake ... pandas dataframe parquet, feather In order to use the more advanced operations on data I described earlier, the data formats we use need to support them. None of the simple formats like csv, json, yaml do. There is one category of applications that had to deal with those things for decades though: databases. So I think it pays to look a bit at how they handle storing data, and what kind of trade-offs they are making. Basically, a database is a system that lets you persist (mostly) structured data on disk, and gives you an easy, memory- and processing-efficient way to query and retrieve it back. To do that, they have different ways to persist data, add indexes, cache 'hot' data, and so on. As it turns out, there are 2 main ways data can be stored on disk for efficient retrieval: row-based, and column-based (I'm ignoring document/'nosql' databases here, since -- for almost all practical use-cases -- they are inferior to relational ones).","title":"Structured (binary) data layout strategies"},{"location":"architecture/data/data_formats/#row-oriented-databases","text":"The most common database type is 'row-oriented'. This means that data is stored in a way so that each row represents a continuous block of disk (or memory). Data is quick and easy to read (if you are interested in a subset of rows) and it is very easy and fast to add new rows/records. This fits the most common requirements businesses have for a database, since new items are added constantly, which is why most databases we encounter in the wild are row-based. Examples for such databases are: Postgres, sqlite, MySQL.","title":"Row-oriented databases"},{"location":"architecture/data/data_formats/#column-oriented-databases","text":"Column-oriented databases have existed for a long time, but they are not as prevalent as their row-based cousins, and often ignored by developers who haven't been exposed to them and their advantages. Instead of storing data row-by-row, they store data column-by-column. This means that column-cells are layed out next to each other on disk, and different columns occupy different regions of the storage (not necessarily close to each other at all). The querying logic is quite different for this type of database, the main advantage is that a certain type of analytical query is really fast (speedups of 10x or even 100x are quite possible), also it is very easy to request whole columns from such a database without it having to access any irrelevant parts of the data. Compressing data is also easier with column-oriented databases, so usually those occupy less disk space than their row-based peers. The disadvantage of those databases is that it's harder and slower to add new data, so they are more common for situations where one either has a fixed dataset, or updates are rare, and come in big(er) batches. Also, certain types of queries are less suited for that layout, which makes it always a good idea to think about what you need out of your data before deciding on a database/database type.","title":"Column-oriented databases"},{"location":"architecture/data/data_formats/#row-basedcolumn-based-in-data-science","text":"How is this relevant? Well, because in data science we are dealing mostly with fixed datasets, and the queries we do on them are mostly analytical in a way that fits column-oriented data layouts; although exceptions from that rule are not uncommon. So it makes sense to depart from the 'common wisdom' of using a row-based approach. In fact, Numpy arrays and Pandas dataframes (which depend on them) are kept in memory using the column-based approach. This is important to know, because it helps us using and querying data correctly in our code. For example, it's not a good idea and very slow to add 'rows' to a Pandas dataframe. Instead, we should initialize the Dataframe with the whole dataset once at the beginning, and then only add columns to it (which is very fast), but no new rows if at all possible. One issue with Numpy/Pandas is that commonly a dataset is loaded into memory as a whole. There are ways around that (for example by processing a csv file in batches), but very often those are not used. In reality, it's probably not that big an issue in the DH field, since datasets seem to be smaller on average. But it is still a good idea to be as efficient as possible in this regard, esp. for our purpose, since we won't have any knowledge or guarantees in advance about the data we'll be handling (which might very well be bigger than the availble memory). Also, since we are building an interactive application, it makes a difference whether a query comes back within a second, or ten.","title":"Row-based/column-based in data science"},{"location":"architecture/data/data_formats/#more-random-benchmarks-this-time-with-pandas","text":"clear_system_cache () import pandas as pd def load_csv ( path ): counter = 0 df = pd . read_csv ( path ) for _ in df [ \"year_month\" ]: counter = counter + 1 return counter profile_read_csv = MemoryRecorder . profile_func ( \"Reading a csv file with Pandas as a whole.\" , \"This is ok, as long as the dataset is not too big.\" , False , load_csv , file_path ) print ( profile_read_csv . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading a csv file with Pandas as a whole. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This is ok, as long as the dataset is not too big. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 load_csv \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 63.625 MB \u2502 \u2502 execution time \u2502 342 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def load_csv (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 df = pd . read_csv(path) \u2502 \u2502 \u2502 \u2502 for _ in df[ \"year_month\" ]: \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return counter \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f clear_system_cache () import pandas as pd def load_csv ( path ): counter = 0 df = pd . read_csv ( path ) for index , row in df . iterrows (): counter = counter + 1 return counter profile_read_csv = MemoryRecorder . profile_func ( \"Reading a csv file with Pandas, iterating over rows\" , \"As one can see, this is very very slow, and not a good idea at all to do in Pandas.\" , False , load_csv , file_path ) print ( profile_read_csv . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading a csv file with Pandas, iterating over rows \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 As one can see, this is very very slow, and not a good idea at all to do in Pandas. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 load_csv \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 80.875 MB \u2502 \u2502 execution time \u2502 21704 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def load_csv (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 df = pd . read_csv(path) \u2502 \u2502 \u2502 \u2502 for index, row in df . iterrows(): \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return counter \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f def load_csv_in_chunks ( path ): counter = 0 chunksize = 1000 with pd . read_csv ( path , chunksize = chunksize ) as reader : for chunk_df in reader : for _ in chunk_df [ \"year_month\" ]: counter = counter + 1 return counter profile_read_csv = MemoryRecorder . profile_func ( \"Reading a csv file with Pandas, in chunks.\" , \"This is a good approach when dealing with a dataset that is large, and we don't need it except for a single operation on a single column. We can optimize the execution-time/memory-usage by adjusting the 'chunksize' value.\" , False , load_csv_in_chunks , file_path ) print ( profile_read_csv . report ) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading a csv file with Pandas, in chunks. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This is a good approach when dealing with a dataset that is large, and we don't need it \u2502 \u2502 except for a single operation on a single column. We can optimize the \u2502 \u2502 execution-time/memory-usage by adjusting the 'chunksize' value. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 load_csv_in_chunks \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 6.0390625 MB \u2502 \u2502 execution time \u2502 892 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def load_csv_in_chunks (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 chunksize =1000 \u2502 \u2502 \u2502 \u2502 with pd . read_csv(path, chunksize = chunksize) as reader: \u2502 \u2502 \u2502 \u2502 for chunk_df in reader: \u2502 \u2502 \u2502 \u2502 for _ in chunk_df[ \"year_month\" ]: \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return counter \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f","title":"More random benchmarks, this time with Pandas"},{"location":"architecture/data/dev/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());","title":"Dev"},{"location":"architecture/data/persistence/","text":"Data persistence \u00b6 This is a document to describe my plans for storing data (and metadata) in kiara . (Almost) nothing I describe here is inmplemented yet, so it only reflects my current thinking. I think the overall strategy will hold, but there might be changes here and there. The problem \u00b6 kiara s main functionality centers around transforming input data sets to output data sets. Those outputs need to be stored, to be of any use later on. Obviously. When deciding how to do this, we must take into account concerns about performance, disk- and memory-usage, data versioning, which metadata to attach, in what way, how to deal with metadata schemas (and versioning of both), etc. The solution \u00b6 Well, solution. This is my current thinking of how to tackle the problem in a way that takes into account all of the aspects described above, while still being flexible enough to hopefully be able to incorporate solutions for future unforsseen issues. I am having trouble coming up with a good structure for this document, so I think I'll just try to tell the story from the point of view of data. Starting from a state where data exists outside of kiara , to when it is in a state to be ready to be published. As with everything I'm writing here as an explanation of generic and abstract concepts, some of the technical details I'm describing might be simplified to the point of being incorrect... The 7 stages of data \u00b6 One thing I'd like to say before I start to describe those stages: the transformation of a dataset, from one stage to the next, always always always happens by piping the dataset through a kiara module. At absolutely no point is this done without kiara s involvement and knowledge. The dataset is used as input for a module, and the result (technically a new dataset) is a representation of the dataset in its next stage. This is important to keep in mind, as it is crucial for us so we can track data lineage. I'll write more on the specifics of this below, where it makes more sense. 1) Unmanaged \u00b6 At the beginning, there was csv. Whether I like it or not, csv is the most predominant form data comes in. Csv is bad in a lot of ways, but in my mind the worst thing about it is that it is schema-less. True, in some cases you have a header-line, which gives you column-names, but that's not a requirement. Also, in a lot of cases you can auto-determine the type of each column, and luckily libraries like Pandas or Apache Arrow solved that problem for us so we don't have to do it ourselves every time. But those auto-parsers are not fool-proof, and you end up with integers where you wanted floats (or doubles), or integers where you wanted strings, or vice versa. In some cases we get data in a form that includes at least a semi-schema. Like a sqlite database file (which is more 'strongly' typed). But it's a lucky day when we get data that contains metadata about authorship, how and when it was created, from what sources, etc. 2) Onboarded \u00b6 This is the first thing we need to do to unmanaged data: we need to 'onboard' it, so kiara knows the data exists, and what exact bytes it consists of. This last thing is very important: we have to be able to make sure the data we are talking about is not being changed externally, a lot of things in kiara s approach to data depend on this. Practically, in most cases this means kiara will copy one or several files into a protected area that no other application can/should access. That way we always have a reference version of the dataset (the bytes) we are talking about. One thing kiara does at this stage is give the dataset a uniuqe id, which can be used to reference it later (by users, or other objects/functions). Another thing is to collect some basic metadata: when the file/folder was imported, from what original path, what the filenames are, mime-type, size of files, original file attributes (creation data, permissions, etc.). This can all be captured automatically. We can also record who it was that imported the dataset, if we have some app-global configuration about the current user, like a full name and email-address. Note, that this might or might not be the person who created the dataset. So, at this stage all we did was copy a file(set) into a protected area to sort of 'freeze' it, and augment it with very basic metadata. We don't know anything about the nature of the dataset yet, all we know is the bytes the datasets consists of. It is important to point out that we would not have to store those chunks of bytes as files again, using the same structure as the original set of files. The dataset ceased to be 'files' here for us, we are only interested in the chunks of bytes (and their meaning) from here on out. We could store the data in an object store, zipped, tarred and feathered (pun intended). Or as byte-stream directly on block storage, if we were crazy enough. A side-note that makes things a bit more complicated, but it is probably necessary to address potential concerns: no, we don't actually need to copy the files, and can leave them in place and only generate the metadata and id for them. This might be necessary in cases where the source data is very big (photos, movies, audio-files, other large datasets). I don't think we need to figure out how exactly we deal with this scenario right now, but it basically comes down to making the user aware of what is happening, and what the implications are if the source data is changed externally (inconsistent metadata and potential incorrect result data-sets further down the line). There are strategies to help prevent some of those potential issues (checksums, for example), but overall we have to acknowledge that working with large-sized datasets is always a challenge, and in some cases we might just have to say: \"sorry, this is too big for us right now\". 3) Augmented with more (basic) metadata \u00b6 To recapitulate: at this stage we have data (chunks of bytes -- not files!!! hit yourself over the head twice with something semi-heavy if you are still think in terms of files from here on out!) in a protected area, some very basic metadata, and an id for each dataset. We might or might not have authorship metadata (arguably one of the most important pieces of metadata), depending on whether who 'onboarded' the dataset actually created it. So, as a first step and following good practice, at this stage we should try to get the user to tell us about authorship and other core metadata about our dataset (licensing, copyright, ...). I don't think we can make this step mandatory, in practice, but we should push fairly hard, even if that means a slight decrease in user experience. It is very important information to have... So, one thing we could do was to have a checkbox that lets the user confirm: I created the data (in which case we can just copy the 'imported-by' field). 3) Typed \u00b6 Chunks of bytes are not very useful by itself. They need to be interpreted to be of use. This means: determining in some way what the structure of the chunks of bytes is, and then applying common conventions for that particular structure (aka data type/format) when reading the chunks of bytes. Therefore 'interpreting' the chunks of bytes. This is a very obvious thing that happens all the time we use computers, but I think it makes sense to point it out here, because usually this is transparent to the user when they click an 'Open file' button in an application, and even some developers are ignorant to the underlying concept (and can afford to be, since they usually work several abstraction layers above where that is happening). To encapsulate this concept, we will create a 'data type' for each important group of datasets that share some important characteristics. Examples for very simple data types are strings, integers, booleans. I'll ignore those, because those are trivial to use, and that triviality actually makes it harder to explain the concept I'm talking about. More relevant data types are: 'table', 'network graph', 'text corpus', 'photo collection'. Every data type inherently contains a description of, well, the 'type' of data represented by it, and, with that, information about how a user or code caqn access the actual data, and/or some of its properties. From here on out, I'll focus on tabular data in some form or other, since I expect that this will be one of our most important (base-) data types. I expect the reader to 'translate' whatever I'm saying below to other types, and extrapolate the practical differences. So, to explain this step I decided to look at three different use-cases (mostly because we use them in 2 of our example workflows, so people should be familiar with them): a csv file with tabular data an imported folder of text files (a corpus) two imported csv files containing edge and node information to form a network graph Example: tabular data \u00b6 This is the simplest case, and very common: we have a csv file, and need to have some sort of tabular data structure that we can use to query and analyze the data contained in it. Let's assume we have onboarded a csv file using kiara , so we have a dataset id that we use to point to it. Technically, this dataset is already 'typed': it has the type 'file'. This is not a very useful type, all it allows can tell us is a file name (which in a way is metadata), and the file content. We can ask kiara to interpret the content of this file as table, though, because we know it must be one. This means we 'overlay' a different, more specific data type on top of the same data. Under the hood, kiara will use the Apache Arrow read_csv helper method, which is very smart and fast, and it can create an Arrow Table object out of a csv file. It can figure out file encoding, column names (if present), column types, seperator characters. This detection is not fool-proof, but should work good enough in practice that we don't need to worry about it here. What really happens here is that the read_csv method is not just reading our data, but also, at the same time, is adding some important metadata to our dataset. Pandas can do the same with its csv import method. Even though this adding of metadata is more or less transparent to the user -- so they are not really aware of it -- it happens, and it is a very important thing that must happen to make our dataset useful. In our application, we might or might not want to ask users whether the extracted column names and types are correct, but this is a UI-specific implementation detail. So, considering all this, the important point here is that at this stage we have actual 'table' data, there is no need for the original csv file anymore (except as a reference for data lineage purposes). Our dataset is now of the data type 'table'. Which means we have an assurance that we can query it for table metadata properties (number of rows, number and name of columns, column types, size in bytes, etc.). And we can apply functions against it that are commonly applied against tabular data (sql queries, filters, etc.). That means, a 'data type' is really just a convention, a marker, that tells us how the bytes are organized for a particular set of bytes, and how to interact with it. This is all users and 3rd party-code needs to worry about. Implementation details about how this data is stored or loaded are irrelevant on this level of abstraction. This reduces complexity for kiara s external facing API, while, of course, introducing some extra complexity internally. Example: text corpus \u00b6 The source data comes as a folder of files, each file contains (just) text (not structured like json, csv, etc.). When we do the 'onboarding' step for this data, all we do is copy the files verbatim into a new location. There could be some metadata implicit in the relative paths of each file (e.g. languages -- files for the same language live in a subfolder named after the language), and there can also be some metadata in the file names. We preserve all that metadata by copying the folder one-to-one, without changing anything. But it is important to note that this metadata, as of yet, is still uncaptured. The 'soul' of this dataset (meaning: the properties of the dataset we are interested in and we want to use in our investigation, and which will hopefully answer our research question) is in the content of each text file (aka the unicode encoded chunks of bytes). It is important to say again: at this stage the dataset ceased to be a set of files! It is a dataset within kiara that has an id (a single one! not one for every text!), and it has a basic set of metadata fields (the ones we could collect automatically). Yes, the dataset is backed by a set of files in the kiara data store, but that is an implementation detail nobody needs to know about, and I think we should try hard to hide from users. If you haven't noticed so far: I strongly believe the file metaphor is a distraction, and not necessary for us, except when import/export is concerned. Anyway, kiara does not know much about the dataset at this stage. To be of actual use, we need to interpret the data. In this case, we know we want to interpret the data as a text corpus. The most basic shape we can imagine a text corpus to look like is a list of strings (an array, or a single-column table). For making it easier to work with the text corpus in the future, let's make up a convention to save in tabular form, and the column containing the text items is always named text_content . If we use, for example Apache Arrow to store that table, it makes the stored chunks of data much smaller (in comparison to text files), and it also makes the whole thing easier (at least faster) to query. It also allows us to easily attach more (meta-)data to the dataset. Note The distinction between data and metadata becomes a bit blurry here. In a lot of cases, when I say metadata, it is metadata from the point of view of the research proess, not metadata for kiara . I don't know how to make it clear which I'm talking about in each case without making this whole thing even more unreadable as it already is, so I will just have to ask you to figure it out yourself, in each case :-) Because we didn't lose any of the implied metadata when onboarding our folder of text files, it would be a shame if we wouldn't actually capture it. In this case, let's assume we didn't have any subfolders (so no metadata in their name), but our files are named in a special way: [publication_id]_[date_of_publishing]_[other_stuff_we_are_not_interested_in] Note The information about the format is important (in a way it is also an input) and we need to retrieve it somehow. This is a real problem that doesn't look like a big problem. But it is, for us. I'll ignore this here, because it would complicate things too much and is only of tangential relevance. This means, we can extract the publication id and the date of publishing with a simple regular expression, and we can add a column for each one to our table that so far only contains the actual text for each item. The publication id will be of type string (even though some of the ids might be integers -- we don't care), and the publication date will be a time format. Now we have a table with 3 columns, and we can already filter the texts by date easily, which is pretty useful! We wouldn't, strictly speaking those two additional columns to have a dataset of type 'text corpus' but it's much more useful that way. As a general rule: if we have metadata like that, it should be extracted and attached to the data in this stage. It's cheap to do in a lot of cases, and we never know when it will be needed later. What we have at this stage is data that has the attributes of a table (columns with name and type info, as well as rows representing an item and it's metadata). This is basically the definition of our 'text corpus' data type: something that allows us to access text content items (the actual important data) using a mandatory column named text_content , and that has zero to N metadata properties for each of those text items. In addition, we can access other metadata that is inherited from the base type (table): number of rows, size in bytes, etc, as well as its lineage (via a reference to the original onboarded dataset). Internally, we'll store this data type as an Arrow table, but again, this is an implementation detail, and neither user nor frontend needs to know about this (exceptions apply, of course, but lets not get bogged down by those just now -- none of them are deal-breakers, as far as I can see). Example: network graph data \u00b6 Similar to the text corpus case above, let's think about what a basic definition of a network graph data type would look like. It would have to include a list of nodes, and a list of edges (that tell us how those nodes are connected). Actually, the list of nodes is implied in the list of edges, so we don't need to provide that if we don't feel like it (although, that doesn't apply if we have nodes that are not part of any edge). In addition, both nodes and edges can have attributes, but those are optional. So, our network graph data type would, at a minimum, need to be able to give us this information about all this via its interface. networkx is one of the most used Python libaries in this space, so let's decide that internally, for us, a network graph is represented as an object of the Graph class, or one of its subclasses. This class will give us a lot of useful methods and properties to access and query, one problem left is: how do we create an object of this class in a way that fits with our overall strategy? We can't save and load a networkx object directly ( pickling would be a bad idea for several reasons), so we need to create (and re-create) it via some other way. For this, lets look at the constructor arguments of this class, as well as what sort of data types we have available that we can use to feed those arguments. One option apparently is to use a list of edges contained in a Pandas dataframe as input, along with a name of columns representing the names of source and target column name, something like: graph: nx.DiGraph = nx.from_pandas_edgelist( pandas_table, source_column, target_column, ) This could work for us: as in the other example, we can use a table as the 'backing' data type for our graph object. Considering a graph without any node attributes, we can have a table with a minimum of two columns, and via a convention that we just made up, we say that the source column should be called edge_source , and the target column edge_target . We wrap all this in an Arrow table again, and save it as such. And later load it again, assuming the same convention (which, basically, saves us from asking for 2 column names every time). If our graph also includes node attributes, all we do is extend the implementation of our network graph data type to create a second table with a required column node_id , and one or several more columns that hold node attributes, similar to the metadata in our 'text corpus' example from above. 4) Transformed \u00b6 With all that out of the way, we can finally do something interesting with the data. Everything up to this point was more or less housekeeping: importing, tagging, marking, organizing datasets. We still are operating on the same actual data as was contained in the original files (whatever type they were). But we now know exactly what we can do with it without having to ask questions. Using the 3 example from above, we now know we have 3 datasets: one table, one text corpus (which is also a table, but a more specific one), and a network graph. And each of those datasets also comes with metadata, and we know what metadata files are available for what data types, and what the metadata means in each context. A first thing we can do is automatically matching datasets to available workflows: we know what input types a workflow takes (that is included in each workflow metadata). So all we need to do is check the input types of each available workflow against the type of a dataset. This works even with different specificity: give me all workflows that take as input a generic graph. Or: give me all workflows that take a directed graph as input (this is information that is included in the metadata of each network graph dataset).","title":"Data persistence"},{"location":"architecture/data/persistence/#data-persistence","text":"This is a document to describe my plans for storing data (and metadata) in kiara . (Almost) nothing I describe here is inmplemented yet, so it only reflects my current thinking. I think the overall strategy will hold, but there might be changes here and there.","title":"Data persistence"},{"location":"architecture/data/persistence/#the-problem","text":"kiara s main functionality centers around transforming input data sets to output data sets. Those outputs need to be stored, to be of any use later on. Obviously. When deciding how to do this, we must take into account concerns about performance, disk- and memory-usage, data versioning, which metadata to attach, in what way, how to deal with metadata schemas (and versioning of both), etc.","title":"The problem"},{"location":"architecture/data/persistence/#the-solution","text":"Well, solution. This is my current thinking of how to tackle the problem in a way that takes into account all of the aspects described above, while still being flexible enough to hopefully be able to incorporate solutions for future unforsseen issues. I am having trouble coming up with a good structure for this document, so I think I'll just try to tell the story from the point of view of data. Starting from a state where data exists outside of kiara , to when it is in a state to be ready to be published. As with everything I'm writing here as an explanation of generic and abstract concepts, some of the technical details I'm describing might be simplified to the point of being incorrect...","title":"The solution"},{"location":"architecture/data/persistence/#the-7-stages-of-data","text":"One thing I'd like to say before I start to describe those stages: the transformation of a dataset, from one stage to the next, always always always happens by piping the dataset through a kiara module. At absolutely no point is this done without kiara s involvement and knowledge. The dataset is used as input for a module, and the result (technically a new dataset) is a representation of the dataset in its next stage. This is important to keep in mind, as it is crucial for us so we can track data lineage. I'll write more on the specifics of this below, where it makes more sense.","title":"The 7 stages of data"},{"location":"architecture/data/persistence/#1-unmanaged","text":"At the beginning, there was csv. Whether I like it or not, csv is the most predominant form data comes in. Csv is bad in a lot of ways, but in my mind the worst thing about it is that it is schema-less. True, in some cases you have a header-line, which gives you column-names, but that's not a requirement. Also, in a lot of cases you can auto-determine the type of each column, and luckily libraries like Pandas or Apache Arrow solved that problem for us so we don't have to do it ourselves every time. But those auto-parsers are not fool-proof, and you end up with integers where you wanted floats (or doubles), or integers where you wanted strings, or vice versa. In some cases we get data in a form that includes at least a semi-schema. Like a sqlite database file (which is more 'strongly' typed). But it's a lucky day when we get data that contains metadata about authorship, how and when it was created, from what sources, etc.","title":"1) Unmanaged"},{"location":"architecture/data/persistence/#2-onboarded","text":"This is the first thing we need to do to unmanaged data: we need to 'onboard' it, so kiara knows the data exists, and what exact bytes it consists of. This last thing is very important: we have to be able to make sure the data we are talking about is not being changed externally, a lot of things in kiara s approach to data depend on this. Practically, in most cases this means kiara will copy one or several files into a protected area that no other application can/should access. That way we always have a reference version of the dataset (the bytes) we are talking about. One thing kiara does at this stage is give the dataset a uniuqe id, which can be used to reference it later (by users, or other objects/functions). Another thing is to collect some basic metadata: when the file/folder was imported, from what original path, what the filenames are, mime-type, size of files, original file attributes (creation data, permissions, etc.). This can all be captured automatically. We can also record who it was that imported the dataset, if we have some app-global configuration about the current user, like a full name and email-address. Note, that this might or might not be the person who created the dataset. So, at this stage all we did was copy a file(set) into a protected area to sort of 'freeze' it, and augment it with very basic metadata. We don't know anything about the nature of the dataset yet, all we know is the bytes the datasets consists of. It is important to point out that we would not have to store those chunks of bytes as files again, using the same structure as the original set of files. The dataset ceased to be 'files' here for us, we are only interested in the chunks of bytes (and their meaning) from here on out. We could store the data in an object store, zipped, tarred and feathered (pun intended). Or as byte-stream directly on block storage, if we were crazy enough. A side-note that makes things a bit more complicated, but it is probably necessary to address potential concerns: no, we don't actually need to copy the files, and can leave them in place and only generate the metadata and id for them. This might be necessary in cases where the source data is very big (photos, movies, audio-files, other large datasets). I don't think we need to figure out how exactly we deal with this scenario right now, but it basically comes down to making the user aware of what is happening, and what the implications are if the source data is changed externally (inconsistent metadata and potential incorrect result data-sets further down the line). There are strategies to help prevent some of those potential issues (checksums, for example), but overall we have to acknowledge that working with large-sized datasets is always a challenge, and in some cases we might just have to say: \"sorry, this is too big for us right now\".","title":"2) Onboarded"},{"location":"architecture/data/persistence/#3-augmented-with-more-basic-metadata","text":"To recapitulate: at this stage we have data (chunks of bytes -- not files!!! hit yourself over the head twice with something semi-heavy if you are still think in terms of files from here on out!) in a protected area, some very basic metadata, and an id for each dataset. We might or might not have authorship metadata (arguably one of the most important pieces of metadata), depending on whether who 'onboarded' the dataset actually created it. So, as a first step and following good practice, at this stage we should try to get the user to tell us about authorship and other core metadata about our dataset (licensing, copyright, ...). I don't think we can make this step mandatory, in practice, but we should push fairly hard, even if that means a slight decrease in user experience. It is very important information to have... So, one thing we could do was to have a checkbox that lets the user confirm: I created the data (in which case we can just copy the 'imported-by' field).","title":"3) Augmented with more (basic) metadata"},{"location":"architecture/data/persistence/#3-typed","text":"Chunks of bytes are not very useful by itself. They need to be interpreted to be of use. This means: determining in some way what the structure of the chunks of bytes is, and then applying common conventions for that particular structure (aka data type/format) when reading the chunks of bytes. Therefore 'interpreting' the chunks of bytes. This is a very obvious thing that happens all the time we use computers, but I think it makes sense to point it out here, because usually this is transparent to the user when they click an 'Open file' button in an application, and even some developers are ignorant to the underlying concept (and can afford to be, since they usually work several abstraction layers above where that is happening). To encapsulate this concept, we will create a 'data type' for each important group of datasets that share some important characteristics. Examples for very simple data types are strings, integers, booleans. I'll ignore those, because those are trivial to use, and that triviality actually makes it harder to explain the concept I'm talking about. More relevant data types are: 'table', 'network graph', 'text corpus', 'photo collection'. Every data type inherently contains a description of, well, the 'type' of data represented by it, and, with that, information about how a user or code caqn access the actual data, and/or some of its properties. From here on out, I'll focus on tabular data in some form or other, since I expect that this will be one of our most important (base-) data types. I expect the reader to 'translate' whatever I'm saying below to other types, and extrapolate the practical differences. So, to explain this step I decided to look at three different use-cases (mostly because we use them in 2 of our example workflows, so people should be familiar with them): a csv file with tabular data an imported folder of text files (a corpus) two imported csv files containing edge and node information to form a network graph","title":"3) Typed"},{"location":"architecture/data/persistence/#4-transformed","text":"With all that out of the way, we can finally do something interesting with the data. Everything up to this point was more or less housekeeping: importing, tagging, marking, organizing datasets. We still are operating on the same actual data as was contained in the original files (whatever type they were). But we now know exactly what we can do with it without having to ask questions. Using the 3 example from above, we now know we have 3 datasets: one table, one text corpus (which is also a table, but a more specific one), and a network graph. And each of those datasets also comes with metadata, and we know what metadata files are available for what data types, and what the metadata means in each context. A first thing we can do is automatically matching datasets to available workflows: we know what input types a workflow takes (that is included in each workflow metadata). So all we need to do is check the input types of each available workflow against the type of a dataset. This works even with different specificity: give me all workflows that take as input a generic graph. Or: give me all workflows that take a directed graph as input (this is information that is included in the metadata of each network graph dataset).","title":"4) Transformed"},{"location":"architecture/data/requirements/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import os import dharpa from rich.jupyter import print from dharpa import DHARPA_TOOLBOX_DEFAULT_WORKFLOWS_FOLDER from dharpa.graphs.utils import graph_to_image from dharpa.utils import get_data_from_file Context & Requirements \u00b6 Types of data \u00b6 scalars (mostly user inputs, booleans, strings, enums, numbers) lists of items of the same type tabular data (a collection of lists of items of the same type, each with the same number of items, incl. schema) binary data: images, videos, audio files In our application, we'll deal with a few basic types of data: scalars (mostly user inputs, booleans, strings, enums, numbers) lists of items of the same type tabular data (a collection of lists of items of the same type, each with the same number of items, incl. schema) binary data: images, videos, audio files Sidenote : I consider every kind of user input as data, since it is conceptually the same thing and needs to be recorded and managed the same way. For our purpose, we can ignore scalars because they are easy and cheap to handle, and can be attached to any sort of data or metadata in a few different ways. Also, let's ignore binary data for now, while acknowledging that we will need a strategy to deal with efficiently, in a way that is not too different from how we deal with other types. Which leaves us with lists and tabular data. Those are different to scalars, because there is no telling in advance how many rows they will have, and how large its cells will be (aka 'how many bytes are we dealing with, KBs, MBs, GBs, TBs?'). List (arrays) will be our main data type, along with tables (dataframes) -- the latter are really just lists of lists (including a schema/description of the type of each list). In a lot of cases a module will receive a table, and the output will be a list of the same length as the table. When using Pandas, we usually assign dataframes to variables, this is handy because we have access to the whole dataset via a single variable, and can access the columns seperately via their names. For our case, because we will have connected modules, we will probably deal with 2 scenarios: a module changes the data in a dataframe in one or several columns: this will be rare, but in this case the result of such a module will be a new dataframe a module adds one or several column to a dataset: this is much more common. It doesn't make much sense to have dataframes as outputs in this case, since those would contain the same data as the input. There is no need to allocate double the amount of memory, for an exact copy of something we already have available (for read purposes). So, in those cases the output will be one or several lists, with the same amount of rows as the input dataframe. Those lists can then be easily assembled into a dataframe at a later stage, if the need arises. Requirements \u00b6 Since data will be the central object our application handles, we need to decide on an internal (as well as import/export) data format. The obvious thing to do would be to use the most common format (probably json), and just use that. For several reasons (layed out in the data_formats document ), I don't think this is a good idea in our case. I think we can anticipate our main requirements on a data format before writing any code, which is why I created this document: to list those requirements, and to come up with a recommendation that is based upon them. Technical requirements \u00b6 schema-aware (ideally included in the format) binary format (performance, filesize) column-based (for tabular data -- analytics query performance) zero-copy, memory-mapping compression in-build (preferrable) possible to use from different programming languages (at least Python & JS) as little cpu, memory, and disk utilization as possible The first group of requirements is technical: we are creating an interactive application, which means we should at least spend some time optimizing for speed (in those instances where it's possbile). In addition: the more we know about our data and its 'shape', the less complex our code has to be, since that removes the need for investigating and validating data at multiple points along its journey. The latter can be achieved by using a data format that is schema aware (e.g. not csv), and ideally includes that schema as metadata in its specification, so we can query the data(-bytes) directly, without having to read seperate, external specifications. For the performance requirements, it's fairly easy to see why we should be looking for a binary, column-based format, that ideally has extra features like memory-mapping and compression. Last but not least we want to be able to access our data from different programming languages. Python and JavaScript support will be mandatatory, but being able to read from Julia and R would also be highly desirable. General requirements \u00b6 option to attach metadata versioning of datasets versioning of metadata we want to be able to treat all data the same way, independent of size, format, other characteristics we want all of this to be more or less transparent to our end-users! Because it's in the nature of our application that we won't exactly know hob big and what shape the data we will be dealing with will have, we have to anticipate a wide range of types and sizes. In order to not have to deal with those differentely each time, it would be highly adventageous if we can come up with a standard 'interface' for our datasets, that lets us, as a minimum, query minimal required metadata (schema, authors, size), and which allows us to forward the dataset to other parts of our application (other modules, frontend), without having to convert or serialize/deserialize it. Most importantly, we will have to figure out a way to make most of this transparent to users. This is probably nothing a data format can help us with directly, but there might be factors in the periphery which can make this easier, or harder (e.g.: how common is that data-format, how much tooling exists for it?) One of our main requirements is to be able to easily attach metadata to our datasets. In addition we want it to be as easy as possible to 'version' the containing data, as well as the attached metadata. Those requirements stem from the need for good research data practices, and should not need further explanation. Let's look at those two points in a bit more detail: Technical metadata (automatic) \u00b6 data type schema (if tabular data) statistics on columns, rows (number of rows, min in column, max in column) data specific indicators/summaries (e.g. geographic range, resolution, ...) digest / checksum (unique id!) The most important metadata we'll be dealing with is the type of data, and its schema in case its in tabular form. As was mentioned above, ideally this would be forced and included by/in the data format we choose, so we can rely on it to be available, always. In addition, in a lot of cases it aids performance if certain characteristics of a dataset are known without having to actually touch it. One example would be min/max values for numeric columns. Geographic range, resolution could be interesting for location data, creation date for photos, and so on. A special item of metadata is a checksum: that enables us to confirm the bytes of a dataset haven't changed since we last checked, and it also makes things like caching or lookups easier. All of those metadata items can be created more or less automatically, without any user input. This is important to differentiate, because that means we don't need to worry about providing a user-interface so they can be added/attached. Other metadata (requires user input) \u00b6 provenance / lineage / heritage author(s)/creator(s) incl. contact info creation / modification date comments, annotations \"ALL THE METADATA\" (Angela R. Cunningham, PhD) The second category of metadata is defined by the necessity for manual user input (at least in parts). Which of course means we need to somehow provide a metadata editing facility for those items. Authorship information as well has the provenance-related metadata is arguably the most important one here. But I imagine we'll come up with quite a few more metadata fields we will want to be able to attach. It's probabaly a good idea to talk to our colleagues who develop Zotero and Tropy for some input in this regard. Dataset versioning \u00b6 versioning of the 'actual' data: new data added existing data changed/fixed existing data removed metadata versioning: independent of actual data changes (except for last modification dates, new authors added, checksum) new metadata added existing metadata changed/fixed metadata removed no new dataset version necessary Data versioning is usually a bit overlooked (although that seems to be changing now, and there are some 'git for data' services and tools cropping up). But it's crucial for good data practices. In order to always know how result data was created, we need to know exactly which inputs were used, and what exactly was done to them. If any of the inputs changes, and we don't record it, then there will be confusion later, when someone tries to recreate a result with the changed input. This implies we have a way to point to datasets using some sort of identifier, something like a DOI -- but it does not need to be globally unique, just locally (unless the data gets shared/exported). Contexts in which we handle data \u00b6 'onboarding' data: external data gets fed into a workflow / into our app we store a copy internally (to prevent changes we are not aware of) some minimal metadata needs to be provided (but can be at least partly determined automatically) gets unique id / alias(es) & checksum & version '1' internal data transformation & transfer: each module processes input data and creates output data output data gets fed into the input of another module input/output data is requested by frontend for display purposes (viz, statistics, table-view, ...) exporting data: researcher needs data in a specific format (csv, Excel, json...) for further processing, publishing, etc. Along with listing requirements, it makes sense to think about in which contexts we deal with data, and how. I think we can seperate three main areas: data onboarding internal data transformation & transfer data export For the first and last items the 'interface' of the data is important, which means we are concerned about how to translate external dataformat into our internal one, as well as the other way around. For the second item we only deal with our internal format, so performance and code complexity are more important considerations. For data onboarding, one thing is important is that we store a copy of the dataset the user points us to in a space where we can be sure the data doesn't get changed by external means. We would also add some automatic metadata, and might or might not require the user to provide some basic required metadata-fields manually. We would also give a newly onboarded dataset a version '1' (or maybe '1.0'). Data export is the least problematic area: since we have a minimal set of required metadata for every piece of data we use internally, it should be fairly trivial to export it into any viable export format (csv, excel, json, parquet,...). Data onboarding and export could also be combined in some scenarios: for example if we don't provide a tool to 'clean' up data (or do something else that would require a version change on the dataset) and users would have to do it externally, we could export the dataset into a temporary folder, let the user do their thing, and then re-import the changed dataset into a new version of the existing, internal one, copying the existing metadata with some additions that describe what was done to the data. Solution proposal \u00b6 Apache Arrow \u00b6 binary, column-based, language-independent in-memory format well defined schema and data types, rudimentary custom metadata support native support for 2 on-disk formats: feather (same as in-memory format), parquet client implementations for most relevant languages growing ecosystem: Arrow Flight (fast data transport framework) Plasma (In-Memory object store) Vaex (native support for memory-mapped feather files, memory-mapped querying) duckdb (column-based, python-native sql engine) easy import/export to NumPy/Pandas types (Arrays, DataFrames) -- still some serialization cost likely to be the standard format for data exchange in data science/data engineering in the future In my research, Apache Arrow came closest to match our technical requirements, and should let us implement most of the other ones too. It is a binary, column based in-memory format that comes with implementations in a number of programming languages (incl. the ones we are interested in). From the Arrow website: Apache Arrow is a software development platform for building high performance applications that process and transport large data sets. It is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system or programming language to another. A critical component of Apache Arrow is its in-memory columnar format, a standardized, language-agnostic specification for representing structured, table-like datasets in-memory. This data format has a rich data type system (included nested and user-defined data types) designed to support the needs of analytic database systems, data frame libraries, and more. In addition to the efficient in-memory format, it supports 2 on-disk formats: feather & parquet. The former one is basically the same as the in-memory format (with all the advantages that come with that), and the latter is a fairly standard format to exchange large(-ish) datasets between processes and infrastructure components. In my opinion (and I'm not alone), Arrow will be the de-facto standard data format for tabular data in the future, in both data science and data engineering. It is well designed, and a lot of the reasons why it came about line up fairly well with our own requirements (althought, at a different scale obviously). Because of that, there is a rich tooling ecosystem growing around Apache Arrow at the moment, which I think we can expect to satisfy to most of our current and future needs in the near to medium-term future, if not already. Esp. vaex and duckdb look like very interesting developments. Pandas and Numpy import/export is very well supported, and as well optimized as seems possible. Apache Arrow Flight and the Plasma Object store look like good contenders that could handle our potential data transport needs in the future. Identifying and versioning datasets \u00b6 every dataset gets it's unique id (uuid) as well as one or several user-defined and automatic aliases a new version of a dataset is created when its data content changes (content can be entirely different) a user can 'designate' a new version of data, in some cases it can be done by our application automatically versioning of metadata is independent of dataset version allows us to discover 'out-of-date' results (via their connected input-ids), and recreating them with updated input dataset frontend must be able to list, access and query datasets/inputs/outputs via unique id & version number It should be obvious that and why we need some sort of (at least internal) unique identifier for each dataset. The main scenario where users will come in touch with such an identifier is when they are asked to choose an input dataset for a module/workflow. It's possible to make that 100% transparent to the user, and let them for example select a folder of csv files, which we would then copy into our internal data repository, assign it an id, and use that for our calculation. That would mean though, that the next time the user wants to use the same dataset again, we would do the same again, basically duplicating our internal dataset. We probably could be smart about it, and recognize those sort of duplicates, but that would involve fairly complex and fragile code I think we should rather avoid, and come up with an interface metaphor/language that makes users aware what is going on, and which empowers them with proper tooling to manage their research data according using best practices (metadata, versioning, etc.). So, I propose that we should have a 'data management' section in our application UI, which could be used to both 'onboard' and manage datasets independent of a workflow, but also within the context of a workflow (for example by re-using some of the file selection widgets and filling in a newly create dataset id into a workflow input, right after onboarding). How that would look like exactly, we'd have to figure out and I think it would be a work-item on itself. The same goes for dataset versioning. One way I can imagine this working is to have a .<major>.<minor> postfix to our unique dataset identifier, where the minor part gets incremented with every metadata version change, and the major part for when the actual data changes. Another point to consider is whether to only use version number increases, or also have a concept of 'branching', where the versions of datasets can diverge, from a common parent. I think there is a point to be made for not making things to complicated unless really necessary, so most of this can be solved with a simple versioning scheme, and assigning totally new datasets id if something significant changes in the data of a dataset (while potentially preserving the lineage information by storing the 'parent id' in the new datasets metadata). But, as I said above, I think this would be a good item to investigate independently. Storing module results \u00b6 requirements: workflow history & snapshots & long running processes need for caching of at least the latest results This section includes a quick recapitulation how our workflows are described and managed by the backend, as well as an outline how to handle and store temporary as well as final workflow outputs. This is important, because having access to already computed results is necessary for some of our requirements (derived from our user-stories): - workflow history: enable the user to move back in the history of input sets of a workflow session - snapshots: 'tag' certain input sets (basically creating a snapshot of that particular workflow state) - support for long running processes: a user will want to have access to computational results, even if the had other workflow sessions inbetween (while a particularly long running job was running) Quick recap: workflow modularity \u00b6 Every module has: - one or several named inputs - one or several named outputs - as well as schema information for each input and output A workflow is a user-facing entity that: - can also be used as a module (has inputs, outputs, schema) - contains one or several modules - where some inputs of some (internal) modules can be connected to an output of another (internal) module - inputs of modules that are not connected to an output of another (internal) module are user inputs In this example we'll use a workflow that is simlates a nand logic-gate. Such a logic gate can be created by using and and not logic gates one after the other. Below you can see a short description of the modules and their inputs, as well as how that would be configured in a workflow description json file. The important part is the modules value. example module: nand \u00b6 consists of two other modules: and inputs: a & b (booleans) output: y (boolean - true if both inputs are true, otherwise false) not : input: a (boolean - connected to y output of and ) output: y (boolean - negated input) two inputs: a & b (booleans, connect directly to and inputs) one output: y (false if 'a' & 'b' are true, otherwise true -- connects to y output of not module) print ( \"Module description: [b]nand[/b]\" ) print ( get_data_from_file ( os . path . join ( DHARPA_TOOLBOX_DEFAULT_WORKFLOWS_FOLDER , \"logic_gates\" , \"nand.json\" ))) Module description: nand { 'modules' : [ { 'module_type' : 'and' } , { 'module_type' : 'not' , 'input_links' : { 'a' : 'and.y' }} ] , 'input_aliases' : { 'and__a' : 'a' , 'and__b' : 'b' } , 'output_aliases' : { 'not__y' : 'y' } , 'module_type_name' : 'nand' , 'meta' : { 'doc' : \"Returns 'True' if both inputs are 'False'.\" } } After creating the workflow description file, we create the workflow object in code, and for illustration purposes, we display the execution order and the state graph of the workflow (in its inital, stale state without any inputs). workflow = dharpa . create_workflow ( \"nand\" ) graph_to_image ( workflow . structure . execution_graph ) graph_to_image ( workflow . create_state_graph ( show_structure = True )) Now, we set the inputs (both True , which means the end-result should be False ). As you can see from the state graph, the workflow inputs are directly connected to the module inputs of the and module. workflow . inputs . a = True workflow . inputs . b = True await workflow . process () processing started: nand.nand processing started: nand.and processing finished: nand.and processing started: nand.not processing finished: nand.not processing finished: nand.nand Again, lets look at the workflow state, this time we display it using a json data structure, not a network graph: state = workflow . to_dict ( include_structure = True ) print ( state ) { 'alias' : 'nand' , 'address' : 'nand.nand' , 'type' : 'nand' , 'is_pipeline' : True , 'state' : 'results_ready' , 'inputs' : { 'a' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } , 'b' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'outputs' : { 'y' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : False }} , 'execution_stage' : None , 'doc' : \"Returns 'True' if both inputs are 'False'.\" , 'pipeline_structure' : { 'workflow_id' : 'nand' , 'modules' : [ { 'module' : { 'alias' : 'and' , 'address' : 'nand.and' , 'type' : 'and' , 'is_pipeline' : False , 'state' : 'results_ready' , 'inputs' : { 'a' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } , 'b' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'outputs' : { 'y' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'execution_stage' : 1 , 'doc' : \"Returns 'True' if both inputs are 'True'.\" , 'pipeline_structure' : None } , 'input_connections' : { 'a' : '__parent__.a' , 'b' : '__parent__.b' } , 'output_connections' : { 'y' : [ 'not.a' ]} } , { 'module' : { 'alias' : 'not' , 'address' : 'nand.not' , 'type' : 'not' , 'is_pipeline' : False , 'state' : 'results_ready' , 'inputs' : { 'a' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'outputs' : { 'y' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : False } } , 'execution_stage' : 2 , 'doc' : 'Negates the input.' , 'pipeline_structure' : None } , 'input_connections' : { 'a' : 'and.y' } , 'output_connections' : { 'y' : [ '__parent__.y' ]} } ] , 'workflow_input_connections' : { 'a' : [ 'and.a' ] , 'b' : [ 'and.b' ]} , 'workflow_output_connections' : { 'y' : 'not.y' } } } How to actually deal with workflow/module outputs? \u00b6 why not store all results? smart way of storing/deleting/managing storage: compression efficient module design cleanup process only store results if good execution time/result size ratio, otherwise just re-process To satisfy the above mentioned requirements, my current plan is to just store all results of all module runs, instead of coming up with a complicated caching scheme. There will have to be some sort of 'result-cleaning' and consolidation, but I think if we are being smart about it this might be the most promising strategy, which will introduce the least amount of complexity. A folder structure to accomodate that would probably look something like this: each module has its own name/id, all results for a module will be stored under same folder 'result.feather' has one or several columns that represent output values also, one column with runtime metadata (execution time, version of workflow, etc.) this works well with the 'dataset' API in Apache Arrow: https://arrow.apache.org/docs/python/dataset.html (which means we can lazy-load all results of a workflow/module into the same dataframe, and do 'meta'-queries and -analysis on that if we choose to) debatable whether 'workflow-results' have to be stored at all, since they are just copies of 'module-results' In order to not waste too much hard-disk space (which would be the most obvious concern here), I think we have a few different options. For one, we'd store all results with compression enabled. We would implement our modules in an efficient way that is aware of how we store results. We might have a cleanup process running in the background that is aware of how often a result is accessed, and how it's compute-time/result-size ratio is. In some cases where that ratio leans very much towards result-size, we might decide to not store those results at all, but re-process every time. Streaming module results \u00b6 TBD This is an area I haven't done too much work on yet, but in general: we will want to have access to intermediate results (or, rather: partial results in real-time), so we can provide the user with information they can use to determine whether to cancel a running process or not. Even though we will probably not have that functionality available in our initial, first version, I think we should anticipate that requirement, and design our data management with it in mind, so it can be added later without having to re-write a lot of code. Default data format (for import/export) \u00b6 every result can be described by specifying: the input dataset(s) and other inputs the workflow (and workflow version) that was used to produce it -> theoretically, every (result) dataset can be described by very small json file/metadata set proposal: invent our own (small) set of file formats (including version-nr, metadata schema, payload) Apache Arrow based for tabular/scalar data folder/zip based for binary data all our import modules would create files in that format provide tooling (and modules) to convert/export those to all common data formats possibility of data registries: very simple implementation compared to products like dataverse, ckan high performance data transfer (using Apache Flight) different levels: local (within our app), organization-wide, global (aka default registry) The last thing to decide is whether we want to provide a 'standard' data format for our application. This will be modelled closely upon the format we will use internally, but with some added metadata fields and possibly restrictions. This is mostly for the purpose of sharing, transferring, and publishing data. In principle, there is a really lightweight way to share our work: since we can describe everything we do by specifying the workflow, and listing all the inputs we use with it. Assuming all inputs are either scalars or, in case of datasets, available via download, this description could be very lightweight: it's just a json file containing the workflow structure (incl. maybe version information), and input-data urls. With that, everyone with access to the data can in theory replicate any end- and intermediate result. In theory, that json structure can also be attached to every result dataset, which means that our results will always come with information how they were produced (and how to re-produce them). Since all this is very dependent on being able to have access to metadata alongside the 'actual' data, and because in my experience systems and architectures that store metadata seperately to data are either fairly complex, specific and hard to maintain, I would propose we come up with a way to package our data in a way that allows for our metadata to always be included, and where it's easy to access both data and metadata without having to open the whole file. Arrow gets us a long way toward that (for tabular data), the only thing that is missing is a standard way to include metadata. For that we have two options: use the Arrow 'metadata' field (which is fairly limited, it only takes encoded byte-arrays as keys/values), or store our metadata in a seperate column. Currently, I'm leaning toward the latter option, but this is something we'll have to try out and play with to get a better idea how feasable it is. For other types of data (binary blobs, images, etc.), I propose we use an archive format (zip, tar, ...) with a json file at a standard location (e.g. './.metadata.json') that includes the same metadata schema a tabular dataset would use. That way our datasets always have the same 'interface'. And we can provide a set of standard tools (which could be implemented as workflow modules and workflows) to import and export 'our' data from/to commonly used formats like csv, excel, etc (which in most cases would not include metadata at all).","title":"Requirements"},{"location":"architecture/data/requirements/#context-requirements","text":"","title":"Context &amp; Requirements"},{"location":"architecture/data/requirements/#types-of-data","text":"scalars (mostly user inputs, booleans, strings, enums, numbers) lists of items of the same type tabular data (a collection of lists of items of the same type, each with the same number of items, incl. schema) binary data: images, videos, audio files In our application, we'll deal with a few basic types of data: scalars (mostly user inputs, booleans, strings, enums, numbers) lists of items of the same type tabular data (a collection of lists of items of the same type, each with the same number of items, incl. schema) binary data: images, videos, audio files Sidenote : I consider every kind of user input as data, since it is conceptually the same thing and needs to be recorded and managed the same way. For our purpose, we can ignore scalars because they are easy and cheap to handle, and can be attached to any sort of data or metadata in a few different ways. Also, let's ignore binary data for now, while acknowledging that we will need a strategy to deal with efficiently, in a way that is not too different from how we deal with other types. Which leaves us with lists and tabular data. Those are different to scalars, because there is no telling in advance how many rows they will have, and how large its cells will be (aka 'how many bytes are we dealing with, KBs, MBs, GBs, TBs?'). List (arrays) will be our main data type, along with tables (dataframes) -- the latter are really just lists of lists (including a schema/description of the type of each list). In a lot of cases a module will receive a table, and the output will be a list of the same length as the table. When using Pandas, we usually assign dataframes to variables, this is handy because we have access to the whole dataset via a single variable, and can access the columns seperately via their names. For our case, because we will have connected modules, we will probably deal with 2 scenarios: a module changes the data in a dataframe in one or several columns: this will be rare, but in this case the result of such a module will be a new dataframe a module adds one or several column to a dataset: this is much more common. It doesn't make much sense to have dataframes as outputs in this case, since those would contain the same data as the input. There is no need to allocate double the amount of memory, for an exact copy of something we already have available (for read purposes). So, in those cases the output will be one or several lists, with the same amount of rows as the input dataframe. Those lists can then be easily assembled into a dataframe at a later stage, if the need arises.","title":"Types of data"},{"location":"architecture/data/requirements/#requirements","text":"Since data will be the central object our application handles, we need to decide on an internal (as well as import/export) data format. The obvious thing to do would be to use the most common format (probably json), and just use that. For several reasons (layed out in the data_formats document ), I don't think this is a good idea in our case. I think we can anticipate our main requirements on a data format before writing any code, which is why I created this document: to list those requirements, and to come up with a recommendation that is based upon them.","title":"Requirements"},{"location":"architecture/data/requirements/#technical-requirements","text":"schema-aware (ideally included in the format) binary format (performance, filesize) column-based (for tabular data -- analytics query performance) zero-copy, memory-mapping compression in-build (preferrable) possible to use from different programming languages (at least Python & JS) as little cpu, memory, and disk utilization as possible The first group of requirements is technical: we are creating an interactive application, which means we should at least spend some time optimizing for speed (in those instances where it's possbile). In addition: the more we know about our data and its 'shape', the less complex our code has to be, since that removes the need for investigating and validating data at multiple points along its journey. The latter can be achieved by using a data format that is schema aware (e.g. not csv), and ideally includes that schema as metadata in its specification, so we can query the data(-bytes) directly, without having to read seperate, external specifications. For the performance requirements, it's fairly easy to see why we should be looking for a binary, column-based format, that ideally has extra features like memory-mapping and compression. Last but not least we want to be able to access our data from different programming languages. Python and JavaScript support will be mandatatory, but being able to read from Julia and R would also be highly desirable.","title":"Technical requirements"},{"location":"architecture/data/requirements/#general-requirements","text":"option to attach metadata versioning of datasets versioning of metadata we want to be able to treat all data the same way, independent of size, format, other characteristics we want all of this to be more or less transparent to our end-users! Because it's in the nature of our application that we won't exactly know hob big and what shape the data we will be dealing with will have, we have to anticipate a wide range of types and sizes. In order to not have to deal with those differentely each time, it would be highly adventageous if we can come up with a standard 'interface' for our datasets, that lets us, as a minimum, query minimal required metadata (schema, authors, size), and which allows us to forward the dataset to other parts of our application (other modules, frontend), without having to convert or serialize/deserialize it. Most importantly, we will have to figure out a way to make most of this transparent to users. This is probably nothing a data format can help us with directly, but there might be factors in the periphery which can make this easier, or harder (e.g.: how common is that data-format, how much tooling exists for it?) One of our main requirements is to be able to easily attach metadata to our datasets. In addition we want it to be as easy as possible to 'version' the containing data, as well as the attached metadata. Those requirements stem from the need for good research data practices, and should not need further explanation. Let's look at those two points in a bit more detail:","title":"General requirements"},{"location":"architecture/data/requirements/#dataset-versioning","text":"versioning of the 'actual' data: new data added existing data changed/fixed existing data removed metadata versioning: independent of actual data changes (except for last modification dates, new authors added, checksum) new metadata added existing metadata changed/fixed metadata removed no new dataset version necessary Data versioning is usually a bit overlooked (although that seems to be changing now, and there are some 'git for data' services and tools cropping up). But it's crucial for good data practices. In order to always know how result data was created, we need to know exactly which inputs were used, and what exactly was done to them. If any of the inputs changes, and we don't record it, then there will be confusion later, when someone tries to recreate a result with the changed input. This implies we have a way to point to datasets using some sort of identifier, something like a DOI -- but it does not need to be globally unique, just locally (unless the data gets shared/exported).","title":"Dataset versioning"},{"location":"architecture/data/requirements/#contexts-in-which-we-handle-data","text":"'onboarding' data: external data gets fed into a workflow / into our app we store a copy internally (to prevent changes we are not aware of) some minimal metadata needs to be provided (but can be at least partly determined automatically) gets unique id / alias(es) & checksum & version '1' internal data transformation & transfer: each module processes input data and creates output data output data gets fed into the input of another module input/output data is requested by frontend for display purposes (viz, statistics, table-view, ...) exporting data: researcher needs data in a specific format (csv, Excel, json...) for further processing, publishing, etc. Along with listing requirements, it makes sense to think about in which contexts we deal with data, and how. I think we can seperate three main areas: data onboarding internal data transformation & transfer data export For the first and last items the 'interface' of the data is important, which means we are concerned about how to translate external dataformat into our internal one, as well as the other way around. For the second item we only deal with our internal format, so performance and code complexity are more important considerations. For data onboarding, one thing is important is that we store a copy of the dataset the user points us to in a space where we can be sure the data doesn't get changed by external means. We would also add some automatic metadata, and might or might not require the user to provide some basic required metadata-fields manually. We would also give a newly onboarded dataset a version '1' (or maybe '1.0'). Data export is the least problematic area: since we have a minimal set of required metadata for every piece of data we use internally, it should be fairly trivial to export it into any viable export format (csv, excel, json, parquet,...). Data onboarding and export could also be combined in some scenarios: for example if we don't provide a tool to 'clean' up data (or do something else that would require a version change on the dataset) and users would have to do it externally, we could export the dataset into a temporary folder, let the user do their thing, and then re-import the changed dataset into a new version of the existing, internal one, copying the existing metadata with some additions that describe what was done to the data.","title":"Contexts in which we handle data"},{"location":"architecture/data/requirements/#solution-proposal","text":"","title":"Solution proposal"},{"location":"architecture/data/requirements/#apache-arrow","text":"binary, column-based, language-independent in-memory format well defined schema and data types, rudimentary custom metadata support native support for 2 on-disk formats: feather (same as in-memory format), parquet client implementations for most relevant languages growing ecosystem: Arrow Flight (fast data transport framework) Plasma (In-Memory object store) Vaex (native support for memory-mapped feather files, memory-mapped querying) duckdb (column-based, python-native sql engine) easy import/export to NumPy/Pandas types (Arrays, DataFrames) -- still some serialization cost likely to be the standard format for data exchange in data science/data engineering in the future In my research, Apache Arrow came closest to match our technical requirements, and should let us implement most of the other ones too. It is a binary, column based in-memory format that comes with implementations in a number of programming languages (incl. the ones we are interested in). From the Arrow website: Apache Arrow is a software development platform for building high performance applications that process and transport large data sets. It is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system or programming language to another. A critical component of Apache Arrow is its in-memory columnar format, a standardized, language-agnostic specification for representing structured, table-like datasets in-memory. This data format has a rich data type system (included nested and user-defined data types) designed to support the needs of analytic database systems, data frame libraries, and more. In addition to the efficient in-memory format, it supports 2 on-disk formats: feather & parquet. The former one is basically the same as the in-memory format (with all the advantages that come with that), and the latter is a fairly standard format to exchange large(-ish) datasets between processes and infrastructure components. In my opinion (and I'm not alone), Arrow will be the de-facto standard data format for tabular data in the future, in both data science and data engineering. It is well designed, and a lot of the reasons why it came about line up fairly well with our own requirements (althought, at a different scale obviously). Because of that, there is a rich tooling ecosystem growing around Apache Arrow at the moment, which I think we can expect to satisfy to most of our current and future needs in the near to medium-term future, if not already. Esp. vaex and duckdb look like very interesting developments. Pandas and Numpy import/export is very well supported, and as well optimized as seems possible. Apache Arrow Flight and the Plasma Object store look like good contenders that could handle our potential data transport needs in the future.","title":"Apache Arrow"},{"location":"architecture/data/requirements/#identifying-and-versioning-datasets","text":"every dataset gets it's unique id (uuid) as well as one or several user-defined and automatic aliases a new version of a dataset is created when its data content changes (content can be entirely different) a user can 'designate' a new version of data, in some cases it can be done by our application automatically versioning of metadata is independent of dataset version allows us to discover 'out-of-date' results (via their connected input-ids), and recreating them with updated input dataset frontend must be able to list, access and query datasets/inputs/outputs via unique id & version number It should be obvious that and why we need some sort of (at least internal) unique identifier for each dataset. The main scenario where users will come in touch with such an identifier is when they are asked to choose an input dataset for a module/workflow. It's possible to make that 100% transparent to the user, and let them for example select a folder of csv files, which we would then copy into our internal data repository, assign it an id, and use that for our calculation. That would mean though, that the next time the user wants to use the same dataset again, we would do the same again, basically duplicating our internal dataset. We probably could be smart about it, and recognize those sort of duplicates, but that would involve fairly complex and fragile code I think we should rather avoid, and come up with an interface metaphor/language that makes users aware what is going on, and which empowers them with proper tooling to manage their research data according using best practices (metadata, versioning, etc.). So, I propose that we should have a 'data management' section in our application UI, which could be used to both 'onboard' and manage datasets independent of a workflow, but also within the context of a workflow (for example by re-using some of the file selection widgets and filling in a newly create dataset id into a workflow input, right after onboarding). How that would look like exactly, we'd have to figure out and I think it would be a work-item on itself. The same goes for dataset versioning. One way I can imagine this working is to have a .<major>.<minor> postfix to our unique dataset identifier, where the minor part gets incremented with every metadata version change, and the major part for when the actual data changes. Another point to consider is whether to only use version number increases, or also have a concept of 'branching', where the versions of datasets can diverge, from a common parent. I think there is a point to be made for not making things to complicated unless really necessary, so most of this can be solved with a simple versioning scheme, and assigning totally new datasets id if something significant changes in the data of a dataset (while potentially preserving the lineage information by storing the 'parent id' in the new datasets metadata). But, as I said above, I think this would be a good item to investigate independently.","title":"Identifying and versioning datasets"},{"location":"architecture/data/requirements/#storing-module-results","text":"requirements: workflow history & snapshots & long running processes need for caching of at least the latest results This section includes a quick recapitulation how our workflows are described and managed by the backend, as well as an outline how to handle and store temporary as well as final workflow outputs. This is important, because having access to already computed results is necessary for some of our requirements (derived from our user-stories): - workflow history: enable the user to move back in the history of input sets of a workflow session - snapshots: 'tag' certain input sets (basically creating a snapshot of that particular workflow state) - support for long running processes: a user will want to have access to computational results, even if the had other workflow sessions inbetween (while a particularly long running job was running)","title":"Storing module results"},{"location":"architecture/data/requirements/#quick-recap-workflow-modularity","text":"Every module has: - one or several named inputs - one or several named outputs - as well as schema information for each input and output A workflow is a user-facing entity that: - can also be used as a module (has inputs, outputs, schema) - contains one or several modules - where some inputs of some (internal) modules can be connected to an output of another (internal) module - inputs of modules that are not connected to an output of another (internal) module are user inputs In this example we'll use a workflow that is simlates a nand logic-gate. Such a logic gate can be created by using and and not logic gates one after the other. Below you can see a short description of the modules and their inputs, as well as how that would be configured in a workflow description json file. The important part is the modules value.","title":"Quick recap: workflow modularity"},{"location":"architecture/data/requirements/#example-module-nand","text":"consists of two other modules: and inputs: a & b (booleans) output: y (boolean - true if both inputs are true, otherwise false) not : input: a (boolean - connected to y output of and ) output: y (boolean - negated input) two inputs: a & b (booleans, connect directly to and inputs) one output: y (false if 'a' & 'b' are true, otherwise true -- connects to y output of not module) print ( \"Module description: [b]nand[/b]\" ) print ( get_data_from_file ( os . path . join ( DHARPA_TOOLBOX_DEFAULT_WORKFLOWS_FOLDER , \"logic_gates\" , \"nand.json\" ))) Module description: nand { 'modules' : [ { 'module_type' : 'and' } , { 'module_type' : 'not' , 'input_links' : { 'a' : 'and.y' }} ] , 'input_aliases' : { 'and__a' : 'a' , 'and__b' : 'b' } , 'output_aliases' : { 'not__y' : 'y' } , 'module_type_name' : 'nand' , 'meta' : { 'doc' : \"Returns 'True' if both inputs are 'False'.\" } } After creating the workflow description file, we create the workflow object in code, and for illustration purposes, we display the execution order and the state graph of the workflow (in its inital, stale state without any inputs). workflow = dharpa . create_workflow ( \"nand\" ) graph_to_image ( workflow . structure . execution_graph ) graph_to_image ( workflow . create_state_graph ( show_structure = True )) Now, we set the inputs (both True , which means the end-result should be False ). As you can see from the state graph, the workflow inputs are directly connected to the module inputs of the and module. workflow . inputs . a = True workflow . inputs . b = True await workflow . process () processing started: nand.nand processing started: nand.and processing finished: nand.and processing started: nand.not processing finished: nand.not processing finished: nand.nand Again, lets look at the workflow state, this time we display it using a json data structure, not a network graph: state = workflow . to_dict ( include_structure = True ) print ( state ) { 'alias' : 'nand' , 'address' : 'nand.nand' , 'type' : 'nand' , 'is_pipeline' : True , 'state' : 'results_ready' , 'inputs' : { 'a' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } , 'b' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'outputs' : { 'y' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : False }} , 'execution_stage' : None , 'doc' : \"Returns 'True' if both inputs are 'False'.\" , 'pipeline_structure' : { 'workflow_id' : 'nand' , 'modules' : [ { 'module' : { 'alias' : 'and' , 'address' : 'nand.and' , 'type' : 'and' , 'is_pipeline' : False , 'state' : 'results_ready' , 'inputs' : { 'a' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } , 'b' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'outputs' : { 'y' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'execution_stage' : 1 , 'doc' : \"Returns 'True' if both inputs are 'True'.\" , 'pipeline_structure' : None } , 'input_connections' : { 'a' : '__parent__.a' , 'b' : '__parent__.b' } , 'output_connections' : { 'y' : [ 'not.a' ]} } , { 'module' : { 'alias' : 'not' , 'address' : 'nand.not' , 'type' : 'not' , 'is_pipeline' : False , 'state' : 'results_ready' , 'inputs' : { 'a' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'outputs' : { 'y' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : False } } , 'execution_stage' : 2 , 'doc' : 'Negates the input.' , 'pipeline_structure' : None } , 'input_connections' : { 'a' : 'and.y' } , 'output_connections' : { 'y' : [ '__parent__.y' ]} } ] , 'workflow_input_connections' : { 'a' : [ 'and.a' ] , 'b' : [ 'and.b' ]} , 'workflow_output_connections' : { 'y' : 'not.y' } } }","title":"example module: nand"},{"location":"architecture/data/requirements/#how-to-actually-deal-with-workflowmodule-outputs","text":"why not store all results? smart way of storing/deleting/managing storage: compression efficient module design cleanup process only store results if good execution time/result size ratio, otherwise just re-process To satisfy the above mentioned requirements, my current plan is to just store all results of all module runs, instead of coming up with a complicated caching scheme. There will have to be some sort of 'result-cleaning' and consolidation, but I think if we are being smart about it this might be the most promising strategy, which will introduce the least amount of complexity. A folder structure to accomodate that would probably look something like this: each module has its own name/id, all results for a module will be stored under same folder 'result.feather' has one or several columns that represent output values also, one column with runtime metadata (execution time, version of workflow, etc.) this works well with the 'dataset' API in Apache Arrow: https://arrow.apache.org/docs/python/dataset.html (which means we can lazy-load all results of a workflow/module into the same dataframe, and do 'meta'-queries and -analysis on that if we choose to) debatable whether 'workflow-results' have to be stored at all, since they are just copies of 'module-results' In order to not waste too much hard-disk space (which would be the most obvious concern here), I think we have a few different options. For one, we'd store all results with compression enabled. We would implement our modules in an efficient way that is aware of how we store results. We might have a cleanup process running in the background that is aware of how often a result is accessed, and how it's compute-time/result-size ratio is. In some cases where that ratio leans very much towards result-size, we might decide to not store those results at all, but re-process every time.","title":"How to actually deal with workflow/module outputs?"},{"location":"architecture/data/requirements/#streaming-module-results","text":"TBD This is an area I haven't done too much work on yet, but in general: we will want to have access to intermediate results (or, rather: partial results in real-time), so we can provide the user with information they can use to determine whether to cancel a running process or not. Even though we will probably not have that functionality available in our initial, first version, I think we should anticipate that requirement, and design our data management with it in mind, so it can be added later without having to re-write a lot of code.","title":"Streaming module results"},{"location":"architecture/data/requirements/#default-data-format-for-importexport","text":"every result can be described by specifying: the input dataset(s) and other inputs the workflow (and workflow version) that was used to produce it -> theoretically, every (result) dataset can be described by very small json file/metadata set proposal: invent our own (small) set of file formats (including version-nr, metadata schema, payload) Apache Arrow based for tabular/scalar data folder/zip based for binary data all our import modules would create files in that format provide tooling (and modules) to convert/export those to all common data formats possibility of data registries: very simple implementation compared to products like dataverse, ckan high performance data transfer (using Apache Flight) different levels: local (within our app), organization-wide, global (aka default registry) The last thing to decide is whether we want to provide a 'standard' data format for our application. This will be modelled closely upon the format we will use internally, but with some added metadata fields and possibly restrictions. This is mostly for the purpose of sharing, transferring, and publishing data. In principle, there is a really lightweight way to share our work: since we can describe everything we do by specifying the workflow, and listing all the inputs we use with it. Assuming all inputs are either scalars or, in case of datasets, available via download, this description could be very lightweight: it's just a json file containing the workflow structure (incl. maybe version information), and input-data urls. With that, everyone with access to the data can in theory replicate any end- and intermediate result. In theory, that json structure can also be attached to every result dataset, which means that our results will always come with information how they were produced (and how to re-produce them). Since all this is very dependent on being able to have access to metadata alongside the 'actual' data, and because in my experience systems and architectures that store metadata seperately to data are either fairly complex, specific and hard to maintain, I would propose we come up with a way to package our data in a way that allows for our metadata to always be included, and where it's easy to access both data and metadata without having to open the whole file. Arrow gets us a long way toward that (for tabular data), the only thing that is missing is a standard way to include metadata. For that we have two options: use the Arrow 'metadata' field (which is fairly limited, it only takes encoded byte-arrays as keys/values), or store our metadata in a seperate column. Currently, I'm leaning toward the latter option, but this is something we'll have to try out and play with to get a better idea how feasable it is. For other types of data (binary blobs, images, etc.), I propose we use an archive format (zip, tar, ...) with a json file at a standard location (e.g. './.metadata.json') that includes the same metadata schema a tabular dataset would use. That way our datasets always have the same 'interface'. And we can provide a set of standard tools (which could be implemented as workflow modules and workflows) to import and export 'our' data from/to commonly used formats like csv, excel, etc (which in most cases would not include metadata at all).","title":"Default data format (for import/export)"},{"location":"architecture/workflows/","text":"If we accept the premise that in the computational context we are really only interested in structured data, it follows that there must be also 'things' that do stuff to our structured data. Let's call those things 'workflows'. Definition \u00b6 I will concede that 'doing stuff to data' although entirely accurate is probably not the most useful of definitions. Not Websters, all right? Well, how about: \"A workflow is a tool to transform data into more structured data.\" \"more\" can be read in one or all of those ways: 'more data' -- we'll create what can be considered 'new' data out of the existing set 'better structured' -- improve (and replace) the current structure (fix errors, etc.) 'more structure' -- augment existing data with additional structure In our context, workflows can also have secondary outcomes: present data in different, more intuitive ways (e.g. visualisations), which researchers can use to get different ideas about the data, or new research questions convert structured data into equivalent structured data, just a different format (e.g csv to Excel spreadsheet) ... (I'm sure there's more, just can't think of anything important right now) Deconstructing a workflow \u00b6 I've written more about it here , but conceptually, every data workflow is a collection of interconnected modules, where outputs of some modules are connected to inputs of some other modules. The resulting network graph of modules defines a workflow. That's even the case for Jupyter notebooks (which are really just fancy Python/R/Julia scripts); if you squint a bit you can see it: the modules are functions that you call with some inputs, and you use the outputs of those functions (stored in variables) as inputs to other functions. Move along, nothing to see here: this is really just how (most) programs work. As I see it, there are three main differences to programs that are written in 'normal' computer engineering: the complexity of the resulting interconnected 'network graph' (the interconnection of functions) is usually lower it's a tad easier (or at least possible) to define, separate and re-use the building blocks needed in a majority of workflows (an example would be Numpy or the Pandas libraries, which are basically implementations of abstract problems that crop up often in this domain) it is possible to create workflows entirely out of modules that were previously created, with no or almost no other customization (normally, that customization would be very prominent in a program) -- often-times only some 'glue' code is needed This means that data engineering workflows could be considered relatively simple script-like applications, where advanced concepts like Object-Oriented-Design, Encapsulation, DRY, YAGNI, ... are not necessary or relevant (in most cases they wouldn't hurt though). Data engineering \u00b6 This way of looking at workflows is nothing new, there are quite a few tools and projects in the data engineering space which deal with workflows in one level of abstraction or another. As I'll point out below, the main difference to what we try to implement is that we'll add an element of 'interactivity'. But I believe we can still learn a whole lot by looking at some aspects of those other tools. I encourage everyone remotely interested to look up some of those projects, and maybe not read the whole documentation, but at least the 'Why-we-created-yet-another-data-orchestrator', 'Why-we-are-better-than-comparable-projects' as well as 'What-we-learned'-type documentation pages you come across. 'I-tried-project-XXX-and-it-is-crap'-blog posts as well as hackernews comment-threads related to those projects are usually also interesting. The '/r/dataengineering' and '/r/datascience' sub-reddits are ok. But they are on Reddit, so, signal-to-noise is a bit, well.. Among others, interesting projects include: dagster prefect airflow luigi also relevant, but less data-engineering-y: Node-RED, Apache NiFi, IFTTT, Zapier, Huginn, ... The 'workflow lifecycle' \u00b6 One thing that I think is also worth considering is the different stages in the lifecycle of a workflow. For illustration, I'll describe how each of those stages relates to the way data science is currently done with Jupyter, which is probably the most used tool in this space at the moment. Workflow creation \u00b6 This is the act of designing and implementing a new workflow transformed into one or a set of defined outcomes (which can be new data, or just a visualization, doesn't matter). The actual creation of the workflow is similar to developing a script or application, and offers some freedom on how to implement it (e.g. which supporting libraries to choose, whether and which defaults to set, ...). In the Jupyter-case, this would be the iterative development of a Jupyter notebook, with one cell added after the other. One thing that is different for us is that we will have a much stricter definition of the desired outcome of our workflow, whereas the creation of a Jupyter notebook is typically way more open-ended, and a researcher would easily be able to 'follow some leads' they come across while working on the whole thing. This is a very important distinction that pays to keep in mind, and I can't emphasize this enough: the workflows we are dealing with are a lot more 'static' than typical Jupyter notebooks, because we have decided in advance which ones to implement, and how to implement them. There is not much we can do about this, and it's a trade-off with very little room to negotiate. This has a few important implications on how our product is different from how data science is done by Jupyter users currently. I will probably mention this again and again, because it is not intuitive at first, but has a big impact on how we view what we are building! As per our core assumptions, end-users won't create new workflows, this is done by a group with a yet-to-be-determined 'special' skill set. Workflow execution \u00b6 This is when a 'finished' workflow gets run, with a set of inputs that are chosen by the user. The schema/type of those inputs is a requirement that has to be considered by the user. It's possible that a workflow allows for inputs to be in multiple formats, to make the users life easier (e.g. allow both '.csv' as well as '.json' formats), but that also has to be documented and communicated to users. It is not possible to add elements to a workflow, and make it do different things than it was designed to do. Our workflows are static, they never change (except in an 'iterative-development' sense where we release new versions)! Compare that to a researcher who created their own Jupyter notebook: they will have run the workflow itself countless times by then, while developing it. The execution phase is really only that last run that achieves the desired outcome, and which will 'fill' the notebook output cells with the final results. That notebook state is likely to be attached to a publication. Often the data is 'hardcoded' into the notebook itself (for example by adding the data itself in the git repo, and using a relative path to point to it in a notebook). It is also possible, although not as common (as far as I've seen -- I might be wrong here) that researchers spend a bit more time on the notebook and make the inputs easier to change, in order to be able to execute it with different parameters, quickly. This is more like what we will end up with, although I'd argue that the underlying workflow is still much easier to change, fix, and adapt than will be the case with our application. One difference between workflow creation and execution is that the creation part is more common for 'data scientists', and the execution part is a bigger concern for 'data engineers' (both do both, of course). I think, our specific problem sits more in the data engineering than data science space (because our main products are 'fixed'/'static' workflows), which is why I tend to look more for the tools used in that domain (data orchestrators, ...) than in the other (Jupyter, ..) when I look for guidance. Workflow publication \u00b6 Once a workflow is run with a set of inputs that yield a meaningful outcome for a researcher, it can be attached to a publication in some way. This has one main purpose: to document and explain the research methodologies that were used, on a different level than 'just' plain language. There is a long-term, idealistic goal of being able to replicate results, but the general sentiment is that it is unrealistic to attempt that at this stage. It doesn't hurt to consider it a sort of 'guiding light', though. It is getting more and more common for researchers to attach Jupyter notebooks to a publication. Jupyter notebooks are a decent fit for this purpose, because the contain plain-text documentation, the actual code, as well as the output(s) of the code in a single file, that has a predictable, well specified format (json, along with a required document schema). As our colleagues at the DHJ project have discovered, it's not a perfect fit, but it can be bent to serve as the basis for a formal, digital publication. In our case, it is my understanding that we would like to have an artefact like this too, and even though it's not one of the 'core' requirements, it would be a very nice thing to have. One strong option is for us to re-use Jupyter notebooks for that. Depending on how we implement our solution, we might already have one as our core element that 'holds' a workflow, in which case this is a non-issue. Or, if that is not the case, we could 'render' a notebook from the metadata we have available, which should also not be too hard to do since the target (the notebook) is well spec'ed. If that's the case, there is one thing I'd like to investigate before we commit though: what characteristics exactly are the ones that make notebooks a good choice for that, and which one are detrimental? As I've mentioned, the DHJ project uses notebooks as the base for creating article-(web)pages, and they came across some issues along the way. So I wonder: is there a better way to achieve the 'document and explain research methodologies' goal than by using a Jupyter notebook? How would that look in a perfect world? How much effort would be involved? Interactivity / Long(-ish) running computations \u00b6 One component that is different in our scenario to other implementations is the requirement for interactivity. In data-engineering, this is never an issue, you describe your pipeline, then you or someone else uses that with a set of inputs, and off it goes, without any further interaction. Plomp , notification, results, rinse, repeat. For us that will be different, because we are creating a graphical user interface that reflects the workflow, and its state. By definition, graphical user interfaces are interactive, and when a user triggers an action, they expect that to kick off some instant response in the UI (maybe the change in a visualization, or a progress indicator, whatever). Computationally trivial/non-trivial \u00b6 One main difficulty will be to find a good visual way to express what is happening to the user, ideally in the same way for 2 different scenarios: computations that are computationally trivial, and will return a result back in a few seconds at most computations that take longer In our workflows, I can see a few different ways those interactions can play out, depending on the characteristics of any particular workflow. So, in the case where a user 'uploads' data or changes a setting: if the whole workflow is trivial, computationally : this triggers the whole workflow to execute and return with a new state/result immediately, and the output elements reflect the new state without any noticable delay if only some (or no) components of the workflows are trivial, computationally : this triggers the execution of only parts of the workflow immediately (from the point of user input to the next non-trivial step of the workflow). all computationally non-trivial parts of the workflow will have some sort of \"Process\" button that users have to click manually to kick off those parts of the workflow. Otherwise the UI would be locked for an undefined amount of time after every user input -- which would result in a very bad UX). alternatively, workflows with computationally non-trivial parts could have one 'global' \"Process\" button, which would trigger the execution of the whole workflow with all current inputs/settings. There will be also inputs that don't directly kick off any processing (like for example control buttons in a visualisation). I think we can ignore those for now, because this is what UIs usually do, and this does not present a difficulty in terms of the overall UI/UX (just like the 'computationally trivial' workflow scenario). UI representations for the current workflow state \u00b6 tldr; \u00b6 In some cases it will be impossible for users to use a workflow fully interactively, because one or all workflow steps will take too much time, which means the interactive session has to be interrupted. In those cases (depending on our setup and other circumstances) we might need to include a 'job-management'/'queue' component to our application, which matches running/completed jobs to users and 'sessions'/'experiments' (for lack of a better word). We need to find a visual metaphors for workflows and workflow steps to make that intuitive, ideally in a way so that those scenarios are not handled too differently in comparison to how our 100%-interactive workflows are used and executed. In addition, we have to make sure our backend can deal with all the scenarios we want to support. Details, skip if you want \u00b6 I'll include some suggestions on how all this could look visually, but those are in no way to be taken as gospel. Just the most obvious (to me) visual elements to use, which I hope will make it easier to get my point across. It's probably obvious that the important cases we have to care about are the ones where there is non-trivial computation. I think we can roughly divide them into 4 categories: execution time of a few seconds : in this case a 'spinning-wheel'-progress indidcator is probably enough backend-wise, we (probably) don't have to worry (although, it's not a given this will not crash a hosted app if we have too many users and computations are executed 'in-line') execution time of a few minutes : not long enough so that for example a browser session would expire in this case it would be good UX-wise to have a semi-exact progress indicator that either shows a 'done'-percentage, or remaining time on the backend-side, we need to separate three scenarios: local app: the computation can happen locally, either in a new thread, or a different process (we can also make use of multiple cores if available) hosted jupyter in some form or other: the running Jupyter kernel can execute the computation, which is probably a good enough separation to not affect the hosted UI hosted web app: there needs to exist some infrastructure we can use to offload the computation, it can't run on the same host as our service (which means a lot of added complexity) there is no need yet for authentication apart from that we need to be able to assign the result of the computation to individual sessions execution time of a few hours : long enough that a user will have left the computer in between, or closed a browser, etc. now the separation of backend-scenarios kicks in earlier, and also affects the front-end: local app: as in the case before, the UI would display a progress-indicator of some sort the computation would happen as a background process, and as long as the user does not shut-down or restart the computer there is no issue (the job should even survive a suspend/hibernate) hosted jupyter: difficult to say, the computation could either still happen in the running Jupyter kernel, or would have to be farmed out to an external service one issue to be aware of is that, depending on how it is configured, Jupyter might or might not kill a notebook process (and underlying kernel) if there has been no activity in the browser for a while. We'd have to make sure this does not happen, or that we have some sort of user session management (which should be entirely possible -- but of course increases complexity by quite a bit). The latter will also be necessary if a user comes back to their session after having been disconnected in some way, because otherwise they'd loose their result. ui-wise there needs to be session and compute-job management, and a list of currently running and past jobs and links to the experiments that produced them hosted web app: as with the jupyter case, we'll need session as well as job management execution time of more than a few hours (days, weeks) : in all cases the computation now needs to be outsourced, and submitted to a compute service (cloud, HPC, local dask-cluster, whatever...) all cases need to implement some sort of session authentication and job management (which would probably be a bit more transparent to the user in the local case, but overall it would be implemented in a similar way in each scenario) Externally running computations \u00b6 One thing to stress is that 'outsourcing' computationally intensive tasks comes with a considerable amount of complexity. Nothing that can't be implemented, and there are several ways I can think of to do this. I'd still advise to be very aware of the cost and complexity this incurs. I do believe we will have to add that in some form at some stage though, if we are in any way successful and have people adopting our solution. Which means we have to include the issue in our architecture design, even if we only plan to implement it later.","title":"Workflows"},{"location":"architecture/workflows/#definition","text":"I will concede that 'doing stuff to data' although entirely accurate is probably not the most useful of definitions. Not Websters, all right? Well, how about: \"A workflow is a tool to transform data into more structured data.\" \"more\" can be read in one or all of those ways: 'more data' -- we'll create what can be considered 'new' data out of the existing set 'better structured' -- improve (and replace) the current structure (fix errors, etc.) 'more structure' -- augment existing data with additional structure In our context, workflows can also have secondary outcomes: present data in different, more intuitive ways (e.g. visualisations), which researchers can use to get different ideas about the data, or new research questions convert structured data into equivalent structured data, just a different format (e.g csv to Excel spreadsheet) ... (I'm sure there's more, just can't think of anything important right now)","title":"Definition"},{"location":"architecture/workflows/#deconstructing-a-workflow","text":"I've written more about it here , but conceptually, every data workflow is a collection of interconnected modules, where outputs of some modules are connected to inputs of some other modules. The resulting network graph of modules defines a workflow. That's even the case for Jupyter notebooks (which are really just fancy Python/R/Julia scripts); if you squint a bit you can see it: the modules are functions that you call with some inputs, and you use the outputs of those functions (stored in variables) as inputs to other functions. Move along, nothing to see here: this is really just how (most) programs work. As I see it, there are three main differences to programs that are written in 'normal' computer engineering: the complexity of the resulting interconnected 'network graph' (the interconnection of functions) is usually lower it's a tad easier (or at least possible) to define, separate and re-use the building blocks needed in a majority of workflows (an example would be Numpy or the Pandas libraries, which are basically implementations of abstract problems that crop up often in this domain) it is possible to create workflows entirely out of modules that were previously created, with no or almost no other customization (normally, that customization would be very prominent in a program) -- often-times only some 'glue' code is needed This means that data engineering workflows could be considered relatively simple script-like applications, where advanced concepts like Object-Oriented-Design, Encapsulation, DRY, YAGNI, ... are not necessary or relevant (in most cases they wouldn't hurt though).","title":"Deconstructing a workflow"},{"location":"architecture/workflows/#data-engineering","text":"This way of looking at workflows is nothing new, there are quite a few tools and projects in the data engineering space which deal with workflows in one level of abstraction or another. As I'll point out below, the main difference to what we try to implement is that we'll add an element of 'interactivity'. But I believe we can still learn a whole lot by looking at some aspects of those other tools. I encourage everyone remotely interested to look up some of those projects, and maybe not read the whole documentation, but at least the 'Why-we-created-yet-another-data-orchestrator', 'Why-we-are-better-than-comparable-projects' as well as 'What-we-learned'-type documentation pages you come across. 'I-tried-project-XXX-and-it-is-crap'-blog posts as well as hackernews comment-threads related to those projects are usually also interesting. The '/r/dataengineering' and '/r/datascience' sub-reddits are ok. But they are on Reddit, so, signal-to-noise is a bit, well.. Among others, interesting projects include: dagster prefect airflow luigi also relevant, but less data-engineering-y: Node-RED, Apache NiFi, IFTTT, Zapier, Huginn, ...","title":"Data engineering"},{"location":"architecture/workflows/#the-workflow-lifecycle","text":"One thing that I think is also worth considering is the different stages in the lifecycle of a workflow. For illustration, I'll describe how each of those stages relates to the way data science is currently done with Jupyter, which is probably the most used tool in this space at the moment.","title":"The 'workflow lifecycle'"},{"location":"architecture/workflows/#workflow-creation","text":"This is the act of designing and implementing a new workflow transformed into one or a set of defined outcomes (which can be new data, or just a visualization, doesn't matter). The actual creation of the workflow is similar to developing a script or application, and offers some freedom on how to implement it (e.g. which supporting libraries to choose, whether and which defaults to set, ...). In the Jupyter-case, this would be the iterative development of a Jupyter notebook, with one cell added after the other. One thing that is different for us is that we will have a much stricter definition of the desired outcome of our workflow, whereas the creation of a Jupyter notebook is typically way more open-ended, and a researcher would easily be able to 'follow some leads' they come across while working on the whole thing. This is a very important distinction that pays to keep in mind, and I can't emphasize this enough: the workflows we are dealing with are a lot more 'static' than typical Jupyter notebooks, because we have decided in advance which ones to implement, and how to implement them. There is not much we can do about this, and it's a trade-off with very little room to negotiate. This has a few important implications on how our product is different from how data science is done by Jupyter users currently. I will probably mention this again and again, because it is not intuitive at first, but has a big impact on how we view what we are building! As per our core assumptions, end-users won't create new workflows, this is done by a group with a yet-to-be-determined 'special' skill set.","title":"Workflow creation"},{"location":"architecture/workflows/#workflow-execution","text":"This is when a 'finished' workflow gets run, with a set of inputs that are chosen by the user. The schema/type of those inputs is a requirement that has to be considered by the user. It's possible that a workflow allows for inputs to be in multiple formats, to make the users life easier (e.g. allow both '.csv' as well as '.json' formats), but that also has to be documented and communicated to users. It is not possible to add elements to a workflow, and make it do different things than it was designed to do. Our workflows are static, they never change (except in an 'iterative-development' sense where we release new versions)! Compare that to a researcher who created their own Jupyter notebook: they will have run the workflow itself countless times by then, while developing it. The execution phase is really only that last run that achieves the desired outcome, and which will 'fill' the notebook output cells with the final results. That notebook state is likely to be attached to a publication. Often the data is 'hardcoded' into the notebook itself (for example by adding the data itself in the git repo, and using a relative path to point to it in a notebook). It is also possible, although not as common (as far as I've seen -- I might be wrong here) that researchers spend a bit more time on the notebook and make the inputs easier to change, in order to be able to execute it with different parameters, quickly. This is more like what we will end up with, although I'd argue that the underlying workflow is still much easier to change, fix, and adapt than will be the case with our application. One difference between workflow creation and execution is that the creation part is more common for 'data scientists', and the execution part is a bigger concern for 'data engineers' (both do both, of course). I think, our specific problem sits more in the data engineering than data science space (because our main products are 'fixed'/'static' workflows), which is why I tend to look more for the tools used in that domain (data orchestrators, ...) than in the other (Jupyter, ..) when I look for guidance.","title":"Workflow execution"},{"location":"architecture/workflows/#workflow-publication","text":"Once a workflow is run with a set of inputs that yield a meaningful outcome for a researcher, it can be attached to a publication in some way. This has one main purpose: to document and explain the research methodologies that were used, on a different level than 'just' plain language. There is a long-term, idealistic goal of being able to replicate results, but the general sentiment is that it is unrealistic to attempt that at this stage. It doesn't hurt to consider it a sort of 'guiding light', though. It is getting more and more common for researchers to attach Jupyter notebooks to a publication. Jupyter notebooks are a decent fit for this purpose, because the contain plain-text documentation, the actual code, as well as the output(s) of the code in a single file, that has a predictable, well specified format (json, along with a required document schema). As our colleagues at the DHJ project have discovered, it's not a perfect fit, but it can be bent to serve as the basis for a formal, digital publication. In our case, it is my understanding that we would like to have an artefact like this too, and even though it's not one of the 'core' requirements, it would be a very nice thing to have. One strong option is for us to re-use Jupyter notebooks for that. Depending on how we implement our solution, we might already have one as our core element that 'holds' a workflow, in which case this is a non-issue. Or, if that is not the case, we could 'render' a notebook from the metadata we have available, which should also not be too hard to do since the target (the notebook) is well spec'ed. If that's the case, there is one thing I'd like to investigate before we commit though: what characteristics exactly are the ones that make notebooks a good choice for that, and which one are detrimental? As I've mentioned, the DHJ project uses notebooks as the base for creating article-(web)pages, and they came across some issues along the way. So I wonder: is there a better way to achieve the 'document and explain research methodologies' goal than by using a Jupyter notebook? How would that look in a perfect world? How much effort would be involved?","title":"Workflow publication"},{"location":"architecture/workflows/#interactivity-long-ish-running-computations","text":"One component that is different in our scenario to other implementations is the requirement for interactivity. In data-engineering, this is never an issue, you describe your pipeline, then you or someone else uses that with a set of inputs, and off it goes, without any further interaction. Plomp , notification, results, rinse, repeat. For us that will be different, because we are creating a graphical user interface that reflects the workflow, and its state. By definition, graphical user interfaces are interactive, and when a user triggers an action, they expect that to kick off some instant response in the UI (maybe the change in a visualization, or a progress indicator, whatever).","title":"Interactivity / Long(-ish) running computations"},{"location":"architecture/workflows/#computationally-trivialnon-trivial","text":"One main difficulty will be to find a good visual way to express what is happening to the user, ideally in the same way for 2 different scenarios: computations that are computationally trivial, and will return a result back in a few seconds at most computations that take longer In our workflows, I can see a few different ways those interactions can play out, depending on the characteristics of any particular workflow. So, in the case where a user 'uploads' data or changes a setting: if the whole workflow is trivial, computationally : this triggers the whole workflow to execute and return with a new state/result immediately, and the output elements reflect the new state without any noticable delay if only some (or no) components of the workflows are trivial, computationally : this triggers the execution of only parts of the workflow immediately (from the point of user input to the next non-trivial step of the workflow). all computationally non-trivial parts of the workflow will have some sort of \"Process\" button that users have to click manually to kick off those parts of the workflow. Otherwise the UI would be locked for an undefined amount of time after every user input -- which would result in a very bad UX). alternatively, workflows with computationally non-trivial parts could have one 'global' \"Process\" button, which would trigger the execution of the whole workflow with all current inputs/settings. There will be also inputs that don't directly kick off any processing (like for example control buttons in a visualisation). I think we can ignore those for now, because this is what UIs usually do, and this does not present a difficulty in terms of the overall UI/UX (just like the 'computationally trivial' workflow scenario).","title":"Computationally trivial/non-trivial"},{"location":"architecture/workflows/#ui-representations-for-the-current-workflow-state","text":"","title":"UI representations for the current workflow state"},{"location":"architecture/workflows/#externally-running-computations","text":"One thing to stress is that 'outsourcing' computationally intensive tasks comes with a considerable amount of complexity. Nothing that can't be implemented, and there are several ways I can think of to do this. I'd still advise to be very aware of the cost and complexity this incurs. I do believe we will have to add that in some form at some stage though, if we are in any way successful and have people adopting our solution. Which means we have to include the issue in our architecture design, even if we only plan to implement it later.","title":"Externally running computations"},{"location":"architecture/workflows/modularity/modularity/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % autosave 0 import os from rich.jupyter import print from dharpa_toolbox.modules.utils import list_available_module_names , describe_module , print_module_desc , load_workflows , create_module from dharpa_toolbox.utils import print_file_content , graph_to_image from dharpa_toolbox.modules.workflows import DharpaWorkflow from dharpa_toolbox.rendering.jupyter.renderer import PlainJupyterWorkflowRenderer , ModuleJupyterWorkflowRenderer base_path = os . path . abspath ( \".\" ) var element = $('#7240dd53-a937-4f53-943b-a44917fddab8'); IPython.notebook.set_autosave_interval(0) Autosave disabled What's a workflow, really? \u00b6 Jupyter is a very good tool to create non-trivial exploratory workflows there's a difference between 'dynamic' workflows, and 'static' ones Jupyter is usually used to create workflows in a 'dynamic' way also important (for us): interactivity Currently, Jupyter is one of the most used technologies in digital research to create workflows. Although there are exceptions, in most cases it is used to explore a very specific research question. Jupyter is exceptionally good at that, which is the reason it is so successful. From a computer-engineering perspective, Jupyter notebooks are 'just' simple scripts, and often they include anti-patterns like global- as well as hard-coded variables, little to no encapsulation of functionality, etc. Which means that typically, Jupyter notebooks have (relatively) little value to other researchers, and re-usability is low. This is an acceptable trade-off though, because the problems they are solving are (usually) very niche and specific, so there is little downside to tailor the code to the exact problem one is having. In addition, Jupyter notebooks are very good to document the workflow itself, and communicate what is happening to the data (which is important for publication). If we want to create a tool that lets users run pre-created workflows, that equation changes though. Because, now the assumption is that the (comparatively few) workflows we create will be useful in not just a very specific way. The goal is to identify areas where people have (roughly) the same problem, and to solve that problem in a generic way that is useful to a larger group of people. The workflow will typically be less important in relation to the overall research project a researcher is working on (compared to a tailored, specific one), but from the perspective of a reasearcher it will also be much less hassle and expensive to use, since they don't have to create the workflow themselves, and someone else already has thought about all the options and parameters that make sense, has done the validation and testing, etc. Also, they don't have to learn programming if they don't already know it... This means that we are dealing now with a very 'static' workflow, compared to the 'dynamic' ones researchers with programming skills can create and change themselves very easily. Everything that can happen in a workflow is known in advance, and even though there can be 'forks' in the flow of data, those have to be defined, implemented and documented in advance. And that difference is why we should not assume that Jupyter notebooks are as good a vessel to implement such a workflow as they are in the other case, where all that can happen 'on the go'. It's still possible notebooks are a good fit here too, but we can't use our normal experience with -- and intuition about -- Jupyter to make that case. One other point that is important to note is user interactivity. Usually, when developing a Jupyter notebook inputs (data as well as parameters) are either hardcoded, or factored out into a variable that is changed on top of the notebook (or in some cells further down). And by running or re-running certain cells, those variables are re-set or changed. This works fine for dynamically creating a workflow (although, it's sometimes confusing, and one of the main criticisms against the Jupyter notebook approach). But, in a 'static' workflow, we need to make sure that a user can set or change all those inputs at any time, while making sure that the 'internal' state of our workflow is known to our engine. At a minimum, we need to know that our state is currently inconsistent after a user-input, and have a way to communicating that to the user so they can kick off some re-processing manually, to make it consistent again. Jupyter supports interactivity via widgets, but the 'cell-based' approach in notebooks is not a very good fit for that, because it forces a very simple one-after-the other processing model, that would make it hard to implement the efficient execution of even remotely non-trivial workflows (for example having parallel execution of some cells, or skipping the execution of parts that don't need to be executed currently, etc.). Prior art \u00b6 workflow/pipeline modelling and execution is a solved problem in programming: flow-based programming (FBP) requires well defined, modular entities (with 'ports': input and output values) lots of (partial) implementations in data engineering: airflow luigi dagster prefect many others: Node-RED, Apache NiFi, IFTTT, Zapier, Huginn, ... one subtle (although important) difference with our project, again: interactivity There is a form of programming that fits our problem space fairly well: flow-based programming (FBP) . Like functional programming, it's probably older than all of us, and it is gaining some notable traction again in recent years (although with much less hype around it, and without being explicitly mentioned by name). A lot of the data orchestration tools and frameworks that cropped up in recent years use some form or aspects of FBP, for example: airflow luigi dagster prefect One thing that FBP requires are well defined entities ('modules', 'nodes'), that have 'ports' (meaning: known inputs, and outputs). A Jupyter notebook for example does not typically have that, which makes it hard to 'combine' notebooks in an FBP-like manner. There are attempts to 'formalize' Jupyter notebooks in a way that would make them better fits in such scenarios ( papermill , orchest ), but in my opinion, although they kind of work, those attempts are a bit clunky, and not very user-friendly (because they try to bend Jupyter into something it was not designed to do). Also, they typically only deal with inputs; outputs are not very well defined at all. Compare that for example with how a 'proper' data-orchestration tool like dagster handles inputs and outputs , which should make clear how many more options someone who implements a workflow execution and rendering framework (which is basically what we are building) has when that sort of metadata is available. As was the case in the section above, one difference in our case is interactivity. Most tools in that space assume they'll get the input values for a workflow execution at the start, and then they can proceed to go through the workflow, batch-processing style (meaning, no further user input half-way through). This is different for us, since we want users to be able to interactively explore their data (within the limits of a 'static' workflow). This means we will have to consider how to deal with long-running computations whose results wil be available after minutes, hours weeks. The good thing is though, whatever we come up with, we'll get a 'traditional workflow execution engine' for free, because every workflow that can be executed interactively, will also be able to do 'batch-style'. This will let us re-use and 'move' our workflows to other execution environments (HPC clusters, 'the cloud', ...) and do other interesting things with them if the need arises (monte-carlo style experiments, automated-testing of workflows and modules, ...). Modelling workflows \u00b6 research data is more useful when it's structured, so why would workflow definitions be different? so: can we model a workflow as code, or even better: as data? So, assuming everyone agrees this is a reasonable avenue to explore, we have to think about how we want to model our workflows. We should definitely look at how other, similar frameworks do this, but I think one approach is very tempting: Describe workflows as structured data! There are several reasons for why I think this would be a good idea: structured data can be processed by every programming language in existence we would have one 'main' library that does the actual workflow execution/data processing (probably in Python) we could use other languages to do different other things in our 'ecosystem': e.g. JavaScript for dynamically rendering a frontend we can (largely) work independent from each other, the only thing to consult about is the schema of the workflow data such structured data can be displayed as a network graph, which is much easier to grasp than code automated testing of every workflow and model is easy, can be done in CI/CD Jupyter notebooks are, as I've explained above, pretty good at creating and manipulating structured data there are a lot of researchers out there who know how to use Jupyter: those could all be potential \"DHARPA-workflow\" creators in addition to that, we can decide to create a visual 'workflow editor/creator', that is independent from the 'workflow executor' part, and 100% optional Code! \u00b6 The following is using prototype-quality code to illustrate how a 'workflow-as-data' model could look like in practice. Only a few modules are implemented, the goal is to recreate the first part of the 'Topic-modelling' workflow: load some text files, tokenize them, then do some processing (lowercasing, removal of stopwords). Definitions \u00b6 module : a module is an atomic entity that contains a fixed set of defined inputs and outputs, as well as a processing unit that converts the set of inputs to outputs, in a predicable way workflow : a workflow contains a set of modules which are connected in a specific way. A workflow is conceptually also a module, because it also contains a set of inputs/outputs as well as processing unit, and it can be used in other, 'parent' workflows in the same ways a normal module can. # we can list all available modules (and workflows) list_available_module_names () ['corpus_processing', 'corpus_processing_simple', 'dharpa_workflow', 'file_reader', 'input_files_processing', 'lowercase_corpus', 'remove_stopwords_from_corpus', 'tokenize_corpus'] # we can investigate each modules inputs and output specs print_module_desc ( 'tokenize_corpus' ) { 'tokenize_corpus' : { 'inputs' : { 'text_map' : 'Dict' } , 'outputs' : { 'tokenized_text' : 'Dict' }}} # a workflow configuration is basically just a list of modules, incl. their input/output connections workflow_config = f ' { base_path } /workflows/corpus_processing_simple.yaml' print_file_content ( workflow_config ) --- modules : - type : tokenize_corpus - type : lowercase_corpus input_map : tokenized_text : tokenize_corpus.tokenized_text - type : remove_stopwords_from_corpus input_map : tokenized_text : lowercase_corpus.tokenized_text workflow_outputs : tokenized_text : processed_text_corpus # we create a 'workflow' object using the configuration data workflow : DharpaWorkflow = DharpaWorkflow . from_file ( workflow_config ) # we can investigate each workflows available input and output names print ( workflow . input_names ) [ 'lowercase_corpus__enabled' , 'remove_stopwords_from_corpus__enabled' , 'remove_stopwords_from_corpus__stopwords_list' , 'tokenize_corpus__text_map' ] print ( workflow . output_names ) [ 'processed_text_corpus' ] # we can display the execution and data-flow structures of a workflow graphically graph_to_image ( workflow . execution_graph ) # graph_to_image(workflow.data_flow_graph) # print the workflow input/output spec print_module_desc ( workflow ) { 'dharpa_workflow' : { 'inputs' : { 'lowercase_corpus__enabled' : 'Bool' , 'remove_stopwords_from_corpus__enabled' : 'Bool' , 'remove_stopwords_from_corpus__stopwords_list' : 'List' , 'tokenize_corpus__text_map' : 'Dict' } , 'outputs' : { 'processed_text_corpus' : 'Dict' } } } # using the spec, we can set a workflows inputs and outputs manually text_map = { \"1\" : \"Hello World!\" , \"2\" : \"Hello DHARPA!\" } stopwords = [ \"hello\" , \"!\" ] workflow . set_input ( \"tokenize_corpus__text_map\" , text_map ) workflow . set_input ( \"lowercase_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__stopwords_list\" , stopwords ) # the workflow state is processed automatically, so we can always query the current output output1 = workflow . get_output ( \"processed_text_corpus\" ) print ( output1 ) { '1' : [ 'world' ] , '2' : [ 'dharpa' ]} # we can load workflows from json/yaml files on the file-system, and convert them to Python classes load_workflows ( f \" { base_path } /workflows\" ) print ( list_available_module_names ()) [ 'corpus_processing' , 'corpus_processing_simple' , 'dharpa_workflow' , 'file_reader' , 'input_files_processing' , 'lowercase_corpus' , 'remove_stopwords_from_corpus' , 'tokenize_corpus' ] # display the module spec for the 'input_files_processing' workflow print_module_desc ( \"input_files_processing\" ) { 'input_files_processing' : { 'inputs' : { 'files' : 'Any' , 'make_lowercase' : 'Bool' , 'remove_stopwords' : 'Bool' , 'stopwords' : 'List' } , 'outputs' : { 'processed_text_corpus' : 'Dict' } } } # create the workflow object ifp_workflow = create_module ( 'input_files_processing' ) # display the internal structure of the workflow print ( ifp_workflow . _workflow_config ) { 'modules' : [ { 'type' : 'file_reader' , 'input_map' : { 'files' : '__workflow_input__.files' } , 'id' : 'file_reader' } , { 'type' : 'corpus_processing' , 'input_map' : { 'text_map' : 'file_reader.content_map' , 'make_lowercase' : '__workflow_input__.make_lowercase' , 'remove_stopwords' : '__workflow_input__.remove_stopwords' , 'stopwords' : '__workflow_input__.stopwords' } , 'workflow_outputs' : { 'processed_text_corpus' : 'processed_text_corpus' } , 'id' : 'corpus_processing' } ] } # display the execution graph for the workflow graph_to_image ( ifp_workflow . execution_graph ) # graph_to_image(ifp_workflow.data_flow_graph) # auto-render input and output-widgets for the workflow (only works when executed in Jupyter) renderer = PlainJupyterWorkflowRenderer ( workflow = ifp_workflow ) renderer . render () var element = $('#cfcb1776-8b08-4baf-bb51-6d32aa74b2e6'); var element = $('#b405ce5f-f153-4d0c-95f5-5a94beafda1a'); Advantages \u00b6 each interactive workflow is automatically a 'batch-style' one ability to work on different parts of the application almost 100% independently (workflow engine, workflow renderers (UIs), plugins (submitting long running jobs to a cluster, metadata augmentation/data lineage, metric gathering, ...) comparatively little dependencies for the base system / lightweight, but very extensible no dependency on any one UI/frontend technology (can use Jupyter/React/QT/...), can run without any UI at all complex workflow can be broken up into separate pieces, and developed and tested individually fairly high re-usablity easy to use in an agile, iterative development process Disadvantages \u00b6 potentially lower performance due to having to create copies of inputs/outputs to make sure they (or items contained within them) are not changed by subsequent steps harder to create very customized UIs there's a limit to the complexity of workflows that can be supported realistically (e.g. hard to implement control structures like if-then-else or loops on a workflow level/outside of modules -- but in a lot of cases that is probably not necessary)","title":"Modularity"},{"location":"architecture/workflows/modularity/modularity/#whats-a-workflow-really","text":"Jupyter is a very good tool to create non-trivial exploratory workflows there's a difference between 'dynamic' workflows, and 'static' ones Jupyter is usually used to create workflows in a 'dynamic' way also important (for us): interactivity Currently, Jupyter is one of the most used technologies in digital research to create workflows. Although there are exceptions, in most cases it is used to explore a very specific research question. Jupyter is exceptionally good at that, which is the reason it is so successful. From a computer-engineering perspective, Jupyter notebooks are 'just' simple scripts, and often they include anti-patterns like global- as well as hard-coded variables, little to no encapsulation of functionality, etc. Which means that typically, Jupyter notebooks have (relatively) little value to other researchers, and re-usability is low. This is an acceptable trade-off though, because the problems they are solving are (usually) very niche and specific, so there is little downside to tailor the code to the exact problem one is having. In addition, Jupyter notebooks are very good to document the workflow itself, and communicate what is happening to the data (which is important for publication). If we want to create a tool that lets users run pre-created workflows, that equation changes though. Because, now the assumption is that the (comparatively few) workflows we create will be useful in not just a very specific way. The goal is to identify areas where people have (roughly) the same problem, and to solve that problem in a generic way that is useful to a larger group of people. The workflow will typically be less important in relation to the overall research project a researcher is working on (compared to a tailored, specific one), but from the perspective of a reasearcher it will also be much less hassle and expensive to use, since they don't have to create the workflow themselves, and someone else already has thought about all the options and parameters that make sense, has done the validation and testing, etc. Also, they don't have to learn programming if they don't already know it... This means that we are dealing now with a very 'static' workflow, compared to the 'dynamic' ones researchers with programming skills can create and change themselves very easily. Everything that can happen in a workflow is known in advance, and even though there can be 'forks' in the flow of data, those have to be defined, implemented and documented in advance. And that difference is why we should not assume that Jupyter notebooks are as good a vessel to implement such a workflow as they are in the other case, where all that can happen 'on the go'. It's still possible notebooks are a good fit here too, but we can't use our normal experience with -- and intuition about -- Jupyter to make that case. One other point that is important to note is user interactivity. Usually, when developing a Jupyter notebook inputs (data as well as parameters) are either hardcoded, or factored out into a variable that is changed on top of the notebook (or in some cells further down). And by running or re-running certain cells, those variables are re-set or changed. This works fine for dynamically creating a workflow (although, it's sometimes confusing, and one of the main criticisms against the Jupyter notebook approach). But, in a 'static' workflow, we need to make sure that a user can set or change all those inputs at any time, while making sure that the 'internal' state of our workflow is known to our engine. At a minimum, we need to know that our state is currently inconsistent after a user-input, and have a way to communicating that to the user so they can kick off some re-processing manually, to make it consistent again. Jupyter supports interactivity via widgets, but the 'cell-based' approach in notebooks is not a very good fit for that, because it forces a very simple one-after-the other processing model, that would make it hard to implement the efficient execution of even remotely non-trivial workflows (for example having parallel execution of some cells, or skipping the execution of parts that don't need to be executed currently, etc.).","title":"What's a workflow, really?"},{"location":"architecture/workflows/modularity/modularity/#prior-art","text":"workflow/pipeline modelling and execution is a solved problem in programming: flow-based programming (FBP) requires well defined, modular entities (with 'ports': input and output values) lots of (partial) implementations in data engineering: airflow luigi dagster prefect many others: Node-RED, Apache NiFi, IFTTT, Zapier, Huginn, ... one subtle (although important) difference with our project, again: interactivity There is a form of programming that fits our problem space fairly well: flow-based programming (FBP) . Like functional programming, it's probably older than all of us, and it is gaining some notable traction again in recent years (although with much less hype around it, and without being explicitly mentioned by name). A lot of the data orchestration tools and frameworks that cropped up in recent years use some form or aspects of FBP, for example: airflow luigi dagster prefect One thing that FBP requires are well defined entities ('modules', 'nodes'), that have 'ports' (meaning: known inputs, and outputs). A Jupyter notebook for example does not typically have that, which makes it hard to 'combine' notebooks in an FBP-like manner. There are attempts to 'formalize' Jupyter notebooks in a way that would make them better fits in such scenarios ( papermill , orchest ), but in my opinion, although they kind of work, those attempts are a bit clunky, and not very user-friendly (because they try to bend Jupyter into something it was not designed to do). Also, they typically only deal with inputs; outputs are not very well defined at all. Compare that for example with how a 'proper' data-orchestration tool like dagster handles inputs and outputs , which should make clear how many more options someone who implements a workflow execution and rendering framework (which is basically what we are building) has when that sort of metadata is available. As was the case in the section above, one difference in our case is interactivity. Most tools in that space assume they'll get the input values for a workflow execution at the start, and then they can proceed to go through the workflow, batch-processing style (meaning, no further user input half-way through). This is different for us, since we want users to be able to interactively explore their data (within the limits of a 'static' workflow). This means we will have to consider how to deal with long-running computations whose results wil be available after minutes, hours weeks. The good thing is though, whatever we come up with, we'll get a 'traditional workflow execution engine' for free, because every workflow that can be executed interactively, will also be able to do 'batch-style'. This will let us re-use and 'move' our workflows to other execution environments (HPC clusters, 'the cloud', ...) and do other interesting things with them if the need arises (monte-carlo style experiments, automated-testing of workflows and modules, ...).","title":"Prior art"},{"location":"architecture/workflows/modularity/modularity/#modelling-workflows","text":"research data is more useful when it's structured, so why would workflow definitions be different? so: can we model a workflow as code, or even better: as data? So, assuming everyone agrees this is a reasonable avenue to explore, we have to think about how we want to model our workflows. We should definitely look at how other, similar frameworks do this, but I think one approach is very tempting: Describe workflows as structured data! There are several reasons for why I think this would be a good idea: structured data can be processed by every programming language in existence we would have one 'main' library that does the actual workflow execution/data processing (probably in Python) we could use other languages to do different other things in our 'ecosystem': e.g. JavaScript for dynamically rendering a frontend we can (largely) work independent from each other, the only thing to consult about is the schema of the workflow data such structured data can be displayed as a network graph, which is much easier to grasp than code automated testing of every workflow and model is easy, can be done in CI/CD Jupyter notebooks are, as I've explained above, pretty good at creating and manipulating structured data there are a lot of researchers out there who know how to use Jupyter: those could all be potential \"DHARPA-workflow\" creators in addition to that, we can decide to create a visual 'workflow editor/creator', that is independent from the 'workflow executor' part, and 100% optional","title":"Modelling workflows"},{"location":"architecture/workflows/modularity/modularity/#code","text":"The following is using prototype-quality code to illustrate how a 'workflow-as-data' model could look like in practice. Only a few modules are implemented, the goal is to recreate the first part of the 'Topic-modelling' workflow: load some text files, tokenize them, then do some processing (lowercasing, removal of stopwords).","title":"Code!"},{"location":"architecture/workflows/modularity/modularity/#definitions","text":"module : a module is an atomic entity that contains a fixed set of defined inputs and outputs, as well as a processing unit that converts the set of inputs to outputs, in a predicable way workflow : a workflow contains a set of modules which are connected in a specific way. A workflow is conceptually also a module, because it also contains a set of inputs/outputs as well as processing unit, and it can be used in other, 'parent' workflows in the same ways a normal module can. # we can list all available modules (and workflows) list_available_module_names () ['corpus_processing', 'corpus_processing_simple', 'dharpa_workflow', 'file_reader', 'input_files_processing', 'lowercase_corpus', 'remove_stopwords_from_corpus', 'tokenize_corpus'] # we can investigate each modules inputs and output specs print_module_desc ( 'tokenize_corpus' ) { 'tokenize_corpus' : { 'inputs' : { 'text_map' : 'Dict' } , 'outputs' : { 'tokenized_text' : 'Dict' }}} # a workflow configuration is basically just a list of modules, incl. their input/output connections workflow_config = f ' { base_path } /workflows/corpus_processing_simple.yaml' print_file_content ( workflow_config ) --- modules : - type : tokenize_corpus - type : lowercase_corpus input_map : tokenized_text : tokenize_corpus.tokenized_text - type : remove_stopwords_from_corpus input_map : tokenized_text : lowercase_corpus.tokenized_text workflow_outputs : tokenized_text : processed_text_corpus # we create a 'workflow' object using the configuration data workflow : DharpaWorkflow = DharpaWorkflow . from_file ( workflow_config ) # we can investigate each workflows available input and output names print ( workflow . input_names ) [ 'lowercase_corpus__enabled' , 'remove_stopwords_from_corpus__enabled' , 'remove_stopwords_from_corpus__stopwords_list' , 'tokenize_corpus__text_map' ] print ( workflow . output_names ) [ 'processed_text_corpus' ] # we can display the execution and data-flow structures of a workflow graphically graph_to_image ( workflow . execution_graph ) # graph_to_image(workflow.data_flow_graph) # print the workflow input/output spec print_module_desc ( workflow ) { 'dharpa_workflow' : { 'inputs' : { 'lowercase_corpus__enabled' : 'Bool' , 'remove_stopwords_from_corpus__enabled' : 'Bool' , 'remove_stopwords_from_corpus__stopwords_list' : 'List' , 'tokenize_corpus__text_map' : 'Dict' } , 'outputs' : { 'processed_text_corpus' : 'Dict' } } } # using the spec, we can set a workflows inputs and outputs manually text_map = { \"1\" : \"Hello World!\" , \"2\" : \"Hello DHARPA!\" } stopwords = [ \"hello\" , \"!\" ] workflow . set_input ( \"tokenize_corpus__text_map\" , text_map ) workflow . set_input ( \"lowercase_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__stopwords_list\" , stopwords ) # the workflow state is processed automatically, so we can always query the current output output1 = workflow . get_output ( \"processed_text_corpus\" ) print ( output1 ) { '1' : [ 'world' ] , '2' : [ 'dharpa' ]} # we can load workflows from json/yaml files on the file-system, and convert them to Python classes load_workflows ( f \" { base_path } /workflows\" ) print ( list_available_module_names ()) [ 'corpus_processing' , 'corpus_processing_simple' , 'dharpa_workflow' , 'file_reader' , 'input_files_processing' , 'lowercase_corpus' , 'remove_stopwords_from_corpus' , 'tokenize_corpus' ] # display the module spec for the 'input_files_processing' workflow print_module_desc ( \"input_files_processing\" ) { 'input_files_processing' : { 'inputs' : { 'files' : 'Any' , 'make_lowercase' : 'Bool' , 'remove_stopwords' : 'Bool' , 'stopwords' : 'List' } , 'outputs' : { 'processed_text_corpus' : 'Dict' } } } # create the workflow object ifp_workflow = create_module ( 'input_files_processing' ) # display the internal structure of the workflow print ( ifp_workflow . _workflow_config ) { 'modules' : [ { 'type' : 'file_reader' , 'input_map' : { 'files' : '__workflow_input__.files' } , 'id' : 'file_reader' } , { 'type' : 'corpus_processing' , 'input_map' : { 'text_map' : 'file_reader.content_map' , 'make_lowercase' : '__workflow_input__.make_lowercase' , 'remove_stopwords' : '__workflow_input__.remove_stopwords' , 'stopwords' : '__workflow_input__.stopwords' } , 'workflow_outputs' : { 'processed_text_corpus' : 'processed_text_corpus' } , 'id' : 'corpus_processing' } ] } # display the execution graph for the workflow graph_to_image ( ifp_workflow . execution_graph ) # graph_to_image(ifp_workflow.data_flow_graph) # auto-render input and output-widgets for the workflow (only works when executed in Jupyter) renderer = PlainJupyterWorkflowRenderer ( workflow = ifp_workflow ) renderer . render () var element = $('#cfcb1776-8b08-4baf-bb51-6d32aa74b2e6'); var element = $('#b405ce5f-f153-4d0c-95f5-5a94beafda1a');","title":"Definitions"},{"location":"architecture/workflows/modularity/modularity/#advantages","text":"each interactive workflow is automatically a 'batch-style' one ability to work on different parts of the application almost 100% independently (workflow engine, workflow renderers (UIs), plugins (submitting long running jobs to a cluster, metadata augmentation/data lineage, metric gathering, ...) comparatively little dependencies for the base system / lightweight, but very extensible no dependency on any one UI/frontend technology (can use Jupyter/React/QT/...), can run without any UI at all complex workflow can be broken up into separate pieces, and developed and tested individually fairly high re-usablity easy to use in an agile, iterative development process","title":"Advantages"},{"location":"architecture/workflows/modularity/modularity/#disadvantages","text":"potentially lower performance due to having to create copies of inputs/outputs to make sure they (or items contained within them) are not changed by subsequent steps harder to create very customized UIs there's a limit to the complexity of workflows that can be supported realistically (e.g. hard to implement control structures like if-then-else or loops on a workflow level/outside of modules -- but in a lot of cases that is probably not necessary)","title":"Disadvantages"},{"location":"architecture/workflows/modularity/modularity_2/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % autosave 0 import os from rich.jupyter import print from dharpa_toolbox.modules.utils import list_available_module_names , describe_module , print_module_desc , load_workflows , create_module , create_workflow from dharpa_toolbox.utils import print_file_content , graph_to_image from dharpa_toolbox.modules.workflows import DharpaWorkflow from dharpa_toolbox.rendering.jupyter.renderer import PlainJupyterWorkflowRenderer , ModuleJupyterWorkflowRenderer base_path = os . path . abspath ( \".\" ) var element = $('#8c61c18c-94a8-425a-8185-8b7bf5146a45'); IPython.notebook.set_autosave_interval(0) Autosave disabled What's a workflow? \u00b6 a workflow is a tool to transform data into more structured data 'more data' -- we'll create what can be considered 'new' data out of the existing set 'better structured' -- improve (and replace) the current structure (fix errors, etc.) 'more structure' -- augment existing data with additional structure secondary outcomes (insight, new research questions, etc...) workflows are 'just' simple scripts no or only very minimal control structures, except from input/output connections low computational complexity for the workflow itself high(er) computational complexity within modules (but hidden from workflow creator) Workflows in our context \u00b6 workflow creators and workflow users are different, distinct roles Jupyter is a very good tool to create non-trivial exploratory workflows there's a difference between 'dynamic' workflows, and 'static' ones Jupyter is usually used to create workflows in a 'dynamic' way more 'data engineering' than 'data science' interactivity: not an issue in 'data engineering' not very well supported in Jupyter (cell-based approach not useful for us) Two options: monolithic & modular \u00b6 Monolithic \u00b6 complexity spread out across the workflow well integrated, no restrictions UI-wise Modular \u00b6 complexity concentrated in the framework, but simple modules some restrictions on the UI no difference in how to handle data, metadata backend modules don't necessarily map onto frontend modules suggested approach: 100% modular backend modular frontend, incl. optional monolithic frontend approach for high-value workflows Notes: only important for the workflow part, there will be features that won't be affected by this at all (metadata, data) Definitions \u00b6 module : a module is an atomic entity that contains a fixed set of defined inputs and outputs, as well as a processing unit that converts the set of inputs to outputs, in a predicable way workflow : a workflow contains a set of modules which are connected in a specific way a workflow is conceptually also a module, because it also contains a set of inputs/outputs as well as processing unit it can be used in other, 'parent' workflows in the same ways a normal module can. Modelling workflows \u00b6 research data is more useful when it's structured, so why would workflow definitions be different? so: can we model a workflow as data? Notes: monolithic workflows are not structured (comparison: Advantages \u00b6 scalability disposible middleware and frontends (only important to be able to use the created workflows) Examples \u00b6 # we can list all available modules (and workflows) list_available_module_names () ['dharpa_workflow', 'file_reader', 'lowercase_corpus', 'remove_stopwords_from_corpus', 'tokenize_corpus'] # we can investigate each modules inputs and output specs print_module_desc ( 'tokenize_corpus' ) { 'tokenize_corpus' : { 'inputs' : { 'text_map' : 'Dict' } , 'outputs' : { 'tokenized_text' : 'Dict' }}} # a workflow configuration is basically just a list of modules, incl. their input/output connections workflow_config = f ' { base_path } /workflows/corpus_processing_simple.yaml' print_file_content ( workflow_config ) --- modules : - type : tokenize_corpus - type : lowercase_corpus input_map : tokenized_text : tokenize_corpus.tokenized_text - type : remove_stopwords_from_corpus input_map : tokenized_text : lowercase_corpus.tokenized_text workflow_outputs : tokenized_text : processed_text_corpus # we create a 'workflow' object using the configuration data workflow = DharpaWorkflow . from_file ( workflow_config ) # we can investigate each workflows available input and output names print ( workflow . input_names ) [ 'lowercase_corpus__enabled' , 'remove_stopwords_from_corpus__enabled' , 'remove_stopwords_from_corpus__stopwords_list' , 'tokenize_corpus__text_map' ] print ( workflow . output_names ) [ 'processed_text_corpus' ] # we can display the execution and data-flow structures of a workflow graphically graph_to_image ( workflow . execution_graph ) # graph_to_image(workflow.data_flow_graph) text_map = { \"1\" : \"Hello World!\" , \"2\" : \"Hello DHARPA!\" } stopwords = [ \"hello\" , \"!\" ] workflow = DharpaWorkflow . from_file ( workflow_config ) workflow . set_input ( \"tokenize_corpus__text_map\" , text_map ) workflow . set_input ( \"lowercase_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__stopwords_list\" , stopwords ) # the workflow state is processed automatically, so we can always query the current output output1 = workflow . get_output ( \"processed_text_corpus\" ) print ( output1 ) { '1' : [ 'world' ] , '2' : [ 'dharpa' ]} # text_map: {\"one\": \"Hello World!\", \"two\": \"Hello DHARPA!\"} # stopword_list: hello workflow = DharpaWorkflow . from_file ( workflow_config ) renderer = PlainJupyterWorkflowRenderer ( workflow = workflow ) renderer . render () var element = $('#4729e5ed-d8d7-4e2e-a724-41f1092426d2'); var element = $('#ab709a80-a011-4081-8eb6-5594ee11eb7b'); # we can load workflows from json/yaml files on the file-system, and convert them to Python classes load_workflows ( f \" { base_path } /workflows\" ) print ( list_available_module_names ()) [ 'corpus_processing' , 'corpus_processing_simple' , 'dharpa_workflow' , 'file_reader' , 'input_files_processing' , 'lowercase_corpus' , 'remove_stopwords_from_corpus' , 'tokenize_corpus' ] # display the module spec for the 'input_files_processing' workflow print_module_desc ( \"input_files_processing\" ) { 'input_files_processing' : { 'inputs' : { 'files' : 'Any' , 'make_lowercase' : 'Bool' , 'remove_stopwords' : 'Bool' , 'stopwords' : 'List' } , 'outputs' : { 'processed_text_corpus' : 'Dict' } } } # create the workflow object ifp_workflow = create_workflow ( 'input_files_processing' ) # display the internal structure of the workflow print ( ifp_workflow . _workflow_config ) { 'modules' : [ { 'type' : 'file_reader' , 'input_map' : { 'files' : '__workflow_input__.files' } , 'id' : 'file_reader' } , { 'type' : 'corpus_processing' , 'input_map' : { 'text_map' : 'file_reader.content_map' , 'make_lowercase' : '__workflow_input__.make_lowercase' , 'remove_stopwords' : '__workflow_input__.remove_stopwords' , 'stopwords' : '__workflow_input__.stopwords' } , 'workflow_outputs' : { 'processed_text_corpus' : 'processed_text_corpus' } , 'id' : 'corpus_processing' } ] } # display the execution graph for the workflow graph_to_image ( ifp_workflow . execution_graph ) # graph_to_image(ifp_workflow.data_flow_graph) # auto-render input and output-widgets for the workflow (only works when executed in Jupyter) renderer = PlainJupyterWorkflowRenderer ( workflow = ifp_workflow ) renderer . render () var element = $('#876f7e09-d431-4aab-ae5e-b2c47b44dc0f'); var element = $('#34ba6f28-d1ae-4fbf-bb62-dd7b2a0c4a71');","title":"Modularity 2"},{"location":"architecture/workflows/modularity/modularity_2/#whats-a-workflow","text":"a workflow is a tool to transform data into more structured data 'more data' -- we'll create what can be considered 'new' data out of the existing set 'better structured' -- improve (and replace) the current structure (fix errors, etc.) 'more structure' -- augment existing data with additional structure secondary outcomes (insight, new research questions, etc...) workflows are 'just' simple scripts no or only very minimal control structures, except from input/output connections low computational complexity for the workflow itself high(er) computational complexity within modules (but hidden from workflow creator)","title":"What's a workflow?"},{"location":"architecture/workflows/modularity/modularity_2/#workflows-in-our-context","text":"workflow creators and workflow users are different, distinct roles Jupyter is a very good tool to create non-trivial exploratory workflows there's a difference between 'dynamic' workflows, and 'static' ones Jupyter is usually used to create workflows in a 'dynamic' way more 'data engineering' than 'data science' interactivity: not an issue in 'data engineering' not very well supported in Jupyter (cell-based approach not useful for us)","title":"Workflows in our context"},{"location":"architecture/workflows/modularity/modularity_2/#two-options-monolithic-modular","text":"","title":"Two options: monolithic &amp; modular"},{"location":"architecture/workflows/modularity/modularity_2/#monolithic","text":"complexity spread out across the workflow well integrated, no restrictions UI-wise","title":"Monolithic"},{"location":"architecture/workflows/modularity/modularity_2/#modular","text":"complexity concentrated in the framework, but simple modules some restrictions on the UI no difference in how to handle data, metadata backend modules don't necessarily map onto frontend modules suggested approach: 100% modular backend modular frontend, incl. optional monolithic frontend approach for high-value workflows Notes: only important for the workflow part, there will be features that won't be affected by this at all (metadata, data)","title":"Modular"},{"location":"architecture/workflows/modularity/modularity_2/#definitions","text":"module : a module is an atomic entity that contains a fixed set of defined inputs and outputs, as well as a processing unit that converts the set of inputs to outputs, in a predicable way workflow : a workflow contains a set of modules which are connected in a specific way a workflow is conceptually also a module, because it also contains a set of inputs/outputs as well as processing unit it can be used in other, 'parent' workflows in the same ways a normal module can.","title":"Definitions"},{"location":"architecture/workflows/modularity/modularity_2/#modelling-workflows","text":"research data is more useful when it's structured, so why would workflow definitions be different? so: can we model a workflow as data? Notes: monolithic workflows are not structured (comparison:","title":"Modelling workflows"},{"location":"architecture/workflows/modularity/modularity_2/#advantages","text":"scalability disposible middleware and frontends (only important to be able to use the created workflows)","title":"Advantages"},{"location":"architecture/workflows/modularity/modularity_2/#examples","text":"# we can list all available modules (and workflows) list_available_module_names () ['dharpa_workflow', 'file_reader', 'lowercase_corpus', 'remove_stopwords_from_corpus', 'tokenize_corpus'] # we can investigate each modules inputs and output specs print_module_desc ( 'tokenize_corpus' ) { 'tokenize_corpus' : { 'inputs' : { 'text_map' : 'Dict' } , 'outputs' : { 'tokenized_text' : 'Dict' }}} # a workflow configuration is basically just a list of modules, incl. their input/output connections workflow_config = f ' { base_path } /workflows/corpus_processing_simple.yaml' print_file_content ( workflow_config ) --- modules : - type : tokenize_corpus - type : lowercase_corpus input_map : tokenized_text : tokenize_corpus.tokenized_text - type : remove_stopwords_from_corpus input_map : tokenized_text : lowercase_corpus.tokenized_text workflow_outputs : tokenized_text : processed_text_corpus # we create a 'workflow' object using the configuration data workflow = DharpaWorkflow . from_file ( workflow_config ) # we can investigate each workflows available input and output names print ( workflow . input_names ) [ 'lowercase_corpus__enabled' , 'remove_stopwords_from_corpus__enabled' , 'remove_stopwords_from_corpus__stopwords_list' , 'tokenize_corpus__text_map' ] print ( workflow . output_names ) [ 'processed_text_corpus' ] # we can display the execution and data-flow structures of a workflow graphically graph_to_image ( workflow . execution_graph ) # graph_to_image(workflow.data_flow_graph) text_map = { \"1\" : \"Hello World!\" , \"2\" : \"Hello DHARPA!\" } stopwords = [ \"hello\" , \"!\" ] workflow = DharpaWorkflow . from_file ( workflow_config ) workflow . set_input ( \"tokenize_corpus__text_map\" , text_map ) workflow . set_input ( \"lowercase_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__stopwords_list\" , stopwords ) # the workflow state is processed automatically, so we can always query the current output output1 = workflow . get_output ( \"processed_text_corpus\" ) print ( output1 ) { '1' : [ 'world' ] , '2' : [ 'dharpa' ]} # text_map: {\"one\": \"Hello World!\", \"two\": \"Hello DHARPA!\"} # stopword_list: hello workflow = DharpaWorkflow . from_file ( workflow_config ) renderer = PlainJupyterWorkflowRenderer ( workflow = workflow ) renderer . render () var element = $('#4729e5ed-d8d7-4e2e-a724-41f1092426d2'); var element = $('#ab709a80-a011-4081-8eb6-5594ee11eb7b'); # we can load workflows from json/yaml files on the file-system, and convert them to Python classes load_workflows ( f \" { base_path } /workflows\" ) print ( list_available_module_names ()) [ 'corpus_processing' , 'corpus_processing_simple' , 'dharpa_workflow' , 'file_reader' , 'input_files_processing' , 'lowercase_corpus' , 'remove_stopwords_from_corpus' , 'tokenize_corpus' ] # display the module spec for the 'input_files_processing' workflow print_module_desc ( \"input_files_processing\" ) { 'input_files_processing' : { 'inputs' : { 'files' : 'Any' , 'make_lowercase' : 'Bool' , 'remove_stopwords' : 'Bool' , 'stopwords' : 'List' } , 'outputs' : { 'processed_text_corpus' : 'Dict' } } } # create the workflow object ifp_workflow = create_workflow ( 'input_files_processing' ) # display the internal structure of the workflow print ( ifp_workflow . _workflow_config ) { 'modules' : [ { 'type' : 'file_reader' , 'input_map' : { 'files' : '__workflow_input__.files' } , 'id' : 'file_reader' } , { 'type' : 'corpus_processing' , 'input_map' : { 'text_map' : 'file_reader.content_map' , 'make_lowercase' : '__workflow_input__.make_lowercase' , 'remove_stopwords' : '__workflow_input__.remove_stopwords' , 'stopwords' : '__workflow_input__.stopwords' } , 'workflow_outputs' : { 'processed_text_corpus' : 'processed_text_corpus' } , 'id' : 'corpus_processing' } ] } # display the execution graph for the workflow graph_to_image ( ifp_workflow . execution_graph ) # graph_to_image(ifp_workflow.data_flow_graph) # auto-render input and output-widgets for the workflow (only works when executed in Jupyter) renderer = PlainJupyterWorkflowRenderer ( workflow = ifp_workflow ) renderer . render () var element = $('#876f7e09-d431-4aab-ae5e-b2c47b44dc0f'); var element = $('#34ba6f28-d1ae-4fbf-bb62-dd7b2a0c4a71');","title":"Examples"},{"location":"development/entities/","text":"Schemas overviews \u00b6 This page contains an overview of the available models and their associated schemas used in kiara . Values \u00b6 ValueSchema : The schema of a value. Value : The underlying base class for all values. PipelineValueInfo : Convenience wrapper to make the PipelineState json/dict export prettier. PipelineValuesInfo : Convenience wrapper to make the PipelineState json/dict export prettier. StepValueAddress : Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure. StepInputRef : An input to a step. StepOutputRef : An output to a step. PipelineInputRef : An input to a pipeline. PipelineOutputRef : An output to a pipeline. Modules \u00b6 ModuleTypeConfigSchema : Base class that describes the configuration a KiaraModule class accepts. PipelineConfig : A class to hold the configuration for a PipelineModule . PipelineStep : A step within a pipeline-structure, includes information about it's connection(s) and other metadata. PipelineStructureDesc : Outlines the internal structure of a Pipeline . PipelineState : Describes the current state of a pipeline. ModuleConfig : A class to hold the type and configuration for a module instance. Events \u00b6 StepInputEvent : Event that gets fired when one or several inputs for steps within a pipeline have changed. StepOutputEvent : Event that gets fired when one or several outputs for steps within a pipeline have changed. PipelineInputEvent : Event that gets fired when one or several inputs for the pipeline itself have changed. PipelineOutputEvent : Event that gets fired when one or several outputs for the pipeline itself have changed.","title":"Schemas overviews"},{"location":"development/entities/#schemas-overviews","text":"This page contains an overview of the available models and their associated schemas used in kiara .","title":"Schemas overviews"},{"location":"development/entities/#values","text":"ValueSchema : The schema of a value. Value : The underlying base class for all values. PipelineValueInfo : Convenience wrapper to make the PipelineState json/dict export prettier. PipelineValuesInfo : Convenience wrapper to make the PipelineState json/dict export prettier. StepValueAddress : Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure. StepInputRef : An input to a step. StepOutputRef : An output to a step. PipelineInputRef : An input to a pipeline. PipelineOutputRef : An output to a pipeline.","title":"Values"},{"location":"development/entities/#modules","text":"ModuleTypeConfigSchema : Base class that describes the configuration a KiaraModule class accepts. PipelineConfig : A class to hold the configuration for a PipelineModule . PipelineStep : A step within a pipeline-structure, includes information about it's connection(s) and other metadata. PipelineStructureDesc : Outlines the internal structure of a Pipeline . PipelineState : Describes the current state of a pipeline. ModuleConfig : A class to hold the type and configuration for a module instance.","title":"Modules"},{"location":"development/entities/#events","text":"StepInputEvent : Event that gets fired when one or several inputs for steps within a pipeline have changed. StepOutputEvent : Event that gets fired when one or several outputs for steps within a pipeline have changed. PipelineInputEvent : Event that gets fired when one or several inputs for the pipeline itself have changed. PipelineOutputEvent : Event that gets fired when one or several outputs for the pipeline itself have changed.","title":"Events"},{"location":"development/entities/events/","text":"Events \u00b6 StepInputEvent \u00b6 Event that gets fired when one or several inputs for steps within a pipeline have changed. References \u00b6 model class reference: StepInputEvent JSON schema file: StepInputEvent.json JSON schema \u00b6 { \"title\" : \"StepInputEvent\" , \"description\" : \"Event that gets fired when one or several inputs for steps within a pipeline have changed.\" , \"type\" : \"object\" , \"properties\" : { \"pipeline_id\" : { \"title\" : \"Pipeline Id\" , \"type\" : \"string\" }, \"type\" : { \"title\" : \"Type\" , \"default\" : \"step_input\" , \"enum\" : [ \"step_input\" ], \"type\" : \"string\" }, \"updated_step_inputs\" : { \"title\" : \"Updated Step Inputs\" , \"description\" : \"steps (keys) with updated inputs which need re-processing (value is list of updated input names)\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } } }, \"required\" : [ \"pipeline_id\" , \"updated_step_inputs\" ] } StepOutputEvent \u00b6 Event that gets fired when one or several outputs for steps within a pipeline have changed. References \u00b6 model class reference: StepOutputEvent JSON schema file: StepOutputEvent.json JSON schema \u00b6 { \"title\" : \"StepOutputEvent\" , \"description\" : \"Event that gets fired when one or several outputs for steps within a pipeline have changed.\" , \"type\" : \"object\" , \"properties\" : { \"pipeline_id\" : { \"title\" : \"Pipeline Id\" , \"type\" : \"string\" }, \"type\" : { \"title\" : \"Type\" , \"default\" : \"step_output\" , \"enum\" : [ \"step_output\" ], \"type\" : \"string\" }, \"updated_step_outputs\" : { \"title\" : \"Updated Step Outputs\" , \"description\" : \"steps (keys) that finished processing of one, several or all outputs (values are list of 'finished' output fields)\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } } }, \"required\" : [ \"pipeline_id\" , \"updated_step_outputs\" ] } PipelineInputEvent \u00b6 Event that gets fired when one or several inputs for the pipeline itself have changed. References \u00b6 model class reference: PipelineInputEvent JSON schema file: PipelineInputEvent.json JSON schema \u00b6 { \"title\" : \"PipelineInputEvent\" , \"description\" : \"Event that gets fired when one or several inputs for the pipeline itself have changed.\" , \"type\" : \"object\" , \"properties\" : { \"pipeline_id\" : { \"title\" : \"Pipeline Id\" , \"type\" : \"string\" }, \"type\" : { \"title\" : \"Type\" , \"default\" : \"pipeline_input\" , \"enum\" : [ \"pipeline_input\" ], \"type\" : \"string\" }, \"updated_pipeline_inputs\" : { \"title\" : \"Updated Pipeline Inputs\" , \"description\" : \"list of pipeline input names that where changed\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } }, \"required\" : [ \"pipeline_id\" , \"updated_pipeline_inputs\" ] } PipelineOutputEvent \u00b6 Event that gets fired when one or several outputs for the pipeline itself have changed. References \u00b6 model class reference: PipelineOutputEvent JSON schema file: PipelineOutputEvent.json JSON schema \u00b6 { \"title\" : \"PipelineOutputEvent\" , \"description\" : \"Event that gets fired when one or several outputs for the pipeline itself have changed.\" , \"type\" : \"object\" , \"properties\" : { \"pipeline_id\" : { \"title\" : \"Pipeline Id\" , \"type\" : \"string\" }, \"type\" : { \"title\" : \"Type\" , \"default\" : \"pipeline_output\" , \"enum\" : [ \"pipeline_output\" ], \"type\" : \"string\" }, \"updated_pipeline_outputs\" : { \"title\" : \"Updated Pipeline Outputs\" , \"description\" : \"list of pipeline output names that where changed\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } }, \"required\" : [ \"pipeline_id\" , \"updated_pipeline_outputs\" ] }","title":"Events"},{"location":"development/entities/events/#events","text":"","title":"Events"},{"location":"development/entities/events/#stepinputevent","text":"Event that gets fired when one or several inputs for steps within a pipeline have changed.","title":"StepInputEvent"},{"location":"development/entities/events/#stepoutputevent","text":"Event that gets fired when one or several outputs for steps within a pipeline have changed.","title":"StepOutputEvent"},{"location":"development/entities/events/#pipelineinputevent","text":"Event that gets fired when one or several inputs for the pipeline itself have changed.","title":"PipelineInputEvent"},{"location":"development/entities/events/#pipelineoutputevent","text":"Event that gets fired when one or several outputs for the pipeline itself have changed.","title":"PipelineOutputEvent"},{"location":"development/entities/modules/","text":"Modules \u00b6 ModuleTypeConfigSchema \u00b6 Base class that describes the configuration a KiaraModule class accepts. This is stored in the _config_cls class attribute in each KiaraModule class. There are two config options every KiaraModule supports: constants , and defaults Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default values that override the schema defaults, and those can be overwritten by users. If both a constant and a default value is set for an input field, an error is thrown. References \u00b6 model class reference: ModuleTypeConfigSchema JSON schema file: ModuleTypeConfigSchema.json JSON schema \u00b6 { \"title\" : \"ModuleTypeConfigSchema\" , \"description\" : \"Base class that describes the configuration a [``KiaraModule``][kiara.module.KiaraModule] class accepts.\\n\\nThis is stored in the ``_config_cls`` class attribute in each ``KiaraModule`` class.\\n\\nThere are two config options every ``KiaraModule`` supports:\\n\\n - ``constants``, and\\n - ``defaults``\\n\\n Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default\\n values that override the schema defaults, and those can be overwritten by users. If both a constant and a default\\n value is set for an input field, an error is thrown.\" , \"type\" : \"object\" , \"properties\" : { \"constants\" : { \"title\" : \"Constants\" , \"description\" : \"Value constants for this module.\" , \"type\" : \"object\" }, \"defaults\" : { \"title\" : \"Defaults\" , \"description\" : \"Value defaults for this module.\" , \"type\" : \"object\" } }, \"additionalProperties\" : false } PipelineConfig \u00b6 A class to hold the configuration for a PipelineModule . If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify auto , in which case Kiara will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a ``nand`` logic gate (in Python): ``` python and_step = PipelineStepConfig(module_type=\"and\", step_id=\"and\") not_step = PipelineStepConfig(module_type=\"not\", step_id=\"not\", input_links={\"a\": [\"and.y\"]} nand_p_conf = PipelineConfig(doc=\"Returns 'False' if both inputs are 'True'.\", steps=[and_step, not_step], input_aliases={ \"and__a\": \"a\", \"and__b\": \"b\" }, output_aliases={ \"not__y\": \"y\" }} ``` Or, the same thing in json: ``` json { \"module_type_name\": \"nand\", \"doc\": \"Returns 'False' if both inputs are 'True'.\", \"steps\": [ { \"module_type\": \"and\", \"step_id\": \"and\" }, { \"module_type\": \"not\", \"step_id\": \"not\", \"input_links\": { \"a\": \"and.y\" } } ], \"input_aliases\": { \"and__a\": \"a\", \"and__b\": \"b\" }, \"output_aliases\": { \"not__y\": \"y\" } } ``` References \u00b6 model class reference: PipelineConfig JSON schema file: PipelineConfig.json JSON schema \u00b6 { \"title\" : \"PipelineConfig\" , \"description\" : \"A class to hold the configuration for a [PipelineModule][kiara.pipeline.module.PipelineModule].\\n\\nIf you want to control the pipeline input and output names, you need to have to provide a map that uses the\\nautogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name\\nas value. The reason that schema for the autogenerated field names exist is that it's hard to ensure\\nthe uniqueness of each field; some steps can have the same input field names, but will need different input\\nvalues. In some cases, some inputs of different steps need the same input. Those sorts of things.\\nSo, to make sure that we always use the right values, I chose to implement a conservative default approach,\\naccepting that in some cases the user will be prompted for duplicate inputs for the same value.\\n\\nTo remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of\\nthe input/output fields.\\n\\nFurther, because in a lot of cases there won't be any overlapping fields, the creator can specify ``auto``,\\nin which case *Kiara* will automatically create a mapping that tries to map autogenerated field names\\nto the shortest possible names for each case.\\n\\nExamples:\\n\\n Configuration for a pipeline module that functions as a ``nand`` logic gate (in Python):\\n\\n ``` python\\n and_step = PipelineStepConfig(module_type=\\\"and\\\", step_id=\\\"and\\\")\\n not_step = PipelineStepConfig(module_type=\\\"not\\\", step_id=\\\"not\\\", input_links={\\\"a\\\": [\\\"and.y\\\"]}\\n nand_p_conf = PipelineConfig(doc=\\\"Returns 'False' if both inputs are 'True'.\\\",\\n steps=[and_step, not_step],\\n input_aliases={\\n \\\"and__a\\\": \\\"a\\\",\\n \\\"and__b\\\": \\\"b\\\"\\n },\\n output_aliases={\\n \\\"not__y\\\": \\\"y\\\"\\n }}\\n ```\\n\\n Or, the same thing in json:\\n\\n ``` json\\n {\\n \\\"module_type_name\\\": \\\"nand\\\",\\n \\\"doc\\\": \\\"Returns 'False' if both inputs are 'True'.\\\",\\n \\\"steps\\\": [\\n {\\n \\\"module_type\\\": \\\"and\\\",\\n \\\"step_id\\\": \\\"and\\\"\\n },\\n {\\n \\\"module_type\\\": \\\"not\\\",\\n \\\"step_id\\\": \\\"not\\\",\\n \\\"input_links\\\": {\\n \\\"a\\\": \\\"and.y\\\"\\n }\\n }\\n ],\\n \\\"input_aliases\\\": {\\n \\\"and__a\\\": \\\"a\\\",\\n \\\"and__b\\\": \\\"b\\\"\\n },\\n \\\"output_aliases\\\": {\\n \\\"not__y\\\": \\\"y\\\"\\n }\\n }\\n ```\" , \"type\" : \"object\" , \"properties\" : { \"constants\" : { \"title\" : \"Constants\" , \"description\" : \"Value constants for this module.\" , \"type\" : \"object\" }, \"defaults\" : { \"title\" : \"Defaults\" , \"description\" : \"Value defaults for this module.\" , \"type\" : \"object\" }, \"steps\" : { \"title\" : \"Steps\" , \"description\" : \"A list of steps/modules of this pipeline, and their connections.\" , \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/PipelineStepConfig\" } }, \"input_aliases\" : { \"title\" : \"Input Aliases\" , \"description\" : \"A map of input aliases, with the calculated (<step_id>__<input_name> -- double underscore!) name as key, and a string (the resulting workflow input alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.\" , \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"string\" } } ] }, \"output_aliases\" : { \"title\" : \"Output Aliases\" , \"description\" : \"A map of output aliases, with the calculated (<step_id>__<output_name> -- double underscore!) name as key, and a string (the resulting workflow output alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.\" , \"anyOf\" : [ { \"type\" : \"string\" }, { \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"string\" } } ] }, \"documentation\" : { \"title\" : \"Documentation\" , \"description\" : \"Documentation about what the pipeline does.\" , \"default\" : \"-- n/a --\" , \"type\" : \"string\" }, \"context\" : { \"title\" : \"Context\" , \"description\" : \"Metadata for this workflow.\" , \"type\" : \"object\" } }, \"definitions\" : { \"DocumentationMetadataModel\" : { \"title\" : \"DocumentationMetadataModel\" , \"description\" : \"Base class for classes that represent value metadata in kiara.\" , \"type\" : \"object\" , \"properties\" : { \"description\" : { \"title\" : \"Description\" , \"description\" : \"Short description of the item.\" , \"default\" : \"-- n/a --\" , \"type\" : \"string\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"Detailed documentation of the item (in markdown).\" , \"type\" : \"string\" } } }, \"StepValueAddress\" : { \"title\" : \"StepValueAddress\" , \"description\" : \"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.\" , \"type\" : \"object\" , \"properties\" : { \"step_id\" : { \"title\" : \"Step Id\" , \"description\" : \"The id of a step within a pipeline.\" , \"type\" : \"string\" }, \"value_name\" : { \"title\" : \"Value Name\" , \"description\" : \"The name of the value (output name or pipeline input name).\" , \"type\" : \"string\" }, \"sub_value\" : { \"title\" : \"Sub Value\" , \"description\" : \"A reference to a subitem of a value (e.g. column, list item)\" , \"type\" : \"object\" } }, \"required\" : [ \"step_id\" , \"value_name\" ], \"additionalProperties\" : false }, \"PipelineStepConfig\" : { \"title\" : \"PipelineStepConfig\" , \"description\" : \"A class to hold the configuration of one module within a [PipelineModule][kiara.pipeline.module.PipelineModule].\" , \"type\" : \"object\" , \"properties\" : { \"module_type\" : { \"title\" : \"Module Type\" , \"description\" : \"The module type.\" , \"type\" : \"string\" }, \"module_config\" : { \"title\" : \"Module Config\" , \"description\" : \"The configuration for the module.\" , \"type\" : \"object\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"Documentation for this operation.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/DocumentationMetadataModel\" } ] }, \"step_id\" : { \"title\" : \"Step Id\" , \"description\" : \"The id of the step.\" , \"type\" : \"string\" }, \"input_links\" : { \"title\" : \"Input Links\" , \"description\" : \"The map with the name of an input link as key, and the connected module output name(s) as value.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/StepValueAddress\" } } } }, \"required\" : [ \"module_type\" , \"step_id\" ], \"additionalProperties\" : false } } } PipelineStep \u00b6 A step within a pipeline-structure, includes information about it's connection(s) and other metadata. References \u00b6 model class reference: PipelineStep JSON schema file: PipelineStep.json JSON schema \u00b6 { \"title\" : \"PipelineStep\" , \"description\" : \"A step within a pipeline-structure, includes information about it's connection(s) and other metadata.\" , \"type\" : \"object\" , \"properties\" : { \"step_id\" : { \"title\" : \"Step Id\" , \"type\" : \"string\" }, \"module_type\" : { \"title\" : \"Module Type\" , \"description\" : \"The module type.\" , \"type\" : \"string\" }, \"module_config\" : { \"title\" : \"Module Config\" , \"description\" : \"The module config.\" , \"type\" : \"object\" }, \"required\" : { \"title\" : \"Required\" , \"description\" : \"Whether this step is required within the workflow.\\n\\nIn some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required.\" , \"default\" : true , \"type\" : \"boolean\" }, \"processing_stage\" : { \"title\" : \"Processing Stage\" , \"description\" : \"The stage number this step is executed within the pipeline.\" , \"type\" : \"integer\" }, \"input_links\" : { \"title\" : \"Input Links\" , \"description\" : \"The links that connect to inputs of the module.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/StepValueAddress\" } } } }, \"required\" : [ \"step_id\" , \"module_type\" ], \"additionalProperties\" : false , \"definitions\" : { \"StepValueAddress\" : { \"title\" : \"StepValueAddress\" , \"description\" : \"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.\" , \"type\" : \"object\" , \"properties\" : { \"step_id\" : { \"title\" : \"Step Id\" , \"description\" : \"The id of a step within a pipeline.\" , \"type\" : \"string\" }, \"value_name\" : { \"title\" : \"Value Name\" , \"description\" : \"The name of the value (output name or pipeline input name).\" , \"type\" : \"string\" }, \"sub_value\" : { \"title\" : \"Sub Value\" , \"description\" : \"A reference to a subitem of a value (e.g. column, list item)\" , \"type\" : \"object\" } }, \"required\" : [ \"step_id\" , \"value_name\" ], \"additionalProperties\" : false } } } PipelineStructureDesc \u00b6 Outlines the internal structure of a Pipeline . References \u00b6 model class reference: PipelineStructureDesc JSON schema file: PipelineStructureDesc.json JSON schema \u00b6 { \"title\" : \"PipelineStructureDesc\" , \"description\" : \"Outlines the internal structure of a [Pipeline][kiara.pipeline.pipeline.Pipeline].\" , \"type\" : \"object\" , \"properties\" : { \"steps\" : { \"title\" : \"Steps\" , \"description\" : \"The steps contained in this pipeline, with the 'step_id' as key.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/StepDesc\" } }, \"processing_stages\" : { \"title\" : \"Processing Stages\" , \"description\" : \"The order in which this pipeline has to be processed (basically the dependencies of each step on other steps, if any).\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } }, \"pipeline_input_connections\" : { \"title\" : \"Pipeline Input Connections\" , \"description\" : \"The connections of this pipelines input fields. One input field can be connected to one or several step input fields.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } }, \"pipeline_output_connections\" : { \"title\" : \"Pipeline Output Connections\" , \"description\" : \"The connections of this pipelines output fields. Each pipeline output is connected to exactly one step output field.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"string\" } }, \"pipeline_inputs\" : { \"title\" : \"Pipeline Inputs\" , \"description\" : \"The pipeline inputs.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/PipelineInputRef\" } }, \"pipeline_outputs\" : { \"title\" : \"Pipeline Outputs\" , \"description\" : \"The pipeline outputs.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/PipelineOutputRef\" } } }, \"required\" : [ \"steps\" , \"processing_stages\" , \"pipeline_input_connections\" , \"pipeline_output_connections\" , \"pipeline_inputs\" , \"pipeline_outputs\" ], \"additionalProperties\" : false , \"definitions\" : { \"StepValueAddress\" : { \"title\" : \"StepValueAddress\" , \"description\" : \"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.\" , \"type\" : \"object\" , \"properties\" : { \"step_id\" : { \"title\" : \"Step Id\" , \"description\" : \"The id of a step within a pipeline.\" , \"type\" : \"string\" }, \"value_name\" : { \"title\" : \"Value Name\" , \"description\" : \"The name of the value (output name or pipeline input name).\" , \"type\" : \"string\" }, \"sub_value\" : { \"title\" : \"Sub Value\" , \"description\" : \"A reference to a subitem of a value (e.g. column, list item)\" , \"type\" : \"object\" } }, \"required\" : [ \"step_id\" , \"value_name\" ], \"additionalProperties\" : false }, \"PipelineStep\" : { \"title\" : \"PipelineStep\" , \"description\" : \"A step within a pipeline-structure, includes information about it's connection(s) and other metadata.\" , \"type\" : \"object\" , \"properties\" : { \"step_id\" : { \"title\" : \"Step Id\" , \"type\" : \"string\" }, \"module_type\" : { \"title\" : \"Module Type\" , \"description\" : \"The module type.\" , \"type\" : \"string\" }, \"module_config\" : { \"title\" : \"Module Config\" , \"description\" : \"The module config.\" , \"type\" : \"object\" }, \"required\" : { \"title\" : \"Required\" , \"description\" : \"Whether this step is required within the workflow.\\n\\nIn some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required.\" , \"default\" : true , \"type\" : \"boolean\" }, \"processing_stage\" : { \"title\" : \"Processing Stage\" , \"description\" : \"The stage number this step is executed within the pipeline.\" , \"type\" : \"integer\" }, \"input_links\" : { \"title\" : \"Input Links\" , \"description\" : \"The links that connect to inputs of the module.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/StepValueAddress\" } } } }, \"required\" : [ \"step_id\" , \"module_type\" ], \"additionalProperties\" : false }, \"StepDesc\" : { \"title\" : \"StepDesc\" , \"description\" : \"Details of a single [PipelineStep][kiara.pipeline.structure.PipelineStep] (which lives within a [Pipeline][kiara.pipeline.pipeline.Pipeline]\" , \"type\" : \"object\" , \"properties\" : { \"step\" : { \"title\" : \"Step\" , \"description\" : \"Attributes of the step itself.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/PipelineStep\" } ] }, \"processing_stage\" : { \"title\" : \"Processing Stage\" , \"description\" : \"The processing stage of this step within a Pipeline.\" , \"type\" : \"integer\" }, \"input_connections\" : { \"title\" : \"Input Connections\" , \"description\" : \"A map that explains what elements connect to this steps inputs. A connection could either be a Pipeline input (indicated by the ``__pipeline__`` token), or another steps output.\\n\\nExample:\\n``` json\\ninput_connections: {\\n \\\"a\\\": [\\\"__pipeline__.a\\\"],\\n \\\"b\\\": [\\\"step_one.a\\\"]\\n}\\n\\n```\\n \" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } }, \"output_connections\" : { \"title\" : \"Output Connections\" , \"description\" : \"A map that explains what elemnts connect to this steps outputs. A connection could be either a Pipeline output, or another steps input.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } }, \"required\" : { \"title\" : \"Required\" , \"description\" : \"Whether this step is always required, or potentially could be skipped in case some inputs are not available.\" , \"type\" : \"boolean\" } }, \"required\" : [ \"step\" , \"processing_stage\" , \"input_connections\" , \"output_connections\" , \"required\" ], \"additionalProperties\" : false }, \"ValueSchema\" : { \"title\" : \"ValueSchema\" , \"description\" : \"The schema of a value.\\n\\nThe schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that\\nwill be used if no user input was given (yet) for a value.\\n\\nFor more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field.\" , \"type\" : \"object\" , \"properties\" : { \"type\" : { \"title\" : \"Type\" , \"description\" : \"The type of the value.\" , \"type\" : \"string\" }, \"type_config\" : { \"title\" : \"Type Config\" , \"description\" : \"Configuration for the type, in case it's complex.\" , \"type\" : \"object\" }, \"default\" : { \"title\" : \"Default\" , \"description\" : \"A default value.\" , \"default\" : \"__not_set__\" }, \"optional\" : { \"title\" : \"Optional\" , \"description\" : \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"description\" : \"Whether the value is a constant.\" , \"default\" : false , \"type\" : \"boolean\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"A description for the value of this input field.\" , \"default\" : \"-- n/a --\" , \"type\" : \"string\" } }, \"required\" : [ \"type\" ] }, \"PipelineInputRef\" : { \"title\" : \"PipelineInputRef\" , \"description\" : \"An input to a pipeline.\" , \"type\" : \"object\" , \"properties\" : { \"value_name\" : { \"title\" : \"Value Name\" , \"type\" : \"string\" }, \"value_schema\" : { \"$ref\" : \"#/definitions/ValueSchema\" }, \"connected_inputs\" : { \"title\" : \"Connected Inputs\" , \"description\" : \"The step inputs that are connected to this pipeline input\" , \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/StepValueAddress\" } }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"default\" : \"Whether this input is a constant and can't be changed by the user.\" , \"type\" : \"boolean\" } }, \"required\" : [ \"value_name\" , \"value_schema\" ], \"additionalProperties\" : false }, \"PipelineOutputRef\" : { \"title\" : \"PipelineOutputRef\" , \"description\" : \"An output to a pipeline.\" , \"type\" : \"object\" , \"properties\" : { \"value_name\" : { \"title\" : \"Value Name\" , \"type\" : \"string\" }, \"value_schema\" : { \"$ref\" : \"#/definitions/ValueSchema\" }, \"connected_output\" : { \"title\" : \"Connected Output\" , \"description\" : \"Connected step outputs.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/StepValueAddress\" } ] } }, \"required\" : [ \"value_name\" , \"value_schema\" , \"connected_output\" ], \"additionalProperties\" : false } } } PipelineState \u00b6 Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the dict or json methods to convert this object into a generic data structure. References \u00b6 model class reference: PipelineState JSON schema file: PipelineState.json JSON schema \u00b6 { \"title\" : \"PipelineState\" , \"description\" : \"Describes the current state of a pipeline.\\n\\nThis includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well\\nas all current input/output values for the pipeline itself, as well as for all internal steps.\\n\\nUse the ``dict`` or ``json`` methods to convert this object into a generic data structure.\" , \"type\" : \"object\" , \"properties\" : { \"structure\" : { \"title\" : \"Structure\" , \"description\" : \"The structure (interconnections of modules/steps) of the pipeline.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/PipelineStructureDesc\" } ] }, \"pipeline_inputs\" : { \"title\" : \"Pipeline Inputs\" , \"description\" : \"The current (externally facing) input values of this pipeline.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/PipelineValuesInfo\" } ] }, \"pipeline_outputs\" : { \"title\" : \"Pipeline Outputs\" , \"description\" : \"The current (externally facing) output values of this pipeline.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/PipelineValuesInfo\" } ] }, \"step_states\" : { \"description\" : \"The status of each step.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/StepStatus\" } }, \"step_inputs\" : { \"title\" : \"Step Inputs\" , \"description\" : \"The current (internal) input values of each step of this pipeline.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/PipelineValuesInfo\" } }, \"step_outputs\" : { \"title\" : \"Step Outputs\" , \"description\" : \"The current (internal) output values of each step of this pipeline.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/PipelineValuesInfo\" } }, \"status\" : { \"description\" : \"The current overal status of the pipeline.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/StepStatus\" } ] } }, \"required\" : [ \"structure\" , \"pipeline_inputs\" , \"pipeline_outputs\" , \"step_states\" , \"step_inputs\" , \"step_outputs\" , \"status\" ], \"definitions\" : { \"StepValueAddress\" : { \"title\" : \"StepValueAddress\" , \"description\" : \"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.\" , \"type\" : \"object\" , \"properties\" : { \"step_id\" : { \"title\" : \"Step Id\" , \"description\" : \"The id of a step within a pipeline.\" , \"type\" : \"string\" }, \"value_name\" : { \"title\" : \"Value Name\" , \"description\" : \"The name of the value (output name or pipeline input name).\" , \"type\" : \"string\" }, \"sub_value\" : { \"title\" : \"Sub Value\" , \"description\" : \"A reference to a subitem of a value (e.g. column, list item)\" , \"type\" : \"object\" } }, \"required\" : [ \"step_id\" , \"value_name\" ], \"additionalProperties\" : false }, \"PipelineStep\" : { \"title\" : \"PipelineStep\" , \"description\" : \"A step within a pipeline-structure, includes information about it's connection(s) and other metadata.\" , \"type\" : \"object\" , \"properties\" : { \"step_id\" : { \"title\" : \"Step Id\" , \"type\" : \"string\" }, \"module_type\" : { \"title\" : \"Module Type\" , \"description\" : \"The module type.\" , \"type\" : \"string\" }, \"module_config\" : { \"title\" : \"Module Config\" , \"description\" : \"The module config.\" , \"type\" : \"object\" }, \"required\" : { \"title\" : \"Required\" , \"description\" : \"Whether this step is required within the workflow.\\n\\nIn some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required.\" , \"default\" : true , \"type\" : \"boolean\" }, \"processing_stage\" : { \"title\" : \"Processing Stage\" , \"description\" : \"The stage number this step is executed within the pipeline.\" , \"type\" : \"integer\" }, \"input_links\" : { \"title\" : \"Input Links\" , \"description\" : \"The links that connect to inputs of the module.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/StepValueAddress\" } } } }, \"required\" : [ \"step_id\" , \"module_type\" ], \"additionalProperties\" : false }, \"StepDesc\" : { \"title\" : \"StepDesc\" , \"description\" : \"Details of a single [PipelineStep][kiara.pipeline.structure.PipelineStep] (which lives within a [Pipeline][kiara.pipeline.pipeline.Pipeline]\" , \"type\" : \"object\" , \"properties\" : { \"step\" : { \"title\" : \"Step\" , \"description\" : \"Attributes of the step itself.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/PipelineStep\" } ] }, \"processing_stage\" : { \"title\" : \"Processing Stage\" , \"description\" : \"The processing stage of this step within a Pipeline.\" , \"type\" : \"integer\" }, \"input_connections\" : { \"title\" : \"Input Connections\" , \"description\" : \"A map that explains what elements connect to this steps inputs. A connection could either be a Pipeline input (indicated by the ``__pipeline__`` token), or another steps output.\\n\\nExample:\\n``` json\\ninput_connections: {\\n \\\"a\\\": [\\\"__pipeline__.a\\\"],\\n \\\"b\\\": [\\\"step_one.a\\\"]\\n}\\n\\n```\\n \" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } }, \"output_connections\" : { \"title\" : \"Output Connections\" , \"description\" : \"A map that explains what elemnts connect to this steps outputs. A connection could be either a Pipeline output, or another steps input.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } }, \"required\" : { \"title\" : \"Required\" , \"description\" : \"Whether this step is always required, or potentially could be skipped in case some inputs are not available.\" , \"type\" : \"boolean\" } }, \"required\" : [ \"step\" , \"processing_stage\" , \"input_connections\" , \"output_connections\" , \"required\" ], \"additionalProperties\" : false }, \"ValueSchema\" : { \"title\" : \"ValueSchema\" , \"description\" : \"The schema of a value.\\n\\nThe schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that\\nwill be used if no user input was given (yet) for a value.\\n\\nFor more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field.\" , \"type\" : \"object\" , \"properties\" : { \"type\" : { \"title\" : \"Type\" , \"description\" : \"The type of the value.\" , \"type\" : \"string\" }, \"type_config\" : { \"title\" : \"Type Config\" , \"description\" : \"Configuration for the type, in case it's complex.\" , \"type\" : \"object\" }, \"default\" : { \"title\" : \"Default\" , \"description\" : \"A default value.\" , \"default\" : \"__not_set__\" }, \"optional\" : { \"title\" : \"Optional\" , \"description\" : \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"description\" : \"Whether the value is a constant.\" , \"default\" : false , \"type\" : \"boolean\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"A description for the value of this input field.\" , \"default\" : \"-- n/a --\" , \"type\" : \"string\" } }, \"required\" : [ \"type\" ] }, \"PipelineInputRef\" : { \"title\" : \"PipelineInputRef\" , \"description\" : \"An input to a pipeline.\" , \"type\" : \"object\" , \"properties\" : { \"value_name\" : { \"title\" : \"Value Name\" , \"type\" : \"string\" }, \"value_schema\" : { \"$ref\" : \"#/definitions/ValueSchema\" }, \"connected_inputs\" : { \"title\" : \"Connected Inputs\" , \"description\" : \"The step inputs that are connected to this pipeline input\" , \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/StepValueAddress\" } }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"default\" : \"Whether this input is a constant and can't be changed by the user.\" , \"type\" : \"boolean\" } }, \"required\" : [ \"value_name\" , \"value_schema\" ], \"additionalProperties\" : false }, \"PipelineOutputRef\" : { \"title\" : \"PipelineOutputRef\" , \"description\" : \"An output to a pipeline.\" , \"type\" : \"object\" , \"properties\" : { \"value_name\" : { \"title\" : \"Value Name\" , \"type\" : \"string\" }, \"value_schema\" : { \"$ref\" : \"#/definitions/ValueSchema\" }, \"connected_output\" : { \"title\" : \"Connected Output\" , \"description\" : \"Connected step outputs.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/StepValueAddress\" } ] } }, \"required\" : [ \"value_name\" , \"value_schema\" , \"connected_output\" ], \"additionalProperties\" : false }, \"PipelineStructureDesc\" : { \"title\" : \"PipelineStructureDesc\" , \"description\" : \"Outlines the internal structure of a [Pipeline][kiara.pipeline.pipeline.Pipeline].\" , \"type\" : \"object\" , \"properties\" : { \"steps\" : { \"title\" : \"Steps\" , \"description\" : \"The steps contained in this pipeline, with the 'step_id' as key.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/StepDesc\" } }, \"processing_stages\" : { \"title\" : \"Processing Stages\" , \"description\" : \"The order in which this pipeline has to be processed (basically the dependencies of each step on other steps, if any).\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } }, \"pipeline_input_connections\" : { \"title\" : \"Pipeline Input Connections\" , \"description\" : \"The connections of this pipelines input fields. One input field can be connected to one or several step input fields.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } } }, \"pipeline_output_connections\" : { \"title\" : \"Pipeline Output Connections\" , \"description\" : \"The connections of this pipelines output fields. Each pipeline output is connected to exactly one step output field.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"string\" } }, \"pipeline_inputs\" : { \"title\" : \"Pipeline Inputs\" , \"description\" : \"The pipeline inputs.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/PipelineInputRef\" } }, \"pipeline_outputs\" : { \"title\" : \"Pipeline Outputs\" , \"description\" : \"The pipeline outputs.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/PipelineOutputRef\" } } }, \"required\" : [ \"steps\" , \"processing_stages\" , \"pipeline_input_connections\" , \"pipeline_output_connections\" , \"pipeline_inputs\" , \"pipeline_outputs\" ], \"additionalProperties\" : false }, \"PipelineValueInfo\" : { \"title\" : \"PipelineValueInfo\" , \"description\" : \"Convenience wrapper to make the [PipelineState][kiara.pipeline.pipeline.PipelineState] json/dict export prettier.\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"title\" : \"Id\" , \"description\" : \"A unique id for this value.\" , \"type\" : \"string\" }, \"is_valid\" : { \"title\" : \"Is Valid\" , \"description\" : \"Whether the value is set and valid.\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_set\" : { \"title\" : \"Is Set\" , \"description\" : \"Whether the value is set.\" , \"type\" : \"boolean\" }, \"value_schema\" : { \"title\" : \"Value Schema\" , \"description\" : \"The schema of this value.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/ValueSchema\" } ] }, \"metadata\" : { \"title\" : \"Metadata\" , \"description\" : \"Metadata relating to the actual data (size, no. of rows, etc. -- depending on data type).\" , \"type\" : \"object\" } }, \"required\" : [ \"id\" , \"is_set\" , \"value_schema\" ], \"additionalProperties\" : false }, \"PipelineValuesInfo\" : { \"title\" : \"PipelineValuesInfo\" , \"description\" : \"Convenience wrapper to make the [PipelineState][kiara.pipeline.pipeline.PipelineState] json/dict export prettier.\\n\\nThis is basically just a simplified version of the [ValueSet][kiara.data.values.ValueSet] class that is using\\npydantic, in order to make it easy to export to json.\" , \"type\" : \"object\" , \"properties\" : { \"values\" : { \"title\" : \"Values\" , \"description\" : \"Field names are keys, and the data as values.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/PipelineValueInfo\" } } }, \"required\" : [ \"values\" ] }, \"StepStatus\" : { \"title\" : \"StepStatus\" , \"description\" : \"Enum to describe the state of a workflow.\" , \"enum\" : [ \"stale\" , \"inputs_ready\" , \"processing\" , \"results_ready\" ] } } } ModuleConfig \u00b6 A class to hold the type and configuration for a module instance. References \u00b6 model class reference: ModuleConfig JSON schema file: ModuleConfig.json JSON schema \u00b6 { \"title\" : \"ModuleConfig\" , \"description\" : \"A class to hold the type and configuration for a module instance.\" , \"type\" : \"object\" , \"properties\" : { \"module_type\" : { \"title\" : \"Module Type\" , \"description\" : \"The module type.\" , \"type\" : \"string\" }, \"module_config\" : { \"title\" : \"Module Config\" , \"description\" : \"The configuration for the module.\" , \"type\" : \"object\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"Documentation for this operation.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/DocumentationMetadataModel\" } ] } }, \"required\" : [ \"module_type\" ], \"additionalProperties\" : false , \"definitions\" : { \"DocumentationMetadataModel\" : { \"title\" : \"DocumentationMetadataModel\" , \"description\" : \"Base class for classes that represent value metadata in kiara.\" , \"type\" : \"object\" , \"properties\" : { \"description\" : { \"title\" : \"Description\" , \"description\" : \"Short description of the item.\" , \"default\" : \"-- n/a --\" , \"type\" : \"string\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"Detailed documentation of the item (in markdown).\" , \"type\" : \"string\" } } } } }","title":"Modules"},{"location":"development/entities/modules/#modules","text":"","title":"Modules"},{"location":"development/entities/modules/#moduletypeconfigschema","text":"Base class that describes the configuration a KiaraModule class accepts. This is stored in the _config_cls class attribute in each KiaraModule class. There are two config options every KiaraModule supports: constants , and defaults Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default values that override the schema defaults, and those can be overwritten by users. If both a constant and a default value is set for an input field, an error is thrown.","title":"ModuleTypeConfigSchema"},{"location":"development/entities/modules/#pipelineconfig","text":"A class to hold the configuration for a PipelineModule . If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify auto , in which case Kiara will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a ``nand`` logic gate (in Python): ``` python and_step = PipelineStepConfig(module_type=\"and\", step_id=\"and\") not_step = PipelineStepConfig(module_type=\"not\", step_id=\"not\", input_links={\"a\": [\"and.y\"]} nand_p_conf = PipelineConfig(doc=\"Returns 'False' if both inputs are 'True'.\", steps=[and_step, not_step], input_aliases={ \"and__a\": \"a\", \"and__b\": \"b\" }, output_aliases={ \"not__y\": \"y\" }} ``` Or, the same thing in json: ``` json { \"module_type_name\": \"nand\", \"doc\": \"Returns 'False' if both inputs are 'True'.\", \"steps\": [ { \"module_type\": \"and\", \"step_id\": \"and\" }, { \"module_type\": \"not\", \"step_id\": \"not\", \"input_links\": { \"a\": \"and.y\" } } ], \"input_aliases\": { \"and__a\": \"a\", \"and__b\": \"b\" }, \"output_aliases\": { \"not__y\": \"y\" } } ```","title":"PipelineConfig"},{"location":"development/entities/modules/#pipelinestep","text":"A step within a pipeline-structure, includes information about it's connection(s) and other metadata.","title":"PipelineStep"},{"location":"development/entities/modules/#pipelinestructuredesc","text":"Outlines the internal structure of a Pipeline .","title":"PipelineStructureDesc"},{"location":"development/entities/modules/#pipelinestate","text":"Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the dict or json methods to convert this object into a generic data structure.","title":"PipelineState"},{"location":"development/entities/modules/#moduleconfig","text":"A class to hold the type and configuration for a module instance.","title":"ModuleConfig"},{"location":"development/entities/values/","text":"Values \u00b6 ValueSchema \u00b6 The schema of a value. The schema contains the ValueType of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the type_config field. References \u00b6 model class reference: ValueSchema JSON schema file: ValueSchema.json JSON schema \u00b6 { \"title\" : \"ValueSchema\" , \"description\" : \"The schema of a value.\\n\\nThe schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that\\nwill be used if no user input was given (yet) for a value.\\n\\nFor more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field.\" , \"type\" : \"object\" , \"properties\" : { \"type\" : { \"title\" : \"Type\" , \"description\" : \"The type of the value.\" , \"type\" : \"string\" }, \"type_config\" : { \"title\" : \"Type Config\" , \"description\" : \"Configuration for the type, in case it's complex.\" , \"type\" : \"object\" }, \"default\" : { \"title\" : \"Default\" , \"description\" : \"A default value.\" , \"default\" : \"__not_set__\" }, \"optional\" : { \"title\" : \"Optional\" , \"description\" : \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"description\" : \"Whether the value is a constant.\" , \"default\" : false , \"type\" : \"boolean\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"A description for the value of this input field.\" , \"default\" : \"-- n/a --\" , \"type\" : \"string\" } }, \"required\" : [ \"type\" ] } Value \u00b6 The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry. References \u00b6 model class reference: Value JSON schema file: Value.json JSON schema \u00b6 { \"title\" : \"Value\" , \"description\" : \"The underlying base class for all values.\\n\\nThe important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data\\nregistry.\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"title\" : \"Id\" , \"description\" : \"A unique id for this value.\" , \"type\" : \"string\" }, \"value_schema\" : { \"title\" : \"Value Schema\" , \"description\" : \"The schema of this value.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/ValueSchema\" } ] }, \"creation_date\" : { \"title\" : \"Creation Date\" , \"description\" : \"The time this value was created value happened.\" , \"type\" : \"string\" , \"format\" : \"date-time\" }, \"is_set\" : { \"title\" : \"Is Set\" , \"description\" : \"Whether the value was set (in some way: user input, default, constant...).\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_none\" : { \"title\" : \"Is None\" , \"description\" : \"Whether the value is 'None'.\" , \"default\" : true , \"type\" : \"boolean\" }, \"hashes\" : { \"title\" : \"Hashes\" , \"description\" : \"Available hashes relating to the actual value data. This attribute is not populated by default, use the 'get_hashes()' method to request one or several hash items, afterwards those hashes will be reflected in this attribute.\" , \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/ValueHash\" } }, \"metadata\" : { \"title\" : \"Metadata\" , \"description\" : \"Available metadata relating to the actual value data (size, no. of rows, etc. -- depending on data type). This attribute is not populated by default, use the 'get_metadata()' method to request one or several metadata items, afterwards those metadata items will be reflected in this attribute.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"object\" } } }, \"required\" : [ \"id\" , \"value_schema\" ], \"additionalProperties\" : false , \"definitions\" : { \"ValueSchema\" : { \"title\" : \"ValueSchema\" , \"description\" : \"The schema of a value.\\n\\nThe schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that\\nwill be used if no user input was given (yet) for a value.\\n\\nFor more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field.\" , \"type\" : \"object\" , \"properties\" : { \"type\" : { \"title\" : \"Type\" , \"description\" : \"The type of the value.\" , \"type\" : \"string\" }, \"type_config\" : { \"title\" : \"Type Config\" , \"description\" : \"Configuration for the type, in case it's complex.\" , \"type\" : \"object\" }, \"default\" : { \"title\" : \"Default\" , \"description\" : \"A default value.\" , \"default\" : \"__not_set__\" }, \"optional\" : { \"title\" : \"Optional\" , \"description\" : \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"description\" : \"Whether the value is a constant.\" , \"default\" : false , \"type\" : \"boolean\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"A description for the value of this input field.\" , \"default\" : \"-- n/a --\" , \"type\" : \"string\" } }, \"required\" : [ \"type\" ] }, \"ValueHash\" : { \"title\" : \"ValueHash\" , \"type\" : \"object\" , \"properties\" : { \"hash\" : { \"title\" : \"Hash\" , \"description\" : \"The value hash.\" , \"type\" : \"string\" }, \"hash_type\" : { \"title\" : \"Hash Type\" , \"description\" : \"The value hash method.\" , \"type\" : \"string\" } }, \"required\" : [ \"hash\" , \"hash_type\" ] } } } PipelineValueInfo \u00b6 Convenience wrapper to make the PipelineState json/dict export prettier. References \u00b6 model class reference: PipelineValueInfo JSON schema file: PipelineValueInfo.json JSON schema \u00b6 { \"title\" : \"PipelineValueInfo\" , \"description\" : \"Convenience wrapper to make the [PipelineState][kiara.pipeline.pipeline.PipelineState] json/dict export prettier.\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"title\" : \"Id\" , \"description\" : \"A unique id for this value.\" , \"type\" : \"string\" }, \"is_valid\" : { \"title\" : \"Is Valid\" , \"description\" : \"Whether the value is set and valid.\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_set\" : { \"title\" : \"Is Set\" , \"description\" : \"Whether the value is set.\" , \"type\" : \"boolean\" }, \"value_schema\" : { \"title\" : \"Value Schema\" , \"description\" : \"The schema of this value.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/ValueSchema\" } ] }, \"metadata\" : { \"title\" : \"Metadata\" , \"description\" : \"Metadata relating to the actual data (size, no. of rows, etc. -- depending on data type).\" , \"type\" : \"object\" } }, \"required\" : [ \"id\" , \"is_set\" , \"value_schema\" ], \"additionalProperties\" : false , \"definitions\" : { \"ValueSchema\" : { \"title\" : \"ValueSchema\" , \"description\" : \"The schema of a value.\\n\\nThe schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that\\nwill be used if no user input was given (yet) for a value.\\n\\nFor more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field.\" , \"type\" : \"object\" , \"properties\" : { \"type\" : { \"title\" : \"Type\" , \"description\" : \"The type of the value.\" , \"type\" : \"string\" }, \"type_config\" : { \"title\" : \"Type Config\" , \"description\" : \"Configuration for the type, in case it's complex.\" , \"type\" : \"object\" }, \"default\" : { \"title\" : \"Default\" , \"description\" : \"A default value.\" , \"default\" : \"__not_set__\" }, \"optional\" : { \"title\" : \"Optional\" , \"description\" : \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"description\" : \"Whether the value is a constant.\" , \"default\" : false , \"type\" : \"boolean\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"A description for the value of this input field.\" , \"default\" : \"-- n/a --\" , \"type\" : \"string\" } }, \"required\" : [ \"type\" ] } } } PipelineValuesInfo \u00b6 Convenience wrapper to make the PipelineState json/dict export prettier. This is basically just a simplified version of the ValueSet class that is using pydantic, in order to make it easy to export to json. References \u00b6 model class reference: PipelineValuesInfo JSON schema file: PipelineValuesInfo.json JSON schema \u00b6 { \"title\" : \"PipelineValuesInfo\" , \"description\" : \"Convenience wrapper to make the [PipelineState][kiara.pipeline.pipeline.PipelineState] json/dict export prettier.\\n\\nThis is basically just a simplified version of the [ValueSet][kiara.data.values.ValueSet] class that is using\\npydantic, in order to make it easy to export to json.\" , \"type\" : \"object\" , \"properties\" : { \"values\" : { \"title\" : \"Values\" , \"description\" : \"Field names are keys, and the data as values.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/definitions/PipelineValueInfo\" } } }, \"required\" : [ \"values\" ], \"definitions\" : { \"ValueSchema\" : { \"title\" : \"ValueSchema\" , \"description\" : \"The schema of a value.\\n\\nThe schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that\\nwill be used if no user input was given (yet) for a value.\\n\\nFor more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field.\" , \"type\" : \"object\" , \"properties\" : { \"type\" : { \"title\" : \"Type\" , \"description\" : \"The type of the value.\" , \"type\" : \"string\" }, \"type_config\" : { \"title\" : \"Type Config\" , \"description\" : \"Configuration for the type, in case it's complex.\" , \"type\" : \"object\" }, \"default\" : { \"title\" : \"Default\" , \"description\" : \"A default value.\" , \"default\" : \"__not_set__\" }, \"optional\" : { \"title\" : \"Optional\" , \"description\" : \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"description\" : \"Whether the value is a constant.\" , \"default\" : false , \"type\" : \"boolean\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"A description for the value of this input field.\" , \"default\" : \"-- n/a --\" , \"type\" : \"string\" } }, \"required\" : [ \"type\" ] }, \"PipelineValueInfo\" : { \"title\" : \"PipelineValueInfo\" , \"description\" : \"Convenience wrapper to make the [PipelineState][kiara.pipeline.pipeline.PipelineState] json/dict export prettier.\" , \"type\" : \"object\" , \"properties\" : { \"id\" : { \"title\" : \"Id\" , \"description\" : \"A unique id for this value.\" , \"type\" : \"string\" }, \"is_valid\" : { \"title\" : \"Is Valid\" , \"description\" : \"Whether the value is set and valid.\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_set\" : { \"title\" : \"Is Set\" , \"description\" : \"Whether the value is set.\" , \"type\" : \"boolean\" }, \"value_schema\" : { \"title\" : \"Value Schema\" , \"description\" : \"The schema of this value.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/ValueSchema\" } ] }, \"metadata\" : { \"title\" : \"Metadata\" , \"description\" : \"Metadata relating to the actual data (size, no. of rows, etc. -- depending on data type).\" , \"type\" : \"object\" } }, \"required\" : [ \"id\" , \"is_set\" , \"value_schema\" ], \"additionalProperties\" : false } } } StepValueAddress \u00b6 Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure. References \u00b6 model class reference: StepValueAddress JSON schema file: StepValueAddress.json JSON schema \u00b6 { \"title\" : \"StepValueAddress\" , \"description\" : \"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.\" , \"type\" : \"object\" , \"properties\" : { \"step_id\" : { \"title\" : \"Step Id\" , \"description\" : \"The id of a step within a pipeline.\" , \"type\" : \"string\" }, \"value_name\" : { \"title\" : \"Value Name\" , \"description\" : \"The name of the value (output name or pipeline input name).\" , \"type\" : \"string\" }, \"sub_value\" : { \"title\" : \"Sub Value\" , \"description\" : \"A reference to a subitem of a value (e.g. column, list item)\" , \"type\" : \"object\" } }, \"required\" : [ \"step_id\" , \"value_name\" ], \"additionalProperties\" : false } StepInputRef \u00b6 An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both. References \u00b6 model class reference: StepInputRef JSON schema file: StepInputRef.json JSON schema \u00b6 { \"title\" : \"StepInputRef\" , \"description\" : \"An input to a step.\\n\\nThis object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both.\" , \"type\" : \"object\" , \"properties\" : { \"value_name\" : { \"title\" : \"Value Name\" , \"type\" : \"string\" }, \"value_schema\" : { \"$ref\" : \"#/definitions/ValueSchema\" }, \"step_id\" : { \"title\" : \"Step Id\" , \"description\" : \"The step id.\" , \"type\" : \"string\" }, \"connected_outputs\" : { \"title\" : \"Connected Outputs\" , \"description\" : \"A potential connected list of one or several module outputs.\" , \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/StepValueAddress\" } }, \"connected_pipeline_input\" : { \"title\" : \"Connected Pipeline Input\" , \"description\" : \"A potential pipeline input.\" , \"type\" : \"string\" }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"default\" : \"Whether this input is a constant and can't be changed by the user.\" , \"type\" : \"boolean\" } }, \"required\" : [ \"value_name\" , \"value_schema\" , \"step_id\" ], \"additionalProperties\" : false , \"definitions\" : { \"ValueSchema\" : { \"title\" : \"ValueSchema\" , \"description\" : \"The schema of a value.\\n\\nThe schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that\\nwill be used if no user input was given (yet) for a value.\\n\\nFor more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field.\" , \"type\" : \"object\" , \"properties\" : { \"type\" : { \"title\" : \"Type\" , \"description\" : \"The type of the value.\" , \"type\" : \"string\" }, \"type_config\" : { \"title\" : \"Type Config\" , \"description\" : \"Configuration for the type, in case it's complex.\" , \"type\" : \"object\" }, \"default\" : { \"title\" : \"Default\" , \"description\" : \"A default value.\" , \"default\" : \"__not_set__\" }, \"optional\" : { \"title\" : \"Optional\" , \"description\" : \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"description\" : \"Whether the value is a constant.\" , \"default\" : false , \"type\" : \"boolean\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"A description for the value of this input field.\" , \"default\" : \"-- n/a --\" , \"type\" : \"string\" } }, \"required\" : [ \"type\" ] }, \"StepValueAddress\" : { \"title\" : \"StepValueAddress\" , \"description\" : \"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.\" , \"type\" : \"object\" , \"properties\" : { \"step_id\" : { \"title\" : \"Step Id\" , \"description\" : \"The id of a step within a pipeline.\" , \"type\" : \"string\" }, \"value_name\" : { \"title\" : \"Value Name\" , \"description\" : \"The name of the value (output name or pipeline input name).\" , \"type\" : \"string\" }, \"sub_value\" : { \"title\" : \"Sub Value\" , \"description\" : \"A reference to a subitem of a value (e.g. column, list item)\" , \"type\" : \"object\" } }, \"required\" : [ \"step_id\" , \"value_name\" ], \"additionalProperties\" : false } } } StepOutputRef \u00b6 An output to a step. References \u00b6 model class reference: StepOutputRef JSON schema file: StepOutputRef.json JSON schema \u00b6 { \"title\" : \"StepOutputRef\" , \"description\" : \"An output to a step.\" , \"type\" : \"object\" , \"properties\" : { \"value_name\" : { \"title\" : \"Value Name\" , \"type\" : \"string\" }, \"value_schema\" : { \"$ref\" : \"#/definitions/ValueSchema\" }, \"step_id\" : { \"title\" : \"Step Id\" , \"description\" : \"The step id.\" , \"type\" : \"string\" }, \"pipeline_output\" : { \"title\" : \"Pipeline Output\" , \"description\" : \"The connected pipeline output.\" , \"type\" : \"string\" }, \"connected_inputs\" : { \"title\" : \"Connected Inputs\" , \"description\" : \"The step inputs that are connected to this step output\" , \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/StepValueAddress\" } } }, \"required\" : [ \"value_name\" , \"value_schema\" , \"step_id\" ], \"additionalProperties\" : false , \"definitions\" : { \"ValueSchema\" : { \"title\" : \"ValueSchema\" , \"description\" : \"The schema of a value.\\n\\nThe schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that\\nwill be used if no user input was given (yet) for a value.\\n\\nFor more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field.\" , \"type\" : \"object\" , \"properties\" : { \"type\" : { \"title\" : \"Type\" , \"description\" : \"The type of the value.\" , \"type\" : \"string\" }, \"type_config\" : { \"title\" : \"Type Config\" , \"description\" : \"Configuration for the type, in case it's complex.\" , \"type\" : \"object\" }, \"default\" : { \"title\" : \"Default\" , \"description\" : \"A default value.\" , \"default\" : \"__not_set__\" }, \"optional\" : { \"title\" : \"Optional\" , \"description\" : \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"description\" : \"Whether the value is a constant.\" , \"default\" : false , \"type\" : \"boolean\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"A description for the value of this input field.\" , \"default\" : \"-- n/a --\" , \"type\" : \"string\" } }, \"required\" : [ \"type\" ] }, \"StepValueAddress\" : { \"title\" : \"StepValueAddress\" , \"description\" : \"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.\" , \"type\" : \"object\" , \"properties\" : { \"step_id\" : { \"title\" : \"Step Id\" , \"description\" : \"The id of a step within a pipeline.\" , \"type\" : \"string\" }, \"value_name\" : { \"title\" : \"Value Name\" , \"description\" : \"The name of the value (output name or pipeline input name).\" , \"type\" : \"string\" }, \"sub_value\" : { \"title\" : \"Sub Value\" , \"description\" : \"A reference to a subitem of a value (e.g. column, list item)\" , \"type\" : \"object\" } }, \"required\" : [ \"step_id\" , \"value_name\" ], \"additionalProperties\" : false } } } PipelineInputRef \u00b6 An input to a pipeline. References \u00b6 model class reference: PipelineInputRef JSON schema file: PipelineInputRef.json JSON schema \u00b6 { \"title\" : \"PipelineInputRef\" , \"description\" : \"An input to a pipeline.\" , \"type\" : \"object\" , \"properties\" : { \"value_name\" : { \"title\" : \"Value Name\" , \"type\" : \"string\" }, \"value_schema\" : { \"$ref\" : \"#/definitions/ValueSchema\" }, \"connected_inputs\" : { \"title\" : \"Connected Inputs\" , \"description\" : \"The step inputs that are connected to this pipeline input\" , \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/StepValueAddress\" } }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"default\" : \"Whether this input is a constant and can't be changed by the user.\" , \"type\" : \"boolean\" } }, \"required\" : [ \"value_name\" , \"value_schema\" ], \"additionalProperties\" : false , \"definitions\" : { \"ValueSchema\" : { \"title\" : \"ValueSchema\" , \"description\" : \"The schema of a value.\\n\\nThe schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that\\nwill be used if no user input was given (yet) for a value.\\n\\nFor more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field.\" , \"type\" : \"object\" , \"properties\" : { \"type\" : { \"title\" : \"Type\" , \"description\" : \"The type of the value.\" , \"type\" : \"string\" }, \"type_config\" : { \"title\" : \"Type Config\" , \"description\" : \"Configuration for the type, in case it's complex.\" , \"type\" : \"object\" }, \"default\" : { \"title\" : \"Default\" , \"description\" : \"A default value.\" , \"default\" : \"__not_set__\" }, \"optional\" : { \"title\" : \"Optional\" , \"description\" : \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"description\" : \"Whether the value is a constant.\" , \"default\" : false , \"type\" : \"boolean\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"A description for the value of this input field.\" , \"default\" : \"-- n/a --\" , \"type\" : \"string\" } }, \"required\" : [ \"type\" ] }, \"StepValueAddress\" : { \"title\" : \"StepValueAddress\" , \"description\" : \"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.\" , \"type\" : \"object\" , \"properties\" : { \"step_id\" : { \"title\" : \"Step Id\" , \"description\" : \"The id of a step within a pipeline.\" , \"type\" : \"string\" }, \"value_name\" : { \"title\" : \"Value Name\" , \"description\" : \"The name of the value (output name or pipeline input name).\" , \"type\" : \"string\" }, \"sub_value\" : { \"title\" : \"Sub Value\" , \"description\" : \"A reference to a subitem of a value (e.g. column, list item)\" , \"type\" : \"object\" } }, \"required\" : [ \"step_id\" , \"value_name\" ], \"additionalProperties\" : false } } } PipelineOutputRef \u00b6 An output to a pipeline. References \u00b6 model class reference: PipelineOutputRef JSON schema file: PipelineOutputRef.json JSON schema \u00b6 { \"title\" : \"PipelineOutputRef\" , \"description\" : \"An output to a pipeline.\" , \"type\" : \"object\" , \"properties\" : { \"value_name\" : { \"title\" : \"Value Name\" , \"type\" : \"string\" }, \"value_schema\" : { \"$ref\" : \"#/definitions/ValueSchema\" }, \"connected_output\" : { \"title\" : \"Connected Output\" , \"description\" : \"Connected step outputs.\" , \"allOf\" : [ { \"$ref\" : \"#/definitions/StepValueAddress\" } ] } }, \"required\" : [ \"value_name\" , \"value_schema\" , \"connected_output\" ], \"additionalProperties\" : false , \"definitions\" : { \"ValueSchema\" : { \"title\" : \"ValueSchema\" , \"description\" : \"The schema of a value.\\n\\nThe schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that\\nwill be used if no user input was given (yet) for a value.\\n\\nFor more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field.\" , \"type\" : \"object\" , \"properties\" : { \"type\" : { \"title\" : \"Type\" , \"description\" : \"The type of the value.\" , \"type\" : \"string\" }, \"type_config\" : { \"title\" : \"Type Config\" , \"description\" : \"Configuration for the type, in case it's complex.\" , \"type\" : \"object\" }, \"default\" : { \"title\" : \"Default\" , \"description\" : \"A default value.\" , \"default\" : \"__not_set__\" }, \"optional\" : { \"title\" : \"Optional\" , \"description\" : \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , \"default\" : false , \"type\" : \"boolean\" }, \"is_constant\" : { \"title\" : \"Is Constant\" , \"description\" : \"Whether the value is a constant.\" , \"default\" : false , \"type\" : \"boolean\" }, \"doc\" : { \"title\" : \"Doc\" , \"description\" : \"A description for the value of this input field.\" , \"default\" : \"-- n/a --\" , \"type\" : \"string\" } }, \"required\" : [ \"type\" ] }, \"StepValueAddress\" : { \"title\" : \"StepValueAddress\" , \"description\" : \"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.\" , \"type\" : \"object\" , \"properties\" : { \"step_id\" : { \"title\" : \"Step Id\" , \"description\" : \"The id of a step within a pipeline.\" , \"type\" : \"string\" }, \"value_name\" : { \"title\" : \"Value Name\" , \"description\" : \"The name of the value (output name or pipeline input name).\" , \"type\" : \"string\" }, \"sub_value\" : { \"title\" : \"Sub Value\" , \"description\" : \"A reference to a subitem of a value (e.g. column, list item)\" , \"type\" : \"object\" } }, \"required\" : [ \"step_id\" , \"value_name\" ], \"additionalProperties\" : false } } }","title":"Values"},{"location":"development/entities/values/#values","text":"","title":"Values"},{"location":"development/entities/values/#valueschema","text":"The schema of a value. The schema contains the ValueType of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the type_config field.","title":"ValueSchema"},{"location":"development/entities/values/#value","text":"The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry.","title":"Value"},{"location":"development/entities/values/#pipelinevalueinfo","text":"Convenience wrapper to make the PipelineState json/dict export prettier.","title":"PipelineValueInfo"},{"location":"development/entities/values/#pipelinevaluesinfo","text":"Convenience wrapper to make the PipelineState json/dict export prettier. This is basically just a simplified version of the ValueSet class that is using pydantic, in order to make it easy to export to json.","title":"PipelineValuesInfo"},{"location":"development/entities/values/#stepvalueaddress","text":"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.","title":"StepValueAddress"},{"location":"development/entities/values/#stepinputref","text":"An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both.","title":"StepInputRef"},{"location":"development/entities/values/#stepoutputref","text":"An output to a step.","title":"StepOutputRef"},{"location":"development/entities/values/#pipelineinputref","text":"An input to a pipeline.","title":"PipelineInputRef"},{"location":"development/entities/values/#pipelineoutputref","text":"An output to a pipeline.","title":"PipelineOutputRef"}]}