{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"kiara \u00b6 A research data management and orchestration framework. kiara user documentation: https://dharpa.org/kiara.documentation Code: https://github.com/DHARPA-Project/kiara Development documentation for this repo: https://dharpa.org/kiara Description \u00b6 Kiara is the data orchestration engine developed by the DHARPA project. It uses a modular approach to let users re-use tried and tested data orchestration pipelines, as well as create new ones from existing building blocks. It also helps you manage your research data, and augment it with automatically-, semi-automatically-, and manually- created metadata. Most of this is not yet implemented. Development \u00b6 Requirements \u00b6 Python (version >=3.6 -- some make targets only work for Python >=3.7, but kiara itself should work on 3.6) pip, virtualenv git make direnv (optional) Prepare development environment \u00b6 git clone https://github.com/DHARPA-Project/kiara.git cd kiara python3 -m venv .venv source .venv/bin/activate make init If you use direnv , you can alternatively do: git clone https://github.com/DHARPA-Project/kiara.git cd kiara cp .envrc.disabled .envrc direnv allow make init Note : you might want to adjust the Python version in .envrc (should not be necessary in most cases though) make targets \u00b6 init : init development project (install project & dev dependencies into virtualenv, as well as pre-commit git hook) update-modules : update default kiara modules package from git flake : run flake8 tests mypy : run mypy tests test : run unit tests docs : create static documentation pages (under build/site ) serve-docs : serve documentation pages (incl. auto-reload) for getting direct feedback when working on documentation clean : clean build directories For details (and other, minor targets), check the Makefile . Running tests \u00b6 > make test # or > make coverage Copyright & license \u00b6 This project is MPL v2.0 licensed, for the license text please check the LICENSE file in this repository. Copyright (c) 2021 DHARPA project","title":"Home"},{"location":"#kiara","text":"A research data management and orchestration framework. kiara user documentation: https://dharpa.org/kiara.documentation Code: https://github.com/DHARPA-Project/kiara Development documentation for this repo: https://dharpa.org/kiara","title":"kiara"},{"location":"#description","text":"Kiara is the data orchestration engine developed by the DHARPA project. It uses a modular approach to let users re-use tried and tested data orchestration pipelines, as well as create new ones from existing building blocks. It also helps you manage your research data, and augment it with automatically-, semi-automatically-, and manually- created metadata. Most of this is not yet implemented.","title":"Description"},{"location":"#development","text":"","title":"Development"},{"location":"#requirements","text":"Python (version >=3.6 -- some make targets only work for Python >=3.7, but kiara itself should work on 3.6) pip, virtualenv git make direnv (optional)","title":"Requirements"},{"location":"#prepare-development-environment","text":"git clone https://github.com/DHARPA-Project/kiara.git cd kiara python3 -m venv .venv source .venv/bin/activate make init If you use direnv , you can alternatively do: git clone https://github.com/DHARPA-Project/kiara.git cd kiara cp .envrc.disabled .envrc direnv allow make init Note : you might want to adjust the Python version in .envrc (should not be necessary in most cases though)","title":"Prepare development environment"},{"location":"#make-targets","text":"init : init development project (install project & dev dependencies into virtualenv, as well as pre-commit git hook) update-modules : update default kiara modules package from git flake : run flake8 tests mypy : run mypy tests test : run unit tests docs : create static documentation pages (under build/site ) serve-docs : serve documentation pages (incl. auto-reload) for getting direct feedback when working on documentation clean : clean build directories For details (and other, minor targets), check the Makefile .","title":"make targets"},{"location":"#running-tests","text":"> make test # or > make coverage","title":"Running tests"},{"location":"#copyright-license","text":"This project is MPL v2.0 licensed, for the license text please check the LICENSE file in this repository. Copyright (c) 2021 DHARPA project","title":"Copyright &amp; license"},{"location":"SUMMARY/","text":"Home Install Architecture Value types Modules Operation types API docs","title":"SUMMARY"},{"location":"install/","text":"Installation \u00b6 Python package \u00b6 The python package is currently not available on pypi , so for now you have to install the package directly from the git repo. If you chooose this install method, I assume you know how to install Python packages manually, which is why I only show you an example way of getting kiara onto your machine: > python3 -m venv ~/.venvs/kiara > source ~/.venvs/kiara/bin/activate > pip install git+https://github.com/DHARPA-Project/kiara.git ... ... ... Successfully installed ... ... ... > kiara --help Usage: kiara [OPTIONS] COMMAND [ARGS]... ... ... In addition to the kiara package, you'll need a package containing modules, most likely kiara_modules.default : > pip install git+https://github.com/DHARPA-Project/kiara_modules.default.git kiara test_custom","title":"Install"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#python-package","text":"The python package is currently not available on pypi , so for now you have to install the package directly from the git repo. If you chooose this install method, I assume you know how to install Python packages manually, which is why I only show you an example way of getting kiara onto your machine: > python3 -m venv ~/.venvs/kiara > source ~/.venvs/kiara/bin/activate > pip install git+https://github.com/DHARPA-Project/kiara.git ... ... ... Successfully installed ... ... ... > kiara --help Usage: kiara [OPTIONS] COMMAND [ARGS]... ... ... In addition to the kiara package, you'll need a package containing modules, most likely kiara_modules.default : > pip install git+https://github.com/DHARPA-Project/kiara_modules.default.git kiara test_custom","title":"Python package"},{"location":"architecture/","text":"Architecture documents \u00b6 This section contains architecture-related documents for the Kiara project. Not all of those might be up-to-date, but they should help to understand certain design decisions, and why they were made.","title":"Architecture"},{"location":"architecture/#architecture-documents","text":"This section contains architecture-related documents for the Kiara project. Not all of those might be up-to-date, but they should help to understand certain design decisions, and why they were made.","title":"Architecture documents"},{"location":"architecture/assumptions/","text":"Assumptions & considerations \u00b6 Core assumptions \u00b6 I consider the following assumptions a given. They are not fuelled by user stories, but are the 'minimal' requirements that emerged after initially presenting the 'open questions', and in other discussions with Sean and the team. If any of those assumptions are wrong, some of the conclusions below will have to be adjusted. our (only) target audience (for now) are digital historians (and maybe also other digital humanity researchers) who can't code themselves the most important outcome of our project is for our target audience to be able to execute workflows in order to explore, explain, transform or augment their data we want the creation of workflows to be as easy and frictionless as possible, although not at the expense of end-user usability we want our product to be used by all DH researchers around the word, independent of their affiliation(s) q- collaboration/sharing of data is not a priority, most of our target audience are either individuals, sometimes small teams (sharing of results and sharing of workflows are different issues, and not included in this assumption) Considerations around adoption \u00b6 One way to look at how to prioritize and implement some of our user stories is through the lens of ease-of-adoption: which characteristics make our application more likely to be adopted, by a larger group of researchers? Those ones are obvious (at least to me) -- in no particular order: ease of workflow use ease of file-management use ease of installation (if there is one involved) whether there is a login/account creation requirement how well it integrates and plays with tools researchers already use day to day provides relevant (to them) workflows the cheaper to use the better (free/monthly cost/pay-per-usage) stability / reliability performance (most importantly on the compute side, but also UI) how easy it is to create workflows, and what skills are necessary to do that (easier creation -> more workflows) whether and how easy it will be to share, re-use and adapt workflows (different to sharing data)","title":"Assumptions & considerations"},{"location":"architecture/assumptions/#assumptions-considerations","text":"","title":"Assumptions &amp; considerations"},{"location":"architecture/assumptions/#core-assumptions","text":"I consider the following assumptions a given. They are not fuelled by user stories, but are the 'minimal' requirements that emerged after initially presenting the 'open questions', and in other discussions with Sean and the team. If any of those assumptions are wrong, some of the conclusions below will have to be adjusted. our (only) target audience (for now) are digital historians (and maybe also other digital humanity researchers) who can't code themselves the most important outcome of our project is for our target audience to be able to execute workflows in order to explore, explain, transform or augment their data we want the creation of workflows to be as easy and frictionless as possible, although not at the expense of end-user usability we want our product to be used by all DH researchers around the word, independent of their affiliation(s) q- collaboration/sharing of data is not a priority, most of our target audience are either individuals, sometimes small teams (sharing of results and sharing of workflows are different issues, and not included in this assumption)","title":"Core assumptions"},{"location":"architecture/assumptions/#considerations-around-adoption","text":"One way to look at how to prioritize and implement some of our user stories is through the lens of ease-of-adoption: which characteristics make our application more likely to be adopted, by a larger group of researchers? Those ones are obvious (at least to me) -- in no particular order: ease of workflow use ease of file-management use ease of installation (if there is one involved) whether there is a login/account creation requirement how well it integrates and plays with tools researchers already use day to day provides relevant (to them) workflows the cheaper to use the better (free/monthly cost/pay-per-usage) stability / reliability performance (most importantly on the compute side, but also UI) how easy it is to create workflows, and what skills are necessary to do that (easier creation -> more workflows) whether and how easy it will be to share, re-use and adapt workflows (different to sharing data)","title":"Considerations around adoption"},{"location":"architecture/decisions/","text":"Decisions \u00b6 This page lists a few of the main decisions that were taken, what the considerations around them were, their impact, as well as why they were made. Supporting two sorts of modules: 'core', and 'pipeline' modules \u00b6 When starting to write code, we didn't have yet many examples of modules and how specific/broad they would be in their utility. I think we should have done a better job gathering those and coming up with a set of modules that would be sufficient for our first 10 or so workflows before writing any code, but alas, we didn't. One way I saw to lower the risk of us implementing us ourselves into a corner was to make our modules as flexible, re-usable and 'combinable' as possible. And the best way I could think of to do that was to have a very simple interface for each module (each module has a defined set of input/output fields, and one main function to transform inputs into outputs), and to allow several modules to be combined into a 'new' module that has those same characteristics/interface. Advantages: - easy to declaratively create re-usable modules with just json/yaml - easy to re-use modules/other pipelines for UI-specific subtasks (like data previews/querying) - in most cases, the higher-level backend code does not know about core- and pipeline- modules, since they can be treated the same Disadvantages: - the lower-level backend code needs to implement two different ways to assemble/create modules, depending on whether it's a core-module, or a pipeline Use of subclassing in general \u00b6 Across kiara , I'm using subclassing and inheritance in some instances, esp. important base classes are KiaraModule and PipelineController . I'm aware that this is considered bad practice in a lot of cases, and I have read my share of opinions and thoughts about the matter. In principle I agree, and I'm not 100% happy with every decision I made (or thought I had to made) in this area for kiara , but overall I decided to allow for some inheritance and class-based code sharing in the code, partly to speed up my implementation work, partly because I thought some of the disadvantages (like having to search base classes for some function definitions) are not as bad in a certain context than in others. I can totally see how others would disagree here, though, and there are a few things I would like to change/improve later on, if I find the time. One of the main advantages I get out of using inheritance is being able to automatically discover subclasses of a base class. This is done for multiple of those, like: KiaraModule ValueType MetadataModel Using auto-discovery in a Python virtualenv removes the need for workflow/module developers to understand Python packaging and entry_points. I've written a project template that sets up all the basics, and developers focus on creating new classes (basically plugins), with no extra registration work to be done. I hope this will aid adoption. And that I've managed to design those base classes well enough so that they are easy to use and understand, so that some of the main drawbacks of subclassing won't matter all that much. Requiring to subclass an abstract base class when creating a module \u00b6 The main class that uses a subclassing-strategy is KiaraModule . At it's heart, it's basically just a wrapper around a pure function, with some utility methods describing it's input and output. One reason I decided to not just create a decorator that wraps any function was the need to be able to describe the input the function takes, and the output it produces in a stricter way than would have been possible with just type hints. Another reason is that this way it is possible to add configuration to a module object, which should make module code much more flexible and re-usable, and developers do not have to implement separate modules for just slightly different use-cases. This design decision does not prevent to allow for more 'loose' implementations of a module, like the above mentioned function with a decorator. Those would be dynamically converted into a KiaraModule subclass/object, with potential downsides of not being able to version control it properly (or as easliy). The point is, though, that the default way of doing things will give us the best guarantees (and metadata). Advantages: - relatively easy to manage 'plugin-like' architecture, discovery of modules - being able to describe module input/output fields in detail - module versioning: by requiring the subclassing of a base class, and also having to add modules as entry_points, it will be possible describe exactly which version of the module was used in a workflow (as well as which version of the base class) Disadvantages: - more abstraction layers than strictly necessary - other, usual disadvantages associated with subclassing/inheritance Separating data from the Python objects that describe them / Data registry \u00b6 TBD Advantages: - efficiency, option to save on memory and IO - (hopefully) decrease of complexity for non trivial scenarios like multi-process or remote job execution Disadvantages: - extra level of abstraction - increase in complexity (at least for simple use-cases)","title":"Decisions"},{"location":"architecture/decisions/#decisions","text":"This page lists a few of the main decisions that were taken, what the considerations around them were, their impact, as well as why they were made.","title":"Decisions"},{"location":"architecture/decisions/#supporting-two-sorts-of-modules-core-and-pipeline-modules","text":"When starting to write code, we didn't have yet many examples of modules and how specific/broad they would be in their utility. I think we should have done a better job gathering those and coming up with a set of modules that would be sufficient for our first 10 or so workflows before writing any code, but alas, we didn't. One way I saw to lower the risk of us implementing us ourselves into a corner was to make our modules as flexible, re-usable and 'combinable' as possible. And the best way I could think of to do that was to have a very simple interface for each module (each module has a defined set of input/output fields, and one main function to transform inputs into outputs), and to allow several modules to be combined into a 'new' module that has those same characteristics/interface. Advantages: - easy to declaratively create re-usable modules with just json/yaml - easy to re-use modules/other pipelines for UI-specific subtasks (like data previews/querying) - in most cases, the higher-level backend code does not know about core- and pipeline- modules, since they can be treated the same Disadvantages: - the lower-level backend code needs to implement two different ways to assemble/create modules, depending on whether it's a core-module, or a pipeline","title":"Supporting two sorts of modules: 'core', and 'pipeline' modules"},{"location":"architecture/decisions/#use-of-subclassing-in-general","text":"Across kiara , I'm using subclassing and inheritance in some instances, esp. important base classes are KiaraModule and PipelineController . I'm aware that this is considered bad practice in a lot of cases, and I have read my share of opinions and thoughts about the matter. In principle I agree, and I'm not 100% happy with every decision I made (or thought I had to made) in this area for kiara , but overall I decided to allow for some inheritance and class-based code sharing in the code, partly to speed up my implementation work, partly because I thought some of the disadvantages (like having to search base classes for some function definitions) are not as bad in a certain context than in others. I can totally see how others would disagree here, though, and there are a few things I would like to change/improve later on, if I find the time. One of the main advantages I get out of using inheritance is being able to automatically discover subclasses of a base class. This is done for multiple of those, like: KiaraModule ValueType MetadataModel Using auto-discovery in a Python virtualenv removes the need for workflow/module developers to understand Python packaging and entry_points. I've written a project template that sets up all the basics, and developers focus on creating new classes (basically plugins), with no extra registration work to be done. I hope this will aid adoption. And that I've managed to design those base classes well enough so that they are easy to use and understand, so that some of the main drawbacks of subclassing won't matter all that much.","title":"Use of subclassing in general"},{"location":"architecture/decisions/#requiring-to-subclass-an-abstract-base-class-when-creating-a-module","text":"The main class that uses a subclassing-strategy is KiaraModule . At it's heart, it's basically just a wrapper around a pure function, with some utility methods describing it's input and output. One reason I decided to not just create a decorator that wraps any function was the need to be able to describe the input the function takes, and the output it produces in a stricter way than would have been possible with just type hints. Another reason is that this way it is possible to add configuration to a module object, which should make module code much more flexible and re-usable, and developers do not have to implement separate modules for just slightly different use-cases. This design decision does not prevent to allow for more 'loose' implementations of a module, like the above mentioned function with a decorator. Those would be dynamically converted into a KiaraModule subclass/object, with potential downsides of not being able to version control it properly (or as easliy). The point is, though, that the default way of doing things will give us the best guarantees (and metadata). Advantages: - relatively easy to manage 'plugin-like' architecture, discovery of modules - being able to describe module input/output fields in detail - module versioning: by requiring the subclassing of a base class, and also having to add modules as entry_points, it will be possible describe exactly which version of the module was used in a workflow (as well as which version of the base class) Disadvantages: - more abstraction layers than strictly necessary - other, usual disadvantages associated with subclassing/inheritance","title":"Requiring to subclass an abstract base class when creating a module"},{"location":"architecture/decisions/#separating-data-from-the-python-objects-that-describe-them-data-registry","text":"TBD Advantages: - efficiency, option to save on memory and IO - (hopefully) decrease of complexity for non trivial scenarios like multi-process or remote job execution Disadvantages: - extra level of abstraction - increase in complexity (at least for simple use-cases)","title":"Separating data from the Python objects that describe them / Data registry"},{"location":"architecture/metadata/","text":"Metadata \u00b6 Metadata is more important in research than in other fields. Metadata can be used to, among other things, track provenance of data, describe authorship, time of creation, location of creation, describing the 'shape' of data (schemas, etc.). In some cases it's not easy to determine what's data and what's metadata. Sometimes metadata becomes data (\"One persons metadata...\"). Handling metadata is difficult, and it regularly gets lost somewhere in the process. Creating metadata in the first place can be very time-consuming, I would wager that is more true in the digital humanities than in the harder sciences. With the growing popularity of the open data movement, people are getting more aware of the importance of metadata, and there is a growing infrastructure and services around all of this (DOIs, RDF, 'linked data', Dublin core, ...). None of it is easy or intuitive to use, but I guess that's just the nature of the beast. I think it is safe to say that whatever we come up with has to be able to create and handle metadata in some way or form, and personally, I think we should 'bake' metadata handling in from the beginning. Looking at the user-stories it's quite clear that this an important topic. How exactly that will look, I think there is some leeway, but all architecture proposals should at least include some indication on how this would be handled. Schema information \u00b6 One important piece of metadata is often schema information: what exactly is the shape of the data, how can I read it? In some cases this can be inferred from the data easily, sometimes it's even obvious. But often that is not the case at all, which makes things like creating generic data exploration tools very hard, if not impossible. We would have, if we choose to create and attach it, all that information available, always, which would mean it would be easy to create generic, peripheral tools like a generic data explorer. It will, of course, also make it easier to re-use such data in other workflows, because users would not have to explicitly specify what their data is; we could infer that from the attached schema. Workflow metadata \u00b6 One thing that is specific to our application is that we have full control over every part of the data-flow. So, we can attach metadata of all inputs and previous steps to each result (or intermediate result) along the way. Which is quite an unique opportunity; this is often not available at all, or has to be done manually by the researcher. There is a lot that can be done with such annotated (result-)data. For example, each data set can include pointers to all the original data that was involved in creating it (or it could even include that data itself), as well as a description of all the transformation steps it went through. This means that one could potentially create a full visual representation of what happened to the data since it was created, just by looking at the attached metadata. This is usually impossible, because there is never a sort of 'unbroken cold-chain' of metadata available. Of course, this would also help with reproducability and related issues. This possibility is something I'm particularly excited about, even though it does not directly appear in any of our user stories (so would not be a core requirement). But it's one of the things I would have liked to have available often in the past.","title":"Metadata"},{"location":"architecture/metadata/#metadata","text":"Metadata is more important in research than in other fields. Metadata can be used to, among other things, track provenance of data, describe authorship, time of creation, location of creation, describing the 'shape' of data (schemas, etc.). In some cases it's not easy to determine what's data and what's metadata. Sometimes metadata becomes data (\"One persons metadata...\"). Handling metadata is difficult, and it regularly gets lost somewhere in the process. Creating metadata in the first place can be very time-consuming, I would wager that is more true in the digital humanities than in the harder sciences. With the growing popularity of the open data movement, people are getting more aware of the importance of metadata, and there is a growing infrastructure and services around all of this (DOIs, RDF, 'linked data', Dublin core, ...). None of it is easy or intuitive to use, but I guess that's just the nature of the beast. I think it is safe to say that whatever we come up with has to be able to create and handle metadata in some way or form, and personally, I think we should 'bake' metadata handling in from the beginning. Looking at the user-stories it's quite clear that this an important topic. How exactly that will look, I think there is some leeway, but all architecture proposals should at least include some indication on how this would be handled.","title":"Metadata"},{"location":"architecture/metadata/#schema-information","text":"One important piece of metadata is often schema information: what exactly is the shape of the data, how can I read it? In some cases this can be inferred from the data easily, sometimes it's even obvious. But often that is not the case at all, which makes things like creating generic data exploration tools very hard, if not impossible. We would have, if we choose to create and attach it, all that information available, always, which would mean it would be easy to create generic, peripheral tools like a generic data explorer. It will, of course, also make it easier to re-use such data in other workflows, because users would not have to explicitly specify what their data is; we could infer that from the attached schema.","title":"Schema information"},{"location":"architecture/metadata/#workflow-metadata","text":"One thing that is specific to our application is that we have full control over every part of the data-flow. So, we can attach metadata of all inputs and previous steps to each result (or intermediate result) along the way. Which is quite an unique opportunity; this is often not available at all, or has to be done manually by the researcher. There is a lot that can be done with such annotated (result-)data. For example, each data set can include pointers to all the original data that was involved in creating it (or it could even include that data itself), as well as a description of all the transformation steps it went through. This means that one could potentially create a full visual representation of what happened to the data since it was created, just by looking at the attached metadata. This is usually impossible, because there is never a sort of 'unbroken cold-chain' of metadata available. Of course, this would also help with reproducability and related issues. This possibility is something I'm particularly excited about, even though it does not directly appear in any of our user stories (so would not be a core requirement). But it's one of the things I would have liked to have available often in the past.","title":"Workflow metadata"},{"location":"architecture/data/","text":"From looking at the user stories, and after listening to the interviews Lorella conducted and also considering my own personal experience in eResearch, I think its save to say that the central topic we are dealing with is data. Without data, none of the other topics (workflows, visualisation, metadata...) would even exist. Because of its central nature I want to lay out the different forms it comes in, and which characteristics of it are important in our context. What's data? \u00b6 Data is created from sources. Sources come in different forms (analog, digital) and can be anything from handwritten documents in an archive to a twitter feed. Photos, cave-paintings, what have you. I'm not websters dictionary, but I think one usable working definition of data could be a 'materialized source', in our context 'materialized source in digital form'. From here on out I'll assume we are talking about 'digital' data when I mention data. One thing I'll leave out in this discussion is what is usually called 'dirty data' in data engineering, although it is an important topic. Most of the issues there map fairly well to the structured/unstructured thing below. There are a few differences, but in the interest of clarity let's ignore those for now... Structured data / Data transformations \u00b6 Important for us is that data can come in two different formats: unstructured, and, who'd have guessed... structured. The same piece of data can be theoretically expressed in structured as well as unstructured form: the meaning to a researcher would be 100% the same, but the ways to handle, digest and operate with the data can differ, and in most scenarios adding structure opens up possibilities to work with the data that weren't there before. In my head I call those two forms 'useless', and 'useful' data, but researcher usually get a bit agitated when I do, so I have learned to not do that in public anymore. For researchers, the most (and arguably only) important feature of 'structure' is that it enables them to do more with the data they already possess. By means of computation. I think it's fair to say that only structured data can be used in a meaningful way in a computational context. With the exception that unstructured data is useful input to create structured data. One more thing to mention is that the line between structured and un-structured is sometimes hard to draw, and can depend entirely on context. \"One persons structured data is another persons unstructured data.\", something like that. In addition, in some instances unstructured data can be converted to structured data trivially, meaning without much effort or any user-interaction. I'd argue we can consider those sorts of datasets basically 'structured'. Example \u00b6 Lets use a simple example to illustrate all that: a digital image of a document . Depending on what you are interested in, such an image might already be structured data. For example it could contain geo-tags, and a timestamp, which are both digitally readable. If you want to visualize on a map where a document is from, you can do that instantly. Structured data, yay! Similarly, if you are interested in the color of the paper of the document (ok, I'm stretching my argument here as this seems fairly unlikely, but this is really just to illustrate...), you might get the color histogram of the image (which is trivial to extract, but needs some batch-computation), and for your purposes you would also consider the image file structured data. Now, if you are interested in the text content of the document, things get more interesting. You will have to jump through some hoops, and feed the image file to an OCR pipeline that will spit out a text file for example. The data itself would still be the same, but now computers can access not only some probably irrelevant metadata, but also the text content, which, in almost all cases, is where the 'soul' of the data is. It could be argued that 'just' a text file is not actually structured. I'd say that groups of ascii-characters that can be found in english-language dictionaries, separated by whitespaces and new-lines can be considered a structure, even if only barely. The new format certainly allows the researcher to interact with the data in other ways (e.g. full-text search). We can go further, and might be interested in characteristics of the text content (language, topics, etc.). This is where the actual magic happens, everything before that is just rote data preparation: turning unstructured (or 'other-ly' structured) data into (meaningful) structured data... On a technical level, those two parts (preparation/computation) of a research workflow might look (or be) the same, but I think there is a difference worth keeping in mind. If I don't forget I'll elaborate on that later. 'Big-ish' data \u00b6 I'm not talking about real 'Big data'-big data here, just largish files, or lots of them, or both. I don't think we'll encounter many use-cases where we have to move or analyze terabytes of data, but I wouldn't be surprised if we come across a few gigabytes worth of it every now and then. There are a few things we have to be prepared for, in those cases: transferring that sort of data is not trivial (esp. from home internet connections with limited upload bandwidth) -- and we will most likely have to be able to offer some sort of resumable-upload (and download) option (in case of a hosted solution) if we offer a hosted service, we will have to take into account and plan for this, so we don't run out of storage space (we might have to impose quotas, for example) computation-wise, we need to make sure we are prepared for large datasets and handle that in a smart way (if we load a huge dataset into memory, it can crash the machine where that is done) similarly, when we feed large datasets into a pipeline, we might not be able to just duplicate and edit the dataset like we could do for small amounts of data (too expensive, storage-wise) -- so we might need to have different strategies in place on how to execute a workflow, depending on file sizes (for example some sort of copy-on-write)","title":"Data"},{"location":"architecture/data/#whats-data","text":"Data is created from sources. Sources come in different forms (analog, digital) and can be anything from handwritten documents in an archive to a twitter feed. Photos, cave-paintings, what have you. I'm not websters dictionary, but I think one usable working definition of data could be a 'materialized source', in our context 'materialized source in digital form'. From here on out I'll assume we are talking about 'digital' data when I mention data. One thing I'll leave out in this discussion is what is usually called 'dirty data' in data engineering, although it is an important topic. Most of the issues there map fairly well to the structured/unstructured thing below. There are a few differences, but in the interest of clarity let's ignore those for now...","title":"What's data?"},{"location":"architecture/data/#structured-data-data-transformations","text":"Important for us is that data can come in two different formats: unstructured, and, who'd have guessed... structured. The same piece of data can be theoretically expressed in structured as well as unstructured form: the meaning to a researcher would be 100% the same, but the ways to handle, digest and operate with the data can differ, and in most scenarios adding structure opens up possibilities to work with the data that weren't there before. In my head I call those two forms 'useless', and 'useful' data, but researcher usually get a bit agitated when I do, so I have learned to not do that in public anymore. For researchers, the most (and arguably only) important feature of 'structure' is that it enables them to do more with the data they already possess. By means of computation. I think it's fair to say that only structured data can be used in a meaningful way in a computational context. With the exception that unstructured data is useful input to create structured data. One more thing to mention is that the line between structured and un-structured is sometimes hard to draw, and can depend entirely on context. \"One persons structured data is another persons unstructured data.\", something like that. In addition, in some instances unstructured data can be converted to structured data trivially, meaning without much effort or any user-interaction. I'd argue we can consider those sorts of datasets basically 'structured'.","title":"Structured data / Data transformations"},{"location":"architecture/data/#example","text":"Lets use a simple example to illustrate all that: a digital image of a document . Depending on what you are interested in, such an image might already be structured data. For example it could contain geo-tags, and a timestamp, which are both digitally readable. If you want to visualize on a map where a document is from, you can do that instantly. Structured data, yay! Similarly, if you are interested in the color of the paper of the document (ok, I'm stretching my argument here as this seems fairly unlikely, but this is really just to illustrate...), you might get the color histogram of the image (which is trivial to extract, but needs some batch-computation), and for your purposes you would also consider the image file structured data. Now, if you are interested in the text content of the document, things get more interesting. You will have to jump through some hoops, and feed the image file to an OCR pipeline that will spit out a text file for example. The data itself would still be the same, but now computers can access not only some probably irrelevant metadata, but also the text content, which, in almost all cases, is where the 'soul' of the data is. It could be argued that 'just' a text file is not actually structured. I'd say that groups of ascii-characters that can be found in english-language dictionaries, separated by whitespaces and new-lines can be considered a structure, even if only barely. The new format certainly allows the researcher to interact with the data in other ways (e.g. full-text search). We can go further, and might be interested in characteristics of the text content (language, topics, etc.). This is where the actual magic happens, everything before that is just rote data preparation: turning unstructured (or 'other-ly' structured) data into (meaningful) structured data... On a technical level, those two parts (preparation/computation) of a research workflow might look (or be) the same, but I think there is a difference worth keeping in mind. If I don't forget I'll elaborate on that later.","title":"Example"},{"location":"architecture/data/#big-ish-data","text":"I'm not talking about real 'Big data'-big data here, just largish files, or lots of them, or both. I don't think we'll encounter many use-cases where we have to move or analyze terabytes of data, but I wouldn't be surprised if we come across a few gigabytes worth of it every now and then. There are a few things we have to be prepared for, in those cases: transferring that sort of data is not trivial (esp. from home internet connections with limited upload bandwidth) -- and we will most likely have to be able to offer some sort of resumable-upload (and download) option (in case of a hosted solution) if we offer a hosted service, we will have to take into account and plan for this, so we don't run out of storage space (we might have to impose quotas, for example) computation-wise, we need to make sure we are prepared for large datasets and handle that in a smart way (if we load a huge dataset into memory, it can crash the machine where that is done) similarly, when we feed large datasets into a pipeline, we might not be able to just duplicate and edit the dataset like we could do for small amounts of data (too expensive, storage-wise) -- so we might need to have different strategies in place on how to execute a workflow, depending on file sizes (for example some sort of copy-on-write)","title":"'Big-ish' data"},{"location":"architecture/data/data_centric_approach/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); A (parallel?) data centric approach for kiara/lumy \u00b6 - decision between Workflow creation and Workflow execution \u00b6","title":"Data centric approach"},{"location":"architecture/data/data_centric_approach/#a-parallel-data-centric-approach-for-kiaralumy","text":"","title":"A (parallel?) data centric approach for kiara/lumy"},{"location":"architecture/data/data_centric_approach/#-decision-between-workflow-creation-and-workflow-execution","text":"","title":"- decision between Workflow creation and Workflow execution"},{"location":"architecture/data/data_formats/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); from dharpa.benchmarking.data import clear_system_cache , MemoryRecorder , get_example_file from rich.jupyter import print This document is a primer on data formats and structures, and how and why those affect our project. I have no idea about how much of this is common knowledge, and how much is news to the majority. I have noticed a few common misconceptions and assumptions about some of the topics in here, so I figured it makes sense to try to get everyone on the same page. I've tried to keep this simple and short, so there are some things in here that are over-simplified bordering on incorrect. My educated guess is that in our project we will mostly be concerned about structured, tabular data, which is why I'll be focussing on that. I might add a companion document about 'binary-blob' data later on. Data serialization and storage \u00b6 data lives in memory or on disk lots of 0's and 1's -- binary format only 'decoding' gets you a useful representation 'text' is just an often used encoding format The natural habitat of (digital) data is computer memory or on disk. Data is always stored in binary form, and there is always some sort of decoding involved to make data usable in one way or another (with the exception of booleans maybe). Even when we talk about text files (seemingly implying that those are not binary since they are 'text'), we are dealing with binary data. It's just such a standard data encoding format that tools to decode that sort of data are available everywhere. Decoding text is by no means trivial, but luckily our tools have evolved so much by now -- and we have standards like utf-8 commonly available -- that we as users hardly ever come across decoding issues anymore. At least not to the extent we used to. It still happens, and I would imagine quite a bit more in the Digital Humanities than in your everyday 'business' use-case. So it helps to be aware of at least the basics involved in text encoding standards, and I would recommend anyone doing any sort of programming to read up on it. Tabular data (serialization) formats \u00b6 serialization/deserialization binary or not, here I come: avro, protobuf, pickle csv, json, yaml, xml 'structured formats' with schema, or without: avro, protobuf, thrift, flatbuffers, xml csv, json, yaml, messagepack, pickle zero-copy, memory-mapping? The (arguably) most important type of data we'll be dealing with is structured, tabular data. So it pays to think about how its digital form is represented in memory, and what issues are involved when trying to store, manipulate and transfer it. In our context, tabular data is always a 2 dimensional matrix (rows, columns), where each column has the same number of items, and each column contains items of the same data type (or union of data types). Tabular data can have a column that can be used as index, but that is optional. Each table can be described with a schema, which is basically a list of column names, and the data types associated with each of those columns. Serialization / deserialization \u00b6 Every programming language represents its basic data types differently in memory. That's especially true for the arguably most common data type: the string. Also, that in-memory representation is (almost) always different to the format of the (functially) same data when exported into a file on disk. This means that, if we want to export our data in our code, we need to do a step that is called 'serializing' (or 'marshalling'): we convert the data into a commonly accepted representation of a commonly accepted set of data-types. This serialization is usually expensive, computationally speaking. We want to avoid it, if at all possible, or at least always postpone it until the last possible moment, when we are sure the data won't change anymore, so we only have to do it once. The same goes for de-serializing data, just in the other direction: we only want to do it once, then keep it in memory in our native representation (if the size of the data allows it), so we don't have to read it again. Even if the content of a file is in the OS (page) cache (which would mean we don't actually have to read the file-content from disk) we'd still have to spend the cpu-cycles for de-serialization. So, big no-no, bad data-scientist! Format types \u00b6 For serialization, we have two basic options: text, and binary (let's just not nitpick and assume that this distinction makes sense). Text-based formats \u00b6 Serializing into a text-based format usually means taking all the elements our tabular data consists of, one by one, then serialize each element into its textual representation (like for example \"hello world\" for a string, 5 for an integer, true for a boolean in json), and then assembling one big 'meta'-string out of all those sub-elements. For csv, that might include a header-row, and adding delimiters like ',' in between the elements. For json it would be adding list (' [ ', ' ] ') or dictionary (' { ', ' } ') indicators, as well as deliminters and other elements as outlined in the JSON specification. I haven't done any research on it, but I'd imagine csv would be one of the oldest widely-used data storage formats. Csv is a text based tabular format, and it allows you to specify an optional header to describe column names. It allows for different deliminters between row cells (whole rows are delimited by the end-of-line special character). Other commonly used text-based formats are json, yaml, toml, xml. Those are not strictly tabular data formats, they can also contain just scalars, dictionaries, or lists (tabular data is always a list of dictionaries of the same shape). Binary formats \u00b6 Serializing into a binary format is usually very specific to the format itself, so there are not really any common tools to read more than one of them (like there are for text-based formats, where you could use any text editor and at least display the content in a meaningful way), and different formats have different priorites (like small size of the resulting blob, quick random read/write access, suitability for streaming, etc). Binary formats often have compression built-in, whereas text formats never have (but can be usually compressed well enough by external tools due to certain characteristics of encoded strings). Also, they usually are a lot easier on the cpu for serialization/deserialization purposes, since it's easier to optimize them for that scenario. Binary formats existed for a long time, but in recent years they are used more widely again. Decentralized software architecture (microservices) as well as 'big(-ish) data' played a huge part in that. Because, as it turns out that, while serializing a few items of data per seconds into json and back is not that much of a problem, doing the same thing for millions of large (or even small) chunks of data actually is. In some cases that serialization step can take more time than the actual computation that was done on the data. To counter that issue, people came up with formats like 'Avro', 'Thrift', 'ProtoBuf'. Pythons 'pickle' can also be considerd a binary serialization format. Schema \u00b6 Another useful way to separate data formats is to check whether they include a (native) schema that describes the value types of the data they hold, or not. If schema information is present, it can either be included in the resulting file, or be stored elsewhere. Having schema information for a dataset is highly desirable, because it tells us exactly what type of data we are dealing with (is it a string, integer, float? what precision?). Whereas most text-based data formats don't include a schema definition format, there are sometimes external efforts to remedy that (JSON-schema, for example). None of the text-based formats I can think of at the top of my head include the schema in a resulting file. This is important, because the complexity of tools that handle data increases if they need to worry about secondary, 'side-car' files for incoming data. Slight detour: csv \u00b6 Csv is bit special in that it can contain a 'header' row in the first line, which can be used to determine column names. Since this row is optional, it is not always present which of course complicates the import process. Because csv files are so common in data science, most of the tools we use include some csv-import method that more or less smartly determines the (text-)encoding of the file, whether it has a header row, as well as the schema of the included data. This method usually serializes the data into the appropriate internal representations of the column types after it is reasonably sure the inferred schema is correct-ish. Without a definite, (externally provided) schema it is not possible to guess this reliably in every case, so a developer should always assert the correct types are present after such an import. Streaming, zero-copy, memory-mapping \u00b6 One thing that everyone working semi-seriously in data science and handling big-ish data should be aware of is that in most OS'es you can read (or 'load') data in more ways than one. The 'one' way is usually something like: file = open ( 'dataset.csv' ) lines = file . read () # or file.readlines() When using Pandas, it'll probably take the form of: import pandas as pd pd . read_csv ( 'dataset.csv' ) Both of those read the whole file into memory. Which will be fine if the dataset is small or there is a need for it to be in memory in full. Depending on the situation, it might be wasteful, though. For example when calculating the mean for a column of integers. In that case it's a better strategy to read one line of the file, process the column we are interested in, eject the line from memory, then read the next line, only keeping the current total and the number of items we processed so far. That way we'll never allocate more memory than what is needed for a single line. We can even process datasets that are larger than the available memory of or our workstation. As a matter of fact, we could do even better if we would know the offset and length of the column we are interested in, in that case, we would only have to read the bytes that hold the integer value we need, and could ignore the other cells of a row completely. Again, this might or might be an issue depending on the size of the data in a row, but if we have a dataset with a lot of large columns, the I/O operations we would not have to do by only reading the exact data we need could improve the speed of processing considerably. Doing that doesn't really work for csv files, for example. Since there is no good way for us to know the exact offset of length of the column we are interested in. There are data formats that support that kind of operation though. Along with those fairly simple strategies to deal with data efficiently, there are more advanced ones that also deal with data and how it is handled in a system memory as well as on disk. For those of you who are interested, I would recommend looking up the terms 'memory-mapping', and 'zero-copy'. Some random benchmarks, to illustrate \u00b6 clear_system_cache () file_path = get_example_file () def count_lines ( path ): f = open ( path ) counter = 0 length = 0 # ignores '\\n' characters lines = f . readlines () for line in lines : counter = counter + 1 length = length + len ( line ) return { \"no_lines\" : counter , \"size\" : length } profile_count_lines = MemoryRecorder . profile_func ( \"Reading the whole file\" , \"This iterates through all lines in memory, keeping all of them in memory at the same time.\" , False , count_lines , file_path ) print ( profile_count_lines . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading the whole file \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This iterates through all lines in memory, keeping all of them in memory at the same \u2502 \u2502 time. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 count_lines \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 53.75 MB \u2502 \u2502 execution time \u2502 144 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def count_lines (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 f = open (path) \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 length = 0 # ignores '\\n' characters \u2502 \u2502 \u2502 \u2502 lines = f . readlines() \u2502 \u2502 \u2502 \u2502 for line in lines: \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 length = length + len (line) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return { \"no_lines\" : counter, \"size\" : length} \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f clear_system_cache () file_obj = open ( file_path , buffering = True ) def count_lines ( f ): counter = 0 length = 0 # ignores '\\n' characters for line in f : # when using open like we do here, it returns an iterator not a materialized list counter = counter + 1 length = length + len ( line ) return { \"no_lines\" : counter , \"size\" : length } profile_count_lines = MemoryRecorder . profile_func ( \"Reading the file line by line\" , \"This allocates only very little memory, since once a line is read and processed, it will be disregarded.\" , False , count_lines , file_obj ) file_obj . close () print ( profile_count_lines . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading the file line by line \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This allocates only very little memory, since once a line is read and processed, it will \u2502 \u2502 be disregarded. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 count_lines \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 0.0 MB \u2502 \u2502 execution time \u2502 115 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def count_lines (f): \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 length = 0 # ignores '\\n' characters \u2502 \u2502 \u2502 \u2502 for line in f: # when using open like we do here, it returns an iterator not a \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 length = length + len (line) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return { \"no_lines\" : counter, \"size\" : length} \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Structured (binary) data layout strategies \u00b6 Row-based: (most commonly used dbs): sqlite, Postgres, MySQL, ... Avro Column-based OLAP dbs: duckdb, ClickHouse, BigQuery, Snowflake ... pandas dataframe parquet, feather In order to use the more advanced operations on data I described earlier, the data formats we use need to support them. None of the simple formats like csv, json, yaml do. There is one category of applications that had to deal with those things for decades though: databases. So I think it pays to look a bit at how they handle storing data, and what kind of trade-offs they are making. Basically, a database is a system that lets you persist (mostly) structured data on disk, and gives you an easy, memory- and processing-efficient way to query and retrieve it back. To do that, they have different ways to persist data, add indexes, cache 'hot' data, and so on. As it turns out, there are 2 main ways data can be stored on disk for efficient retrieval: row-based, and column-based (I'm ignoring document/'nosql' databases here, since -- for almost all practical use-cases -- they are inferior to relational ones). Row-oriented databases \u00b6 The most common database type is 'row-oriented'. This means that data is stored in a way so that each row represents a continuous block of disk (or memory). Data is quick and easy to read (if you are interested in a subset of rows) and it is very easy and fast to add new rows/records. This fits the most common requirements businesses have for a database, since new items are added constantly, which is why most databases we encounter in the wild are row-based. Examples for such databases are: Postgres, sqlite, MySQL. Column-oriented databases \u00b6 Column-oriented databases have existed for a long time, but they are not as prevalent as their row-based cousins, and often ignored by developers who haven't been exposed to them and their advantages. Instead of storing data row-by-row, they store data column-by-column. This means that column-cells are layed out next to each other on disk, and different columns occupy different regions of the storage (not necessarily close to each other at all). The querying logic is quite different for this type of database, the main advantage is that a certain type of analytical query is really fast (speedups of 10x or even 100x are quite possible), also it is very easy to request whole columns from such a database without it having to access any irrelevant parts of the data. Compressing data is also easier with column-oriented databases, so usually those occupy less disk space than their row-based peers. The disadvantage of those databases is that it's harder and slower to add new data, so they are more common for situations where one either has a fixed dataset, or updates are rare, and come in big(er) batches. Also, certain types of queries are less suited for that layout, which makes it always a good idea to think about what you need out of your data before deciding on a database/database type. Row-based/column-based in data science \u00b6 How is this relevant? Well, because in data science we are dealing mostly with fixed datasets, and the queries we do on them are mostly analytical in a way that fits column-oriented data layouts; although exceptions from that rule are not uncommon. So it makes sense to depart from the 'common wisdom' of using a row-based approach. In fact, Numpy arrays and Pandas dataframes (which depend on them) are kept in memory using the column-based approach. This is important to know, because it helps us using and querying data correctly in our code. For example, it's not a good idea and very slow to add 'rows' to a Pandas dataframe. Instead, we should initialize the Dataframe with the whole dataset once at the beginning, and then only add columns to it (which is very fast), but no new rows if at all possible. One issue with Numpy/Pandas is that commonly a dataset is loaded into memory as a whole. There are ways around that (for example by processing a csv file in batches), but very often those are not used. In reality, it's probably not that big an issue in the DH field, since datasets seem to be smaller on average. But it is still a good idea to be as efficient as possible in this regard, esp. for our purpose, since we won't have any knowledge or guarantees in advance about the data we'll be handling (which might very well be bigger than the availble memory). Also, since we are building an interactive application, it makes a difference whether a query comes back within a second, or ten. More random benchmarks, this time with Pandas \u00b6 clear_system_cache () import pandas as pd def load_csv ( path ): counter = 0 df = pd . read_csv ( path ) for _ in df [ \"year_month\" ]: counter = counter + 1 return counter profile_read_csv = MemoryRecorder . profile_func ( \"Reading a csv file with Pandas as a whole.\" , \"This is ok, as long as the dataset is not too big.\" , False , load_csv , file_path ) print ( profile_read_csv . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading a csv file with Pandas as a whole. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This is ok, as long as the dataset is not too big. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 load_csv \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 63.625 MB \u2502 \u2502 execution time \u2502 342 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def load_csv (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 df = pd . read_csv(path) \u2502 \u2502 \u2502 \u2502 for _ in df[ \"year_month\" ]: \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return counter \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f clear_system_cache () import pandas as pd def load_csv ( path ): counter = 0 df = pd . read_csv ( path ) for index , row in df . iterrows (): counter = counter + 1 return counter profile_read_csv = MemoryRecorder . profile_func ( \"Reading a csv file with Pandas, iterating over rows\" , \"As one can see, this is very very slow, and not a good idea at all to do in Pandas.\" , False , load_csv , file_path ) print ( profile_read_csv . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading a csv file with Pandas, iterating over rows \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 As one can see, this is very very slow, and not a good idea at all to do in Pandas. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 load_csv \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 80.875 MB \u2502 \u2502 execution time \u2502 21704 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def load_csv (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 df = pd . read_csv(path) \u2502 \u2502 \u2502 \u2502 for index, row in df . iterrows(): \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return counter \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f def load_csv_in_chunks ( path ): counter = 0 chunksize = 1000 with pd . read_csv ( path , chunksize = chunksize ) as reader : for chunk_df in reader : for _ in chunk_df [ \"year_month\" ]: counter = counter + 1 return counter profile_read_csv = MemoryRecorder . profile_func ( \"Reading a csv file with Pandas, in chunks.\" , \"This is a good approach when dealing with a dataset that is large, and we don't need it except for a single operation on a single column. We can optimize the execution-time/memory-usage by adjusting the 'chunksize' value.\" , False , load_csv_in_chunks , file_path ) print ( profile_read_csv . report ) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading a csv file with Pandas, in chunks. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This is a good approach when dealing with a dataset that is large, and we don't need it \u2502 \u2502 except for a single operation on a single column. We can optimize the \u2502 \u2502 execution-time/memory-usage by adjusting the 'chunksize' value. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 load_csv_in_chunks \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 6.0390625 MB \u2502 \u2502 execution time \u2502 892 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def load_csv_in_chunks (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 chunksize =1000 \u2502 \u2502 \u2502 \u2502 with pd . read_csv(path, chunksize = chunksize) as reader: \u2502 \u2502 \u2502 \u2502 for chunk_df in reader: \u2502 \u2502 \u2502 \u2502 for _ in chunk_df[ \"year_month\" ]: \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return counter \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f","title":"Data formats"},{"location":"architecture/data/data_formats/#data-serialization-and-storage","text":"data lives in memory or on disk lots of 0's and 1's -- binary format only 'decoding' gets you a useful representation 'text' is just an often used encoding format The natural habitat of (digital) data is computer memory or on disk. Data is always stored in binary form, and there is always some sort of decoding involved to make data usable in one way or another (with the exception of booleans maybe). Even when we talk about text files (seemingly implying that those are not binary since they are 'text'), we are dealing with binary data. It's just such a standard data encoding format that tools to decode that sort of data are available everywhere. Decoding text is by no means trivial, but luckily our tools have evolved so much by now -- and we have standards like utf-8 commonly available -- that we as users hardly ever come across decoding issues anymore. At least not to the extent we used to. It still happens, and I would imagine quite a bit more in the Digital Humanities than in your everyday 'business' use-case. So it helps to be aware of at least the basics involved in text encoding standards, and I would recommend anyone doing any sort of programming to read up on it.","title":"Data serialization and storage"},{"location":"architecture/data/data_formats/#tabular-data-serialization-formats","text":"serialization/deserialization binary or not, here I come: avro, protobuf, pickle csv, json, yaml, xml 'structured formats' with schema, or without: avro, protobuf, thrift, flatbuffers, xml csv, json, yaml, messagepack, pickle zero-copy, memory-mapping? The (arguably) most important type of data we'll be dealing with is structured, tabular data. So it pays to think about how its digital form is represented in memory, and what issues are involved when trying to store, manipulate and transfer it. In our context, tabular data is always a 2 dimensional matrix (rows, columns), where each column has the same number of items, and each column contains items of the same data type (or union of data types). Tabular data can have a column that can be used as index, but that is optional. Each table can be described with a schema, which is basically a list of column names, and the data types associated with each of those columns.","title":"Tabular data (serialization) formats"},{"location":"architecture/data/data_formats/#serialization-deserialization","text":"Every programming language represents its basic data types differently in memory. That's especially true for the arguably most common data type: the string. Also, that in-memory representation is (almost) always different to the format of the (functially) same data when exported into a file on disk. This means that, if we want to export our data in our code, we need to do a step that is called 'serializing' (or 'marshalling'): we convert the data into a commonly accepted representation of a commonly accepted set of data-types. This serialization is usually expensive, computationally speaking. We want to avoid it, if at all possible, or at least always postpone it until the last possible moment, when we are sure the data won't change anymore, so we only have to do it once. The same goes for de-serializing data, just in the other direction: we only want to do it once, then keep it in memory in our native representation (if the size of the data allows it), so we don't have to read it again. Even if the content of a file is in the OS (page) cache (which would mean we don't actually have to read the file-content from disk) we'd still have to spend the cpu-cycles for de-serialization. So, big no-no, bad data-scientist!","title":"Serialization / deserialization"},{"location":"architecture/data/data_formats/#format-types","text":"For serialization, we have two basic options: text, and binary (let's just not nitpick and assume that this distinction makes sense).","title":"Format types"},{"location":"architecture/data/data_formats/#schema","text":"Another useful way to separate data formats is to check whether they include a (native) schema that describes the value types of the data they hold, or not. If schema information is present, it can either be included in the resulting file, or be stored elsewhere. Having schema information for a dataset is highly desirable, because it tells us exactly what type of data we are dealing with (is it a string, integer, float? what precision?). Whereas most text-based data formats don't include a schema definition format, there are sometimes external efforts to remedy that (JSON-schema, for example). None of the text-based formats I can think of at the top of my head include the schema in a resulting file. This is important, because the complexity of tools that handle data increases if they need to worry about secondary, 'side-car' files for incoming data.","title":"Schema"},{"location":"architecture/data/data_formats/#streaming-zero-copy-memory-mapping","text":"One thing that everyone working semi-seriously in data science and handling big-ish data should be aware of is that in most OS'es you can read (or 'load') data in more ways than one. The 'one' way is usually something like: file = open ( 'dataset.csv' ) lines = file . read () # or file.readlines() When using Pandas, it'll probably take the form of: import pandas as pd pd . read_csv ( 'dataset.csv' ) Both of those read the whole file into memory. Which will be fine if the dataset is small or there is a need for it to be in memory in full. Depending on the situation, it might be wasteful, though. For example when calculating the mean for a column of integers. In that case it's a better strategy to read one line of the file, process the column we are interested in, eject the line from memory, then read the next line, only keeping the current total and the number of items we processed so far. That way we'll never allocate more memory than what is needed for a single line. We can even process datasets that are larger than the available memory of or our workstation. As a matter of fact, we could do even better if we would know the offset and length of the column we are interested in, in that case, we would only have to read the bytes that hold the integer value we need, and could ignore the other cells of a row completely. Again, this might or might be an issue depending on the size of the data in a row, but if we have a dataset with a lot of large columns, the I/O operations we would not have to do by only reading the exact data we need could improve the speed of processing considerably. Doing that doesn't really work for csv files, for example. Since there is no good way for us to know the exact offset of length of the column we are interested in. There are data formats that support that kind of operation though. Along with those fairly simple strategies to deal with data efficiently, there are more advanced ones that also deal with data and how it is handled in a system memory as well as on disk. For those of you who are interested, I would recommend looking up the terms 'memory-mapping', and 'zero-copy'.","title":"Streaming, zero-copy, memory-mapping"},{"location":"architecture/data/data_formats/#some-random-benchmarks-to-illustrate","text":"clear_system_cache () file_path = get_example_file () def count_lines ( path ): f = open ( path ) counter = 0 length = 0 # ignores '\\n' characters lines = f . readlines () for line in lines : counter = counter + 1 length = length + len ( line ) return { \"no_lines\" : counter , \"size\" : length } profile_count_lines = MemoryRecorder . profile_func ( \"Reading the whole file\" , \"This iterates through all lines in memory, keeping all of them in memory at the same time.\" , False , count_lines , file_path ) print ( profile_count_lines . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading the whole file \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This iterates through all lines in memory, keeping all of them in memory at the same \u2502 \u2502 time. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 count_lines \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 53.75 MB \u2502 \u2502 execution time \u2502 144 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def count_lines (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 f = open (path) \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 length = 0 # ignores '\\n' characters \u2502 \u2502 \u2502 \u2502 lines = f . readlines() \u2502 \u2502 \u2502 \u2502 for line in lines: \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 length = length + len (line) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return { \"no_lines\" : counter, \"size\" : length} \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f clear_system_cache () file_obj = open ( file_path , buffering = True ) def count_lines ( f ): counter = 0 length = 0 # ignores '\\n' characters for line in f : # when using open like we do here, it returns an iterator not a materialized list counter = counter + 1 length = length + len ( line ) return { \"no_lines\" : counter , \"size\" : length } profile_count_lines = MemoryRecorder . profile_func ( \"Reading the file line by line\" , \"This allocates only very little memory, since once a line is read and processed, it will be disregarded.\" , False , count_lines , file_obj ) file_obj . close () print ( profile_count_lines . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading the file line by line \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This allocates only very little memory, since once a line is read and processed, it will \u2502 \u2502 be disregarded. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 count_lines \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 0.0 MB \u2502 \u2502 execution time \u2502 115 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def count_lines (f): \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 length = 0 # ignores '\\n' characters \u2502 \u2502 \u2502 \u2502 for line in f: # when using open like we do here, it returns an iterator not a \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 length = length + len (line) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return { \"no_lines\" : counter, \"size\" : length} \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f","title":"Some random benchmarks, to illustrate"},{"location":"architecture/data/data_formats/#structured-binary-data-layout-strategies","text":"Row-based: (most commonly used dbs): sqlite, Postgres, MySQL, ... Avro Column-based OLAP dbs: duckdb, ClickHouse, BigQuery, Snowflake ... pandas dataframe parquet, feather In order to use the more advanced operations on data I described earlier, the data formats we use need to support them. None of the simple formats like csv, json, yaml do. There is one category of applications that had to deal with those things for decades though: databases. So I think it pays to look a bit at how they handle storing data, and what kind of trade-offs they are making. Basically, a database is a system that lets you persist (mostly) structured data on disk, and gives you an easy, memory- and processing-efficient way to query and retrieve it back. To do that, they have different ways to persist data, add indexes, cache 'hot' data, and so on. As it turns out, there are 2 main ways data can be stored on disk for efficient retrieval: row-based, and column-based (I'm ignoring document/'nosql' databases here, since -- for almost all practical use-cases -- they are inferior to relational ones).","title":"Structured (binary) data layout strategies"},{"location":"architecture/data/data_formats/#row-oriented-databases","text":"The most common database type is 'row-oriented'. This means that data is stored in a way so that each row represents a continuous block of disk (or memory). Data is quick and easy to read (if you are interested in a subset of rows) and it is very easy and fast to add new rows/records. This fits the most common requirements businesses have for a database, since new items are added constantly, which is why most databases we encounter in the wild are row-based. Examples for such databases are: Postgres, sqlite, MySQL.","title":"Row-oriented databases"},{"location":"architecture/data/data_formats/#column-oriented-databases","text":"Column-oriented databases have existed for a long time, but they are not as prevalent as their row-based cousins, and often ignored by developers who haven't been exposed to them and their advantages. Instead of storing data row-by-row, they store data column-by-column. This means that column-cells are layed out next to each other on disk, and different columns occupy different regions of the storage (not necessarily close to each other at all). The querying logic is quite different for this type of database, the main advantage is that a certain type of analytical query is really fast (speedups of 10x or even 100x are quite possible), also it is very easy to request whole columns from such a database without it having to access any irrelevant parts of the data. Compressing data is also easier with column-oriented databases, so usually those occupy less disk space than their row-based peers. The disadvantage of those databases is that it's harder and slower to add new data, so they are more common for situations where one either has a fixed dataset, or updates are rare, and come in big(er) batches. Also, certain types of queries are less suited for that layout, which makes it always a good idea to think about what you need out of your data before deciding on a database/database type.","title":"Column-oriented databases"},{"location":"architecture/data/data_formats/#row-basedcolumn-based-in-data-science","text":"How is this relevant? Well, because in data science we are dealing mostly with fixed datasets, and the queries we do on them are mostly analytical in a way that fits column-oriented data layouts; although exceptions from that rule are not uncommon. So it makes sense to depart from the 'common wisdom' of using a row-based approach. In fact, Numpy arrays and Pandas dataframes (which depend on them) are kept in memory using the column-based approach. This is important to know, because it helps us using and querying data correctly in our code. For example, it's not a good idea and very slow to add 'rows' to a Pandas dataframe. Instead, we should initialize the Dataframe with the whole dataset once at the beginning, and then only add columns to it (which is very fast), but no new rows if at all possible. One issue with Numpy/Pandas is that commonly a dataset is loaded into memory as a whole. There are ways around that (for example by processing a csv file in batches), but very often those are not used. In reality, it's probably not that big an issue in the DH field, since datasets seem to be smaller on average. But it is still a good idea to be as efficient as possible in this regard, esp. for our purpose, since we won't have any knowledge or guarantees in advance about the data we'll be handling (which might very well be bigger than the availble memory). Also, since we are building an interactive application, it makes a difference whether a query comes back within a second, or ten.","title":"Row-based/column-based in data science"},{"location":"architecture/data/data_formats/#more-random-benchmarks-this-time-with-pandas","text":"clear_system_cache () import pandas as pd def load_csv ( path ): counter = 0 df = pd . read_csv ( path ) for _ in df [ \"year_month\" ]: counter = counter + 1 return counter profile_read_csv = MemoryRecorder . profile_func ( \"Reading a csv file with Pandas as a whole.\" , \"This is ok, as long as the dataset is not too big.\" , False , load_csv , file_path ) print ( profile_read_csv . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading a csv file with Pandas as a whole. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This is ok, as long as the dataset is not too big. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 load_csv \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 63.625 MB \u2502 \u2502 execution time \u2502 342 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def load_csv (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 df = pd . read_csv(path) \u2502 \u2502 \u2502 \u2502 for _ in df[ \"year_month\" ]: \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return counter \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f clear_system_cache () import pandas as pd def load_csv ( path ): counter = 0 df = pd . read_csv ( path ) for index , row in df . iterrows (): counter = counter + 1 return counter profile_read_csv = MemoryRecorder . profile_func ( \"Reading a csv file with Pandas, iterating over rows\" , \"As one can see, this is very very slow, and not a good idea at all to do in Pandas.\" , False , load_csv , file_path ) print ( profile_read_csv . report ) -- system cache cleared -- \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading a csv file with Pandas, iterating over rows \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 As one can see, this is very very slow, and not a good idea at all to do in Pandas. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 load_csv \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 80.875 MB \u2502 \u2502 execution time \u2502 21704 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def load_csv (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 df = pd . read_csv(path) \u2502 \u2502 \u2502 \u2502 for index, row in df . iterrows(): \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return counter \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f def load_csv_in_chunks ( path ): counter = 0 chunksize = 1000 with pd . read_csv ( path , chunksize = chunksize ) as reader : for chunk_df in reader : for _ in chunk_df [ \"year_month\" ]: counter = counter + 1 return counter profile_read_csv = MemoryRecorder . profile_func ( \"Reading a csv file with Pandas, in chunks.\" , \"This is a good approach when dealing with a dataset that is large, and we don't need it except for a single operation on a single column. We can optimize the execution-time/memory-usage by adjusting the 'chunksize' value.\" , False , load_csv_in_chunks , file_path ) print ( profile_read_csv . report ) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Reading a csv file with Pandas, in chunks. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 This is a good approach when dealing with a dataset that is large, and we don't need it \u2502 \u2502 except for a single operation on a single column. We can optimize the \u2502 \u2502 execution-time/memory-usage by adjusting the 'chunksize' value. \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2577 \u2502 \u2502 Profiled function \u2502 load_csv_in_chunks \u2502 \u2502 \u2576\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2574 \u2502 \u2502 max memory \u2502 6.0390625 MB \u2502 \u2502 execution time \u2502 892 ms \u2502 \u2502 \u2575 \u2502 \u2502 \u256d\u2500 Code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 def load_csv_in_chunks (path): \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 counter = 0 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 chunksize =1000 \u2502 \u2502 \u2502 \u2502 with pd . read_csv(path, chunksize = chunksize) as reader: \u2502 \u2502 \u2502 \u2502 for chunk_df in reader: \u2502 \u2502 \u2502 \u2502 for _ in chunk_df[ \"year_month\" ]: \u2502 \u2502 \u2502 \u2502 counter = counter + 1 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 return counter \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f","title":"More random benchmarks, this time with Pandas"},{"location":"architecture/data/dev/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());","title":"Dev"},{"location":"architecture/data/persistence/","text":"Data persistence \u00b6 This is a document to describe my plans for storing data (and metadata) in kiara . (Almost) nothing I describe here is inmplemented yet, so it only reflects my current thinking. I think the overall strategy will hold, but there might be changes here and there. The problem \u00b6 kiara s main functionality centers around transforming input data sets to output data sets. Those outputs need to be stored, to be of any use later on. Obviously. When deciding how to do this, we must take into account concerns about performance, disk- and memory-usage, data versioning, which metadata to attach, in what way, how to deal with metadata schemas (and versioning of both), etc. The solution \u00b6 Well, solution. This is my current thinking of how to tackle the problem in a way that takes into account all of the aspects described above, while still being flexible enough to hopefully be able to incorporate solutions for future unforsseen issues. I am having trouble coming up with a good structure for this document, so I think I'll just try to tell the story from the point of view of data. Starting from a state where data exists outside of kiara , to when it is in a state to be ready to be published. As with everything I'm writing here as an explanation of generic and abstract concepts, some of the technical details I'm describing might be simplified to the point of being incorrect... The 7 stages of data \u00b6 One thing I'd like to say before I start to describe those stages: the transformation of a dataset, from one stage to the next, always always always happens by piping the dataset through a kiara module. At absolutely no point is this done without kiara s involvement and knowledge. The dataset is used as input for a module, and the result (technically a new dataset) is a representation of the dataset in its next stage. This is important to keep in mind, as it is crucial for us so we can track data lineage. I'll write more on the specifics of this below, where it makes more sense. 1) Unmanaged \u00b6 At the beginning, there was csv. Whether I like it or not, csv is the most predominant form data comes in. Csv is bad in a lot of ways, but in my mind the worst thing about it is that it is schema-less. True, in some cases you have a header-line, which gives you column-names, but that's not a requirement. Also, in a lot of cases you can auto-determine the type of each column, and luckily libraries like Pandas or Apache Arrow solved that problem for us so we don't have to do it ourselves every time. But those auto-parsers are not fool-proof, and you end up with integers where you wanted floats (or doubles), or integers where you wanted strings, or vice versa. In some cases we get data in a form that includes at least a semi-schema. Like a sqlite database file (which is more 'strongly' typed). But it's a lucky day when we get data that contains metadata about authorship, how and when it was created, from what sources, etc. 2) Onboarded \u00b6 This is the first thing we need to do to unmanaged data: we need to 'onboard' it, so kiara knows the data exists, and what exact bytes it consists of. This last thing is very important: we have to be able to make sure the data we are talking about is not being changed externally, a lot of things in kiara s approach to data depend on this. Practically, in most cases this means kiara will copy one or several files into a protected area that no other application can/should access. That way we always have a reference version of the dataset (the bytes) we are talking about. One thing kiara does at this stage is give the dataset a uniuqe id, which can be used to reference it later (by users, or other objects/functions). Another thing is to collect some basic metadata: when the file/folder was imported, from what original path, what the filenames are, mime-type, size of files, original file attributes (creation data, permissions, etc.). This can all be captured automatically. We can also record who it was that imported the dataset, if we have some app-global configuration about the current user, like a full name and email-address. Note, that this might or might not be the person who created the dataset. So, at this stage all we did was copy a file(set) into a protected area to sort of 'freeze' it, and augment it with very basic metadata. We don't know anything about the nature of the dataset yet, all we know is the bytes the datasets consists of. It is important to point out that we would not have to store those chunks of bytes as files again, using the same structure as the original set of files. The dataset ceased to be 'files' here for us, we are only interested in the chunks of bytes (and their meaning) from here on out. We could store the data in an object store, zipped, tarred and feathered (pun intended). Or as byte-stream directly on block storage, if we were crazy enough. A side-note that makes things a bit more complicated, but it is probably necessary to address potential concerns: no, we don't actually need to copy the files, and can leave them in place and only generate the metadata and id for them. This might be necessary in cases where the source data is very big (photos, movies, audio-files, other large datasets). I don't think we need to figure out how exactly we deal with this scenario right now, but it basically comes down to making the user aware of what is happening, and what the implications are if the source data is changed externally (inconsistent metadata and potential incorrect result data-sets further down the line). There are strategies to help prevent some of those potential issues (checksums, for example), but overall we have to acknowledge that working with large-sized datasets is always a challenge, and in some cases we might just have to say: \"sorry, this is too big for us right now\". 3) Augmented with more (basic) metadata \u00b6 To recapitulate: at this stage we have data (chunks of bytes -- not files!!! hit yourself over the head twice with something semi-heavy if you are still think in terms of files from here on out!) in a protected area, some very basic metadata, and an id for each dataset. We might or might not have authorship metadata (arguably one of the most important pieces of metadata), depending on whether who 'onboarded' the dataset actually created it. So, as a first step and following good practice, at this stage we should try to get the user to tell us about authorship and other core metadata about our dataset (licensing, copyright, ...). I don't think we can make this step mandatory, in practice, but we should push fairly hard, even if that means a slight decrease in user experience. It is very important information to have... So, one thing we could do was to have a checkbox that lets the user confirm: I created the data (in which case we can just copy the 'imported-by' field). 3) Typed \u00b6 Chunks of bytes are not very useful by itself. They need to be interpreted to be of use. This means: determining in some way what the structure of the chunks of bytes is, and then applying common conventions for that particular structure (aka data type/format) when reading the chunks of bytes. Therefore 'interpreting' the chunks of bytes. This is a very obvious thing that happens all the time we use computers, but I think it makes sense to point it out here, because usually this is transparent to the user when they click an 'Open file' button in an application, and even some developers are ignorant to the underlying concept (and can afford to be, since they usually work several abstraction layers above where that is happening). To encapsulate this concept, we will create a 'data type' for each important group of datasets that share some important characteristics. Examples for very simple data types are strings, integers, booleans. I'll ignore those, because those are trivial to use, and that triviality actually makes it harder to explain the concept I'm talking about. More relevant data types are: 'table', 'network graph', 'text corpus', 'photo collection'. Every data type inherently contains a description of, well, the 'type' of data represented by it, and, with that, information about how a user or code caqn access the actual data, and/or some of its properties. From here on out, I'll focus on tabular data in some form or other, since I expect that this will be one of our most important (base-) data types. I expect the reader to 'translate' whatever I'm saying below to other types, and extrapolate the practical differences. So, to explain this step I decided to look at three different use-cases (mostly because we use them in 2 of our example workflows, so people should be familiar with them): a csv file with tabular data an imported folder of text files (a corpus) two imported csv files containing edge and node information to form a network graph Example: tabular data \u00b6 This is the simplest case, and very common: we have a csv file, and need to have some sort of tabular data structure that we can use to query and analyze the data contained in it. Let's assume we have onboarded a csv file using kiara , so we have a dataset id that we use to point to it. Technically, this dataset is already 'typed': it has the type 'file'. This is not a very useful type, all it allows can tell us is a file name (which in a way is metadata), and the file content. We can ask kiara to interpret the content of this file as table, though, because we know it must be one. This means we 'overlay' a different, more specific data type on top of the same data. Under the hood, kiara will use the Apache Arrow read_csv helper method, which is very smart and fast, and it can create an Arrow Table object out of a csv file. It can figure out file encoding, column names (if present), column types, seperator characters. This detection is not fool-proof, but should work good enough in practice that we don't need to worry about it here. What really happens here is that the read_csv method is not just reading our data, but also, at the same time, is adding some important metadata to our dataset. Pandas can do the same with its csv import method. Even though this adding of metadata is more or less transparent to the user -- so they are not really aware of it -- it happens, and it is a very important thing that must happen to make our dataset useful. In our application, we might or might not want to ask users whether the extracted column names and types are correct, but this is a UI-specific implementation detail. So, considering all this, the important point here is that at this stage we have actual 'table' data, there is no need for the original csv file anymore (except as a reference for data lineage purposes). Our dataset is now of the data type 'table'. Which means we have an assurance that we can query it for table metadata properties (number of rows, number and name of columns, column types, size in bytes, etc.). And we can apply functions against it that are commonly applied against tabular data (sql queries, filters, etc.). That means, a 'data type' is really just a convention, a marker, that tells us how the bytes are organized for a particular set of bytes, and how to interact with it. This is all users and 3rd party-code needs to worry about. Implementation details about how this data is stored or loaded are irrelevant on this level of abstraction. This reduces complexity for kiara s external facing API, while, of course, introducing some extra complexity internally. Example: text corpus \u00b6 The source data comes as a folder of files, each file contains (just) text (not structured like json, csv, etc.). When we do the 'onboarding' step for this data, all we do is copy the files verbatim into a new location. There could be some metadata implicit in the relative paths of each file (e.g. languages -- files for the same language live in a subfolder named after the language), and there can also be some metadata in the file names. We preserve all that metadata by copying the folder one-to-one, without changing anything. But it is important to note that this metadata, as of yet, is still uncaptured. The 'soul' of this dataset (meaning: the properties of the dataset we are interested in and we want to use in our investigation, and which will hopefully answer our research question) is in the content of each text file (aka the unicode encoded chunks of bytes). It is important to say again: at this stage the dataset ceased to be a set of files! It is a dataset within kiara that has an id (a single one! not one for every text!), and it has a basic set of metadata fields (the ones we could collect automatically). Yes, the dataset is backed by a set of files in the kiara data store, but that is an implementation detail nobody needs to know about, and I think we should try hard to hide from users. If you haven't noticed so far: I strongly believe the file metaphor is a distraction, and not necessary for us, except when import/export is concerned. Anyway, kiara does not know much about the dataset at this stage. To be of actual use, we need to interpret the data. In this case, we know we want to interpret the data as a text corpus. The most basic shape we can imagine a text corpus to look like is a list of strings (an array, or a single-column table). For making it easier to work with the text corpus in the future, let's make up a convention to save in tabular form, and the column containing the text items is always named text_content . If we use, for example Apache Arrow to store that table, it makes the stored chunks of data much smaller (in comparison to text files), and it also makes the whole thing easier (at least faster) to query. It also allows us to easily attach more (meta-)data to the dataset. Note The distinction between data and metadata becomes a bit blurry here. In a lot of cases, when I say metadata, it is metadata from the point of view of the research proess, not metadata for kiara . I don't know how to make it clear which I'm talking about in each case without making this whole thing even more unreadable as it already is, so I will just have to ask you to figure it out yourself, in each case :-) Because we didn't lose any of the implied metadata when onboarding our folder of text files, it would be a shame if we wouldn't actually capture it. In this case, let's assume we didn't have any subfolders (so no metadata in their name), but our files are named in a special way: [publication_id]_[date_of_publishing]_[other_stuff_we_are_not_interested_in] Note The information about the format is important (in a way it is also an input) and we need to retrieve it somehow. This is a real problem that doesn't look like a big problem. But it is, for us. I'll ignore this here, because it would complicate things too much and is only of tangential relevance. This means, we can extract the publication id and the date of publishing with a simple regular expression, and we can add a column for each one to our table that so far only contains the actual text for each item. The publication id will be of type string (even though some of the ids might be integers -- we don't care), and the publication date will be a time format. Now we have a table with 3 columns, and we can already filter the texts by date easily, which is pretty useful! We wouldn't, strictly speaking those two additional columns to have a dataset of type 'text corpus' but it's much more useful that way. As a general rule: if we have metadata like that, it should be extracted and attached to the data in this stage. It's cheap to do in a lot of cases, and we never know when it will be needed later. What we have at this stage is data that has the attributes of a table (columns with name and type info, as well as rows representing an item and it's metadata). This is basically the definition of our 'text corpus' data type: something that allows us to access text content items (the actual important data) using a mandatory column named text_content , and that has zero to N metadata properties for each of those text items. In addition, we can access other metadata that is inherited from the base type (table): number of rows, size in bytes, etc, as well as its lineage (via a reference to the original onboarded dataset). Internally, we'll store this data type as an Arrow table, but again, this is an implementation detail, and neither user nor frontend needs to know about this (exceptions apply, of course, but lets not get bogged down by those just now -- none of them are deal-breakers, as far as I can see). Example: network graph data \u00b6 Similar to the text corpus case above, let's think about what a basic definition of a network graph data type would look like. It would have to include a list of nodes, and a list of edges (that tell us how those nodes are connected). Actually, the list of nodes is implied in the list of edges, so we don't need to provide that if we don't feel like it (although, that doesn't apply if we have nodes that are not part of any edge). In addition, both nodes and edges can have attributes, but those are optional. So, our network graph data type would, at a minimum, need to be able to give us this information about all this via its interface. networkx is one of the most used Python libaries in this space, so let's decide that internally, for us, a network graph is represented as an object of the Graph class, or one of its subclasses. This class will give us a lot of useful methods and properties to access and query, one problem left is: how do we create an object of this class in a way that fits with our overall strategy? We can't save and load a networkx object directly ( pickling would be a bad idea for several reasons), so we need to create (and re-create) it via some other way. For this, lets look at the constructor arguments of this class, as well as what sort of data types we have available that we can use to feed those arguments. One option apparently is to use a list of edges contained in a Pandas dataframe as input, along with a name of columns representing the names of source and target column name, something like: graph: nx.DiGraph = nx.from_pandas_edgelist( pandas_table, source_column, target_column, ) This could work for us: as in the other example, we can use a table as the 'backing' data type for our graph object. Considering a graph without any node attributes, we can have a table with a minimum of two columns, and via a convention that we just made up, we say that the source column should be called edge_source , and the target column edge_target . We wrap all this in an Arrow table again, and save it as such. And later load it again, assuming the same convention (which, basically, saves us from asking for 2 column names every time). If our graph also includes node attributes, all we do is extend the implementation of our network graph data type to create a second table with a required column node_id , and one or several more columns that hold node attributes, similar to the metadata in our 'text corpus' example from above. 4) Transformed \u00b6 With all that out of the way, we can finally do something interesting with the data. Everything up to this point was more or less housekeeping: importing, tagging, marking, organizing datasets. We still are operating on the same actual data as was contained in the original files (whatever type they were). But we now know exactly what we can do with it without having to ask questions. Using the 3 example from above, we now know we have 3 datasets: one table, one text corpus (which is also a table, but a more specific one), and a network graph. And each of those datasets also comes with metadata, and we know what metadata files are available for what data types, and what the metadata means in each context. A first thing we can do is automatically matching datasets to available workflows: we know what input types a workflow takes (that is included in each workflow metadata). So all we need to do is check the input types of each available workflow against the type of a dataset. This works even with different specificity: give me all workflows that take as input a generic graph. Or: give me all workflows that take a directed graph as input (this is information that is included in the metadata of each network graph dataset).","title":"Data persistence"},{"location":"architecture/data/persistence/#data-persistence","text":"This is a document to describe my plans for storing data (and metadata) in kiara . (Almost) nothing I describe here is inmplemented yet, so it only reflects my current thinking. I think the overall strategy will hold, but there might be changes here and there.","title":"Data persistence"},{"location":"architecture/data/persistence/#the-problem","text":"kiara s main functionality centers around transforming input data sets to output data sets. Those outputs need to be stored, to be of any use later on. Obviously. When deciding how to do this, we must take into account concerns about performance, disk- and memory-usage, data versioning, which metadata to attach, in what way, how to deal with metadata schemas (and versioning of both), etc.","title":"The problem"},{"location":"architecture/data/persistence/#the-solution","text":"Well, solution. This is my current thinking of how to tackle the problem in a way that takes into account all of the aspects described above, while still being flexible enough to hopefully be able to incorporate solutions for future unforsseen issues. I am having trouble coming up with a good structure for this document, so I think I'll just try to tell the story from the point of view of data. Starting from a state where data exists outside of kiara , to when it is in a state to be ready to be published. As with everything I'm writing here as an explanation of generic and abstract concepts, some of the technical details I'm describing might be simplified to the point of being incorrect...","title":"The solution"},{"location":"architecture/data/persistence/#the-7-stages-of-data","text":"One thing I'd like to say before I start to describe those stages: the transformation of a dataset, from one stage to the next, always always always happens by piping the dataset through a kiara module. At absolutely no point is this done without kiara s involvement and knowledge. The dataset is used as input for a module, and the result (technically a new dataset) is a representation of the dataset in its next stage. This is important to keep in mind, as it is crucial for us so we can track data lineage. I'll write more on the specifics of this below, where it makes more sense.","title":"The 7 stages of data"},{"location":"architecture/data/persistence/#1-unmanaged","text":"At the beginning, there was csv. Whether I like it or not, csv is the most predominant form data comes in. Csv is bad in a lot of ways, but in my mind the worst thing about it is that it is schema-less. True, in some cases you have a header-line, which gives you column-names, but that's not a requirement. Also, in a lot of cases you can auto-determine the type of each column, and luckily libraries like Pandas or Apache Arrow solved that problem for us so we don't have to do it ourselves every time. But those auto-parsers are not fool-proof, and you end up with integers where you wanted floats (or doubles), or integers where you wanted strings, or vice versa. In some cases we get data in a form that includes at least a semi-schema. Like a sqlite database file (which is more 'strongly' typed). But it's a lucky day when we get data that contains metadata about authorship, how and when it was created, from what sources, etc.","title":"1) Unmanaged"},{"location":"architecture/data/persistence/#2-onboarded","text":"This is the first thing we need to do to unmanaged data: we need to 'onboard' it, so kiara knows the data exists, and what exact bytes it consists of. This last thing is very important: we have to be able to make sure the data we are talking about is not being changed externally, a lot of things in kiara s approach to data depend on this. Practically, in most cases this means kiara will copy one or several files into a protected area that no other application can/should access. That way we always have a reference version of the dataset (the bytes) we are talking about. One thing kiara does at this stage is give the dataset a uniuqe id, which can be used to reference it later (by users, or other objects/functions). Another thing is to collect some basic metadata: when the file/folder was imported, from what original path, what the filenames are, mime-type, size of files, original file attributes (creation data, permissions, etc.). This can all be captured automatically. We can also record who it was that imported the dataset, if we have some app-global configuration about the current user, like a full name and email-address. Note, that this might or might not be the person who created the dataset. So, at this stage all we did was copy a file(set) into a protected area to sort of 'freeze' it, and augment it with very basic metadata. We don't know anything about the nature of the dataset yet, all we know is the bytes the datasets consists of. It is important to point out that we would not have to store those chunks of bytes as files again, using the same structure as the original set of files. The dataset ceased to be 'files' here for us, we are only interested in the chunks of bytes (and their meaning) from here on out. We could store the data in an object store, zipped, tarred and feathered (pun intended). Or as byte-stream directly on block storage, if we were crazy enough. A side-note that makes things a bit more complicated, but it is probably necessary to address potential concerns: no, we don't actually need to copy the files, and can leave them in place and only generate the metadata and id for them. This might be necessary in cases where the source data is very big (photos, movies, audio-files, other large datasets). I don't think we need to figure out how exactly we deal with this scenario right now, but it basically comes down to making the user aware of what is happening, and what the implications are if the source data is changed externally (inconsistent metadata and potential incorrect result data-sets further down the line). There are strategies to help prevent some of those potential issues (checksums, for example), but overall we have to acknowledge that working with large-sized datasets is always a challenge, and in some cases we might just have to say: \"sorry, this is too big for us right now\".","title":"2) Onboarded"},{"location":"architecture/data/persistence/#3-augmented-with-more-basic-metadata","text":"To recapitulate: at this stage we have data (chunks of bytes -- not files!!! hit yourself over the head twice with something semi-heavy if you are still think in terms of files from here on out!) in a protected area, some very basic metadata, and an id for each dataset. We might or might not have authorship metadata (arguably one of the most important pieces of metadata), depending on whether who 'onboarded' the dataset actually created it. So, as a first step and following good practice, at this stage we should try to get the user to tell us about authorship and other core metadata about our dataset (licensing, copyright, ...). I don't think we can make this step mandatory, in practice, but we should push fairly hard, even if that means a slight decrease in user experience. It is very important information to have... So, one thing we could do was to have a checkbox that lets the user confirm: I created the data (in which case we can just copy the 'imported-by' field).","title":"3) Augmented with more (basic) metadata"},{"location":"architecture/data/persistence/#3-typed","text":"Chunks of bytes are not very useful by itself. They need to be interpreted to be of use. This means: determining in some way what the structure of the chunks of bytes is, and then applying common conventions for that particular structure (aka data type/format) when reading the chunks of bytes. Therefore 'interpreting' the chunks of bytes. This is a very obvious thing that happens all the time we use computers, but I think it makes sense to point it out here, because usually this is transparent to the user when they click an 'Open file' button in an application, and even some developers are ignorant to the underlying concept (and can afford to be, since they usually work several abstraction layers above where that is happening). To encapsulate this concept, we will create a 'data type' for each important group of datasets that share some important characteristics. Examples for very simple data types are strings, integers, booleans. I'll ignore those, because those are trivial to use, and that triviality actually makes it harder to explain the concept I'm talking about. More relevant data types are: 'table', 'network graph', 'text corpus', 'photo collection'. Every data type inherently contains a description of, well, the 'type' of data represented by it, and, with that, information about how a user or code caqn access the actual data, and/or some of its properties. From here on out, I'll focus on tabular data in some form or other, since I expect that this will be one of our most important (base-) data types. I expect the reader to 'translate' whatever I'm saying below to other types, and extrapolate the practical differences. So, to explain this step I decided to look at three different use-cases (mostly because we use them in 2 of our example workflows, so people should be familiar with them): a csv file with tabular data an imported folder of text files (a corpus) two imported csv files containing edge and node information to form a network graph","title":"3) Typed"},{"location":"architecture/data/persistence/#4-transformed","text":"With all that out of the way, we can finally do something interesting with the data. Everything up to this point was more or less housekeeping: importing, tagging, marking, organizing datasets. We still are operating on the same actual data as was contained in the original files (whatever type they were). But we now know exactly what we can do with it without having to ask questions. Using the 3 example from above, we now know we have 3 datasets: one table, one text corpus (which is also a table, but a more specific one), and a network graph. And each of those datasets also comes with metadata, and we know what metadata files are available for what data types, and what the metadata means in each context. A first thing we can do is automatically matching datasets to available workflows: we know what input types a workflow takes (that is included in each workflow metadata). So all we need to do is check the input types of each available workflow against the type of a dataset. This works even with different specificity: give me all workflows that take as input a generic graph. Or: give me all workflows that take a directed graph as input (this is information that is included in the metadata of each network graph dataset).","title":"4) Transformed"},{"location":"architecture/data/requirements/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import os import dharpa from rich.jupyter import print from dharpa import DHARPA_TOOLBOX_DEFAULT_WORKFLOWS_FOLDER from dharpa.graphs.utils import graph_to_image from dharpa.utils import get_data_from_file Context & Requirements \u00b6 Types of data \u00b6 scalars (mostly user inputs, booleans, strings, enums, numbers) lists of items of the same type tabular data (a collection of lists of items of the same type, each with the same number of items, incl. schema) binary data: images, videos, audio files In our application, we'll deal with a few basic types of data: scalars (mostly user inputs, booleans, strings, enums, numbers) lists of items of the same type tabular data (a collection of lists of items of the same type, each with the same number of items, incl. schema) binary data: images, videos, audio files Sidenote : I consider every kind of user input as data, since it is conceptually the same thing and needs to be recorded and managed the same way. For our purpose, we can ignore scalars because they are easy and cheap to handle, and can be attached to any sort of data or metadata in a few different ways. Also, let's ignore binary data for now, while acknowledging that we will need a strategy to deal with efficiently, in a way that is not too different from how we deal with other types. Which leaves us with lists and tabular data. Those are different to scalars, because there is no telling in advance how many rows they will have, and how large its cells will be (aka 'how many bytes are we dealing with, KBs, MBs, GBs, TBs?'). List (arrays) will be our main data type, along with tables (dataframes) -- the latter are really just lists of lists (including a schema/description of the type of each list). In a lot of cases a module will receive a table, and the output will be a list of the same length as the table. When using Pandas, we usually assign dataframes to variables, this is handy because we have access to the whole dataset via a single variable, and can access the columns seperately via their names. For our case, because we will have connected modules, we will probably deal with 2 scenarios: a module changes the data in a dataframe in one or several columns: this will be rare, but in this case the result of such a module will be a new dataframe a module adds one or several column to a dataset: this is much more common. It doesn't make much sense to have dataframes as outputs in this case, since those would contain the same data as the input. There is no need to allocate double the amount of memory, for an exact copy of something we already have available (for read purposes). So, in those cases the output will be one or several lists, with the same amount of rows as the input dataframe. Those lists can then be easily assembled into a dataframe at a later stage, if the need arises. Requirements \u00b6 Since data will be the central object our application handles, we need to decide on an internal (as well as import/export) data format. The obvious thing to do would be to use the most common format (probably json), and just use that. For several reasons (layed out in the data_formats document ), I don't think this is a good idea in our case. I think we can anticipate our main requirements on a data format before writing any code, which is why I created this document: to list those requirements, and to come up with a recommendation that is based upon them. Technical requirements \u00b6 schema-aware (ideally included in the format) binary format (performance, filesize) column-based (for tabular data -- analytics query performance) zero-copy, memory-mapping compression in-build (preferrable) possible to use from different programming languages (at least Python & JS) as little cpu, memory, and disk utilization as possible The first group of requirements is technical: we are creating an interactive application, which means we should at least spend some time optimizing for speed (in those instances where it's possbile). In addition: the more we know about our data and its 'shape', the less complex our code has to be, since that removes the need for investigating and validating data at multiple points along its journey. The latter can be achieved by using a data format that is schema aware (e.g. not csv), and ideally includes that schema as metadata in its specification, so we can query the data(-bytes) directly, without having to read seperate, external specifications. For the performance requirements, it's fairly easy to see why we should be looking for a binary, column-based format, that ideally has extra features like memory-mapping and compression. Last but not least we want to be able to access our data from different programming languages. Python and JavaScript support will be mandatatory, but being able to read from Julia and R would also be highly desirable. General requirements \u00b6 option to attach metadata versioning of datasets versioning of metadata we want to be able to treat all data the same way, independent of size, format, other characteristics we want all of this to be more or less transparent to our end-users! Because it's in the nature of our application that we won't exactly know hob big and what shape the data we will be dealing with will have, we have to anticipate a wide range of types and sizes. In order to not have to deal with those differentely each time, it would be highly adventageous if we can come up with a standard 'interface' for our datasets, that lets us, as a minimum, query minimal required metadata (schema, authors, size), and which allows us to forward the dataset to other parts of our application (other modules, frontend), without having to convert or serialize/deserialize it. Most importantly, we will have to figure out a way to make most of this transparent to users. This is probably nothing a data format can help us with directly, but there might be factors in the periphery which can make this easier, or harder (e.g.: how common is that data-format, how much tooling exists for it?) One of our main requirements is to be able to easily attach metadata to our datasets. In addition we want it to be as easy as possible to 'version' the containing data, as well as the attached metadata. Those requirements stem from the need for good research data practices, and should not need further explanation. Let's look at those two points in a bit more detail: Technical metadata (automatic) \u00b6 data type schema (if tabular data) statistics on columns, rows (number of rows, min in column, max in column) data specific indicators/summaries (e.g. geographic range, resolution, ...) digest / checksum (unique id!) The most important metadata we'll be dealing with is the type of data, and its schema in case its in tabular form. As was mentioned above, ideally this would be forced and included by/in the data format we choose, so we can rely on it to be available, always. In addition, in a lot of cases it aids performance if certain characteristics of a dataset are known without having to actually touch it. One example would be min/max values for numeric columns. Geographic range, resolution could be interesting for location data, creation date for photos, and so on. A special item of metadata is a checksum: that enables us to confirm the bytes of a dataset haven't changed since we last checked, and it also makes things like caching or lookups easier. All of those metadata items can be created more or less automatically, without any user input. This is important to differentiate, because that means we don't need to worry about providing a user-interface so they can be added/attached. Other metadata (requires user input) \u00b6 provenance / lineage / heritage author(s)/creator(s) incl. contact info creation / modification date comments, annotations \"ALL THE METADATA\" (Angela R. Cunningham, PhD) The second category of metadata is defined by the necessity for manual user input (at least in parts). Which of course means we need to somehow provide a metadata editing facility for those items. Authorship information as well has the provenance-related metadata is arguably the most important one here. But I imagine we'll come up with quite a few more metadata fields we will want to be able to attach. It's probabaly a good idea to talk to our colleagues who develop Zotero and Tropy for some input in this regard. Dataset versioning \u00b6 versioning of the 'actual' data: new data added existing data changed/fixed existing data removed metadata versioning: independent of actual data changes (except for last modification dates, new authors added, checksum) new metadata added existing metadata changed/fixed metadata removed no new dataset version necessary Data versioning is usually a bit overlooked (although that seems to be changing now, and there are some 'git for data' services and tools cropping up). But it's crucial for good data practices. In order to always know how result data was created, we need to know exactly which inputs were used, and what exactly was done to them. If any of the inputs changes, and we don't record it, then there will be confusion later, when someone tries to recreate a result with the changed input. This implies we have a way to point to datasets using some sort of identifier, something like a DOI -- but it does not need to be globally unique, just locally (unless the data gets shared/exported). Contexts in which we handle data \u00b6 'onboarding' data: external data gets fed into a workflow / into our app we store a copy internally (to prevent changes we are not aware of) some minimal metadata needs to be provided (but can be at least partly determined automatically) gets unique id / alias(es) & checksum & version '1' internal data transformation & transfer: each module processes input data and creates output data output data gets fed into the input of another module input/output data is requested by frontend for display purposes (viz, statistics, table-view, ...) exporting data: researcher needs data in a specific format (csv, Excel, json...) for further processing, publishing, etc. Along with listing requirements, it makes sense to think about in which contexts we deal with data, and how. I think we can seperate three main areas: data onboarding internal data transformation & transfer data export For the first and last items the 'interface' of the data is important, which means we are concerned about how to translate external dataformat into our internal one, as well as the other way around. For the second item we only deal with our internal format, so performance and code complexity are more important considerations. For data onboarding, one thing is important is that we store a copy of the dataset the user points us to in a space where we can be sure the data doesn't get changed by external means. We would also add some automatic metadata, and might or might not require the user to provide some basic required metadata-fields manually. We would also give a newly onboarded dataset a version '1' (or maybe '1.0'). Data export is the least problematic area: since we have a minimal set of required metadata for every piece of data we use internally, it should be fairly trivial to export it into any viable export format (csv, excel, json, parquet,...). Data onboarding and export could also be combined in some scenarios: for example if we don't provide a tool to 'clean' up data (or do something else that would require a version change on the dataset) and users would have to do it externally, we could export the dataset into a temporary folder, let the user do their thing, and then re-import the changed dataset into a new version of the existing, internal one, copying the existing metadata with some additions that describe what was done to the data. Solution proposal \u00b6 Apache Arrow \u00b6 binary, column-based, language-independent in-memory format well defined schema and data types, rudimentary custom metadata support native support for 2 on-disk formats: feather (same as in-memory format), parquet client implementations for most relevant languages growing ecosystem: Arrow Flight (fast data transport framework) Plasma (In-Memory object store) Vaex (native support for memory-mapped feather files, memory-mapped querying) duckdb (column-based, python-native sql engine) easy import/export to NumPy/Pandas types (Arrays, DataFrames) -- still some serialization cost likely to be the standard format for data exchange in data science/data engineering in the future In my research, Apache Arrow came closest to match our technical requirements, and should let us implement most of the other ones too. It is a binary, column based in-memory format that comes with implementations in a number of programming languages (incl. the ones we are interested in). From the Arrow website: Apache Arrow is a software development platform for building high performance applications that process and transport large data sets. It is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system or programming language to another. A critical component of Apache Arrow is its in-memory columnar format, a standardized, language-agnostic specification for representing structured, table-like datasets in-memory. This data format has a rich data type system (included nested and user-defined data types) designed to support the needs of analytic database systems, data frame libraries, and more. In addition to the efficient in-memory format, it supports 2 on-disk formats: feather & parquet. The former one is basically the same as the in-memory format (with all the advantages that come with that), and the latter is a fairly standard format to exchange large(-ish) datasets between processes and infrastructure components. In my opinion (and I'm not alone), Arrow will be the de-facto standard data format for tabular data in the future, in both data science and data engineering. It is well designed, and a lot of the reasons why it came about line up fairly well with our own requirements (althought, at a different scale obviously). Because of that, there is a rich tooling ecosystem growing around Apache Arrow at the moment, which I think we can expect to satisfy to most of our current and future needs in the near to medium-term future, if not already. Esp. vaex and duckdb look like very interesting developments. Pandas and Numpy import/export is very well supported, and as well optimized as seems possible. Apache Arrow Flight and the Plasma Object store look like good contenders that could handle our potential data transport needs in the future. Identifying and versioning datasets \u00b6 every dataset gets it's unique id (uuid) as well as one or several user-defined and automatic aliases a new version of a dataset is created when its data content changes (content can be entirely different) a user can 'designate' a new version of data, in some cases it can be done by our application automatically versioning of metadata is independent of dataset version allows us to discover 'out-of-date' results (via their connected input-ids), and recreating them with updated input dataset frontend must be able to list, access and query datasets/inputs/outputs via unique id & version number It should be obvious that and why we need some sort of (at least internal) unique identifier for each dataset. The main scenario where users will come in touch with such an identifier is when they are asked to choose an input dataset for a module/workflow. It's possible to make that 100% transparent to the user, and let them for example select a folder of csv files, which we would then copy into our internal data repository, assign it an id, and use that for our calculation. That would mean though, that the next time the user wants to use the same dataset again, we would do the same again, basically duplicating our internal dataset. We probably could be smart about it, and recognize those sort of duplicates, but that would involve fairly complex and fragile code I think we should rather avoid, and come up with an interface metaphor/language that makes users aware what is going on, and which empowers them with proper tooling to manage their research data according using best practices (metadata, versioning, etc.). So, I propose that we should have a 'data management' section in our application UI, which could be used to both 'onboard' and manage datasets independent of a workflow, but also within the context of a workflow (for example by re-using some of the file selection widgets and filling in a newly create dataset id into a workflow input, right after onboarding). How that would look like exactly, we'd have to figure out and I think it would be a work-item on itself. The same goes for dataset versioning. One way I can imagine this working is to have a .<major>.<minor> postfix to our unique dataset identifier, where the minor part gets incremented with every metadata version change, and the major part for when the actual data changes. Another point to consider is whether to only use version number increases, or also have a concept of 'branching', where the versions of datasets can diverge, from a common parent. I think there is a point to be made for not making things to complicated unless really necessary, so most of this can be solved with a simple versioning scheme, and assigning totally new datasets id if something significant changes in the data of a dataset (while potentially preserving the lineage information by storing the 'parent id' in the new datasets metadata). But, as I said above, I think this would be a good item to investigate independently. Storing module results \u00b6 requirements: workflow history & snapshots & long running processes need for caching of at least the latest results This section includes a quick recapitulation how our workflows are described and managed by the backend, as well as an outline how to handle and store temporary as well as final workflow outputs. This is important, because having access to already computed results is necessary for some of our requirements (derived from our user-stories): - workflow history: enable the user to move back in the history of input sets of a workflow session - snapshots: 'tag' certain input sets (basically creating a snapshot of that particular workflow state) - support for long running processes: a user will want to have access to computational results, even if the had other workflow sessions inbetween (while a particularly long running job was running) Quick recap: workflow modularity \u00b6 Every module has: - one or several named inputs - one or several named outputs - as well as schema information for each input and output A workflow is a user-facing entity that: - can also be used as a module (has inputs, outputs, schema) - contains one or several modules - where some inputs of some (internal) modules can be connected to an output of another (internal) module - inputs of modules that are not connected to an output of another (internal) module are user inputs In this example we'll use a workflow that is simlates a nand logic-gate. Such a logic gate can be created by using and and not logic gates one after the other. Below you can see a short description of the modules and their inputs, as well as how that would be configured in a workflow description json file. The important part is the modules value. example module: nand \u00b6 consists of two other modules: and inputs: a & b (booleans) output: y (boolean - true if both inputs are true, otherwise false) not : input: a (boolean - connected to y output of and ) output: y (boolean - negated input) two inputs: a & b (booleans, connect directly to and inputs) one output: y (false if 'a' & 'b' are true, otherwise true -- connects to y output of not module) print ( \"Module description: [b]nand[/b]\" ) print ( get_data_from_file ( os . path . join ( DHARPA_TOOLBOX_DEFAULT_WORKFLOWS_FOLDER , \"logic_gates\" , \"nand.json\" ))) Module description: nand { 'modules' : [ { 'module_type' : 'and' } , { 'module_type' : 'not' , 'input_links' : { 'a' : 'and.y' }} ] , 'input_aliases' : { 'and__a' : 'a' , 'and__b' : 'b' } , 'output_aliases' : { 'not__y' : 'y' } , 'module_type_name' : 'nand' , 'meta' : { 'doc' : \"Returns 'True' if both inputs are 'False'.\" } } After creating the workflow description file, we create the workflow object in code, and for illustration purposes, we display the execution order and the state graph of the workflow (in its inital, stale state without any inputs). workflow = dharpa . create_workflow ( \"nand\" ) graph_to_image ( workflow . structure . execution_graph ) graph_to_image ( workflow . create_state_graph ( show_structure = True )) Now, we set the inputs (both True , which means the end-result should be False ). As you can see from the state graph, the workflow inputs are directly connected to the module inputs of the and module. workflow . inputs . a = True workflow . inputs . b = True await workflow . process () processing started: nand.nand processing started: nand.and processing finished: nand.and processing started: nand.not processing finished: nand.not processing finished: nand.nand Again, lets look at the workflow state, this time we display it using a json data structure, not a network graph: state = workflow . to_dict ( include_structure = True ) print ( state ) { 'alias' : 'nand' , 'address' : 'nand.nand' , 'type' : 'nand' , 'is_pipeline' : True , 'state' : 'results_ready' , 'inputs' : { 'a' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } , 'b' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'outputs' : { 'y' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : False }} , 'execution_stage' : None , 'doc' : \"Returns 'True' if both inputs are 'False'.\" , 'pipeline_structure' : { 'workflow_id' : 'nand' , 'modules' : [ { 'module' : { 'alias' : 'and' , 'address' : 'nand.and' , 'type' : 'and' , 'is_pipeline' : False , 'state' : 'results_ready' , 'inputs' : { 'a' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } , 'b' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'outputs' : { 'y' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'execution_stage' : 1 , 'doc' : \"Returns 'True' if both inputs are 'True'.\" , 'pipeline_structure' : None } , 'input_connections' : { 'a' : '__parent__.a' , 'b' : '__parent__.b' } , 'output_connections' : { 'y' : [ 'not.a' ]} } , { 'module' : { 'alias' : 'not' , 'address' : 'nand.not' , 'type' : 'not' , 'is_pipeline' : False , 'state' : 'results_ready' , 'inputs' : { 'a' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'outputs' : { 'y' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : False } } , 'execution_stage' : 2 , 'doc' : 'Negates the input.' , 'pipeline_structure' : None } , 'input_connections' : { 'a' : 'and.y' } , 'output_connections' : { 'y' : [ '__parent__.y' ]} } ] , 'workflow_input_connections' : { 'a' : [ 'and.a' ] , 'b' : [ 'and.b' ]} , 'workflow_output_connections' : { 'y' : 'not.y' } } } How to actually deal with workflow/module outputs? \u00b6 why not store all results? smart way of storing/deleting/managing storage: compression efficient module design cleanup process only store results if good execution time/result size ratio, otherwise just re-process To satisfy the above mentioned requirements, my current plan is to just store all results of all module runs, instead of coming up with a complicated caching scheme. There will have to be some sort of 'result-cleaning' and consolidation, but I think if we are being smart about it this might be the most promising strategy, which will introduce the least amount of complexity. A folder structure to accomodate that would probably look something like this: each module has its own name/id, all results for a module will be stored under same folder 'result.feather' has one or several columns that represent output values also, one column with runtime metadata (execution time, version of workflow, etc.) this works well with the 'dataset' API in Apache Arrow: https://arrow.apache.org/docs/python/dataset.html (which means we can lazy-load all results of a workflow/module into the same dataframe, and do 'meta'-queries and -analysis on that if we choose to) debatable whether 'workflow-results' have to be stored at all, since they are just copies of 'module-results' In order to not waste too much hard-disk space (which would be the most obvious concern here), I think we have a few different options. For one, we'd store all results with compression enabled. We would implement our modules in an efficient way that is aware of how we store results. We might have a cleanup process running in the background that is aware of how often a result is accessed, and how it's compute-time/result-size ratio is. In some cases where that ratio leans very much towards result-size, we might decide to not store those results at all, but re-process every time. Streaming module results \u00b6 TBD This is an area I haven't done too much work on yet, but in general: we will want to have access to intermediate results (or, rather: partial results in real-time), so we can provide the user with information they can use to determine whether to cancel a running process or not. Even though we will probably not have that functionality available in our initial, first version, I think we should anticipate that requirement, and design our data management with it in mind, so it can be added later without having to re-write a lot of code. Default data format (for import/export) \u00b6 every result can be described by specifying: the input dataset(s) and other inputs the workflow (and workflow version) that was used to produce it -> theoretically, every (result) dataset can be described by very small json file/metadata set proposal: invent our own (small) set of file formats (including version-nr, metadata schema, payload) Apache Arrow based for tabular/scalar data folder/zip based for binary data all our import modules would create files in that format provide tooling (and modules) to convert/export those to all common data formats possibility of data registries: very simple implementation compared to products like dataverse, ckan high performance data transfer (using Apache Flight) different levels: local (within our app), organization-wide, global (aka default registry) The last thing to decide is whether we want to provide a 'standard' data format for our application. This will be modelled closely upon the format we will use internally, but with some added metadata fields and possibly restrictions. This is mostly for the purpose of sharing, transferring, and publishing data. In principle, there is a really lightweight way to share our work: since we can describe everything we do by specifying the workflow, and listing all the inputs we use with it. Assuming all inputs are either scalars or, in case of datasets, available via download, this description could be very lightweight: it's just a json file containing the workflow structure (incl. maybe version information), and input-data urls. With that, everyone with access to the data can in theory replicate any end- and intermediate result. In theory, that json structure can also be attached to every result dataset, which means that our results will always come with information how they were produced (and how to re-produce them). Since all this is very dependent on being able to have access to metadata alongside the 'actual' data, and because in my experience systems and architectures that store metadata seperately to data are either fairly complex, specific and hard to maintain, I would propose we come up with a way to package our data in a way that allows for our metadata to always be included, and where it's easy to access both data and metadata without having to open the whole file. Arrow gets us a long way toward that (for tabular data), the only thing that is missing is a standard way to include metadata. For that we have two options: use the Arrow 'metadata' field (which is fairly limited, it only takes encoded byte-arrays as keys/values), or store our metadata in a seperate column. Currently, I'm leaning toward the latter option, but this is something we'll have to try out and play with to get a better idea how feasable it is. For other types of data (binary blobs, images, etc.), I propose we use an archive format (zip, tar, ...) with a json file at a standard location (e.g. './.metadata.json') that includes the same metadata schema a tabular dataset would use. That way our datasets always have the same 'interface'. And we can provide a set of standard tools (which could be implemented as workflow modules and workflows) to import and export 'our' data from/to commonly used formats like csv, excel, etc (which in most cases would not include metadata at all).","title":"Requirements"},{"location":"architecture/data/requirements/#context-requirements","text":"","title":"Context &amp; Requirements"},{"location":"architecture/data/requirements/#types-of-data","text":"scalars (mostly user inputs, booleans, strings, enums, numbers) lists of items of the same type tabular data (a collection of lists of items of the same type, each with the same number of items, incl. schema) binary data: images, videos, audio files In our application, we'll deal with a few basic types of data: scalars (mostly user inputs, booleans, strings, enums, numbers) lists of items of the same type tabular data (a collection of lists of items of the same type, each with the same number of items, incl. schema) binary data: images, videos, audio files Sidenote : I consider every kind of user input as data, since it is conceptually the same thing and needs to be recorded and managed the same way. For our purpose, we can ignore scalars because they are easy and cheap to handle, and can be attached to any sort of data or metadata in a few different ways. Also, let's ignore binary data for now, while acknowledging that we will need a strategy to deal with efficiently, in a way that is not too different from how we deal with other types. Which leaves us with lists and tabular data. Those are different to scalars, because there is no telling in advance how many rows they will have, and how large its cells will be (aka 'how many bytes are we dealing with, KBs, MBs, GBs, TBs?'). List (arrays) will be our main data type, along with tables (dataframes) -- the latter are really just lists of lists (including a schema/description of the type of each list). In a lot of cases a module will receive a table, and the output will be a list of the same length as the table. When using Pandas, we usually assign dataframes to variables, this is handy because we have access to the whole dataset via a single variable, and can access the columns seperately via their names. For our case, because we will have connected modules, we will probably deal with 2 scenarios: a module changes the data in a dataframe in one or several columns: this will be rare, but in this case the result of such a module will be a new dataframe a module adds one or several column to a dataset: this is much more common. It doesn't make much sense to have dataframes as outputs in this case, since those would contain the same data as the input. There is no need to allocate double the amount of memory, for an exact copy of something we already have available (for read purposes). So, in those cases the output will be one or several lists, with the same amount of rows as the input dataframe. Those lists can then be easily assembled into a dataframe at a later stage, if the need arises.","title":"Types of data"},{"location":"architecture/data/requirements/#requirements","text":"Since data will be the central object our application handles, we need to decide on an internal (as well as import/export) data format. The obvious thing to do would be to use the most common format (probably json), and just use that. For several reasons (layed out in the data_formats document ), I don't think this is a good idea in our case. I think we can anticipate our main requirements on a data format before writing any code, which is why I created this document: to list those requirements, and to come up with a recommendation that is based upon them.","title":"Requirements"},{"location":"architecture/data/requirements/#technical-requirements","text":"schema-aware (ideally included in the format) binary format (performance, filesize) column-based (for tabular data -- analytics query performance) zero-copy, memory-mapping compression in-build (preferrable) possible to use from different programming languages (at least Python & JS) as little cpu, memory, and disk utilization as possible The first group of requirements is technical: we are creating an interactive application, which means we should at least spend some time optimizing for speed (in those instances where it's possbile). In addition: the more we know about our data and its 'shape', the less complex our code has to be, since that removes the need for investigating and validating data at multiple points along its journey. The latter can be achieved by using a data format that is schema aware (e.g. not csv), and ideally includes that schema as metadata in its specification, so we can query the data(-bytes) directly, without having to read seperate, external specifications. For the performance requirements, it's fairly easy to see why we should be looking for a binary, column-based format, that ideally has extra features like memory-mapping and compression. Last but not least we want to be able to access our data from different programming languages. Python and JavaScript support will be mandatatory, but being able to read from Julia and R would also be highly desirable.","title":"Technical requirements"},{"location":"architecture/data/requirements/#general-requirements","text":"option to attach metadata versioning of datasets versioning of metadata we want to be able to treat all data the same way, independent of size, format, other characteristics we want all of this to be more or less transparent to our end-users! Because it's in the nature of our application that we won't exactly know hob big and what shape the data we will be dealing with will have, we have to anticipate a wide range of types and sizes. In order to not have to deal with those differentely each time, it would be highly adventageous if we can come up with a standard 'interface' for our datasets, that lets us, as a minimum, query minimal required metadata (schema, authors, size), and which allows us to forward the dataset to other parts of our application (other modules, frontend), without having to convert or serialize/deserialize it. Most importantly, we will have to figure out a way to make most of this transparent to users. This is probably nothing a data format can help us with directly, but there might be factors in the periphery which can make this easier, or harder (e.g.: how common is that data-format, how much tooling exists for it?) One of our main requirements is to be able to easily attach metadata to our datasets. In addition we want it to be as easy as possible to 'version' the containing data, as well as the attached metadata. Those requirements stem from the need for good research data practices, and should not need further explanation. Let's look at those two points in a bit more detail:","title":"General requirements"},{"location":"architecture/data/requirements/#dataset-versioning","text":"versioning of the 'actual' data: new data added existing data changed/fixed existing data removed metadata versioning: independent of actual data changes (except for last modification dates, new authors added, checksum) new metadata added existing metadata changed/fixed metadata removed no new dataset version necessary Data versioning is usually a bit overlooked (although that seems to be changing now, and there are some 'git for data' services and tools cropping up). But it's crucial for good data practices. In order to always know how result data was created, we need to know exactly which inputs were used, and what exactly was done to them. If any of the inputs changes, and we don't record it, then there will be confusion later, when someone tries to recreate a result with the changed input. This implies we have a way to point to datasets using some sort of identifier, something like a DOI -- but it does not need to be globally unique, just locally (unless the data gets shared/exported).","title":"Dataset versioning"},{"location":"architecture/data/requirements/#contexts-in-which-we-handle-data","text":"'onboarding' data: external data gets fed into a workflow / into our app we store a copy internally (to prevent changes we are not aware of) some minimal metadata needs to be provided (but can be at least partly determined automatically) gets unique id / alias(es) & checksum & version '1' internal data transformation & transfer: each module processes input data and creates output data output data gets fed into the input of another module input/output data is requested by frontend for display purposes (viz, statistics, table-view, ...) exporting data: researcher needs data in a specific format (csv, Excel, json...) for further processing, publishing, etc. Along with listing requirements, it makes sense to think about in which contexts we deal with data, and how. I think we can seperate three main areas: data onboarding internal data transformation & transfer data export For the first and last items the 'interface' of the data is important, which means we are concerned about how to translate external dataformat into our internal one, as well as the other way around. For the second item we only deal with our internal format, so performance and code complexity are more important considerations. For data onboarding, one thing is important is that we store a copy of the dataset the user points us to in a space where we can be sure the data doesn't get changed by external means. We would also add some automatic metadata, and might or might not require the user to provide some basic required metadata-fields manually. We would also give a newly onboarded dataset a version '1' (or maybe '1.0'). Data export is the least problematic area: since we have a minimal set of required metadata for every piece of data we use internally, it should be fairly trivial to export it into any viable export format (csv, excel, json, parquet,...). Data onboarding and export could also be combined in some scenarios: for example if we don't provide a tool to 'clean' up data (or do something else that would require a version change on the dataset) and users would have to do it externally, we could export the dataset into a temporary folder, let the user do their thing, and then re-import the changed dataset into a new version of the existing, internal one, copying the existing metadata with some additions that describe what was done to the data.","title":"Contexts in which we handle data"},{"location":"architecture/data/requirements/#solution-proposal","text":"","title":"Solution proposal"},{"location":"architecture/data/requirements/#apache-arrow","text":"binary, column-based, language-independent in-memory format well defined schema and data types, rudimentary custom metadata support native support for 2 on-disk formats: feather (same as in-memory format), parquet client implementations for most relevant languages growing ecosystem: Arrow Flight (fast data transport framework) Plasma (In-Memory object store) Vaex (native support for memory-mapped feather files, memory-mapped querying) duckdb (column-based, python-native sql engine) easy import/export to NumPy/Pandas types (Arrays, DataFrames) -- still some serialization cost likely to be the standard format for data exchange in data science/data engineering in the future In my research, Apache Arrow came closest to match our technical requirements, and should let us implement most of the other ones too. It is a binary, column based in-memory format that comes with implementations in a number of programming languages (incl. the ones we are interested in). From the Arrow website: Apache Arrow is a software development platform for building high performance applications that process and transport large data sets. It is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system or programming language to another. A critical component of Apache Arrow is its in-memory columnar format, a standardized, language-agnostic specification for representing structured, table-like datasets in-memory. This data format has a rich data type system (included nested and user-defined data types) designed to support the needs of analytic database systems, data frame libraries, and more. In addition to the efficient in-memory format, it supports 2 on-disk formats: feather & parquet. The former one is basically the same as the in-memory format (with all the advantages that come with that), and the latter is a fairly standard format to exchange large(-ish) datasets between processes and infrastructure components. In my opinion (and I'm not alone), Arrow will be the de-facto standard data format for tabular data in the future, in both data science and data engineering. It is well designed, and a lot of the reasons why it came about line up fairly well with our own requirements (althought, at a different scale obviously). Because of that, there is a rich tooling ecosystem growing around Apache Arrow at the moment, which I think we can expect to satisfy to most of our current and future needs in the near to medium-term future, if not already. Esp. vaex and duckdb look like very interesting developments. Pandas and Numpy import/export is very well supported, and as well optimized as seems possible. Apache Arrow Flight and the Plasma Object store look like good contenders that could handle our potential data transport needs in the future.","title":"Apache Arrow"},{"location":"architecture/data/requirements/#identifying-and-versioning-datasets","text":"every dataset gets it's unique id (uuid) as well as one or several user-defined and automatic aliases a new version of a dataset is created when its data content changes (content can be entirely different) a user can 'designate' a new version of data, in some cases it can be done by our application automatically versioning of metadata is independent of dataset version allows us to discover 'out-of-date' results (via their connected input-ids), and recreating them with updated input dataset frontend must be able to list, access and query datasets/inputs/outputs via unique id & version number It should be obvious that and why we need some sort of (at least internal) unique identifier for each dataset. The main scenario where users will come in touch with such an identifier is when they are asked to choose an input dataset for a module/workflow. It's possible to make that 100% transparent to the user, and let them for example select a folder of csv files, which we would then copy into our internal data repository, assign it an id, and use that for our calculation. That would mean though, that the next time the user wants to use the same dataset again, we would do the same again, basically duplicating our internal dataset. We probably could be smart about it, and recognize those sort of duplicates, but that would involve fairly complex and fragile code I think we should rather avoid, and come up with an interface metaphor/language that makes users aware what is going on, and which empowers them with proper tooling to manage their research data according using best practices (metadata, versioning, etc.). So, I propose that we should have a 'data management' section in our application UI, which could be used to both 'onboard' and manage datasets independent of a workflow, but also within the context of a workflow (for example by re-using some of the file selection widgets and filling in a newly create dataset id into a workflow input, right after onboarding). How that would look like exactly, we'd have to figure out and I think it would be a work-item on itself. The same goes for dataset versioning. One way I can imagine this working is to have a .<major>.<minor> postfix to our unique dataset identifier, where the minor part gets incremented with every metadata version change, and the major part for when the actual data changes. Another point to consider is whether to only use version number increases, or also have a concept of 'branching', where the versions of datasets can diverge, from a common parent. I think there is a point to be made for not making things to complicated unless really necessary, so most of this can be solved with a simple versioning scheme, and assigning totally new datasets id if something significant changes in the data of a dataset (while potentially preserving the lineage information by storing the 'parent id' in the new datasets metadata). But, as I said above, I think this would be a good item to investigate independently.","title":"Identifying and versioning datasets"},{"location":"architecture/data/requirements/#storing-module-results","text":"requirements: workflow history & snapshots & long running processes need for caching of at least the latest results This section includes a quick recapitulation how our workflows are described and managed by the backend, as well as an outline how to handle and store temporary as well as final workflow outputs. This is important, because having access to already computed results is necessary for some of our requirements (derived from our user-stories): - workflow history: enable the user to move back in the history of input sets of a workflow session - snapshots: 'tag' certain input sets (basically creating a snapshot of that particular workflow state) - support for long running processes: a user will want to have access to computational results, even if the had other workflow sessions inbetween (while a particularly long running job was running)","title":"Storing module results"},{"location":"architecture/data/requirements/#quick-recap-workflow-modularity","text":"Every module has: - one or several named inputs - one or several named outputs - as well as schema information for each input and output A workflow is a user-facing entity that: - can also be used as a module (has inputs, outputs, schema) - contains one or several modules - where some inputs of some (internal) modules can be connected to an output of another (internal) module - inputs of modules that are not connected to an output of another (internal) module are user inputs In this example we'll use a workflow that is simlates a nand logic-gate. Such a logic gate can be created by using and and not logic gates one after the other. Below you can see a short description of the modules and their inputs, as well as how that would be configured in a workflow description json file. The important part is the modules value.","title":"Quick recap: workflow modularity"},{"location":"architecture/data/requirements/#example-module-nand","text":"consists of two other modules: and inputs: a & b (booleans) output: y (boolean - true if both inputs are true, otherwise false) not : input: a (boolean - connected to y output of and ) output: y (boolean - negated input) two inputs: a & b (booleans, connect directly to and inputs) one output: y (false if 'a' & 'b' are true, otherwise true -- connects to y output of not module) print ( \"Module description: [b]nand[/b]\" ) print ( get_data_from_file ( os . path . join ( DHARPA_TOOLBOX_DEFAULT_WORKFLOWS_FOLDER , \"logic_gates\" , \"nand.json\" ))) Module description: nand { 'modules' : [ { 'module_type' : 'and' } , { 'module_type' : 'not' , 'input_links' : { 'a' : 'and.y' }} ] , 'input_aliases' : { 'and__a' : 'a' , 'and__b' : 'b' } , 'output_aliases' : { 'not__y' : 'y' } , 'module_type_name' : 'nand' , 'meta' : { 'doc' : \"Returns 'True' if both inputs are 'False'.\" } } After creating the workflow description file, we create the workflow object in code, and for illustration purposes, we display the execution order and the state graph of the workflow (in its inital, stale state without any inputs). workflow = dharpa . create_workflow ( \"nand\" ) graph_to_image ( workflow . structure . execution_graph ) graph_to_image ( workflow . create_state_graph ( show_structure = True )) Now, we set the inputs (both True , which means the end-result should be False ). As you can see from the state graph, the workflow inputs are directly connected to the module inputs of the and module. workflow . inputs . a = True workflow . inputs . b = True await workflow . process () processing started: nand.nand processing started: nand.and processing finished: nand.and processing started: nand.not processing finished: nand.not processing finished: nand.nand Again, lets look at the workflow state, this time we display it using a json data structure, not a network graph: state = workflow . to_dict ( include_structure = True ) print ( state ) { 'alias' : 'nand' , 'address' : 'nand.nand' , 'type' : 'nand' , 'is_pipeline' : True , 'state' : 'results_ready' , 'inputs' : { 'a' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } , 'b' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'outputs' : { 'y' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : False }} , 'execution_stage' : None , 'doc' : \"Returns 'True' if both inputs are 'False'.\" , 'pipeline_structure' : { 'workflow_id' : 'nand' , 'modules' : [ { 'module' : { 'alias' : 'and' , 'address' : 'nand.and' , 'type' : 'and' , 'is_pipeline' : False , 'state' : 'results_ready' , 'inputs' : { 'a' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } , 'b' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'outputs' : { 'y' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'execution_stage' : 1 , 'doc' : \"Returns 'True' if both inputs are 'True'.\" , 'pipeline_structure' : None } , 'input_connections' : { 'a' : '__parent__.a' , 'b' : '__parent__.b' } , 'output_connections' : { 'y' : [ 'not.a' ]} } , { 'module' : { 'alias' : 'not' , 'address' : 'nand.not' , 'type' : 'not' , 'is_pipeline' : False , 'state' : 'results_ready' , 'inputs' : { 'a' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : True } } , 'outputs' : { 'y' : { 'schema' : { 'type' : 'boolean' , 'default' : None } , 'value' : False } } , 'execution_stage' : 2 , 'doc' : 'Negates the input.' , 'pipeline_structure' : None } , 'input_connections' : { 'a' : 'and.y' } , 'output_connections' : { 'y' : [ '__parent__.y' ]} } ] , 'workflow_input_connections' : { 'a' : [ 'and.a' ] , 'b' : [ 'and.b' ]} , 'workflow_output_connections' : { 'y' : 'not.y' } } }","title":"example module: nand"},{"location":"architecture/data/requirements/#how-to-actually-deal-with-workflowmodule-outputs","text":"why not store all results? smart way of storing/deleting/managing storage: compression efficient module design cleanup process only store results if good execution time/result size ratio, otherwise just re-process To satisfy the above mentioned requirements, my current plan is to just store all results of all module runs, instead of coming up with a complicated caching scheme. There will have to be some sort of 'result-cleaning' and consolidation, but I think if we are being smart about it this might be the most promising strategy, which will introduce the least amount of complexity. A folder structure to accomodate that would probably look something like this: each module has its own name/id, all results for a module will be stored under same folder 'result.feather' has one or several columns that represent output values also, one column with runtime metadata (execution time, version of workflow, etc.) this works well with the 'dataset' API in Apache Arrow: https://arrow.apache.org/docs/python/dataset.html (which means we can lazy-load all results of a workflow/module into the same dataframe, and do 'meta'-queries and -analysis on that if we choose to) debatable whether 'workflow-results' have to be stored at all, since they are just copies of 'module-results' In order to not waste too much hard-disk space (which would be the most obvious concern here), I think we have a few different options. For one, we'd store all results with compression enabled. We would implement our modules in an efficient way that is aware of how we store results. We might have a cleanup process running in the background that is aware of how often a result is accessed, and how it's compute-time/result-size ratio is. In some cases where that ratio leans very much towards result-size, we might decide to not store those results at all, but re-process every time.","title":"How to actually deal with workflow/module outputs?"},{"location":"architecture/data/requirements/#streaming-module-results","text":"TBD This is an area I haven't done too much work on yet, but in general: we will want to have access to intermediate results (or, rather: partial results in real-time), so we can provide the user with information they can use to determine whether to cancel a running process or not. Even though we will probably not have that functionality available in our initial, first version, I think we should anticipate that requirement, and design our data management with it in mind, so it can be added later without having to re-write a lot of code.","title":"Streaming module results"},{"location":"architecture/data/requirements/#default-data-format-for-importexport","text":"every result can be described by specifying: the input dataset(s) and other inputs the workflow (and workflow version) that was used to produce it -> theoretically, every (result) dataset can be described by very small json file/metadata set proposal: invent our own (small) set of file formats (including version-nr, metadata schema, payload) Apache Arrow based for tabular/scalar data folder/zip based for binary data all our import modules would create files in that format provide tooling (and modules) to convert/export those to all common data formats possibility of data registries: very simple implementation compared to products like dataverse, ckan high performance data transfer (using Apache Flight) different levels: local (within our app), organization-wide, global (aka default registry) The last thing to decide is whether we want to provide a 'standard' data format for our application. This will be modelled closely upon the format we will use internally, but with some added metadata fields and possibly restrictions. This is mostly for the purpose of sharing, transferring, and publishing data. In principle, there is a really lightweight way to share our work: since we can describe everything we do by specifying the workflow, and listing all the inputs we use with it. Assuming all inputs are either scalars or, in case of datasets, available via download, this description could be very lightweight: it's just a json file containing the workflow structure (incl. maybe version information), and input-data urls. With that, everyone with access to the data can in theory replicate any end- and intermediate result. In theory, that json structure can also be attached to every result dataset, which means that our results will always come with information how they were produced (and how to re-produce them). Since all this is very dependent on being able to have access to metadata alongside the 'actual' data, and because in my experience systems and architectures that store metadata seperately to data are either fairly complex, specific and hard to maintain, I would propose we come up with a way to package our data in a way that allows for our metadata to always be included, and where it's easy to access both data and metadata without having to open the whole file. Arrow gets us a long way toward that (for tabular data), the only thing that is missing is a standard way to include metadata. For that we have two options: use the Arrow 'metadata' field (which is fairly limited, it only takes encoded byte-arrays as keys/values), or store our metadata in a seperate column. Currently, I'm leaning toward the latter option, but this is something we'll have to try out and play with to get a better idea how feasable it is. For other types of data (binary blobs, images, etc.), I propose we use an archive format (zip, tar, ...) with a json file at a standard location (e.g. './.metadata.json') that includes the same metadata schema a tabular dataset would use. That way our datasets always have the same 'interface'. And we can provide a set of standard tools (which could be implemented as workflow modules and workflows) to import and export 'our' data from/to commonly used formats like csv, excel, etc (which in most cases would not include metadata at all).","title":"Default data format (for import/export)"},{"location":"architecture/workflows/","text":"If we accept the premise that in the computational context we are really only interested in structured data, it follows that there must be also 'things' that do stuff to our structured data. Let's call those things 'workflows'. Definition \u00b6 I will concede that 'doing stuff to data' although entirely accurate is probably not the most useful of definitions. Not Websters, all right? Well, how about: \"A workflow is a tool to transform data into more structured data.\" \"more\" can be read in one or all of those ways: 'more data' -- we'll create what can be considered 'new' data out of the existing set 'better structured' -- improve (and replace) the current structure (fix errors, etc.) 'more structure' -- augment existing data with additional structure In our context, workflows can also have secondary outcomes: present data in different, more intuitive ways (e.g. visualisations), which researchers can use to get different ideas about the data, or new research questions convert structured data into equivalent structured data, just a different format (e.g csv to Excel spreadsheet) ... (I'm sure there's more, just can't think of anything important right now) Deconstructing a workflow \u00b6 I've written more about it here , but conceptually, every data workflow is a collection of interconnected modules, where outputs of some modules are connected to inputs of some other modules. The resulting network graph of modules defines a workflow. That's even the case for Jupyter notebooks (which are really just fancy Python/R/Julia scripts); if you squint a bit you can see it: the modules are functions that you call with some inputs, and you use the outputs of those functions (stored in variables) as inputs to other functions. Move along, nothing to see here: this is really just how (most) programs work. As I see it, there are three main differences to programs that are written in 'normal' computer engineering: the complexity of the resulting interconnected 'network graph' (the interconnection of functions) is usually lower it's a tad easier (or at least possible) to define, separate and re-use the building blocks needed in a majority of workflows (an example would be Numpy or the Pandas libraries, which are basically implementations of abstract problems that crop up often in this domain) it is possible to create workflows entirely out of modules that were previously created, with no or almost no other customization (normally, that customization would be very prominent in a program) -- often-times only some 'glue' code is needed This means that data engineering workflows could be considered relatively simple script-like applications, where advanced concepts like Object-Oriented-Design, Encapsulation, DRY, YAGNI, ... are not necessary or relevant (in most cases they wouldn't hurt though). Data engineering \u00b6 This way of looking at workflows is nothing new, there are quite a few tools and projects in the data engineering space which deal with workflows in one level of abstraction or another. As I'll point out below, the main difference to what we try to implement is that we'll add an element of 'interactivity'. But I believe we can still learn a whole lot by looking at some aspects of those other tools. I encourage everyone remotely interested to look up some of those projects, and maybe not read the whole documentation, but at least the 'Why-we-created-yet-another-data-orchestrator', 'Why-we-are-better-than-comparable-projects' as well as 'What-we-learned'-type documentation pages you come across. 'I-tried-project-XXX-and-it-is-crap'-blog posts as well as hackernews comment-threads related to those projects are usually also interesting. The '/r/dataengineering' and '/r/datascience' sub-reddits are ok. But they are on Reddit, so, signal-to-noise is a bit, well.. Among others, interesting projects include: dagster prefect airflow luigi also relevant, but less data-engineering-y: Node-RED, Apache NiFi, IFTTT, Zapier, Huginn, ... The 'workflow lifecycle' \u00b6 One thing that I think is also worth considering is the different stages in the lifecycle of a workflow. For illustration, I'll describe how each of those stages relates to the way data science is currently done with Jupyter, which is probably the most used tool in this space at the moment. Workflow creation \u00b6 This is the act of designing and implementing a new workflow transformed into one or a set of defined outcomes (which can be new data, or just a visualization, doesn't matter). The actual creation of the workflow is similar to developing a script or application, and offers some freedom on how to implement it (e.g. which supporting libraries to choose, whether and which defaults to set, ...). In the Jupyter-case, this would be the iterative development of a Jupyter notebook, with one cell added after the other. One thing that is different for us is that we will have a much stricter definition of the desired outcome of our workflow, whereas the creation of a Jupyter notebook is typically way more open-ended, and a researcher would easily be able to 'follow some leads' they come across while working on the whole thing. This is a very important distinction that pays to keep in mind, and I can't emphasize this enough: the workflows we are dealing with are a lot more 'static' than typical Jupyter notebooks, because we have decided in advance which ones to implement, and how to implement them. There is not much we can do about this, and it's a trade-off with very little room to negotiate. This has a few important implications on how our product is different from how data science is done by Jupyter users currently. I will probably mention this again and again, because it is not intuitive at first, but has a big impact on how we view what we are building! As per our core assumptions, end-users won't create new workflows, this is done by a group with a yet-to-be-determined 'special' skill set. Workflow execution \u00b6 This is when a 'finished' workflow gets run, with a set of inputs that are chosen by the user. The schema/type of those inputs is a requirement that has to be considered by the user. It's possible that a workflow allows for inputs to be in multiple formats, to make the users life easier (e.g. allow both '.csv' as well as '.json' formats), but that also has to be documented and communicated to users. It is not possible to add elements to a workflow, and make it do different things than it was designed to do. Our workflows are static, they never change (except in an 'iterative-development' sense where we release new versions)! Compare that to a researcher who created their own Jupyter notebook: they will have run the workflow itself countless times by then, while developing it. The execution phase is really only that last run that achieves the desired outcome, and which will 'fill' the notebook output cells with the final results. That notebook state is likely to be attached to a publication. Often the data is 'hardcoded' into the notebook itself (for example by adding the data itself in the git repo, and using a relative path to point to it in a notebook). It is also possible, although not as common (as far as I've seen -- I might be wrong here) that researchers spend a bit more time on the notebook and make the inputs easier to change, in order to be able to execute it with different parameters, quickly. This is more like what we will end up with, although I'd argue that the underlying workflow is still much easier to change, fix, and adapt than will be the case with our application. One difference between workflow creation and execution is that the creation part is more common for 'data scientists', and the execution part is a bigger concern for 'data engineers' (both do both, of course). I think, our specific problem sits more in the data engineering than data science space (because our main products are 'fixed'/'static' workflows), which is why I tend to look more for the tools used in that domain (data orchestrators, ...) than in the other (Jupyter, ..) when I look for guidance. Workflow publication \u00b6 Once a workflow is run with a set of inputs that yield a meaningful outcome for a researcher, it can be attached to a publication in some way. This has one main purpose: to document and explain the research methodologies that were used, on a different level than 'just' plain language. There is a long-term, idealistic goal of being able to replicate results, but the general sentiment is that it is unrealistic to attempt that at this stage. It doesn't hurt to consider it a sort of 'guiding light', though. It is getting more and more common for researchers to attach Jupyter notebooks to a publication. Jupyter notebooks are a decent fit for this purpose, because the contain plain-text documentation, the actual code, as well as the output(s) of the code in a single file, that has a predictable, well specified format (json, along with a required document schema). As our colleagues at the DHJ project have discovered, it's not a perfect fit, but it can be bent to serve as the basis for a formal, digital publication. In our case, it is my understanding that we would like to have an artefact like this too, and even though it's not one of the 'core' requirements, it would be a very nice thing to have. One strong option is for us to re-use Jupyter notebooks for that. Depending on how we implement our solution, we might already have one as our core element that 'holds' a workflow, in which case this is a non-issue. Or, if that is not the case, we could 'render' a notebook from the metadata we have available, which should also not be too hard to do since the target (the notebook) is well spec'ed. If that's the case, there is one thing I'd like to investigate before we commit though: what characteristics exactly are the ones that make notebooks a good choice for that, and which one are detrimental? As I've mentioned, the DHJ project uses notebooks as the base for creating article-(web)pages, and they came across some issues along the way. So I wonder: is there a better way to achieve the 'document and explain research methodologies' goal than by using a Jupyter notebook? How would that look in a perfect world? How much effort would be involved? Interactivity / Long(-ish) running computations \u00b6 One component that is different in our scenario to other implementations is the requirement for interactivity. In data-engineering, this is never an issue, you describe your pipeline, then you or someone else uses that with a set of inputs, and off it goes, without any further interaction. Plomp , notification, results, rinse, repeat. For us that will be different, because we are creating a graphical user interface that reflects the workflow, and its state. By definition, graphical user interfaces are interactive, and when a user triggers an action, they expect that to kick off some instant response in the UI (maybe the change in a visualization, or a progress indicator, whatever). Computationally trivial/non-trivial \u00b6 One main difficulty will be to find a good visual way to express what is happening to the user, ideally in the same way for 2 different scenarios: computations that are computationally trivial, and will return a result back in a few seconds at most computations that take longer In our workflows, I can see a few different ways those interactions can play out, depending on the characteristics of any particular workflow. So, in the case where a user 'uploads' data or changes a setting: if the whole workflow is trivial, computationally : this triggers the whole workflow to execute and return with a new state/result immediately, and the output elements reflect the new state without any noticable delay if only some (or no) components of the workflows are trivial, computationally : this triggers the execution of only parts of the workflow immediately (from the point of user input to the next non-trivial step of the workflow). all computationally non-trivial parts of the workflow will have some sort of \"Process\" button that users have to click manually to kick off those parts of the workflow. Otherwise the UI would be locked for an undefined amount of time after every user input -- which would result in a very bad UX). alternatively, workflows with computationally non-trivial parts could have one 'global' \"Process\" button, which would trigger the execution of the whole workflow with all current inputs/settings. There will be also inputs that don't directly kick off any processing (like for example control buttons in a visualisation). I think we can ignore those for now, because this is what UIs usually do, and this does not present a difficulty in terms of the overall UI/UX (just like the 'computationally trivial' workflow scenario). UI representations for the current workflow state \u00b6 tldr; \u00b6 In some cases it will be impossible for users to use a workflow fully interactively, because one or all workflow steps will take too much time, which means the interactive session has to be interrupted. In those cases (depending on our setup and other circumstances) we might need to include a 'job-management'/'queue' component to our application, which matches running/completed jobs to users and 'sessions'/'experiments' (for lack of a better word). We need to find a visual metaphors for workflows and workflow steps to make that intuitive, ideally in a way so that those scenarios are not handled too differently in comparison to how our 100%-interactive workflows are used and executed. In addition, we have to make sure our backend can deal with all the scenarios we want to support. Details, skip if you want \u00b6 I'll include some suggestions on how all this could look visually, but those are in no way to be taken as gospel. Just the most obvious (to me) visual elements to use, which I hope will make it easier to get my point across. It's probably obvious that the important cases we have to care about are the ones where there is non-trivial computation. I think we can roughly divide them into 4 categories: execution time of a few seconds : in this case a 'spinning-wheel'-progress indidcator is probably enough backend-wise, we (probably) don't have to worry (although, it's not a given this will not crash a hosted app if we have too many users and computations are executed 'in-line') execution time of a few minutes : not long enough so that for example a browser session would expire in this case it would be good UX-wise to have a semi-exact progress indicator that either shows a 'done'-percentage, or remaining time on the backend-side, we need to separate three scenarios: local app: the computation can happen locally, either in a new thread, or a different process (we can also make use of multiple cores if available) hosted jupyter in some form or other: the running Jupyter kernel can execute the computation, which is probably a good enough separation to not affect the hosted UI hosted web app: there needs to exist some infrastructure we can use to offload the computation, it can't run on the same host as our service (which means a lot of added complexity) there is no need yet for authentication apart from that we need to be able to assign the result of the computation to individual sessions execution time of a few hours : long enough that a user will have left the computer in between, or closed a browser, etc. now the separation of backend-scenarios kicks in earlier, and also affects the front-end: local app: as in the case before, the UI would display a progress-indicator of some sort the computation would happen as a background process, and as long as the user does not shut-down or restart the computer there is no issue (the job should even survive a suspend/hibernate) hosted jupyter: difficult to say, the computation could either still happen in the running Jupyter kernel, or would have to be farmed out to an external service one issue to be aware of is that, depending on how it is configured, Jupyter might or might not kill a notebook process (and underlying kernel) if there has been no activity in the browser for a while. We'd have to make sure this does not happen, or that we have some sort of user session management (which should be entirely possible -- but of course increases complexity by quite a bit). The latter will also be necessary if a user comes back to their session after having been disconnected in some way, because otherwise they'd loose their result. ui-wise there needs to be session and compute-job management, and a list of currently running and past jobs and links to the experiments that produced them hosted web app: as with the jupyter case, we'll need session as well as job management execution time of more than a few hours (days, weeks) : in all cases the computation now needs to be outsourced, and submitted to a compute service (cloud, HPC, local dask-cluster, whatever...) all cases need to implement some sort of session authentication and job management (which would probably be a bit more transparent to the user in the local case, but overall it would be implemented in a similar way in each scenario) Externally running computations \u00b6 One thing to stress is that 'outsourcing' computationally intensive tasks comes with a considerable amount of complexity. Nothing that can't be implemented, and there are several ways I can think of to do this. I'd still advise to be very aware of the cost and complexity this incurs. I do believe we will have to add that in some form at some stage though, if we are in any way successful and have people adopting our solution. Which means we have to include the issue in our architecture design, even if we only plan to implement it later.","title":"Workflows"},{"location":"architecture/workflows/#definition","text":"I will concede that 'doing stuff to data' although entirely accurate is probably not the most useful of definitions. Not Websters, all right? Well, how about: \"A workflow is a tool to transform data into more structured data.\" \"more\" can be read in one or all of those ways: 'more data' -- we'll create what can be considered 'new' data out of the existing set 'better structured' -- improve (and replace) the current structure (fix errors, etc.) 'more structure' -- augment existing data with additional structure In our context, workflows can also have secondary outcomes: present data in different, more intuitive ways (e.g. visualisations), which researchers can use to get different ideas about the data, or new research questions convert structured data into equivalent structured data, just a different format (e.g csv to Excel spreadsheet) ... (I'm sure there's more, just can't think of anything important right now)","title":"Definition"},{"location":"architecture/workflows/#deconstructing-a-workflow","text":"I've written more about it here , but conceptually, every data workflow is a collection of interconnected modules, where outputs of some modules are connected to inputs of some other modules. The resulting network graph of modules defines a workflow. That's even the case for Jupyter notebooks (which are really just fancy Python/R/Julia scripts); if you squint a bit you can see it: the modules are functions that you call with some inputs, and you use the outputs of those functions (stored in variables) as inputs to other functions. Move along, nothing to see here: this is really just how (most) programs work. As I see it, there are three main differences to programs that are written in 'normal' computer engineering: the complexity of the resulting interconnected 'network graph' (the interconnection of functions) is usually lower it's a tad easier (or at least possible) to define, separate and re-use the building blocks needed in a majority of workflows (an example would be Numpy or the Pandas libraries, which are basically implementations of abstract problems that crop up often in this domain) it is possible to create workflows entirely out of modules that were previously created, with no or almost no other customization (normally, that customization would be very prominent in a program) -- often-times only some 'glue' code is needed This means that data engineering workflows could be considered relatively simple script-like applications, where advanced concepts like Object-Oriented-Design, Encapsulation, DRY, YAGNI, ... are not necessary or relevant (in most cases they wouldn't hurt though).","title":"Deconstructing a workflow"},{"location":"architecture/workflows/#data-engineering","text":"This way of looking at workflows is nothing new, there are quite a few tools and projects in the data engineering space which deal with workflows in one level of abstraction or another. As I'll point out below, the main difference to what we try to implement is that we'll add an element of 'interactivity'. But I believe we can still learn a whole lot by looking at some aspects of those other tools. I encourage everyone remotely interested to look up some of those projects, and maybe not read the whole documentation, but at least the 'Why-we-created-yet-another-data-orchestrator', 'Why-we-are-better-than-comparable-projects' as well as 'What-we-learned'-type documentation pages you come across. 'I-tried-project-XXX-and-it-is-crap'-blog posts as well as hackernews comment-threads related to those projects are usually also interesting. The '/r/dataengineering' and '/r/datascience' sub-reddits are ok. But they are on Reddit, so, signal-to-noise is a bit, well.. Among others, interesting projects include: dagster prefect airflow luigi also relevant, but less data-engineering-y: Node-RED, Apache NiFi, IFTTT, Zapier, Huginn, ...","title":"Data engineering"},{"location":"architecture/workflows/#the-workflow-lifecycle","text":"One thing that I think is also worth considering is the different stages in the lifecycle of a workflow. For illustration, I'll describe how each of those stages relates to the way data science is currently done with Jupyter, which is probably the most used tool in this space at the moment.","title":"The 'workflow lifecycle'"},{"location":"architecture/workflows/#workflow-creation","text":"This is the act of designing and implementing a new workflow transformed into one or a set of defined outcomes (which can be new data, or just a visualization, doesn't matter). The actual creation of the workflow is similar to developing a script or application, and offers some freedom on how to implement it (e.g. which supporting libraries to choose, whether and which defaults to set, ...). In the Jupyter-case, this would be the iterative development of a Jupyter notebook, with one cell added after the other. One thing that is different for us is that we will have a much stricter definition of the desired outcome of our workflow, whereas the creation of a Jupyter notebook is typically way more open-ended, and a researcher would easily be able to 'follow some leads' they come across while working on the whole thing. This is a very important distinction that pays to keep in mind, and I can't emphasize this enough: the workflows we are dealing with are a lot more 'static' than typical Jupyter notebooks, because we have decided in advance which ones to implement, and how to implement them. There is not much we can do about this, and it's a trade-off with very little room to negotiate. This has a few important implications on how our product is different from how data science is done by Jupyter users currently. I will probably mention this again and again, because it is not intuitive at first, but has a big impact on how we view what we are building! As per our core assumptions, end-users won't create new workflows, this is done by a group with a yet-to-be-determined 'special' skill set.","title":"Workflow creation"},{"location":"architecture/workflows/#workflow-execution","text":"This is when a 'finished' workflow gets run, with a set of inputs that are chosen by the user. The schema/type of those inputs is a requirement that has to be considered by the user. It's possible that a workflow allows for inputs to be in multiple formats, to make the users life easier (e.g. allow both '.csv' as well as '.json' formats), but that also has to be documented and communicated to users. It is not possible to add elements to a workflow, and make it do different things than it was designed to do. Our workflows are static, they never change (except in an 'iterative-development' sense where we release new versions)! Compare that to a researcher who created their own Jupyter notebook: they will have run the workflow itself countless times by then, while developing it. The execution phase is really only that last run that achieves the desired outcome, and which will 'fill' the notebook output cells with the final results. That notebook state is likely to be attached to a publication. Often the data is 'hardcoded' into the notebook itself (for example by adding the data itself in the git repo, and using a relative path to point to it in a notebook). It is also possible, although not as common (as far as I've seen -- I might be wrong here) that researchers spend a bit more time on the notebook and make the inputs easier to change, in order to be able to execute it with different parameters, quickly. This is more like what we will end up with, although I'd argue that the underlying workflow is still much easier to change, fix, and adapt than will be the case with our application. One difference between workflow creation and execution is that the creation part is more common for 'data scientists', and the execution part is a bigger concern for 'data engineers' (both do both, of course). I think, our specific problem sits more in the data engineering than data science space (because our main products are 'fixed'/'static' workflows), which is why I tend to look more for the tools used in that domain (data orchestrators, ...) than in the other (Jupyter, ..) when I look for guidance.","title":"Workflow execution"},{"location":"architecture/workflows/#workflow-publication","text":"Once a workflow is run with a set of inputs that yield a meaningful outcome for a researcher, it can be attached to a publication in some way. This has one main purpose: to document and explain the research methodologies that were used, on a different level than 'just' plain language. There is a long-term, idealistic goal of being able to replicate results, but the general sentiment is that it is unrealistic to attempt that at this stage. It doesn't hurt to consider it a sort of 'guiding light', though. It is getting more and more common for researchers to attach Jupyter notebooks to a publication. Jupyter notebooks are a decent fit for this purpose, because the contain plain-text documentation, the actual code, as well as the output(s) of the code in a single file, that has a predictable, well specified format (json, along with a required document schema). As our colleagues at the DHJ project have discovered, it's not a perfect fit, but it can be bent to serve as the basis for a formal, digital publication. In our case, it is my understanding that we would like to have an artefact like this too, and even though it's not one of the 'core' requirements, it would be a very nice thing to have. One strong option is for us to re-use Jupyter notebooks for that. Depending on how we implement our solution, we might already have one as our core element that 'holds' a workflow, in which case this is a non-issue. Or, if that is not the case, we could 'render' a notebook from the metadata we have available, which should also not be too hard to do since the target (the notebook) is well spec'ed. If that's the case, there is one thing I'd like to investigate before we commit though: what characteristics exactly are the ones that make notebooks a good choice for that, and which one are detrimental? As I've mentioned, the DHJ project uses notebooks as the base for creating article-(web)pages, and they came across some issues along the way. So I wonder: is there a better way to achieve the 'document and explain research methodologies' goal than by using a Jupyter notebook? How would that look in a perfect world? How much effort would be involved?","title":"Workflow publication"},{"location":"architecture/workflows/#interactivity-long-ish-running-computations","text":"One component that is different in our scenario to other implementations is the requirement for interactivity. In data-engineering, this is never an issue, you describe your pipeline, then you or someone else uses that with a set of inputs, and off it goes, without any further interaction. Plomp , notification, results, rinse, repeat. For us that will be different, because we are creating a graphical user interface that reflects the workflow, and its state. By definition, graphical user interfaces are interactive, and when a user triggers an action, they expect that to kick off some instant response in the UI (maybe the change in a visualization, or a progress indicator, whatever).","title":"Interactivity / Long(-ish) running computations"},{"location":"architecture/workflows/#computationally-trivialnon-trivial","text":"One main difficulty will be to find a good visual way to express what is happening to the user, ideally in the same way for 2 different scenarios: computations that are computationally trivial, and will return a result back in a few seconds at most computations that take longer In our workflows, I can see a few different ways those interactions can play out, depending on the characteristics of any particular workflow. So, in the case where a user 'uploads' data or changes a setting: if the whole workflow is trivial, computationally : this triggers the whole workflow to execute and return with a new state/result immediately, and the output elements reflect the new state without any noticable delay if only some (or no) components of the workflows are trivial, computationally : this triggers the execution of only parts of the workflow immediately (from the point of user input to the next non-trivial step of the workflow). all computationally non-trivial parts of the workflow will have some sort of \"Process\" button that users have to click manually to kick off those parts of the workflow. Otherwise the UI would be locked for an undefined amount of time after every user input -- which would result in a very bad UX). alternatively, workflows with computationally non-trivial parts could have one 'global' \"Process\" button, which would trigger the execution of the whole workflow with all current inputs/settings. There will be also inputs that don't directly kick off any processing (like for example control buttons in a visualisation). I think we can ignore those for now, because this is what UIs usually do, and this does not present a difficulty in terms of the overall UI/UX (just like the 'computationally trivial' workflow scenario).","title":"Computationally trivial/non-trivial"},{"location":"architecture/workflows/#ui-representations-for-the-current-workflow-state","text":"","title":"UI representations for the current workflow state"},{"location":"architecture/workflows/#externally-running-computations","text":"One thing to stress is that 'outsourcing' computationally intensive tasks comes with a considerable amount of complexity. Nothing that can't be implemented, and there are several ways I can think of to do this. I'd still advise to be very aware of the cost and complexity this incurs. I do believe we will have to add that in some form at some stage though, if we are in any way successful and have people adopting our solution. Which means we have to include the issue in our architecture design, even if we only plan to implement it later.","title":"Externally running computations"},{"location":"architecture/workflows/modularity/modularity/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % autosave 0 import os from rich.jupyter import print from dharpa_toolbox.modules.utils import list_available_module_names , describe_module , print_module_desc , load_workflows , create_module from dharpa_toolbox.utils import print_file_content , graph_to_image from dharpa_toolbox.modules.workflows import DharpaWorkflow from dharpa_toolbox.rendering.jupyter.renderer import PlainJupyterWorkflowRenderer , ModuleJupyterWorkflowRenderer base_path = os . path . abspath ( \".\" ) var element = $('#3a0568f4-1716-412a-a157-4b70b7b15c27'); IPython.notebook.set_autosave_interval(0) Autosave disabled What's a workflow, really? \u00b6 Jupyter is a very good tool to create non-trivial exploratory workflows there's a difference between 'dynamic' workflows, and 'static' ones Jupyter is usually used to create workflows in a 'dynamic' way also important (for us): interactivity Currently, Jupyter is one of the most used technologies in digital research to create workflows. Although there are exceptions, in most cases it is used to explore a very specific research question. Jupyter is exceptionally good at that, which is the reason it is so successful. From a computer-engineering perspective, Jupyter notebooks are 'just' simple scripts, and often they include anti-patterns like global- as well as hard-coded variables, little to no encapsulation of functionality, etc. Which means that typically, Jupyter notebooks have (relatively) little value to other researchers, and re-usability is low. This is an acceptable trade-off though, because the problems they are solving are (usually) very niche and specific, so there is little downside to tailor the code to the exact problem one is having. In addition, Jupyter notebooks are very good to document the workflow itself, and communicate what is happening to the data (which is important for publication). If we want to create a tool that lets users run pre-created workflows, that equation changes though. Because, now the assumption is that the (comparatively few) workflows we create will be useful in not just a very specific way. The goal is to identify areas where people have (roughly) the same problem, and to solve that problem in a generic way that is useful to a larger group of people. The workflow will typically be less important in relation to the overall research project a researcher is working on (compared to a tailored, specific one), but from the perspective of a reasearcher it will also be much less hassle and expensive to use, since they don't have to create the workflow themselves, and someone else already has thought about all the options and parameters that make sense, has done the validation and testing, etc. Also, they don't have to learn programming if they don't already know it... This means that we are dealing now with a very 'static' workflow, compared to the 'dynamic' ones researchers with programming skills can create and change themselves very easily. Everything that can happen in a workflow is known in advance, and even though there can be 'forks' in the flow of data, those have to be defined, implemented and documented in advance. And that difference is why we should not assume that Jupyter notebooks are as good a vessel to implement such a workflow as they are in the other case, where all that can happen 'on the go'. It's still possible notebooks are a good fit here too, but we can't use our normal experience with -- and intuition about -- Jupyter to make that case. One other point that is important to note is user interactivity. Usually, when developing a Jupyter notebook inputs (data as well as parameters) are either hardcoded, or factored out into a variable that is changed on top of the notebook (or in some cells further down). And by running or re-running certain cells, those variables are re-set or changed. This works fine for dynamically creating a workflow (although, it's sometimes confusing, and one of the main criticisms against the Jupyter notebook approach). But, in a 'static' workflow, we need to make sure that a user can set or change all those inputs at any time, while making sure that the 'internal' state of our workflow is known to our engine. At a minimum, we need to know that our state is currently inconsistent after a user-input, and have a way to communicating that to the user so they can kick off some re-processing manually, to make it consistent again. Jupyter supports interactivity via widgets, but the 'cell-based' approach in notebooks is not a very good fit for that, because it forces a very simple one-after-the other processing model, that would make it hard to implement the efficient execution of even remotely non-trivial workflows (for example having parallel execution of some cells, or skipping the execution of parts that don't need to be executed currently, etc.). Prior art \u00b6 workflow/pipeline modelling and execution is a solved problem in programming: flow-based programming (FBP) requires well defined, modular entities (with 'ports': input and output values) lots of (partial) implementations in data engineering: airflow luigi dagster prefect many others: Node-RED, Apache NiFi, IFTTT, Zapier, Huginn, ... one subtle (although important) difference with our project, again: interactivity There is a form of programming that fits our problem space fairly well: flow-based programming (FBP) . Like functional programming, it's probably older than all of us, and it is gaining some notable traction again in recent years (although with much less hype around it, and without being explicitly mentioned by name). A lot of the data orchestration tools and frameworks that cropped up in recent years use some form or aspects of FBP, for example: airflow luigi dagster prefect One thing that FBP requires are well defined entities ('modules', 'nodes'), that have 'ports' (meaning: known inputs, and outputs). A Jupyter notebook for example does not typically have that, which makes it hard to 'combine' notebooks in an FBP-like manner. There are attempts to 'formalize' Jupyter notebooks in a way that would make them better fits in such scenarios ( papermill , orchest ), but in my opinion, although they kind of work, those attempts are a bit clunky, and not very user-friendly (because they try to bend Jupyter into something it was not designed to do). Also, they typically only deal with inputs; outputs are not very well defined at all. Compare that for example with how a 'proper' data-orchestration tool like dagster handles inputs and outputs , which should make clear how many more options someone who implements a workflow execution and rendering framework (which is basically what we are building) has when that sort of metadata is available. As was the case in the section above, one difference in our case is interactivity. Most tools in that space assume they'll get the input values for a workflow execution at the start, and then they can proceed to go through the workflow, batch-processing style (meaning, no further user input half-way through). This is different for us, since we want users to be able to interactively explore their data (within the limits of a 'static' workflow). This means we will have to consider how to deal with long-running computations whose results wil be available after minutes, hours weeks. The good thing is though, whatever we come up with, we'll get a 'traditional workflow execution engine' for free, because every workflow that can be executed interactively, will also be able to do 'batch-style'. This will let us re-use and 'move' our workflows to other execution environments (HPC clusters, 'the cloud', ...) and do other interesting things with them if the need arises (monte-carlo style experiments, automated-testing of workflows and modules, ...). Modelling workflows \u00b6 research data is more useful when it's structured, so why would workflow definitions be different? so: can we model a workflow as code, or even better: as data? So, assuming everyone agrees this is a reasonable avenue to explore, we have to think about how we want to model our workflows. We should definitely look at how other, similar frameworks do this, but I think one approach is very tempting: Describe workflows as structured data! There are several reasons for why I think this would be a good idea: structured data can be processed by every programming language in existence we would have one 'main' library that does the actual workflow execution/data processing (probably in Python) we could use other languages to do different other things in our 'ecosystem': e.g. JavaScript for dynamically rendering a frontend we can (largely) work independent from each other, the only thing to consult about is the schema of the workflow data such structured data can be displayed as a network graph, which is much easier to grasp than code automated testing of every workflow and model is easy, can be done in CI/CD Jupyter notebooks are, as I've explained above, pretty good at creating and manipulating structured data there are a lot of researchers out there who know how to use Jupyter: those could all be potential \"DHARPA-workflow\" creators in addition to that, we can decide to create a visual 'workflow editor/creator', that is independent from the 'workflow executor' part, and 100% optional Code! \u00b6 The following is using prototype-quality code to illustrate how a 'workflow-as-data' model could look like in practice. Only a few modules are implemented, the goal is to recreate the first part of the 'Topic-modelling' workflow: load some text files, tokenize them, then do some processing (lowercasing, removal of stopwords). Definitions \u00b6 module : a module is an atomic entity that contains a fixed set of defined inputs and outputs, as well as a processing unit that converts the set of inputs to outputs, in a predicable way workflow : a workflow contains a set of modules which are connected in a specific way. A workflow is conceptually also a module, because it also contains a set of inputs/outputs as well as processing unit, and it can be used in other, 'parent' workflows in the same ways a normal module can. # we can list all available modules (and workflows) list_available_module_names () ['corpus_processing', 'corpus_processing_simple', 'dharpa_workflow', 'file_reader', 'input_files_processing', 'lowercase_corpus', 'remove_stopwords_from_corpus', 'tokenize_corpus'] # we can investigate each modules inputs and output specs print_module_desc ( 'tokenize_corpus' ) { 'tokenize_corpus' : { 'inputs' : { 'text_map' : 'Dict' } , 'outputs' : { 'tokenized_text' : 'Dict' }}} # a workflow configuration is basically just a list of modules, incl. their input/output connections workflow_config = f ' { base_path } /workflows/corpus_processing_simple.yaml' print_file_content ( workflow_config ) --- modules : - type : tokenize_corpus - type : lowercase_corpus input_map : tokenized_text : tokenize_corpus.tokenized_text - type : remove_stopwords_from_corpus input_map : tokenized_text : lowercase_corpus.tokenized_text workflow_outputs : tokenized_text : processed_text_corpus # we create a 'workflow' object using the configuration data workflow : DharpaWorkflow = DharpaWorkflow . from_file ( workflow_config ) # we can investigate each workflows available input and output names print ( workflow . input_names ) [ 'lowercase_corpus__enabled' , 'remove_stopwords_from_corpus__enabled' , 'remove_stopwords_from_corpus__stopwords_list' , 'tokenize_corpus__text_map' ] print ( workflow . output_names ) [ 'processed_text_corpus' ] # we can display the execution and data-flow structures of a workflow graphically graph_to_image ( workflow . execution_graph ) # graph_to_image(workflow.data_flow_graph) # print the workflow input/output spec print_module_desc ( workflow ) { 'dharpa_workflow' : { 'inputs' : { 'lowercase_corpus__enabled' : 'Bool' , 'remove_stopwords_from_corpus__enabled' : 'Bool' , 'remove_stopwords_from_corpus__stopwords_list' : 'List' , 'tokenize_corpus__text_map' : 'Dict' } , 'outputs' : { 'processed_text_corpus' : 'Dict' } } } # using the spec, we can set a workflows inputs and outputs manually text_map = { \"1\" : \"Hello World!\" , \"2\" : \"Hello DHARPA!\" } stopwords = [ \"hello\" , \"!\" ] workflow . set_input ( \"tokenize_corpus__text_map\" , text_map ) workflow . set_input ( \"lowercase_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__stopwords_list\" , stopwords ) # the workflow state is processed automatically, so we can always query the current output output1 = workflow . get_output ( \"processed_text_corpus\" ) print ( output1 ) { '1' : [ 'world' ] , '2' : [ 'dharpa' ]} # we can load workflows from json/yaml files on the file-system, and convert them to Python classes load_workflows ( f \" { base_path } /workflows\" ) print ( list_available_module_names ()) [ 'corpus_processing' , 'corpus_processing_simple' , 'dharpa_workflow' , 'file_reader' , 'input_files_processing' , 'lowercase_corpus' , 'remove_stopwords_from_corpus' , 'tokenize_corpus' ] # display the module spec for the 'input_files_processing' workflow print_module_desc ( \"input_files_processing\" ) { 'input_files_processing' : { 'inputs' : { 'files' : 'Any' , 'make_lowercase' : 'Bool' , 'remove_stopwords' : 'Bool' , 'stopwords' : 'List' } , 'outputs' : { 'processed_text_corpus' : 'Dict' } } } # create the workflow object ifp_workflow = create_module ( 'input_files_processing' ) # display the internal structure of the workflow print ( ifp_workflow . _workflow_config ) { 'modules' : [ { 'type' : 'file_reader' , 'input_map' : { 'files' : '__workflow_input__.files' } , 'id' : 'file_reader' } , { 'type' : 'corpus_processing' , 'input_map' : { 'text_map' : 'file_reader.content_map' , 'make_lowercase' : '__workflow_input__.make_lowercase' , 'remove_stopwords' : '__workflow_input__.remove_stopwords' , 'stopwords' : '__workflow_input__.stopwords' } , 'workflow_outputs' : { 'processed_text_corpus' : 'processed_text_corpus' } , 'id' : 'corpus_processing' } ] } # display the execution graph for the workflow graph_to_image ( ifp_workflow . execution_graph ) # graph_to_image(ifp_workflow.data_flow_graph) # auto-render input and output-widgets for the workflow (only works when executed in Jupyter) renderer = PlainJupyterWorkflowRenderer ( workflow = ifp_workflow ) renderer . render () GridspecLayout(children=(Label(value='files', layout=Layout(grid_area='widget001')), FileUpload(value={}, desc\u2026 Output() Advantages \u00b6 each interactive workflow is automatically a 'batch-style' one ability to work on different parts of the application almost 100% independently (workflow engine, workflow renderers (UIs), plugins (submitting long running jobs to a cluster, metadata augmentation/data lineage, metric gathering, ...) comparatively little dependencies for the base system / lightweight, but very extensible no dependency on any one UI/frontend technology (can use Jupyter/React/QT/...), can run without any UI at all complex workflow can be broken up into separate pieces, and developed and tested individually fairly high re-usablity easy to use in an agile, iterative development process Disadvantages \u00b6 potentially lower performance due to having to create copies of inputs/outputs to make sure they (or items contained within them) are not changed by subsequent steps harder to create very customized UIs there's a limit to the complexity of workflows that can be supported realistically (e.g. hard to implement control structures like if-then-else or loops on a workflow level/outside of modules -- but in a lot of cases that is probably not necessary)","title":"Modularity"},{"location":"architecture/workflows/modularity/modularity/#whats-a-workflow-really","text":"Jupyter is a very good tool to create non-trivial exploratory workflows there's a difference between 'dynamic' workflows, and 'static' ones Jupyter is usually used to create workflows in a 'dynamic' way also important (for us): interactivity Currently, Jupyter is one of the most used technologies in digital research to create workflows. Although there are exceptions, in most cases it is used to explore a very specific research question. Jupyter is exceptionally good at that, which is the reason it is so successful. From a computer-engineering perspective, Jupyter notebooks are 'just' simple scripts, and often they include anti-patterns like global- as well as hard-coded variables, little to no encapsulation of functionality, etc. Which means that typically, Jupyter notebooks have (relatively) little value to other researchers, and re-usability is low. This is an acceptable trade-off though, because the problems they are solving are (usually) very niche and specific, so there is little downside to tailor the code to the exact problem one is having. In addition, Jupyter notebooks are very good to document the workflow itself, and communicate what is happening to the data (which is important for publication). If we want to create a tool that lets users run pre-created workflows, that equation changes though. Because, now the assumption is that the (comparatively few) workflows we create will be useful in not just a very specific way. The goal is to identify areas where people have (roughly) the same problem, and to solve that problem in a generic way that is useful to a larger group of people. The workflow will typically be less important in relation to the overall research project a researcher is working on (compared to a tailored, specific one), but from the perspective of a reasearcher it will also be much less hassle and expensive to use, since they don't have to create the workflow themselves, and someone else already has thought about all the options and parameters that make sense, has done the validation and testing, etc. Also, they don't have to learn programming if they don't already know it... This means that we are dealing now with a very 'static' workflow, compared to the 'dynamic' ones researchers with programming skills can create and change themselves very easily. Everything that can happen in a workflow is known in advance, and even though there can be 'forks' in the flow of data, those have to be defined, implemented and documented in advance. And that difference is why we should not assume that Jupyter notebooks are as good a vessel to implement such a workflow as they are in the other case, where all that can happen 'on the go'. It's still possible notebooks are a good fit here too, but we can't use our normal experience with -- and intuition about -- Jupyter to make that case. One other point that is important to note is user interactivity. Usually, when developing a Jupyter notebook inputs (data as well as parameters) are either hardcoded, or factored out into a variable that is changed on top of the notebook (or in some cells further down). And by running or re-running certain cells, those variables are re-set or changed. This works fine for dynamically creating a workflow (although, it's sometimes confusing, and one of the main criticisms against the Jupyter notebook approach). But, in a 'static' workflow, we need to make sure that a user can set or change all those inputs at any time, while making sure that the 'internal' state of our workflow is known to our engine. At a minimum, we need to know that our state is currently inconsistent after a user-input, and have a way to communicating that to the user so they can kick off some re-processing manually, to make it consistent again. Jupyter supports interactivity via widgets, but the 'cell-based' approach in notebooks is not a very good fit for that, because it forces a very simple one-after-the other processing model, that would make it hard to implement the efficient execution of even remotely non-trivial workflows (for example having parallel execution of some cells, or skipping the execution of parts that don't need to be executed currently, etc.).","title":"What's a workflow, really?"},{"location":"architecture/workflows/modularity/modularity/#prior-art","text":"workflow/pipeline modelling and execution is a solved problem in programming: flow-based programming (FBP) requires well defined, modular entities (with 'ports': input and output values) lots of (partial) implementations in data engineering: airflow luigi dagster prefect many others: Node-RED, Apache NiFi, IFTTT, Zapier, Huginn, ... one subtle (although important) difference with our project, again: interactivity There is a form of programming that fits our problem space fairly well: flow-based programming (FBP) . Like functional programming, it's probably older than all of us, and it is gaining some notable traction again in recent years (although with much less hype around it, and without being explicitly mentioned by name). A lot of the data orchestration tools and frameworks that cropped up in recent years use some form or aspects of FBP, for example: airflow luigi dagster prefect One thing that FBP requires are well defined entities ('modules', 'nodes'), that have 'ports' (meaning: known inputs, and outputs). A Jupyter notebook for example does not typically have that, which makes it hard to 'combine' notebooks in an FBP-like manner. There are attempts to 'formalize' Jupyter notebooks in a way that would make them better fits in such scenarios ( papermill , orchest ), but in my opinion, although they kind of work, those attempts are a bit clunky, and not very user-friendly (because they try to bend Jupyter into something it was not designed to do). Also, they typically only deal with inputs; outputs are not very well defined at all. Compare that for example with how a 'proper' data-orchestration tool like dagster handles inputs and outputs , which should make clear how many more options someone who implements a workflow execution and rendering framework (which is basically what we are building) has when that sort of metadata is available. As was the case in the section above, one difference in our case is interactivity. Most tools in that space assume they'll get the input values for a workflow execution at the start, and then they can proceed to go through the workflow, batch-processing style (meaning, no further user input half-way through). This is different for us, since we want users to be able to interactively explore their data (within the limits of a 'static' workflow). This means we will have to consider how to deal with long-running computations whose results wil be available after minutes, hours weeks. The good thing is though, whatever we come up with, we'll get a 'traditional workflow execution engine' for free, because every workflow that can be executed interactively, will also be able to do 'batch-style'. This will let us re-use and 'move' our workflows to other execution environments (HPC clusters, 'the cloud', ...) and do other interesting things with them if the need arises (monte-carlo style experiments, automated-testing of workflows and modules, ...).","title":"Prior art"},{"location":"architecture/workflows/modularity/modularity/#modelling-workflows","text":"research data is more useful when it's structured, so why would workflow definitions be different? so: can we model a workflow as code, or even better: as data? So, assuming everyone agrees this is a reasonable avenue to explore, we have to think about how we want to model our workflows. We should definitely look at how other, similar frameworks do this, but I think one approach is very tempting: Describe workflows as structured data! There are several reasons for why I think this would be a good idea: structured data can be processed by every programming language in existence we would have one 'main' library that does the actual workflow execution/data processing (probably in Python) we could use other languages to do different other things in our 'ecosystem': e.g. JavaScript for dynamically rendering a frontend we can (largely) work independent from each other, the only thing to consult about is the schema of the workflow data such structured data can be displayed as a network graph, which is much easier to grasp than code automated testing of every workflow and model is easy, can be done in CI/CD Jupyter notebooks are, as I've explained above, pretty good at creating and manipulating structured data there are a lot of researchers out there who know how to use Jupyter: those could all be potential \"DHARPA-workflow\" creators in addition to that, we can decide to create a visual 'workflow editor/creator', that is independent from the 'workflow executor' part, and 100% optional","title":"Modelling workflows"},{"location":"architecture/workflows/modularity/modularity/#code","text":"The following is using prototype-quality code to illustrate how a 'workflow-as-data' model could look like in practice. Only a few modules are implemented, the goal is to recreate the first part of the 'Topic-modelling' workflow: load some text files, tokenize them, then do some processing (lowercasing, removal of stopwords).","title":"Code!"},{"location":"architecture/workflows/modularity/modularity/#definitions","text":"module : a module is an atomic entity that contains a fixed set of defined inputs and outputs, as well as a processing unit that converts the set of inputs to outputs, in a predicable way workflow : a workflow contains a set of modules which are connected in a specific way. A workflow is conceptually also a module, because it also contains a set of inputs/outputs as well as processing unit, and it can be used in other, 'parent' workflows in the same ways a normal module can. # we can list all available modules (and workflows) list_available_module_names () ['corpus_processing', 'corpus_processing_simple', 'dharpa_workflow', 'file_reader', 'input_files_processing', 'lowercase_corpus', 'remove_stopwords_from_corpus', 'tokenize_corpus'] # we can investigate each modules inputs and output specs print_module_desc ( 'tokenize_corpus' ) { 'tokenize_corpus' : { 'inputs' : { 'text_map' : 'Dict' } , 'outputs' : { 'tokenized_text' : 'Dict' }}} # a workflow configuration is basically just a list of modules, incl. their input/output connections workflow_config = f ' { base_path } /workflows/corpus_processing_simple.yaml' print_file_content ( workflow_config ) --- modules : - type : tokenize_corpus - type : lowercase_corpus input_map : tokenized_text : tokenize_corpus.tokenized_text - type : remove_stopwords_from_corpus input_map : tokenized_text : lowercase_corpus.tokenized_text workflow_outputs : tokenized_text : processed_text_corpus # we create a 'workflow' object using the configuration data workflow : DharpaWorkflow = DharpaWorkflow . from_file ( workflow_config ) # we can investigate each workflows available input and output names print ( workflow . input_names ) [ 'lowercase_corpus__enabled' , 'remove_stopwords_from_corpus__enabled' , 'remove_stopwords_from_corpus__stopwords_list' , 'tokenize_corpus__text_map' ] print ( workflow . output_names ) [ 'processed_text_corpus' ] # we can display the execution and data-flow structures of a workflow graphically graph_to_image ( workflow . execution_graph ) # graph_to_image(workflow.data_flow_graph) # print the workflow input/output spec print_module_desc ( workflow ) { 'dharpa_workflow' : { 'inputs' : { 'lowercase_corpus__enabled' : 'Bool' , 'remove_stopwords_from_corpus__enabled' : 'Bool' , 'remove_stopwords_from_corpus__stopwords_list' : 'List' , 'tokenize_corpus__text_map' : 'Dict' } , 'outputs' : { 'processed_text_corpus' : 'Dict' } } } # using the spec, we can set a workflows inputs and outputs manually text_map = { \"1\" : \"Hello World!\" , \"2\" : \"Hello DHARPA!\" } stopwords = [ \"hello\" , \"!\" ] workflow . set_input ( \"tokenize_corpus__text_map\" , text_map ) workflow . set_input ( \"lowercase_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__stopwords_list\" , stopwords ) # the workflow state is processed automatically, so we can always query the current output output1 = workflow . get_output ( \"processed_text_corpus\" ) print ( output1 ) { '1' : [ 'world' ] , '2' : [ 'dharpa' ]} # we can load workflows from json/yaml files on the file-system, and convert them to Python classes load_workflows ( f \" { base_path } /workflows\" ) print ( list_available_module_names ()) [ 'corpus_processing' , 'corpus_processing_simple' , 'dharpa_workflow' , 'file_reader' , 'input_files_processing' , 'lowercase_corpus' , 'remove_stopwords_from_corpus' , 'tokenize_corpus' ] # display the module spec for the 'input_files_processing' workflow print_module_desc ( \"input_files_processing\" ) { 'input_files_processing' : { 'inputs' : { 'files' : 'Any' , 'make_lowercase' : 'Bool' , 'remove_stopwords' : 'Bool' , 'stopwords' : 'List' } , 'outputs' : { 'processed_text_corpus' : 'Dict' } } } # create the workflow object ifp_workflow = create_module ( 'input_files_processing' ) # display the internal structure of the workflow print ( ifp_workflow . _workflow_config ) { 'modules' : [ { 'type' : 'file_reader' , 'input_map' : { 'files' : '__workflow_input__.files' } , 'id' : 'file_reader' } , { 'type' : 'corpus_processing' , 'input_map' : { 'text_map' : 'file_reader.content_map' , 'make_lowercase' : '__workflow_input__.make_lowercase' , 'remove_stopwords' : '__workflow_input__.remove_stopwords' , 'stopwords' : '__workflow_input__.stopwords' } , 'workflow_outputs' : { 'processed_text_corpus' : 'processed_text_corpus' } , 'id' : 'corpus_processing' } ] } # display the execution graph for the workflow graph_to_image ( ifp_workflow . execution_graph ) # graph_to_image(ifp_workflow.data_flow_graph) # auto-render input and output-widgets for the workflow (only works when executed in Jupyter) renderer = PlainJupyterWorkflowRenderer ( workflow = ifp_workflow ) renderer . render () GridspecLayout(children=(Label(value='files', layout=Layout(grid_area='widget001')), FileUpload(value={}, desc\u2026 Output()","title":"Definitions"},{"location":"architecture/workflows/modularity/modularity/#advantages","text":"each interactive workflow is automatically a 'batch-style' one ability to work on different parts of the application almost 100% independently (workflow engine, workflow renderers (UIs), plugins (submitting long running jobs to a cluster, metadata augmentation/data lineage, metric gathering, ...) comparatively little dependencies for the base system / lightweight, but very extensible no dependency on any one UI/frontend technology (can use Jupyter/React/QT/...), can run without any UI at all complex workflow can be broken up into separate pieces, and developed and tested individually fairly high re-usablity easy to use in an agile, iterative development process","title":"Advantages"},{"location":"architecture/workflows/modularity/modularity/#disadvantages","text":"potentially lower performance due to having to create copies of inputs/outputs to make sure they (or items contained within them) are not changed by subsequent steps harder to create very customized UIs there's a limit to the complexity of workflows that can be supported realistically (e.g. hard to implement control structures like if-then-else or loops on a workflow level/outside of modules -- but in a lot of cases that is probably not necessary)","title":"Disadvantages"},{"location":"architecture/workflows/modularity/modularity_2/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % autosave 0 import os from rich.jupyter import print from dharpa_toolbox.modules.utils import list_available_module_names , describe_module , print_module_desc , load_workflows , create_module , create_workflow from dharpa_toolbox.utils import print_file_content , graph_to_image from dharpa_toolbox.modules.workflows import DharpaWorkflow from dharpa_toolbox.rendering.jupyter.renderer import PlainJupyterWorkflowRenderer , ModuleJupyterWorkflowRenderer base_path = os . path . abspath ( \".\" ) var element = $('#09a70951-2010-41c8-979d-7e2436a2ed43'); IPython.notebook.set_autosave_interval(0) Autosave disabled What's a workflow? \u00b6 a workflow is a tool to transform data into more structured data 'more data' -- we'll create what can be considered 'new' data out of the existing set 'better structured' -- improve (and replace) the current structure (fix errors, etc.) 'more structure' -- augment existing data with additional structure secondary outcomes (insight, new research questions, etc...) workflows are 'just' simple scripts no or only very minimal control structures, except from input/output connections low computational complexity for the workflow itself high(er) computational complexity within modules (but hidden from workflow creator) Workflows in our context \u00b6 workflow creators and workflow users are different, distinct roles Jupyter is a very good tool to create non-trivial exploratory workflows there's a difference between 'dynamic' workflows, and 'static' ones Jupyter is usually used to create workflows in a 'dynamic' way more 'data engineering' than 'data science' interactivity: not an issue in 'data engineering' not very well supported in Jupyter (cell-based approach not useful for us) Two options: monolithic & modular \u00b6 Monolithic \u00b6 complexity spread out across the workflow well integrated, no restrictions UI-wise Modular \u00b6 complexity concentrated in the framework, but simple modules some restrictions on the UI no difference in how to handle data, metadata backend modules don't necessarily map onto frontend modules suggested approach: 100% modular backend modular frontend, incl. optional monolithic frontend approach for high-value workflows Notes: only important for the workflow part, there will be features that won't be affected by this at all (metadata, data) Definitions \u00b6 module : a module is an atomic entity that contains a fixed set of defined inputs and outputs, as well as a processing unit that converts the set of inputs to outputs, in a predicable way workflow : a workflow contains a set of modules which are connected in a specific way a workflow is conceptually also a module, because it also contains a set of inputs/outputs as well as processing unit it can be used in other, 'parent' workflows in the same ways a normal module can. Modelling workflows \u00b6 research data is more useful when it's structured, so why would workflow definitions be different? so: can we model a workflow as data? Notes: monolithic workflows are not structured (comparison: Advantages \u00b6 scalability disposible middleware and frontends (only important to be able to use the created workflows) Examples \u00b6 # we can list all available modules (and workflows) list_available_module_names () ['dharpa_workflow', 'file_reader', 'lowercase_corpus', 'remove_stopwords_from_corpus', 'tokenize_corpus'] # we can investigate each modules inputs and output specs print_module_desc ( 'tokenize_corpus' ) { 'tokenize_corpus' : { 'inputs' : { 'text_map' : 'Dict' } , 'outputs' : { 'tokenized_text' : 'Dict' }}} # a workflow configuration is basically just a list of modules, incl. their input/output connections workflow_config = f ' { base_path } /workflows/corpus_processing_simple.yaml' print_file_content ( workflow_config ) --- modules : - type : tokenize_corpus - type : lowercase_corpus input_map : tokenized_text : tokenize_corpus.tokenized_text - type : remove_stopwords_from_corpus input_map : tokenized_text : lowercase_corpus.tokenized_text workflow_outputs : tokenized_text : processed_text_corpus # we create a 'workflow' object using the configuration data workflow = DharpaWorkflow . from_file ( workflow_config ) # we can investigate each workflows available input and output names print ( workflow . input_names ) [ 'lowercase_corpus__enabled' , 'remove_stopwords_from_corpus__enabled' , 'remove_stopwords_from_corpus__stopwords_list' , 'tokenize_corpus__text_map' ] print ( workflow . output_names ) [ 'processed_text_corpus' ] # we can display the execution and data-flow structures of a workflow graphically graph_to_image ( workflow . execution_graph ) # graph_to_image(workflow.data_flow_graph) text_map = { \"1\" : \"Hello World!\" , \"2\" : \"Hello DHARPA!\" } stopwords = [ \"hello\" , \"!\" ] workflow = DharpaWorkflow . from_file ( workflow_config ) workflow . set_input ( \"tokenize_corpus__text_map\" , text_map ) workflow . set_input ( \"lowercase_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__stopwords_list\" , stopwords ) # the workflow state is processed automatically, so we can always query the current output output1 = workflow . get_output ( \"processed_text_corpus\" ) print ( output1 ) { '1' : [ 'world' ] , '2' : [ 'dharpa' ]} # text_map: {\"one\": \"Hello World!\", \"two\": \"Hello DHARPA!\"} # stopword_list: hello workflow = DharpaWorkflow . from_file ( workflow_config ) renderer = PlainJupyterWorkflowRenderer ( workflow = workflow ) renderer . render () GridspecLayout(children=(Label(value='tokenize_corpus__text_map', layout=Layout(grid_area='widget001')), Texta\u2026 Output() # we can load workflows from json/yaml files on the file-system, and convert them to Python classes load_workflows ( f \" { base_path } /workflows\" ) print ( list_available_module_names ()) [ 'corpus_processing' , 'corpus_processing_simple' , 'dharpa_workflow' , 'file_reader' , 'input_files_processing' , 'lowercase_corpus' , 'remove_stopwords_from_corpus' , 'tokenize_corpus' ] # display the module spec for the 'input_files_processing' workflow print_module_desc ( \"input_files_processing\" ) { 'input_files_processing' : { 'inputs' : { 'files' : 'Any' , 'make_lowercase' : 'Bool' , 'remove_stopwords' : 'Bool' , 'stopwords' : 'List' } , 'outputs' : { 'processed_text_corpus' : 'Dict' } } } # create the workflow object ifp_workflow = create_workflow ( 'input_files_processing' ) # display the internal structure of the workflow print ( ifp_workflow . _workflow_config ) { 'modules' : [ { 'type' : 'file_reader' , 'input_map' : { 'files' : '__workflow_input__.files' } , 'id' : 'file_reader' } , { 'type' : 'corpus_processing' , 'input_map' : { 'text_map' : 'file_reader.content_map' , 'make_lowercase' : '__workflow_input__.make_lowercase' , 'remove_stopwords' : '__workflow_input__.remove_stopwords' , 'stopwords' : '__workflow_input__.stopwords' } , 'workflow_outputs' : { 'processed_text_corpus' : 'processed_text_corpus' } , 'id' : 'corpus_processing' } ] } # display the execution graph for the workflow graph_to_image ( ifp_workflow . execution_graph ) # graph_to_image(ifp_workflow.data_flow_graph) # auto-render input and output-widgets for the workflow (only works when executed in Jupyter) renderer = PlainJupyterWorkflowRenderer ( workflow = ifp_workflow ) renderer . render () GridspecLayout(children=(Label(value='files', layout=Layout(grid_area='widget001')), FileUpload(value={}, desc\u2026 Output()","title":"Modularity 2"},{"location":"architecture/workflows/modularity/modularity_2/#whats-a-workflow","text":"a workflow is a tool to transform data into more structured data 'more data' -- we'll create what can be considered 'new' data out of the existing set 'better structured' -- improve (and replace) the current structure (fix errors, etc.) 'more structure' -- augment existing data with additional structure secondary outcomes (insight, new research questions, etc...) workflows are 'just' simple scripts no or only very minimal control structures, except from input/output connections low computational complexity for the workflow itself high(er) computational complexity within modules (but hidden from workflow creator)","title":"What's a workflow?"},{"location":"architecture/workflows/modularity/modularity_2/#workflows-in-our-context","text":"workflow creators and workflow users are different, distinct roles Jupyter is a very good tool to create non-trivial exploratory workflows there's a difference between 'dynamic' workflows, and 'static' ones Jupyter is usually used to create workflows in a 'dynamic' way more 'data engineering' than 'data science' interactivity: not an issue in 'data engineering' not very well supported in Jupyter (cell-based approach not useful for us)","title":"Workflows in our context"},{"location":"architecture/workflows/modularity/modularity_2/#two-options-monolithic-modular","text":"","title":"Two options: monolithic &amp; modular"},{"location":"architecture/workflows/modularity/modularity_2/#monolithic","text":"complexity spread out across the workflow well integrated, no restrictions UI-wise","title":"Monolithic"},{"location":"architecture/workflows/modularity/modularity_2/#modular","text":"complexity concentrated in the framework, but simple modules some restrictions on the UI no difference in how to handle data, metadata backend modules don't necessarily map onto frontend modules suggested approach: 100% modular backend modular frontend, incl. optional monolithic frontend approach for high-value workflows Notes: only important for the workflow part, there will be features that won't be affected by this at all (metadata, data)","title":"Modular"},{"location":"architecture/workflows/modularity/modularity_2/#definitions","text":"module : a module is an atomic entity that contains a fixed set of defined inputs and outputs, as well as a processing unit that converts the set of inputs to outputs, in a predicable way workflow : a workflow contains a set of modules which are connected in a specific way a workflow is conceptually also a module, because it also contains a set of inputs/outputs as well as processing unit it can be used in other, 'parent' workflows in the same ways a normal module can.","title":"Definitions"},{"location":"architecture/workflows/modularity/modularity_2/#modelling-workflows","text":"research data is more useful when it's structured, so why would workflow definitions be different? so: can we model a workflow as data? Notes: monolithic workflows are not structured (comparison:","title":"Modelling workflows"},{"location":"architecture/workflows/modularity/modularity_2/#advantages","text":"scalability disposible middleware and frontends (only important to be able to use the created workflows)","title":"Advantages"},{"location":"architecture/workflows/modularity/modularity_2/#examples","text":"# we can list all available modules (and workflows) list_available_module_names () ['dharpa_workflow', 'file_reader', 'lowercase_corpus', 'remove_stopwords_from_corpus', 'tokenize_corpus'] # we can investigate each modules inputs and output specs print_module_desc ( 'tokenize_corpus' ) { 'tokenize_corpus' : { 'inputs' : { 'text_map' : 'Dict' } , 'outputs' : { 'tokenized_text' : 'Dict' }}} # a workflow configuration is basically just a list of modules, incl. their input/output connections workflow_config = f ' { base_path } /workflows/corpus_processing_simple.yaml' print_file_content ( workflow_config ) --- modules : - type : tokenize_corpus - type : lowercase_corpus input_map : tokenized_text : tokenize_corpus.tokenized_text - type : remove_stopwords_from_corpus input_map : tokenized_text : lowercase_corpus.tokenized_text workflow_outputs : tokenized_text : processed_text_corpus # we create a 'workflow' object using the configuration data workflow = DharpaWorkflow . from_file ( workflow_config ) # we can investigate each workflows available input and output names print ( workflow . input_names ) [ 'lowercase_corpus__enabled' , 'remove_stopwords_from_corpus__enabled' , 'remove_stopwords_from_corpus__stopwords_list' , 'tokenize_corpus__text_map' ] print ( workflow . output_names ) [ 'processed_text_corpus' ] # we can display the execution and data-flow structures of a workflow graphically graph_to_image ( workflow . execution_graph ) # graph_to_image(workflow.data_flow_graph) text_map = { \"1\" : \"Hello World!\" , \"2\" : \"Hello DHARPA!\" } stopwords = [ \"hello\" , \"!\" ] workflow = DharpaWorkflow . from_file ( workflow_config ) workflow . set_input ( \"tokenize_corpus__text_map\" , text_map ) workflow . set_input ( \"lowercase_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__enabled\" , True ) workflow . set_input ( \"remove_stopwords_from_corpus__stopwords_list\" , stopwords ) # the workflow state is processed automatically, so we can always query the current output output1 = workflow . get_output ( \"processed_text_corpus\" ) print ( output1 ) { '1' : [ 'world' ] , '2' : [ 'dharpa' ]} # text_map: {\"one\": \"Hello World!\", \"two\": \"Hello DHARPA!\"} # stopword_list: hello workflow = DharpaWorkflow . from_file ( workflow_config ) renderer = PlainJupyterWorkflowRenderer ( workflow = workflow ) renderer . render () GridspecLayout(children=(Label(value='tokenize_corpus__text_map', layout=Layout(grid_area='widget001')), Texta\u2026 Output() # we can load workflows from json/yaml files on the file-system, and convert them to Python classes load_workflows ( f \" { base_path } /workflows\" ) print ( list_available_module_names ()) [ 'corpus_processing' , 'corpus_processing_simple' , 'dharpa_workflow' , 'file_reader' , 'input_files_processing' , 'lowercase_corpus' , 'remove_stopwords_from_corpus' , 'tokenize_corpus' ] # display the module spec for the 'input_files_processing' workflow print_module_desc ( \"input_files_processing\" ) { 'input_files_processing' : { 'inputs' : { 'files' : 'Any' , 'make_lowercase' : 'Bool' , 'remove_stopwords' : 'Bool' , 'stopwords' : 'List' } , 'outputs' : { 'processed_text_corpus' : 'Dict' } } } # create the workflow object ifp_workflow = create_workflow ( 'input_files_processing' ) # display the internal structure of the workflow print ( ifp_workflow . _workflow_config ) { 'modules' : [ { 'type' : 'file_reader' , 'input_map' : { 'files' : '__workflow_input__.files' } , 'id' : 'file_reader' } , { 'type' : 'corpus_processing' , 'input_map' : { 'text_map' : 'file_reader.content_map' , 'make_lowercase' : '__workflow_input__.make_lowercase' , 'remove_stopwords' : '__workflow_input__.remove_stopwords' , 'stopwords' : '__workflow_input__.stopwords' } , 'workflow_outputs' : { 'processed_text_corpus' : 'processed_text_corpus' } , 'id' : 'corpus_processing' } ] } # display the execution graph for the workflow graph_to_image ( ifp_workflow . execution_graph ) # graph_to_image(ifp_workflow.data_flow_graph) # auto-render input and output-widgets for the workflow (only works when executed in Jupyter) renderer = PlainJupyterWorkflowRenderer ( workflow = ifp_workflow ) renderer . render () GridspecLayout(children=(Label(value='files', layout=Layout(grid_area='widget001')), FileUpload(value={}, desc\u2026 Output()","title":"Examples"},{"location":"modules/","text":"xxxxxxx","title":"Index"},{"location":"modules/SUMMARY/","text":"metadata value","title":"SUMMARY"},{"location":"modules/metadata/","text":"metadata.python_class \u00b6 Documentation Extract metadata about the Python type of a value. Origin Authors Markus Binsteiner (markus@frkl.io) Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://dharpa.org/kiara_documentatio\u2026 module_doc : https://dharpa.org/kiara_documentatio\u2026 source_url : https://github.com/DHARPA-Project/kia\u2026 Module config Field Type Description Required \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants no for this module. defaults object Value defaults for no this module. value_type string The data type this yes module will be used for. Module config -- no config -- Python class class_name ExtractPythonClass module_name kiara.modules.metadata full_name kiara.modules.metadata.ExtractPython\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueSet, outputs: Value\u2026 input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" value = inputs . get_value_obj(input_name) if self . value_type != \"any\" and value . type_nam\u2026 raise KiaraProcessingException( f\"Can't extract metadata for value of \u2026 ) # TODO: if type 'any', validate that the data \u2026 outputs . set_value( \"metadata_item_schema\" , self \u2026 metadata = self . extract_metadata(value) if isinstance(metadata, BaseModel): metadata = metadata . dict(exclude_none = True ) # TODO: validate metadata? outputs . set_value( \"metadata_item\" , metadata) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"metadata"},{"location":"modules/metadata/#metadatapython_class","text":"Documentation Extract metadata about the Python type of a value. Origin Authors Markus Binsteiner (markus@frkl.io) Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://dharpa.org/kiara_documentatio\u2026 module_doc : https://dharpa.org/kiara_documentatio\u2026 source_url : https://github.com/DHARPA-Project/kia\u2026 Module config Field Type Description Required \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants no for this module. defaults object Value defaults for no this module. value_type string The data type this yes module will be used for. Module config -- no config -- Python class class_name ExtractPythonClass module_name kiara.modules.metadata full_name kiara.modules.metadata.ExtractPython\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueSet, outputs: Value\u2026 input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" value = inputs . get_value_obj(input_name) if self . value_type != \"any\" and value . type_nam\u2026 raise KiaraProcessingException( f\"Can't extract metadata for value of \u2026 ) # TODO: if type 'any', validate that the data \u2026 outputs . set_value( \"metadata_item_schema\" , self \u2026 metadata = self . extract_metadata(value) if isinstance(metadata, BaseModel): metadata = metadata . dict(exclude_none = True ) # TODO: validate metadata? outputs . set_value( \"metadata_item\" , metadata) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"metadata.python_class"},{"location":"modules/value/","text":"value.hash \u00b6 Documentation Calculate the hash of a value. Origin Authors Markus Binsteiner (markus@frkl.io) Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://dharpa.org/kiara_documentatio\u2026 module_doc : https://dharpa.org/kiara_documentatio\u2026 source_url : https://github.com/DHARPA-Project/kia\u2026 Module config Field Type Description Required \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants no for this module. defaults object Value defaults for no this module. value_type string The type of the yes value to calculate the hash for. hash_type string The hash type. yes Module config -- no config -- Python class class_name CalculateValueHashModule module_name kiara.operations.calculate_hash full_name kiara.operations.calculate_hash.Calc\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueSet, outputs: Value\u2026 input_name = self . get_config_value( \"value_type\u2026 if input_name == \"any\" : input_name = \"value_item\" value: Value = inputs . get_value_obj(input_name) value_hash = value . get_hash(hash_type = self . get\u2026 outputs . set_value( \"hash\" , value_hash . hash) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 value.pretty_print \u00b6 Documentation -- n/a -- Origin Authors Markus Binsteiner (markus@frkl.io) Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://dharpa.org/kiara_documentatio\u2026 module_doc : https://dharpa.org/kiara_documentatio\u2026 source_url : https://github.com/DHARPA-Project/kia\u2026 Module config Field Type Description Required \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants no for this module. defaults object Value defaults no for this module. value_type string The type of the yes value to save. target_type string The target to no print the value to. Module config -- no config -- Python class class_name PrettyPrintValueModule module_name kiara.operations.pretty_print full_name kiara.operations.pretty_print.Pretty\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueSet, outputs: Value\u2026 value_type: str = self . get_config_value( \"value\u2026 target_type: str = self . get_config_value( \"targ\u2026 if value_type == \"all\" : input_name = \"value_item\" else : input_name = value_type value_obj: Value = inputs . get_value_obj(input_\u2026 print_config = dict(DEFAULT_PRETTY_PRINT_CONFI\u2026 config: typing . Mapping = inputs . get_value_data\u2026 if config: print_config . update(config) func_name = f\"pretty_print_as_{ target_type }\" if not hasattr(value_obj . type_obj, func_name): raise Exception ( f\"Type '{ value_type }' can't be pretty \u2026 ) if not value_obj . is_set: printed = \"-- not set --\" else : func = getattr(value_obj . type_obj, func_na\u2026 # TODO: check signature try : printed = func(value = value_obj, print_\u2026 except Exception as e: if is_debug(): import traceback traceback . print_exc() raise KiaraProcessingException(str(e)) outputs . set_value( \"printed\" , printed) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"value"},{"location":"modules/value/#valuehash","text":"Documentation Calculate the hash of a value. Origin Authors Markus Binsteiner (markus@frkl.io) Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://dharpa.org/kiara_documentatio\u2026 module_doc : https://dharpa.org/kiara_documentatio\u2026 source_url : https://github.com/DHARPA-Project/kia\u2026 Module config Field Type Description Required \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants no for this module. defaults object Value defaults for no this module. value_type string The type of the yes value to calculate the hash for. hash_type string The hash type. yes Module config -- no config -- Python class class_name CalculateValueHashModule module_name kiara.operations.calculate_hash full_name kiara.operations.calculate_hash.Calc\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueSet, outputs: Value\u2026 input_name = self . get_config_value( \"value_type\u2026 if input_name == \"any\" : input_name = \"value_item\" value: Value = inputs . get_value_obj(input_name) value_hash = value . get_hash(hash_type = self . get\u2026 outputs . set_value( \"hash\" , value_hash . hash) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"value.hash"},{"location":"modules/value/#valuepretty_print","text":"Documentation -- n/a -- Origin Authors Markus Binsteiner (markus@frkl.io) Context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kia\u2026 documentation : https://dharpa.org/kiara_documentatio\u2026 module_doc : https://dharpa.org/kiara_documentatio\u2026 source_url : https://github.com/DHARPA-Project/kia\u2026 Module config Field Type Description Required \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 constants object Value constants no for this module. defaults object Value defaults no for this module. value_type string The type of the yes value to save. target_type string The target to no print the value to. Module config -- no config -- Python class class_name PrettyPrintValueModule module_name kiara.operations.pretty_print full_name kiara.operations.pretty_print.Pretty\u2026 Processing source code \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 def process (self, inputs: ValueSet, outputs: Value\u2026 value_type: str = self . get_config_value( \"value\u2026 target_type: str = self . get_config_value( \"targ\u2026 if value_type == \"all\" : input_name = \"value_item\" else : input_name = value_type value_obj: Value = inputs . get_value_obj(input_\u2026 print_config = dict(DEFAULT_PRETTY_PRINT_CONFI\u2026 config: typing . Mapping = inputs . get_value_data\u2026 if config: print_config . update(config) func_name = f\"pretty_print_as_{ target_type }\" if not hasattr(value_obj . type_obj, func_name): raise Exception ( f\"Type '{ value_type }' can't be pretty \u2026 ) if not value_obj . is_set: printed = \"-- not set --\" else : func = getattr(value_obj . type_obj, func_na\u2026 # TODO: check signature try : printed = func(value = value_obj, print_\u2026 except Exception as e: if is_debug(): import traceback traceback . print_exc() raise KiaraProcessingException(str(e)) outputs . set_value( \"printed\" , printed) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"value.pretty_print"},{"location":"operation_types/","text":"xxxxxxx","title":"Index"},{"location":"operation_types/SUMMARY/","text":"all calculate_hash create export extract_metadata import pretty_print sample serialize store_value","title":"SUMMARY"},{"location":"operation_types/all/","text":"all \u00b6 type_name all documentation -- n/a -- origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name AllOperationType module_name kiara.operations full_name kiara.operations.AllOperationType","title":"all"},{"location":"operation_types/all/#all","text":"type_name all documentation -- n/a -- origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name AllOperationType module_name kiara.operations full_name kiara.operations.AllOperationType","title":"all"},{"location":"operation_types/calculate_hash/","text":"calculate_hash \u00b6 type_name calculate_hash documentation Operation type to calculate hash(es) for data. This is mostly used internally, so users usually won't be exposed to operations of this type. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name CalculateHashOperationType module_name kiara.operations.calculate_hash full_name kiara.operations.calculate_hash.Calculate\u2026","title":"calculate_hash"},{"location":"operation_types/calculate_hash/#calculate_hash","text":"type_name calculate_hash documentation Operation type to calculate hash(es) for data. This is mostly used internally, so users usually won't be exposed to operations of this type. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name CalculateHashOperationType module_name kiara.operations.calculate_hash full_name kiara.operations.calculate_hash.Calculate\u2026","title":"calculate_hash"},{"location":"operation_types/create/","text":"create \u00b6 type_name create documentation Operations that create values of specific types from values of certain value profiles. In most cases, source profiles will be 'file' or 'file_bundle' values of some sort, and the created values will exhibit more inherent structure and stricter specs than their sources. The 'create' operation type differs from 'import' in that it expects values that are already onboarded and are all information in the value is stored in a kiara data registry (in most cases the kiara data store ). origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name CreateValueOperationType module_name kiara.operations.create_value full_name kiara.operations.create_value.CreateValue\u2026","title":"create"},{"location":"operation_types/create/#create","text":"type_name create documentation Operations that create values of specific types from values of certain value profiles. In most cases, source profiles will be 'file' or 'file_bundle' values of some sort, and the created values will exhibit more inherent structure and stricter specs than their sources. The 'create' operation type differs from 'import' in that it expects values that are already onboarded and are all information in the value is stored in a kiara data registry (in most cases the kiara data store ). origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name CreateValueOperationType module_name kiara.operations.create_value full_name kiara.operations.create_value.CreateValue\u2026","title":"create"},{"location":"operation_types/export/","text":"export \u00b6 type_name export documentation Export data from kiara . Operations of this type use internally handled datasets, and export it to the local file system. Export operations are created by implementing a class that inherits from DataExportModule , kiara will register it under an operation id following this template: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 <SOURCE_DATA_TYPE>.export_as.<EXPORT_PROFILE> \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 The meaning of the templated fields is: \u2022 EXPORTED_DATA_TYPE : the data type of the value to export \u2022 EXPORT_PROFILE : a short, free-form description of the format the data will be exported as origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name ExportDataOperationType module_name kiara.operations.data_export full_name kiara.operations.data_export.ExportDataOp\u2026","title":"export"},{"location":"operation_types/export/#export","text":"type_name export documentation Export data from kiara . Operations of this type use internally handled datasets, and export it to the local file system. Export operations are created by implementing a class that inherits from DataExportModule , kiara will register it under an operation id following this template: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 <SOURCE_DATA_TYPE>.export_as.<EXPORT_PROFILE> \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 The meaning of the templated fields is: \u2022 EXPORTED_DATA_TYPE : the data type of the value to export \u2022 EXPORT_PROFILE : a short, free-form description of the format the data will be exported as origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name ExportDataOperationType module_name kiara.operations.data_export full_name kiara.operations.data_export.ExportDataOp\u2026","title":"export"},{"location":"operation_types/extract_metadata/","text":"extract_metadata \u00b6 type_name extract_metadata documentation Extract metadata from a dataset. The purpose of this operation type is to be able to extract arbitrary, type-specific metadata from value data. In general, kiara wants to collect (and store along the value) as much metadata related to data as possible, but the extraction process should not take a lot of processing time (since this is done whenever a value is registered into a data registry). As its hard to predict all the types of metadata of a specific type that could be interesting in specific scenarios, kiara supports a pluggable mechanism to add new metadata extraction processes by extending the base class ExtractMetadataModule and adding that implementation somewhere kiara can find it. Once that is done, kiara will automatically add a new operation with an id that follows this template: <VALUE_TYPE>.extract_metadata.<METADATA_KEY> , where METADATA_KEY is a name under which the metadata will be stored within the value object. By default, every value type should have at least one metadata extraction module where the METADATA_KEY is the same as the value type name, and which contains basic, type-specific metadata (e.g. for a 'table', that would be number of rows, column names, column types, etc.). origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name ExtractMetadataOperationType module_name kiara.operations.extract_metadata full_name kiara.operations.extract_metadata.Extract\u2026","title":"extract_metadata"},{"location":"operation_types/extract_metadata/#extract_metadata","text":"type_name extract_metadata documentation Extract metadata from a dataset. The purpose of this operation type is to be able to extract arbitrary, type-specific metadata from value data. In general, kiara wants to collect (and store along the value) as much metadata related to data as possible, but the extraction process should not take a lot of processing time (since this is done whenever a value is registered into a data registry). As its hard to predict all the types of metadata of a specific type that could be interesting in specific scenarios, kiara supports a pluggable mechanism to add new metadata extraction processes by extending the base class ExtractMetadataModule and adding that implementation somewhere kiara can find it. Once that is done, kiara will automatically add a new operation with an id that follows this template: <VALUE_TYPE>.extract_metadata.<METADATA_KEY> , where METADATA_KEY is a name under which the metadata will be stored within the value object. By default, every value type should have at least one metadata extraction module where the METADATA_KEY is the same as the value type name, and which contains basic, type-specific metadata (e.g. for a 'table', that would be number of rows, column names, column types, etc.). origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name ExtractMetadataOperationType module_name kiara.operations.extract_metadata full_name kiara.operations.extract_metadata.Extract\u2026","title":"extract_metadata"},{"location":"operation_types/import/","text":"import \u00b6 type_name import documentation Import data into kiara . Operations of this type take external data, and register it into kiara . External data is different in that it usually does not come with any metadata on how it was created, who created it, when, etc. Import operations are created by implementing a class that inherits from DataImportModule , kiara will register it under an operation id following this template: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 <IMPORTED_DATA_TYPE>.import_from.<IMPORT_PROFILE>.<INPU\u2026 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 The meaning of the templated fields is: \u2022 IMPORTED_DATA_TYPE : the data type of the imported value \u2022 IMPORT_PROFILE : a short, free-form description of where from (or how) the data is imported \u2022 INPUT_TYPE : the data type of the user input that points to the data (like a file path, url, query, etc.) -- in most cases this will be some form of a string or uri There are two main scenarios when an operation of this type is used: \u2022 'onboard' data that was created by a 3rd party, or using external processes \u2022 're-import' data that as created in kiara , then exported to be transformed in an external process, and then imported again into kiara In both of those scenarios, we'll need to have a way to add metadata to fill out 'holes' in the metadata 'chold chain'. We don't have a concept yet as to how to do that, but that is planned for the future. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name ImportDataOperationType module_name kiara.operations.data_import full_name kiara.operations.data_import.ImportDataOp\u2026","title":"import"},{"location":"operation_types/import/#import","text":"type_name import documentation Import data into kiara . Operations of this type take external data, and register it into kiara . External data is different in that it usually does not come with any metadata on how it was created, who created it, when, etc. Import operations are created by implementing a class that inherits from DataImportModule , kiara will register it under an operation id following this template: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 <IMPORTED_DATA_TYPE>.import_from.<IMPORT_PROFILE>.<INPU\u2026 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 The meaning of the templated fields is: \u2022 IMPORTED_DATA_TYPE : the data type of the imported value \u2022 IMPORT_PROFILE : a short, free-form description of where from (or how) the data is imported \u2022 INPUT_TYPE : the data type of the user input that points to the data (like a file path, url, query, etc.) -- in most cases this will be some form of a string or uri There are two main scenarios when an operation of this type is used: \u2022 'onboard' data that was created by a 3rd party, or using external processes \u2022 're-import' data that as created in kiara , then exported to be transformed in an external process, and then imported again into kiara In both of those scenarios, we'll need to have a way to add metadata to fill out 'holes' in the metadata 'chold chain'. We don't have a concept yet as to how to do that, but that is planned for the future. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name ImportDataOperationType module_name kiara.operations.data_import full_name kiara.operations.data_import.ImportDataOp\u2026","title":"import"},{"location":"operation_types/pretty_print/","text":"pretty_print \u00b6 type_name pretty_print documentation This operation type renders values of any type into human readable output for different targets (e.g. terminal, html, ...). The main purpose of this operation type is to show the user as much content of the data as possible for the specific rendering target, without giving any guarantees that all information contained in the data is shown. It may print out the whole content (if the content is small, or the available space large enough), or just a few bits and pieces from the start and/or end of the data, whatever makes most sense for the data itself. For example, for large tables it might be a good idea to print out the first and last 30 rows, so users can see which columns are available, and get a rough overview of the content itself. For network graphs it might make sense to print out some graph properties (number of nodes, edges, available attributes, ...) on the terminal, but render the graph itself when using html as target. Currently, it is only possible to implement a pretty_print renderer if you have access to the source code of the value type you want to render. To add a new render method, add a new instance method to the ValueType class in question, in the format: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 def pretty_print_as_<TARGET_TYPE>(value: Value, \u2502 \u2502 print_config: typing.Mapping[str, typing.Any]) -> \u2502 \u2502 typing.Any: \u2502 \u2502 ... \u2502 \u2502 ... \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 kiara will look at all available ValueType classes for methods that match this signature, and auto-generate operations following this naming template: <SOURCE_TYPE>.pretty_print_as.<TARGET_TYPE> . Currently, only the type renderables is implemented as target type (for printing out to the terminal). It is planned to add output suitable for use in Jupyter notebooks in the near future. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name PrettyPrintOperationType module_name kiara.operations.pretty_print full_name kiara.operations.pretty_print.PrettyPrint\u2026","title":"pretty_print"},{"location":"operation_types/pretty_print/#pretty_print","text":"type_name pretty_print documentation This operation type renders values of any type into human readable output for different targets (e.g. terminal, html, ...). The main purpose of this operation type is to show the user as much content of the data as possible for the specific rendering target, without giving any guarantees that all information contained in the data is shown. It may print out the whole content (if the content is small, or the available space large enough), or just a few bits and pieces from the start and/or end of the data, whatever makes most sense for the data itself. For example, for large tables it might be a good idea to print out the first and last 30 rows, so users can see which columns are available, and get a rough overview of the content itself. For network graphs it might make sense to print out some graph properties (number of nodes, edges, available attributes, ...) on the terminal, but render the graph itself when using html as target. Currently, it is only possible to implement a pretty_print renderer if you have access to the source code of the value type you want to render. To add a new render method, add a new instance method to the ValueType class in question, in the format: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 def pretty_print_as_<TARGET_TYPE>(value: Value, \u2502 \u2502 print_config: typing.Mapping[str, typing.Any]) -> \u2502 \u2502 typing.Any: \u2502 \u2502 ... \u2502 \u2502 ... \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 kiara will look at all available ValueType classes for methods that match this signature, and auto-generate operations following this naming template: <SOURCE_TYPE>.pretty_print_as.<TARGET_TYPE> . Currently, only the type renderables is implemented as target type (for printing out to the terminal). It is planned to add output suitable for use in Jupyter notebooks in the near future. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name PrettyPrintOperationType module_name kiara.operations.pretty_print full_name kiara.operations.pretty_print.PrettyPrint\u2026","title":"pretty_print"},{"location":"operation_types/sample/","text":"sample \u00b6 type_name sample documentation Operation type for sampling data of different types. This is useful to reduce the size of some datasets in previews and test-runs, while adjusting parameters and the like. Operations of this type can implement very simple or complex ways to take samples of the data they are fed. The most important ones are sampling operations relating to tables and arrays, but it might also make sense to sample texts, image-sets and so on. Operations of this type take the value to be sampled as input, as well as a sample size (integer), which can be, for example, a number of rows, or percentage of the whole dataset, depending on sample type. Modules that implement sampling should inherit from SampleValueModule , and will get auto-registered with operation ids following this template: <VALUE_TYPE>.sample.<SAMPLE_TYPE_NAME> , where SAMPLE_TYPE_NAME is a descriptive name what will be sampled, or how sampling will be done. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name SampleValueOperationType module_name kiara.operations.sample full_name kiara.operations.sample.SampleValueOperat\u2026","title":"sample"},{"location":"operation_types/sample/#sample","text":"type_name sample documentation Operation type for sampling data of different types. This is useful to reduce the size of some datasets in previews and test-runs, while adjusting parameters and the like. Operations of this type can implement very simple or complex ways to take samples of the data they are fed. The most important ones are sampling operations relating to tables and arrays, but it might also make sense to sample texts, image-sets and so on. Operations of this type take the value to be sampled as input, as well as a sample size (integer), which can be, for example, a number of rows, or percentage of the whole dataset, depending on sample type. Modules that implement sampling should inherit from SampleValueModule , and will get auto-registered with operation ids following this template: <VALUE_TYPE>.sample.<SAMPLE_TYPE_NAME> , where SAMPLE_TYPE_NAME is a descriptive name what will be sampled, or how sampling will be done. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name SampleValueOperationType module_name kiara.operations.sample full_name kiara.operations.sample.SampleValueOperat\u2026","title":"sample"},{"location":"operation_types/serialize/","text":"serialize \u00b6 type_name serialize documentation Operations that serialize data into formats that can be used for data exchange. NOT USED YET origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name SerializeValueOperationType module_name kiara.operations.serialize full_name kiara.operations.serialize.SerializeValue\u2026","title":"serialize"},{"location":"operation_types/serialize/#serialize","text":"type_name serialize documentation Operations that serialize data into formats that can be used for data exchange. NOT USED YET origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name SerializeValueOperationType module_name kiara.operations.serialize full_name kiara.operations.serialize.SerializeValue\u2026","title":"serialize"},{"location":"operation_types/store_value/","text":"store_value \u00b6 type_name store_value documentation Store a value into a local data store. This is a special operation type, that is used internally by the [LocalDataStore](http://dharpa.org/kiara/latest/api_referen\u2026 data registry implementation. For each value type that should be supported by the persistent kiara data store, there must be an implementation of the StoreValueTypeModule class, which handles the actual persisting on disk. In most cases, end users won't need to interact with this type of operation. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name StoreOperationType module_name kiara.operations.store_value full_name kiara.operations.store_value.StoreOperati\u2026","title":"store_value"},{"location":"operation_types/store_value/#store_value","text":"type_name store_value documentation Store a value into a local data store. This is a special operation type, that is used internally by the [LocalDataStore](http://dharpa.org/kiara/latest/api_referen\u2026 data registry implementation. For each value type that should be supported by the persistent kiara data store, there must be an implementation of the StoreValueTypeModule class, which handles the actual persisting on disk. In most cases, end users won't need to interact with this type of operation. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name StoreOperationType module_name kiara.operations.store_value full_name kiara.operations.store_value.StoreOperati\u2026","title":"store_value"},{"location":"reference/SUMMARY/","text":"kiara config data onboarding batch registry store types core type_mgmt values value_set defaults doc gen_info_pages generate_api_doc mkdocs_macros_cli mkdocs_macros_kiara mkdocstrings collector handler renderer environment operating_system python events examples example_controller exceptions info kiara metadata modules operations pipelines types interfaces cli data commands dev commands environment commands explain info commands metadata commands module commands operation commands pipeline commands run type commands utils python_api controller kiara metadata core_models data mgmt module_models operation_models type_models module module_config module_mgmt merged pipelines python_classes modules metadata pipelines save_value type_conversion operations calculate_hash create_value data_export data_import extract_metadata merge_values pretty_print sample serialize store_value pipeline config controller batch listeners module pipeline structure utils values processing parallel processor synchronous rendering jinja mgmt sessions utils class_loading concurrency doc global_metadata jupyter modules output workflow kiara_workflow wrapped mkdocstrings_handlers kiara","title":"SUMMARY"},{"location":"reference/kiara/__init__/","text":"__author__ special \u00b6 The author of this package. __email__ special \u00b6 Email address of the author. get_version () \u00b6 Return the current version of Kiara . Source code in kiara/__init__.py def get_version () -> str : \"\"\"Return the current version of *Kiara*.\"\"\" from pkg_resources import DistributionNotFound , get_distribution try : # Change here if project is renamed and does not equal the package name dist_name = __name__ __version__ = get_distribution ( dist_name ) . version except DistributionNotFound : try : version_file = os . path . join ( os . path . dirname ( __file__ ), \"version.txt\" ) if os . path . exists ( version_file ): with open ( version_file , encoding = \"utf-8\" ) as vf : __version__ = vf . read () else : __version__ = \"unknown\" except ( Exception ): pass if __version__ is None : __version__ = \"unknown\" return __version__ config \u00b6 KiaraConfig ( BaseSettings ) pydantic-model \u00b6 Source code in kiara/config.py class KiaraConfig ( BaseSettings ): class Config : extra = Extra . forbid env_file_encoding = \"utf-8\" env_prefix = \"kiara_\" @classmethod def customise_sources ( cls , init_settings , env_settings , file_secret_settings , ): return ( init_settings , env_settings , yaml_config_settings_source , file_secret_settings , ) module_managers : typing . Optional [ typing . List [ typing . Union [ PythonModuleManagerConfig , PipelineModuleManagerConfig ] ] ] = Field ( description = \"The module managers to use in this kiara instance.\" , default = None ) default_processor : typing . Optional [ typing . Union [ SynchronousProcessorConfig , ThreadPoolProcessorConfig ] ] = Field ( description = \"The configuration for the default processor to use.\" , default_factory = SynchronousProcessorConfig , ) data_store : str = Field ( description = \"The path to the local kiara data store.\" , default = KIARA_DATA_STORE_DIR , ) extra_pipeline_folders : typing . List [ str ] = Field ( description = \"Paths to local folders that contain kiara pipelines.\" , default_factory = list , ) ignore_errors : bool = Field ( description = \"If set, kiara will try to ignore most errors (that can be ignored).\" , default = False , ) @validator ( \"module_managers\" , pre = True ) def _validate_managers ( cls , v ): if v is None : return [] if isinstance ( v , typing . Mapping ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( item , ModuleManager ): result . append ( item ) else : assert isinstance ( item , typing . Mapping ) mm_type = item . get ( \"module_manager_type\" , None ) if not mm_type : raise ValueError ( f \"No module manager type provided in config: { item } \" ) if mm_type == \"python\" : item_config = PythonModuleManagerConfig ( ** item ) elif mm_type == \"pipeline\" : item_config = PipelineModuleManagerConfig ( ** item ) else : raise ValueError ( f \"Invalid module manager type: { mm_type } \" ) result . append ( item_config ) return result @validator ( \"default_processor\" , pre = True ) def _validate_default_processor ( cls , v ): if not v : return SynchronousProcessorConfig () if isinstance ( v , ( SynchronousProcessorConfig , ThreadPoolProcessorConfig )): return v if v == \"synchronous\" : return SynchronousProcessorConfig () if v == \"multi-threaded\" : return ThreadPoolProcessorConfig () if not isinstance ( v , typing . Mapping ): raise ValueError ( f \"Invalid type ' { type ( v ) } ' for default_processor config: { v } \" ) processor_type = v . get ( \"module_processor_type\" , None ) if not processor_type : raise ValueError ( \"No 'module_processor_type' provided: {config} \" ) if processor_type == \"synchronous\" : config = SynchronousProcessorConfig ( ** v ) elif processor_type == \"multi-threaded\" : config = ThreadPoolProcessorConfig () return config data_store : str pydantic-field \u00b6 The path to the local kiara data store. default_processor : Union [ kiara . processing . synchronous . SynchronousProcessorConfig , kiara . processing . parallel . ThreadPoolProcessorConfig ] pydantic-field \u00b6 The configuration for the default processor to use. extra_pipeline_folders : List [ str ] pydantic-field \u00b6 Paths to local folders that contain kiara pipelines. ignore_errors : bool pydantic-field \u00b6 If set, kiara will try to ignore most errors (that can be ignored). module_managers : List [ Union [ kiara . module_mgmt . python_classes . PythonModuleManagerConfig , kiara . module_mgmt . pipelines . PipelineModuleManagerConfig ]] pydantic-field \u00b6 The module managers to use in this kiara instance. yaml_config_settings_source ( settings ) \u00b6 A simple settings source that loads variables from a JSON file at the project's root. Here we happen to choose to use the env_file_encoding from Config when reading config.json Source code in kiara/config.py def yaml_config_settings_source ( settings : BaseSettings ) -> typing . Dict [ str , typing . Any ]: \"\"\" A simple settings source that loads variables from a JSON file at the project's root. Here we happen to choose to use the `env_file_encoding` from Config when reading `config.json` \"\"\" config_file = os . path . join ( kiara_app_dirs . user_config_dir , \"config.yaml\" ) if os . path . exists ( config_file ): data = get_data_from_file ( config_file ) return data else : return {} data special \u00b6 Data and value related classes for Kiara . onboarding special \u00b6 ValueStoreConfig ( BaseModel ) pydantic-model \u00b6 A configuration that describes which step outputs of a pipeline to save, and how. Source code in kiara/data/onboarding/__init__.py class ValueStoreConfig ( BaseModel ): \"\"\"A configuration that describes which step outputs of a pipeline to save, and how.\"\"\" @classmethod def save_fields ( self , base_id : str , value_set : typing . Mapping [ str , Value ], matchers : typing . Optional [ typing . List [ FieldMatcher ]] = None , ) -> typing . Dict [ Value , typing . List [ str ]]: if matchers is None : matchers = FieldMatcher . create_default_matchers () for matcher in matchers : if matcher . matcher_type not in [ \"glob\" , \"regex\" ]: raise NotImplementedError ( \"Only 'glob' and 'regex' field matcher type implemented yet.\" ) if matcher . matcher_type == \"glob\" : matcher . matcher_type = \"regex\" matcher . match_expr = fnmatch . translate ( matcher . match_expr ) env = Environment () to_save : typing . Dict [ str , typing . Set [ str ]] = {} for field_name in value_set . keys (): for matcher in matchers : if not re . match ( matcher . match_expr , field_name ): continue template = env . from_string ( matcher . alias_template ) rendered = template . render ( field_name = field_name , base_id = base_id ) to_save . setdefault ( field_name , set ()) . add ( rendered ) if not matcher . check_next_on_match : break result : typing . Dict [ Value , typing . List [ str ]] = {} for field_name , aliases in to_save . items (): value = value_set [ field_name ] _v = value . save ( aliases = aliases ) result . setdefault ( _v , []) . extend ( aliases ) return result inputs : typing . List [ FieldMatcher ] = Field ( description = \"Whether and how to save inputs.\" , default_factory = list ) outputs : typing . List [ FieldMatcher ] = Field ( description = \"Whether and how to save inputs.\" , default_factory = FieldMatcher . create_default_matchers , ) steps : typing . List [ \"ValueStoreConfig\" ] = Field ( description = \"Whether and how to save step inputs and outputs.\" , default_factory = list , ) inputs : List [ kiara . data . onboarding . FieldMatcher ] pydantic-field \u00b6 Whether and how to save inputs. outputs : List [ kiara . data . onboarding . FieldMatcher ] pydantic-field \u00b6 Whether and how to save inputs. steps : List [ ValueStoreConfig ] pydantic-field \u00b6 Whether and how to save step inputs and outputs. batch \u00b6 BatchOnboard ( BaseModel ) pydantic-model \u00b6 Source code in kiara/data/onboarding/batch.py class BatchOnboard ( BaseModel ): @classmethod def create ( cls , module_type : str , inputs : typing . Mapping [ str , typing . Any ], store_config : typing . Optional [ ValueStoreConfig ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ Kiara ] = None , ): if kiara is None : kiara = Kiara . instance () operation = Operation . create_operation ( kiara = kiara , operation_id = \"_batch_onboarding_\" , config = module_type , module_config = module_config , ) if store_config is None : store_config = ValueStoreConfig () elif isinstance ( store_config , typing . Mapping ): store_config = ValueStoreConfig ( ** store_config ) return BatchOnboard ( operation = operation , inputs = inputs , store_config = store_config ) inputs : typing . Mapping [ str , typing . Any ] = Field ( description = \"The inputs.\" ) operation : Operation = Field ( description = \"The operation to use with the inputs.\" ) store_config : ValueStoreConfig = Field ( description = \"The configuration for storing operation/pipeline values.\" ) def run ( self , base_id : str ) -> typing . Mapping [ Value , typing . Iterable [ str ]]: run_result = self . operation . module . run ( ** self . inputs ) result : typing . Dict [ Value , typing . List [ str ]] = {} if self . store_config . inputs : raise NotImplementedError ( \"Storing inputs not supported yet.\" ) if self . store_config . steps : raise NotImplementedError ( \"Storing step inputs/outputs not supported yet.\" ) if self . store_config . outputs : r = ValueStoreConfig . save_fields ( base_id = base_id , value_set = run_result , matchers = self . store_config . outputs , ) for value , aliases in r . items (): result . setdefault ( value , []) . extend ( aliases ) return result inputs : Mapping [ str , Any ] pydantic-field required \u00b6 The inputs. operation : Operation pydantic-field required \u00b6 The operation to use with the inputs. store_config : ValueStoreConfig pydantic-field required \u00b6 The configuration for storing operation/pipeline values. registry special \u00b6 BaseDataRegistry ( ABC ) \u00b6 Base class to extend if you want to write a kiara data registry. This outlines the main methods that must be available for an entity that holds some values and their data, as well as their associated aliases. In most cases users will interact with subclasses of the more fully featured DataRegistry class, but there are cases where it will make sense to sub-class this base class directly (like for example read-only 'archive' data registries). Source code in kiara/data/registry/__init__.py class BaseDataRegistry ( abc . ABC ): \"\"\"Base class to extend if you want to write a *kiara* data registry. This outlines the main methods that must be available for an entity that holds some values and their data, as well as their associated aliases. In most cases users will interact with subclasses of the more fully featured [DataRegistry][kiara.data.registry.DataRegistry] class, but there are cases where it will make sense to sub-class this base class directly (like for example read-only 'archive' data registries). \"\"\" def __init__ ( self , kiara : \"Kiara\" ): self . _id : str = str ( uuid . uuid4 ()) self . _kiara : Kiara = kiara # self._hashes: typing.Dict[str, typing.Dict[str, str]] = {} self . _register_tokens : typing . Set = set () @property def id ( self ) -> str : return self . _id @property def value_ids ( self ) -> typing . List [ str ]: return list ( self . _get_available_value_ids ()) @property def alias_names ( self ) -> typing . List [ str ]: return sorted ( self . _get_available_aliases ()) # ====================================================================== # main abstract methods @abc . abstractmethod def _get_available_value_ids ( self ) -> typing . Iterable [ str ]: \"\"\"Return all of the registries available value ids.\"\"\" @abc . abstractmethod def _get_value_obj_for_id ( self , value_id : str ) -> Value : pass @abc . abstractmethod def _get_value_data_for_id ( self , value_item : str ) -> typing . Any : pass @abc . abstractmethod def _get_available_aliases ( self ) -> typing . Iterable [ str ]: pass @abc . abstractmethod def _get_value_slot_for_alias ( self , alias_name : str ) -> ValueSlot : pass @abc . abstractmethod def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: pass def _find_value_for_hashes ( self , * hashes : ValueHash ) -> typing . Optional [ Value ]: \"\"\"Return a value that matches one of the provided hashes. This method does not need to be overwritten, but it is recommended that it is done for performance reasons. Arguments: hashes: a collection of hashes, Returns: A (registered) value that matches one of the hashes \"\"\" return None # ----------------------------------------------------------------------- # main value retrieval methods def _create_alias_obj ( self , alias : str ) -> ValueAlias : if alias . startswith ( \"value:\" ): alias = alias [ 6 :] value_alias = ValueAlias . from_string ( value_alias = alias ) if ( value_alias . alias not in self . _get_available_value_ids () and value_alias . alias not in self . _get_available_aliases () ): raise Exception ( f \"Neither id nor alias ' { alias } ' registered with this registry.\" ) if ( value_alias . alias in self . _get_available_aliases () and value_alias . tag is None and value_alias . version is None ): value_alias . version = self . get_latest_version_for_alias ( value_alias . alias ) return value_alias def get_value_obj ( self , value_item : typing . Union [ str , Value , ValueAlias , ValueSlot ], raise_exception : bool = False , ) -> typing . Optional [ Value ]: if value_item == NO_ID_YET_MARKER : raise Exception ( \"Can't get value object: value not fully registered yet.\" ) if isinstance ( value_item , Value ): value_item = value_item . id elif isinstance ( value_item , ValueSlot ): value_item = value_item . get_latest_value () . id if isinstance ( value_item , str ): value_item = self . _create_alias_obj ( value_item ) elif not isinstance ( value_item , ValueAlias ): raise TypeError ( f \"Invalid type ' { type ( value_item ) } ' for value item parameter.\" ) if value_item . alias in self . _get_available_value_ids (): _value_id = value_item . alias _value : typing . Optional [ Value ] = self . _get_value_obj_for_id ( _value_id ) elif value_item . alias in self . _get_available_aliases (): _value = self . _resolve_alias_to_value ( value_item ) else : _value = None if _value is None and raise_exception : raise Exception ( f \"No value or alias registered in registry for: { value_item } \" ) return _value def get_value_data ( self , value_item : typing . Union [ str , Value , ValueSlot , ValueAlias ] ) -> typing . Any : value_obj = self . get_value_obj ( value_item ) if value_obj is None : raise Exception ( f \"No value registered for: { value_item } \" ) if not value_obj . is_set and value_obj . value_schema . default not in ( SpecialValue . NO_VALUE , SpecialValue . NOT_SET , None , ): return value_obj . value_schema . default elif not value_obj . is_set : raise Exception ( f \"Value not set: { value_obj . id } \" ) data = self . _get_value_data_for_id ( value_obj . id ) if data == SpecialValue . NO_VALUE : return None elif isinstance ( data , Value ): return data . get_value_data () else : return data def get_lineage ( self , value_item : typing . Union [ str , Value , ValueSlot ] ) -> typing . Optional [ ValueLineage ]: value_obj = self . get_value_obj ( value_item = value_item ) if value_obj is None : raise Exception ( f \"No value registered for: { value_item } \" ) return self . _get_value_lineage ( value_obj . id ) # def get_value_info(self, value_item: typing.Union[str, Value, ValueAlias]) -> ValueInfo: # # value_obj = self.get_value_obj(value_item=value_item) # # aliases = self.find_aliases_for_value_id(value_id=value_obj.id, include_all_versions=True, include_tags=True) # # info = ValueInfo(value_id=value_obj.id, value_type=value_obj.value_schema.type, aliases=aliases, metadata=value_obj.get_metadata()) # return info def get_value_slot ( self , alias : typing . Union [ str , ValueSlot ] ) -> typing . Optional [ ValueSlot ]: if isinstance ( alias , ValueSlot ): alias = alias . id if alias not in self . alias_names : return None return self . _get_value_slot_for_alias ( alias_name = alias ) # ----------------------------------------------------------------------- # def _resolve_hash_to_value( # self, hash_str: str, hash_type: typing.Optional[str] = None # ) -> typing.Optional[Value]: # # matches: typing.Set[str] = set() # if hash_type is None: # for hash_type, details in self._hashes.items(): # if hash_str in details.keys(): # matches.add(details[hash_str]) # else: # hashes_for_type = self._hashes.get(hash_type, {}) # if hash_str in hashes_for_type.keys(): # matches.add(hashes_for_type[hash_str]) # # if len(matches) == 0: # return None # elif len(matches) > 1: # raise Exception(f\"Multiple values found for hash '{hash_str}'.\") # # match_id = next(iter(matches)) # value = self.get_value_obj(match_id) # return value def _check_register_token ( self , register_token : uuid . UUID ): return register_token in self . _register_tokens # alias-related methods def _resolve_alias_to_value ( self , alias : ValueAlias ) -> typing . Optional [ Value ]: alias_name = alias . alias alias_version = alias . version alias_tag = alias . tag value_slot = self . get_value_slot ( alias_name ) if not value_slot : raise Exception ( f \"No alias ' { alias_name } ' registered with registry.\" ) if alias_tag : _version = value_slot . tags . get ( alias_tag ) if alias_version : if _version != alias_version : raise Exception ( f \"Value alias object contains both tag and version information, but actual version of tag resolves to different version in the registry: { _version } != { alias_version } \" ) else : alias_version = _version assert alias_version is not None value = value_slot . values . get ( alias_version , None ) return value def _find_aliases_for_value_id ( self , value_id : str ) -> typing . List [ ValueAlias ]: \"\"\"Find all aliases that point to the specified value id. Sub-classes may overwrite this method for performance reasons. \"\"\" result = [] for alias in self . _get_available_aliases (): value_slot = self . get_value_slot ( alias ) assert value_slot aliases = value_slot . find_linked_aliases ( value_id ) result . extend ( aliases ) return result def _get_tags_for_alias ( self , alias : str ) -> typing . Iterable [ str ]: value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { alias } ' registered with registry.\" ) return value_slot . tags . keys () def find_aliases_for_value ( self , value_item : typing . Union [ str , Value ], include_all_versions : bool = False , include_tags : bool = False , ) -> typing . List [ ValueAlias ]: value = self . get_value_obj ( value_item ) if value is None : raise Exception ( f \"Can't find registered value for: { value_item } \" ) aliases = self . _find_aliases_for_value_id ( value_id = value . id ) result = [] latest_cache : typing . Dict [ str , int ] = {} for alias in aliases : if alias . version is not None : if not include_all_versions : if alias . alias in latest_cache . keys (): latest_version = latest_cache [ alias . alias ] else : latest_version = self . get_latest_version_for_alias ( alias . alias ) latest_cache [ alias . alias ] = latest_version if latest_version == alias . version : result . append ( alias ) else : result . append ( alias ) if alias . tag is not None and include_tags : result . append ( alias ) return result def get_latest_version_for_alias ( self , alias : str ) -> int : versions = self . get_versions_for_alias ( alias ) if not versions : return 0 else : return max ( versions ) def get_versions_for_alias ( self , alias : str ) -> typing . Iterable [ int ]: \"\"\"Return all available versions for the specified alias.\"\"\" if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . values . keys ()) def get_tags_for_alias ( self , alias : str ) -> typing . Iterable [ str ]: if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . tags . keys ()) @deprecated ( reason = \"This will be removed soon in favor of a more generic way to query values.\" ) def find_all_values_of_type ( self , value_type : str ) -> typing . Dict [ str , Value ]: \"\"\"Find all values of a certain type.\"\"\" result = {} for value_id in self . value_ids : value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"No value registered for: { value_id } \" ) if value_obj . type_name == value_type : # type: ignore result [ value_id ] = value_obj return result def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , DataRegistry ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id ) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (id= { self . id } \" def __str__ ( self ): return self . __repr__ () find_all_values_of_type ( self , value_type ) \u00b6 Find all values of a certain type. Source code in kiara/data/registry/__init__.py @deprecated ( reason = \"This will be removed soon in favor of a more generic way to query values.\" ) def find_all_values_of_type ( self , value_type : str ) -> typing . Dict [ str , Value ]: \"\"\"Find all values of a certain type.\"\"\" result = {} for value_id in self . value_ids : value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"No value registered for: { value_id } \" ) if value_obj . type_name == value_type : # type: ignore result [ value_id ] = value_obj return result get_versions_for_alias ( self , alias ) \u00b6 Return all available versions for the specified alias. Source code in kiara/data/registry/__init__.py def get_versions_for_alias ( self , alias : str ) -> typing . Iterable [ int ]: \"\"\"Return all available versions for the specified alias.\"\"\" if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . values . keys ()) DataRegistry ( BaseDataRegistry ) \u00b6 Source code in kiara/data/registry/__init__.py class DataRegistry ( BaseDataRegistry ): def __init__ ( self , kiara : \"Kiara\" ): self . _lineages : typing . Dict [ str , typing . Optional [ ValueLineage ]] = {} super () . __init__ ( kiara = kiara ) @abc . abstractmethod def _register_value_and_data ( self , value : Value , data : typing . Any ) -> str : \"\"\"Register data into this registry. Returns the values id. In case the value already has a value set (meaning it's different from string [NO_ID_YET_MARKER][kiara.data.value.NO_ID_YET_MARKER] / '__no_id_yet__' this method must use this id or throw an exception. Otherwise, it is required that the result id is different from ``NO_ID_YET_MARKER`` and a non-empty string. \"\"\" @abc . abstractmethod def _register_remote_value ( self , value : Value ) -> Value : \"\"\"Register an existing value from a different registry into this one. Arguments: value: the original value (with the '_registry' attribute still pointing to the original registry) Returns: either None (in which case the original value object will be copied with the '_registry' and '_kiara' attributes adjusted), or a value object. \"\"\" def register_data ( self , value_data : typing . Any = SpecialValue . NOT_SET , value_schema : typing . Union [ None , ValueSchema , str ] = None , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> Value : # if lineage and not lineage.output_name: # raise Exception(f\"Value lineage for data of type '{value_data.value_schema.type}' does not have 'output_name' field set.\") value_id = str ( uuid . uuid4 ()) if value_id in self . value_ids : raise Exception ( f \"Can't register value: value id ' { value_id } ' already exists.\" ) if value_id in self . alias_names : raise Exception ( f \"Can't register value: value id ' { value_id } ' already exists as alias.\" ) if value_schema is None : if isinstance ( value_data , Value ): value_schema = value_data . value_schema else : raise Exception ( f \"No value schema provided for value: { value_data } \" ) elif isinstance ( value_schema , str ): value_schema = ValueSchema ( type = value_schema ) cls = self . _kiara . get_value_type_cls ( value_schema . type ) _type_obj = cls ( ** value_schema . type_config ) existing_value : typing . Optional [ Value ] = None if isinstance ( value_data , Value ): if value_data . id in self . value_ids and not metadata : if lineage : existing_lineage = self . _lineages . get ( value_data . id , None ) if existing_lineage : log_message ( f \"Overwriting existing value lineage for: { value_data . id } \" ) self . _lineages [ value_data . id ] = lineage # TODO: check it's really the same our_value = self . get_value_obj ( value_data . id ) if our_value is None : raise Exception ( \"Could not retrieve cloned value, this is probably a bug.\" ) return our_value existing_value = self . _find_value_for_hashes ( * value_data . get_hashes ()) if existing_value and not metadata : if lineage and self . _lineages . get ( existing_value . id , None ): log_message ( f \"Overwriting value lineage for: { existing_value . id } \" ) if lineage : self . _lineages [ existing_value . id ] = lineage return existing_value if value_data . id in self . value_ids and metadata : _metadata : typing . Optional [ typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = None if metadata : _metadata = {} for k , v in metadata . items (): _metadata [ k ] = {} _metadata [ k ][ \"metadata_item\" ] = v . dict () _metadata [ k ][ \"metadata_schema\" ] = v . schema_json () # TODO: not resolve value_data ? cloned_new_metadata = self . _create_value_obj ( value_schema = value_data . value_schema , is_set = value_data . is_set , is_none = value_data . is_none , type_obj = value_data . type_obj , metadata = _metadata , value_hashes = None , ) r = self . _register_new_value_obj ( value_obj = cloned_new_metadata , value_data = value_data . get_value_data (), lineage = lineage , # value_hashes=value_hashes, ) return r copied_value = self . _register_remote_value ( value_data ) if value_data . id not in self . value_ids : raise Exception ( f \"Value with id ' { value_data . id } ' wasn't successfully registered. This is most likely a bug.\" ) assert copied_value . _registry == self assert copied_value . _kiara == self . _kiara if value_data . id != copied_value . id : log_message ( f \"Value id for value of type { copied_value . type_name } changed when registering in other registry.\" ) raise Exception ( f \"Imported value object with id ' { value_data . id } ' resulted in copied value with different id. This is a bug.\" ) # for hash_type, value_hash in copied_value.get_hashes().items(): # self._hashes.setdefault(hash_type, {})[value_hash.hash] = value_id self . _lineages [ copied_value . id ] = value_data . get_lineage () return copied_value if value_data in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE , ] and value_schema . default not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE , ]: if metadata : raise NotImplementedError () value_data = copy . deepcopy ( value_schema . default ) if value_data not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ]: # TODO: actually implement that for hash_type in _type_obj . get_supported_hash_types (): hash_str = _type_obj . calculate_value_hash ( value = value_data , hash_type = hash_type ) existing_value = self . _find_value_for_hashes ( ValueHash ( hash_type = hash_type , hash = hash_str ) ) if existing_value : break if existing_value : if metadata : raise NotImplementedError () if lineage and self . _lineages . get ( existing_value . id , None ): log_message ( f \"Overwriting lineage for value ' { existing_value . id } '.\" ) if lineage : self . _lineages [ existing_value . id ] = lineage return existing_value if value_schema . is_constant : if ( value_data not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ] and value_data != value_schema . default ): raise NotImplementedError () value_data = value_schema . default is_set = value_data != SpecialValue . NOT_SET is_none = value_data in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ] _metadata = None if metadata : _metadata = {} for k , v in metadata . items (): _metadata [ k ] = {} _metadata [ k ][ \"metadata_item\" ] = v . dict () _metadata [ k ][ \"metadata_schema\" ] = v . schema_json () # value_hashes: typing.Dict[str] = {} value = self . _create_value_obj ( value_schema = value_schema , is_set = is_set , is_none = is_none , type_obj = _type_obj , metadata = _metadata , value_hashes = None , ) self . _register_new_value_obj ( value_obj = value , value_data = value_data , lineage = lineage , # value_hashes=value_hashes, ) # if not value.is_none: # value.get_metadata() # metadata = self._kiara.metadata_mgmt.get_value_metadata(value=value, also_return_schema=True) # print(metadata) return value def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: return self . _lineages . get ( value_id , None ) def _create_value_obj ( self , value_schema : ValueSchema , is_set : bool , is_none : bool , type_obj : typing . Optional [ ValueType ] = None , value_hashes : typing . Optional [ typing . Iterable [ ValueHash ]] = None , metadata : typing . Optional [ typing . Mapping [ str , typing . Mapping [ str , typing . Any ]] ] = None , ): # if value_schema.is_constant and value_data not in [ # SpecialValue.NO_VALUE, # SpecialValue.NOT_SET, # None, # ]: # raise Exception( # \"Can't create value. Is a constant, but value data was provided.\" # ) if type_obj is None : if value_schema . type not in self . _kiara . value_types : log_message ( f \"Value type ' { value_schema . type } ' not supported in this kiara environment, changing type to 'any'.\" ) type_cls = self . _kiara . get_value_type_cls ( \"any\" ) else : type_cls = self . _kiara . get_value_type_cls ( value_schema . type ) type_obj = type_cls ( ** value_schema . type_config ) register_token = uuid . uuid4 () self . _register_tokens . add ( register_token ) try : value = Value ( registry = self , # type: ignore value_schema = value_schema , type_obj = type_obj , # type: ignore is_set = is_set , is_none = is_none , hashes = value_hashes , metadata = metadata , register_token = register_token , # type: ignore ) return value finally : self . _register_tokens . remove ( register_token ) def _register_new_value_obj ( self , value_obj : Value , value_data : typing . Any , lineage : typing . Optional [ ValueLineage ], # value_hashes: typing.Optional[typing.Mapping[str, ValueHash]] = None, ): # assert value_obj.id == NO_ID_YET_MARKER if value_data not in [ SpecialValue . NO_VALUE , SpecialValue . NOT_SET , SpecialValue . IGNORE , None , ]: # TODO: should we keep the original value? value_data = value_obj . type_obj . import_value ( value_data ) value_id = self . _register_value_and_data ( value = value_obj , data = value_data ) if value_obj . id == NO_ID_YET_MARKER : value_obj . id = value_id if value_obj . id != value_id : raise Exception ( f \"Inconsistent id for value: { value_obj . id } != { value_id } \" ) if value_id not in self . value_ids : raise Exception ( f \"Value id ' { value_id } ' wasn't registered propertly in registry. This is most likely a bug.\" ) if lineage : self . _lineages [ value_id ] = lineage # if value_hashes is None: # value_hashes = {} # for hash_type, value_hash in value_hashes.items(): # self._hashes.setdefault(hash_type, {})[value_hash.hash] = value_id return value_obj def set_value_lineage ( self , value_item : typing . Union [ str , Value ], value_lineage : ValueLineage ): value_obj = self . get_value_obj ( value_item ) if value_obj is None : raise Exception ( f \"No value available for: { value_item } \" ) self . _lineages [ value_obj . id ] = value_lineage def link_alias ( self , value : typing . Union [ str , Value ], alias : str , register_missing_alias : bool = True , ) -> None : value_obj = self . get_value_obj ( value ) if value_obj is None : raise Exception ( f \"No value for: { value } \" ) value_slot = self . get_value_slot ( alias = alias ) if value_slot is None : if not register_missing_alias : raise Exception ( f \"Can't link value to alias ' { alias } ': alias not registered.\" ) else : self . register_alias ( value_or_schema = value_obj , alias_name = alias ) else : value_alias = ValueAlias . from_string ( alias ) if value_alias . tag : value_slot . add_value ( value_obj , tags = [ value_alias . tag ]) else : value_slot . add_value ( value_obj ) def link_aliases ( self , value : typing . Union [ str , Value ], * aliases : str , register_missing_aliases : bool = True , ) -> None : value_obj = self . get_value_obj ( value ) if value_obj is None : raise Exception ( f \"No value for: { value } \" ) if not register_missing_aliases : invalid = [] for alias in aliases : if alias not in self . alias_names : invalid . append ( alias ) if invalid : raise Exception ( f \"Can't link value(s), invalid alias(es): { ', ' . join ( invalid ) } \" ) for alias in aliases : self . link_alias ( value_obj , alias = alias ) def _check_valid_alias ( self , alias_name : str ): match = bool ( re . match ( VALID_ALIAS_PATTERN , alias_name )) return True if match else False @abc . abstractmethod def _register_alias ( self , alias_name : str , value_schema : ValueSchema ) -> ValueSlot : pass def register_aliases ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], aliases : typing . Iterable [ str ], callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> typing . Mapping [ str , ValueSlot ]: invalid = [] for alias in aliases : if not self . _check_valid_alias ( alias_name = alias ): invalid . append ( alias ) if invalid : raise Exception ( f \"Invalid alias(es), only alphanumeric characters, '-', and '_' allowed in alias name: { ', ' . join ( invalid ) } \" ) result = {} for alias in aliases : vs = self . register_alias ( value_or_schema = value_or_schema , alias_name = alias , callbacks = callbacks ) result [ alias ] = vs return result def register_alias ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], # preseed: bool = False, alias_name : typing . Optional [ str ] = None , callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> ValueSlot : \"\"\"Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Arguments: value_or_schema: a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) preseed: whether to add an empty/non-set value to the newly created slot alias_name: the alias name, will be auto-created if not provided callbacks: a list of callbacks to trigger whenever the alias is updated \"\"\" if alias_name is None : alias_name = str ( uuid . uuid4 ()) if not alias_name : raise Exception ( \"Empty alias name not allowed.\" ) match = self . _check_valid_alias ( alias_name ) if not match : raise Exception ( f \"Invalid alias ' { alias_name } ': only alphanumeric characters, '-', and '_' allowed in alias name.\" ) # if the provided value_or_schema argument is a string, it must be a value_id (for now) if isinstance ( value_or_schema , str ): if value_or_schema not in self . value_ids : raise Exception ( f \"Can't register alias ' { alias_name } ', provided string ' { value_or_schema } ' not an existing value id.\" ) _value_or_schema = self . get_value_obj ( value_or_schema ) if _value_or_schema is None : raise Exception ( f \"No value registered for: { value_or_schema } \" ) value_or_schema = _value_or_schema if isinstance ( value_or_schema , Value ): _value_schema = value_or_schema . value_schema _value : typing . Optional [ Value ] = value_or_schema elif isinstance ( value_or_schema , ValueSchema ): _value_schema = value_or_schema _value = None else : raise TypeError ( f \"Invalid value type: { type ( value_or_schema ) } \" ) if alias_name in self . _get_available_aliases (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists.\" ) elif alias_name in self . _get_available_value_ids (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists as value id.\" ) # vs = ValueSlot.from_value(id=_id, value=value_or_schema) vs = self . _register_alias ( alias_name = alias_name , value_schema = _value_schema ) # self._value_slots[vs.id] = vs # if preseed: # _v = Value(value_schema=_value_schema, kiara=self._kiara, registry=self) # vs.add_value(_v) if callbacks : vs . register_callbacks ( * callbacks ) # self.register_callbacks(vs, *callbacks) if _value is not None : vs . add_value ( _value ) return vs # def find_value_slots( # self, value_item: typing.Union[str, Value] # ) -> typing.List[ValueSlot]: # # value_item = self.get_value_obj(value_item) # result = [] # for slot_id, slot in self._value_slots.items(): # if slot.is_latest_value(value_item): # result.append(slot) # return result # def register_callbacks( # self, # alias: typing.Union[str, ValueSlot], # *callbacks: ValueSlotUpdateHandler, # ) -> None: # # _value_slot = self.get_value_slot(alias) # _value_slot.register_callbacks(*callbacks) def update_value_slot ( self , value_slot : typing . Union [ str , Value , ValueSlot ], data : typing . Any , value_lineage : typing . Optional [ ValueLineage ] = None , ) -> bool : # first, resolve a potential string into a value_slot or value if isinstance ( value_slot , str ): if value_slot in self . alias_names : _value_slot : typing . Union [ None , Value , ValueSlot , str ] = self . get_value_slot ( alias = value_slot ) elif value_slot in self . value_ids : _value_slot = self . get_value_obj ( value_slot ) else : _value_slot = value_slot if _value_slot is None : raise Exception ( f \"Can't retrieve target object for id: { value_slot } \" ) value_slot = _value_slot if isinstance ( value_slot , Value ): aliases = self . find_aliases_for_value ( value_slot . id ) # slots = self.find_value_slots(value_slot) if len ( aliases ) == 0 : raise Exception ( f \"No value slot found for value ' { value_slot . id } '.\" ) elif len ( aliases ) > 1 : raise Exception ( f \"Multiple value slots found for value ' { value_slot . id } '. This is not supported (yet).\" ) _value_slot_2 : typing . Optional [ ValueSlot ] = self . get_value_slot ( alias = aliases [ 0 ] . alias ) elif isinstance ( value_slot , ValueSlot ): _value_slot_2 = value_slot else : raise TypeError ( f \"Invalid type for value slot: { type ( value_slot ) } \" ) assert _value_slot_2 is not None if isinstance ( data , Value ): if value_lineage : raise Exception ( \"Can't update value slot with new value lineage data.\" ) _value : Value = data else : _value = self . register_data ( value_data = data , value_schema = value_slot . value_schema , lineage = value_lineage , ) # _value = self.create_value( # value_data=data, # value_schema=_value_slot.value_schema, # value_lineage=value_lineage, # ) return self . _update_value_slot ( value_slot = _value_slot_2 , new_value = _value , trigger_callbacks = True ) def update_value_slots ( self , updated_values : typing . Mapping [ typing . Union [ str , ValueSlot ], typing . Any ], metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> typing . Mapping [ ValueSlot , typing . Union [ bool , Exception ]]: updated : typing . Dict [ str , typing . List [ ValueSlot ]] = {} cb_map : typing . Dict [ str , ValueSlotUpdateHandler ] = {} result : typing . Dict [ ValueSlot , typing . Union [ bool , Exception ]] = {} invalid : typing . Set [ str ] = set () for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id _alias = self . get_value_slot ( alias ) if _alias is None : if isinstance ( alias , ValueSlot ): invalid . add ( alias . id ) else : invalid . add ( alias ) if invalid : raise Exception ( f \"Can't update value slots: invalid alias name(s) ' { ', ' . join ( invalid ) } '.\" ) for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id value_slot = self . get_value_slot ( alias ) if value_slot is None : raise Exception ( f \"Can't retrieve value slot for alias: { alias } \" ) for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id value_slot = self . get_value_slot ( alias ) if value_slot is None : raise Exception ( f \"No value slot for alias: { alias } .\" ) try : if isinstance ( value_item , Value ): _value_item : Value = value_item if _value_item . _registry != self : _value_item = self . register_data ( _value_item , metadata = metadata , lineage = lineage ) elif metadata : _value_item = self . register_data ( _value_item , metadata = metadata , lineage = lineage ) else : _value_item = self . register_data ( value_data = value_item , value_schema = value_slot . value_schema , metadata = metadata , lineage = lineage , ) updated_item = self . _update_value_slot ( value_slot = value_slot , new_value = _value_item , trigger_callbacks = False , ) result [ value_slot ] = updated_item if updated_item : for cb_id , cb in value_slot . _callbacks . items (): cb_map [ cb_id ] = cb updated . setdefault ( cb_id , []) . append ( value_slot ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () result [ value_slot ] = e for cb_id , value_slots in updated . items (): cb = cb_map [ cb_id ] cb . values_updated ( * value_slots ) return result def _update_value_slot ( self , value_slot : ValueSlot , new_value : Value , trigger_callbacks : bool = True ) -> bool : last_version = value_slot . latest_version_nr new_version = value_slot . add_value ( new_value , trigger_callbacks = trigger_callbacks ) updated = last_version != new_version return updated register_alias ( self , value_or_schema , alias_name = None , callbacks = None ) \u00b6 Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Parameters: Name Type Description Default value_or_schema Union[kiara.data.values.Value, kiara.data.values.ValueSchema, str] a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) required preseed whether to add an empty/non-set value to the newly created slot required alias_name Optional[str] the alias name, will be auto-created if not provided None callbacks Optional[Iterable[kiara.data.registry.ValueSlotUpdateHandler]] a list of callbacks to trigger whenever the alias is updated None Source code in kiara/data/registry/__init__.py def register_alias ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], # preseed: bool = False, alias_name : typing . Optional [ str ] = None , callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> ValueSlot : \"\"\"Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Arguments: value_or_schema: a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) preseed: whether to add an empty/non-set value to the newly created slot alias_name: the alias name, will be auto-created if not provided callbacks: a list of callbacks to trigger whenever the alias is updated \"\"\" if alias_name is None : alias_name = str ( uuid . uuid4 ()) if not alias_name : raise Exception ( \"Empty alias name not allowed.\" ) match = self . _check_valid_alias ( alias_name ) if not match : raise Exception ( f \"Invalid alias ' { alias_name } ': only alphanumeric characters, '-', and '_' allowed in alias name.\" ) # if the provided value_or_schema argument is a string, it must be a value_id (for now) if isinstance ( value_or_schema , str ): if value_or_schema not in self . value_ids : raise Exception ( f \"Can't register alias ' { alias_name } ', provided string ' { value_or_schema } ' not an existing value id.\" ) _value_or_schema = self . get_value_obj ( value_or_schema ) if _value_or_schema is None : raise Exception ( f \"No value registered for: { value_or_schema } \" ) value_or_schema = _value_or_schema if isinstance ( value_or_schema , Value ): _value_schema = value_or_schema . value_schema _value : typing . Optional [ Value ] = value_or_schema elif isinstance ( value_or_schema , ValueSchema ): _value_schema = value_or_schema _value = None else : raise TypeError ( f \"Invalid value type: { type ( value_or_schema ) } \" ) if alias_name in self . _get_available_aliases (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists.\" ) elif alias_name in self . _get_available_value_ids (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists as value id.\" ) # vs = ValueSlot.from_value(id=_id, value=value_or_schema) vs = self . _register_alias ( alias_name = alias_name , value_schema = _value_schema ) # self._value_slots[vs.id] = vs # if preseed: # _v = Value(value_schema=_value_schema, kiara=self._kiara, registry=self) # vs.add_value(_v) if callbacks : vs . register_callbacks ( * callbacks ) # self.register_callbacks(vs, *callbacks) if _value is not None : vs . add_value ( _value ) return vs ValueSlotUpdateHandler ( Protocol ) \u00b6 The call signature for callbacks that can be registered as value update handlers. Source code in kiara/data/registry/__init__.py class ValueSlotUpdateHandler ( Protocol ): \"\"\"The call signature for callbacks that can be registered as value update handlers.\"\"\" def values_updated ( self , * items : \"ValueSlot\" ) -> typing . Any : ... store \u00b6 LocalDataStore ( DataRegistry ) \u00b6 An implementation of a kiara data registry that stores data locally, so it can be accessed across sessions. Data is stored under the value for the configured 'base_path' attribute, along with LoadConfig data that describes how the data can be re-loaded into memory. Source code in kiara/data/registry/store.py class LocalDataStore ( DataRegistry ): \"\"\"An implementation of a *kiara* data registry that stores data locally, so it can be accessed across sessions. Data is stored under the value for the configured 'base_path' attribute, along with [LoadConfig][kiara.metadata.data.LoadConfig] data that describes how the data can be re-loaded into memory. \"\"\" def __init__ ( self , kiara : \"Kiara\" , base_path : typing . Union [ str , Path ]): if isinstance ( base_path , str ): base_path = Path ( base_path ) self . _base_path : Path = base_path self . _value_obj_cache : typing . Dict [ str , Value ] = {} self . _value_info_cache : typing . Dict [ str , SavedValueInfo ] = {} self . _data_cache : typing . Dict [ str , Value ] = {} self . _metadata_cache : typing . Dict [ str , typing . Any ] = {} self . _value_slots : typing . Dict [ str , ValueSlot ] = {} super () . __init__ ( kiara = kiara ) @property def base_path ( self ) -> Path : return self . _base_path def _get_saved_value_info ( self , value_id : str ) -> SavedValueInfo : if value_id in self . _value_info_cache . keys (): return self . _value_info_cache [ value_id ] metadata_path = self . get_metadata_path ( value_id = value_id ) if not metadata_path . exists (): raise Exception ( f \"No value with id ' { value_id } ' registered.\" ) info_content = metadata_path . read_text () info_dict = json . loads ( info_content ) value_info = SavedValueInfo ( ** info_dict ) assert value_info . value_id == value_id self . _value_info_cache [ value_id ] = value_info return value_info def _get_value_obj_for_id ( self , value_id : str ) -> Value : if value_id in self . _value_obj_cache . keys (): return self . _value_obj_cache [ value_id ] value_info : SavedValueInfo = self . _get_saved_value_info ( value_id ) value_schema = value_info . value_schema # value_schema = ValueSchema( # type=value_info.value_type, type_config=value_info.value_type_config # ) value_obj = self . _create_value_obj ( value_schema = value_schema , is_set = True , is_none = False , value_hashes = value_info . hashes , metadata = value_info . metadata , ) value_obj . id = value_info . value_id self . _register_new_value_obj ( value_obj = value_obj , value_data = SpecialValue . IGNORE , lineage = value_info . lineage , # value_hashes=value_info.hashes, ) self . _value_obj_cache [ value_obj . id ] = value_obj return value_obj def _get_value_data_for_id ( self , value_id : str ) -> typing . Any : if value_id in self . _data_cache . keys (): return self . _data_cache [ value_id ] . get_value_data () # # value_obj = self.get_value_obj(value_item=value_id) # metadata_path = self.get_metadata_path(value_id=value_id) # if not metadata_path.exists(): # raise Exception(f\"No value with id '{value_id}' registered.\") # # info_content = metadata_path.read_text() # info_dict = json.loads(info_content) # value_info = SavedValueInfo(**info_dict) value_info = self . _get_saved_value_info ( value_id = value_id ) assert value_info . value_id == value_id load_config = value_info . load_config value : Value = self . _kiara . run ( ** load_config . dict ( exclude = { \"value_id\" , \"base_path_input_name\" , \"doc\" })) # type: ignore self . _data_cache [ value_id ] = value return value . get_value_data () def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: return self . _get_saved_value_info ( value_id ) . lineage def _register_value_and_data ( self , value : Value , data : typing . Any ) -> str : if data == SpecialValue . IGNORE : return value . id raise NotImplementedError () # # new_value_id = str(uuid.uuid4()) # # value.id = new_value_id # value_type = value.type_name # # # run the type and repo type specific save module, and retrieve the load_config that is needed to retrieve it again later # save_config = self._prepare_save_config( # value_id=new_value_id, value_type=value_type, value=value # ) # # save_module = save_config.create_module(kiara=self._kiara) # result = save_module.run(**save_config.inputs) # load_config_value: Value = result.get_value_obj(\"load_config\") # # # assemble the value metadata # metadata: typing.Dict[str, typing.Mapping[str, typing.Any]] = dict( # value.get_metadata(also_return_schema=True) # ) def _register_remote_value ( self , value : Value ) -> Value : value_metadata_path = self . get_metadata_path ( value . id ) if value_metadata_path . exists (): raise Exception ( f \"Can't register remote value with id ' { value . id } '. Load config for this id already exists: { value_metadata_path . as_posix () } \" ) value_type = value . type_name # run the type and repo type specific save module, and retrieve the load_config that is needed to retrieve it again later save_config = self . _prepare_save_config ( value_id = value . id , value_type = value_type , value = value ) save_module = save_config . create_module ( kiara = self . _kiara ) result = save_module . run ( ** save_config . inputs ) load_config_value : Value = result . get_value_obj ( \"load_config\" ) field_name = value_type if field_name == \"any\" : field_name = \"value_item\" saved_value : Value = result . get_value_obj ( field_name ) load_config_data = load_config_value . get_value_data () metadata = saved_value . get_metadata ( also_return_schema = True ) value_info = SavedValueInfo ( value_id = value . id , value_schema = saved_value . value_schema , is_valid = saved_value . item_is_valid (), hashes = saved_value . get_hashes (), lineage = value . get_lineage (), save_lineage = saved_value . get_lineage (), metadata = metadata , load_config = load_config_data , ) value_metadata_path . parent . mkdir ( parents = True , exist_ok = True ) value_metadata_path . write_text ( value_info . json ()) copied_value = saved_value . copy () copied_value . _registry = self copied_value . _kiara = self . _kiara if copied_value . id != value . id : log_message ( f \"Saved value id different to original id ' { value . id } .\" ) copied_value . id = value . id self . _value_obj_cache [ copied_value . id ] = copied_value return copied_value def _prepare_save_config ( self , value_id : str , value_type : str , value : \"Value\" ) -> SaveConfig : store_operations : StoreOperationType = self . _kiara . operation_mgmt . get_operations ( \"store_value\" ) # type: ignore op_config = store_operations . get_store_operation_for_type ( value_type ) if not op_config : raise Exception ( f \"Can't save value: no save operation found for value type ' { value_type } '\" ) target_path = self . get_data_path ( value_id = value_id ) metadata_path = self . get_metadata_path ( value_id = value_id ) if os . path . exists ( metadata_path ): raise Exception ( f \"Can't save value, metadata file already exists: { metadata_path } \" ) input_name = value . type_name if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : value . get_value_data (), \"base_path\" : target_path . as_posix (), \"value_id\" : value_id , } # type: ignore save_config = SaveConfig ( module_type = op_config . module_type , module_config = op_config . module_config , inputs = inputs , load_config_output = \"load_config\" , ) return save_config def get_data_path ( self , value_id : str ) -> Path : return self . _base_path / f \"value_ { value_id } \" / \"data\" def get_metadata_path ( self , value_id : str ) -> Path : path = self . _base_path / f \"value_ { value_id } \" / \"value_metadata.json\" return path # def get_load_config_path(self, value_id: str) -> Path: # path = self._base_path / f\"value_{value_id}\" / \"load_config.json\" # return path def _get_value_slot_for_alias ( self , alias_name : str ) -> ValueSlot : if alias_name in self . _value_slots . keys (): return self . _value_slots [ alias_name ] alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" if not alias_file . is_file (): raise Exception ( f \"No alias with name ' { alias_name } ' registered.\" ) file_content = alias_file . read_text () alias_data = json . loads ( file_content ) value_schema = ValueSchema ( ** alias_data [ \"value_schema\" ]) value_slot = ValueSlot ( id = alias_name , value_schema = value_schema , kiara = self . _kiara , registry = self ) value_slot . register_callbacks ( self ) for version in sorted (( int ( x ) for x in alias_data [ \"versions\" ] . keys ())): value_id = alias_data [ \"versions\" ][ str ( version )] value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"Can't find value with id: { value_id } \" ) tags : typing . Optional [ typing . Iterable [ str ]] = None if value_id in alias_data [ \"tags\" ] . values (): tags = [ tag for tag , version in alias_data [ \"tags\" ] . items () if version == version ] new_version = value_slot . add_value ( value_obj , trigger_callbacks = False , tags = tags ) if new_version != int ( version ): raise Exception ( f \"New version different to stored version ( { new_version } != { version } ). This is a bug.\" ) self . _value_slots [ alias_name ] = value_slot return self . _value_slots [ alias_name ] def _get_available_aliases ( self ) -> typing . Iterable [ str ]: alias_path = self . _base_path / \"aliases\" alias_file = alias_path . glob ( \"alias_*.json\" ) result = [] for af in alias_file : if not af . is_file (): log_message ( f \"Ignoring non-file: { af . as_posix () } \" ) continue alias_name = af . name [ 6 : - 5 ] result . append ( alias_name ) return result def _get_available_value_ids ( self ) -> typing . Iterable [ str ]: value_ids = [ x . parent . name [ 6 :] for x in self . _base_path . glob ( \"value_*/value_metadata.json\" ) ] return value_ids def _register_alias ( self , alias_name : str , value_schema : ValueSchema ) -> ValueSlot : alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" if alias_file . exists (): raise Exception ( f \"Alias ' { alias_name } ' already registered.\" ) if alias_name in self . _value_slots . keys (): raise Exception ( f \"Value slot for alias ' { alias_name } ' already exists.\" ) alias_file . parent . mkdir ( parents = True , exist_ok = True ) vs = ValueSlot ( id = alias_name , value_schema = value_schema , kiara = self . _kiara , registry = self ) self . _value_slots [ alias_name ] = vs self . _write_alias_file ( alias_name ) vs . register_callbacks ( self ) return vs def _write_alias_file ( self , alias_name ): alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" value_slot = self . _value_slots [ alias_name ] alias_dict = { \"value_schema\" : value_slot . value_schema . dict (), \"versions\" : {}, \"tags\" : {}, } for version , value in value_slot . values . items (): alias_dict [ \"versions\" ][ version ] = value . id for tag_name , version in value_slot . tags . items (): alias_dict [ \"tags\" ][ tag_name ] = version alias_file . write_text ( json . dumps ( alias_dict )) def values_updated ( self , * items : \"ValueSlot\" ): invalid = [] for item in items : if item . id not in self . _value_slots . keys (): invalid . append ( item . id ) if invalid : raise Exception ( f \"Can't update value(s), invalid aliases: { ', ' . join ( invalid ) } \" ) for item in items : self . _write_alias_file ( alias_name = item . id ) SavedValueInfo ( ValueInfo ) pydantic-model \u00b6 Source code in kiara/data/registry/store.py class SavedValueInfo ( ValueInfo ): load_config : LoadConfig = Field ( description = \"The configuration to load this value from disk (or however it is stored).\" ) save_lineage : ValueLineage = Field ( description = \"Information about how the value was saved.\" ) load_config : LoadConfig pydantic-field required \u00b6 The configuration to load this value from disk (or however it is stored). save_lineage : ValueLineage pydantic-field required \u00b6 Information about how the value was saved. types special \u00b6 This is the base module that contains everything data type-related in kiara . I'm still not 100% sure how to best implement the kiara type system, there are several ways it could be done, for example based on Python type-hints, using JSON-schema, Avro (which is my 2nd favourite option), as well as by implementing a custom type-class hierarchy. Which is what I have choosen to try first. For now, it looks like it'll work out, but there is a chance requirements I haven't forseen will crop up that could make this become ugly. Anyway, the way it works (for now) is that kiara comes with a set of often used types (the standard set of: scalars, list, dict, table & array, etc.) which each come with 2 functions that can serialize and deserialize values of that type in a persistant fashion -- which could be storing as a file on disk, or as a cell/row in a database. Those functions will most likley be kiara modules themselves, with even more restricted input/output type options. In addition, packages that contain modules can implement their own, custom types, if suitable ones are not available in core- kiara . Those can either be 'serialized/deserialized' into kiara -native types (which in turn will serialize them using their own serializing functions), or will have to implement custom serializing functionality (which will probably be discouraged, since this might not be trivial and there are quite a few things to consider). ValueType ( ABC , Generic ) \u00b6 Base class that all kiara types must inherit from. kiara types have 3 main responsibilities: serialize into / deserialize from persistent state data validation metadata extraction Serializing being the arguably most important of those, because without most of the data management features of kiara would be impossible. Validation should not require any explanation. Metadata extraction is important, because that metadata will be available to other components of kiara (or frontends for it), without them having to request the actual data. That will hopefully make kiara very efficient in terms of memory management, as well as data transfer and I/O. Ideally, the actual data (bytes) will only be requested at the last possible moment. For example when a module needs the input data to do processing on it -- and even then it might be that it only requests a part of the data, say a single column of a table. Or when a frontend needs to display/visualize the data. Source code in kiara/data/types/__init__.py class ValueType ( abc . ABC , typing . Generic [ TYPE_PYTHON_CLS , TYPE_CONFIG_CLS ]): \"\"\"Base class that all *kiara* types must inherit from. *kiara* types have 3 main responsibilities: - serialize into / deserialize from persistent state - data validation - metadata extraction Serializing being the arguably most important of those, because without most of the data management features of *kiara* would be impossible. Validation should not require any explanation. Metadata extraction is important, because that metadata will be available to other components of *kiara* (or frontends for it), without them having to request the actual data. That will hopefully make *kiara* very efficient in terms of memory management, as well as data transfer and I/O. Ideally, the actual data (bytes) will only be requested at the last possible moment. For example when a module needs the input data to do processing on it -- and even then it might be that it only requests a part of the data, say a single column of a table. Or when a frontend needs to display/visualize the data. \"\"\" @classmethod def get_type_metadata ( cls ) -> ValueTypeMetadata : return ValueTypeMetadata . from_value_type_class ( cls ) # @classmethod # def doc(cls) -> str: # # return extract_doc_from_cls(cls) # # @classmethod # def desc(cls) -> str: # return extract_doc_from_cls(cls, only_first_line=True) # @classmethod # def conversions( # self, # ) -> typing.Optional[typing.Mapping[str, typing.Mapping[str, typing.Any]]]: # \"\"\"Return a dictionary of configuration for modules that can transform this type. # # The name of the transformation is the key of the result dictionary, the configuration is a module configuration # (dictionary wth 'module_type' and optional 'module_config', 'input_name' and 'output_name' keys). # \"\"\" # # return {\"string\": {\"module_type\": \"string.pretty_print\", \"input_name\": \"item\"}} @classmethod def check_data ( cls , data : typing . Any ) -> typing . Optional [ \"ValueType\" ]: \"\"\"Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). \"\"\" return None @classmethod @abc . abstractmethod def backing_python_type ( cls ) -> typing . Type [ TYPE_PYTHON_CLS ]: pass @classmethod @abc . abstractmethod def type_config_cls ( cls ) -> typing . Type [ TYPE_CONFIG_CLS ]: pass @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return None @classmethod def get_supported_hash_types ( cls ) -> typing . Iterable [ str ]: return [] @classmethod def calculate_value_hash ( cls , value : typing . Any , hash_type : str ) -> str : \"\"\"Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. \"\"\" raise Exception ( f \"Value type ' { cls . _value_type_name } ' does not support hash calculation.\" ) # type: ignore def __init__ ( self , ** type_config : typing . Any ): try : self . _type_config : TYPE_CONFIG_CLS = self . __class__ . type_config_cls ( ** type_config ) # type: ignore # TODO: double-check this is only a mypy issue except ValidationError as ve : raise ValueTypeConfigException ( f \"Error creating object for value_type: { ve } \" , self . __class__ , type_config , ve , ) # self._type_config: typing.Mapping[str, typing.Any] = self # self._transformations: typing.Optional[ # typing.Mapping[str, typing.Mapping[str, typing.Any]] # ] = None @property def type_config ( self ) -> TYPE_CONFIG_CLS : return self . _type_config def import_value ( self , value : typing . Any ) -> typing . Any : assert value is not None try : parsed = self . parse_value ( value ) if parsed is None : parsed = value self . validate ( parsed ) except Exception as e : raise KiaraValueException ( value_type = self . __class__ , value_data = value , exception = e ) return parsed def parse_value ( self , value : typing . Any ) -> typing . Any : \"\"\"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Arguments: v: the value Returns: 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object \"\"\" return None def validate ( cls , value : typing . Any ) -> None : \"\"\"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type).\"\"\" def get_type_hint ( self , context : str = \"python\" ) -> typing . Optional [ typing . Type ]: \"\"\"Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. \"\"\" return None def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : raise NotImplementedError () calculate_value_hash ( value , hash_type ) classmethod \u00b6 Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. Source code in kiara/data/types/__init__.py @classmethod def calculate_value_hash ( cls , value : typing . Any , hash_type : str ) -> str : \"\"\"Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. \"\"\" raise Exception ( f \"Value type ' { cls . _value_type_name } ' does not support hash calculation.\" ) # type: ignore check_data ( data ) classmethod \u00b6 Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). Source code in kiara/data/types/__init__.py @classmethod def check_data ( cls , data : typing . Any ) -> typing . Optional [ \"ValueType\" ]: \"\"\"Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). \"\"\" return None get_type_hint ( self , context = 'python' ) \u00b6 Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. Source code in kiara/data/types/__init__.py def get_type_hint ( self , context : str = \"python\" ) -> typing . Optional [ typing . Type ]: \"\"\"Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. \"\"\" return None parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/__init__.py def parse_value ( self , value : typing . Any ) -> typing . Any : \"\"\"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Arguments: v: the value Returns: 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object \"\"\" return None validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/__init__.py def validate ( cls , value : typing . Any ) -> None : \"\"\"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type).\"\"\" ValueTypeConfigMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/data/types/__init__.py class ValueTypeConfigMetadata ( MetadataModel ): @classmethod def from_config_class ( cls , config_cls : typing . Type [ ValueTypeConfigSchema ], ): flat_models = get_flat_models_from_model ( config_cls ) model_name_map = get_model_name_map ( flat_models ) m_schema , _ , _ = model_process_schema ( config_cls , model_name_map = model_name_map ) fields = m_schema [ \"properties\" ] config_values = {} for field_name , details in fields . items (): type_str = \"-- n/a --\" if \"type\" in details . keys (): type_str = details [ \"type\" ] desc = details . get ( \"description\" , DEFAULT_NO_DESC_VALUE ) default = config_cls . __fields__ [ field_name ] . default if default is None : if callable ( config_cls . __fields__ [ field_name ] . default_factory ): default = config_cls . __fields__ [ field_name ] . default_factory () # type: ignore req = config_cls . __fields__ [ field_name ] . required config_values [ field_name ] = ValueTypeAndDescription ( description = desc , type = type_str , value_default = default , required = req ) python_cls = PythonClassMetadata . from_class ( config_cls ) return ValueTypeConfigMetadata ( python_class = python_cls , config_values = config_values ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this configuration.\" ) config_values : typing . Dict [ str , ValueTypeAndDescription ] = Field ( description = \"The available configuration values.\" ) config_values : Dict [ str , kiara . metadata . ValueTypeAndDescription ] pydantic-field required \u00b6 The available configuration values. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this configuration. ValueTypeConfigSchema ( BaseModel ) pydantic-model \u00b6 Base class that describes the configuration a ValueType class accepts. This is stored in the _config_cls class attribute in each ValueType class. By default, a ValueType is not configurable, unless the _config_cls class attribute points to a sub-class of this class. Source code in kiara/data/types/__init__.py class ValueTypeConfigSchema ( BaseModel ): \"\"\"Base class that describes the configuration a [``ValueType``][kiara.data.types.ValueType] class accepts. This is stored in the ``_config_cls`` class attribute in each ``ValueType`` class. By default, a ``ValueType`` is not configurable, unless the ``_config_cls`` class attribute points to a sub-class of this class. \"\"\" @classmethod def requires_config ( cls ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : return True return False _config_hash : str = PrivateAttr ( default = None ) class Config : extra = Extra . forbid allow_mutation = False def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) @property def config_hash ( self ): if self . _config_hash is None : _d = self . dict () hashes = deepdiff . DeepHash ( _d ) self . _config_hash = hashes [ _d ] return self . _config_hash def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () def __hash__ ( self ): return hash ( self . config_hash ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : my_table = Table ( box = box . MINIMAL , show_header = False ) my_table . add_column ( \"Field name\" , style = \"i\" ) my_table . add_column ( \"Value\" ) for field in self . __fields__ : my_table . add_row ( field , getattr ( self , field )) yield my_table __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/types/__init__.py def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/types/__init__.py def __hash__ ( self ): return hash ( self . config_hash ) get ( self , key ) \u00b6 Get the value for the specified configuation key. Source code in kiara/data/types/__init__.py def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) requires_config () classmethod \u00b6 Return whether this class can be used as-is, or requires configuration before an instance can be created. Source code in kiara/data/types/__init__.py @classmethod def requires_config ( cls ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : return True return False get_type_name ( obj ) \u00b6 Utility function to get a pretty string from the class of an object. Source code in kiara/data/types/__init__.py def get_type_name ( obj : typing . Any ): \"\"\"Utility function to get a pretty string from the class of an object.\"\"\" if obj . __class__ . __module__ == \"builtins\" : return obj . __class__ . __name__ else : return f \" { obj . __class__ . __module__ } . { obj . __class__ . __name__ } \" core \u00b6 AnyType ( ValueType ) \u00b6 Any type / No type information. Source code in kiara/data/types/core.py class AnyType ( ValueType [ object , ValueTypeConfigSchema ]): \"\"\"Any type / No type information.\"\"\" _value_type_name = \"any\" @classmethod def backing_python_type ( cls ) -> typing . Type : return object @classmethod def type_config_cls ( cls ) -> typing . Type [ ValueTypeConfigSchema ]: return ValueTypeConfigSchema def pretty_print_as_renderables ( self , value : \"Value\" , print_config : typing . Mapping [ str , typing . Any ] ) -> typing . Any : data = value . get_value_data () return [ str ( data )] DeserializeConfigData ( KiaraInternalValueType ) \u00b6 A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. Source code in kiara/data/types/core.py class DeserializeConfigData ( KiaraInternalValueType ): \"\"\"A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. \"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , DeserializeConfig ] @classmethod def type_name ( cls ): return \"deserialize_config\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = DeserializeConfig ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , DeserializeConfig ): raise Exception ( f \"Invalid type for deserialize config: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = DeserializeConfig ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , DeserializeConfig ): raise Exception ( f \"Invalid type for deserialize config: { type ( value ) } .\" ) LoadConfigData ( KiaraInternalValueType ) \u00b6 A dictionary representing load config for a kiara value. The load config schema itself can be looked up via the wrapper class ' LoadConfig '. Source code in kiara/data/types/core.py class LoadConfigData ( KiaraInternalValueType ): \"\"\"A dictionary representing load config for a *kiara* value. The load config schema itself can be looked up via the wrapper class '[LoadConfig][kiara.data.persistence.LoadConfig]'. \"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , LoadConfig ] @classmethod def type_name ( cls ): return \"load_config\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = LoadConfig ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , LoadConfig ): raise Exception ( f \"Invalid type for load config: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = LoadConfig ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , LoadConfig ): raise Exception ( f \"Invalid type for load config: { type ( value ) } .\" ) ValueInfoData ( KiaraInternalValueType ) \u00b6 A dictionary representing a kiara ValueInfo object. Source code in kiara/data/types/core.py class ValueInfoData ( KiaraInternalValueType ): \"\"\"A dictionary representing a kiara ValueInfo object.\"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , ValueInfo ] @classmethod def type_name ( cls ): return \"value_info\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueInfo ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueInfo ): raise Exception ( f \"Invalid type for value info: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueInfo ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueInfo ): raise Exception ( f \"Invalid type for value info: { type ( value ) } .\" ) ValueLineageData ( KiaraInternalValueType ) \u00b6 A dictionary representing a kiara ValueLineage. Source code in kiara/data/types/core.py class ValueLineageData ( KiaraInternalValueType ): \"\"\"A dictionary representing a kiara ValueLineage.\"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , ValueLineage ] @classmethod def type_name ( cls ): return \"value_lineage\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueLineage ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueLineage ): raise Exception ( f \"Invalid type for value seed: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueLineage ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueLineage ): raise Exception ( f \"Invalid type for value seed: { type ( value ) } .\" ) type_mgmt \u00b6 TypeMgmt \u00b6 Source code in kiara/data/types/type_mgmt.py class TypeMgmt ( object ): def __init__ ( self , kiara : \"Kiara\" ): self . _kiara : Kiara = kiara self . _value_types : typing . Optional [ bidict [ str , typing . Type [ ValueType ]]] = None self . _value_type_transformations : typing . Dict [ str , typing . Dict [ str , typing . Any ] ] = {} self . _registered_python_classes : typing . Dict [ typing . Type , typing . List [ str ]] = None # type: ignore self . _type_hierarchy : typing . Optional [ nx . DiGraph ] = None def invalidate_types ( self ): self . _value_types = None self . _value_type_transformations . clear () self . _registered_python_classes = None @property def value_types ( self ) -> bidict [ str , typing . Type [ ValueType ]]: if self . _value_types is not None : return self . _value_types self . _value_types = bidict ( find_all_value_types ()) return self . _value_types @property def value_type_hierarchy ( self ) -> \"nx.DiGraph\" : if self . _type_hierarchy is not None : return self . _type_hierarchy def recursive_base_find ( cls : typing . Type , current : typing . Optional [ typing . List [ str ]] = None ): if current is None : current = [] for base in cls . __bases__ : if base in self . value_types . values (): current . append ( self . value_types . inverse [ base ]) recursive_base_find ( base , current = current ) return current bases = {} for name , cls in self . value_types . items (): bases [ name ] = recursive_base_find ( cls ) import networkx as nx hierarchy = nx . DiGraph () for name , _bases in bases . items (): hierarchy . add_node ( name , cls = self . value_types [ name ]) if not _bases : continue # we only need the first parent, all others will be taken care of by the parent of the parent hierarchy . add_edge ( _bases [ 0 ], name ) self . _type_hierarchy = hierarchy return self . _type_hierarchy def get_type_lineage ( self , value_type : str ) -> typing . Iterable [ str ]: \"\"\"Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type.\"\"\" if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx path = nx . shortest_path ( self . value_type_hierarchy , \"any\" , value_type ) return reversed ( path ) def get_sub_types ( self , value_type : str ) -> typing . Set [ str ]: if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx desc = nx . descendants ( self . value_type_hierarchy , value_type ) return desc @property def value_type_names ( self ) -> typing . List [ str ]: return list ( self . value_types . keys ()) @property def registered_python_classes ( self , ) -> typing . Mapping [ typing . Type , typing . Iterable [ str ]]: if self . _registered_python_classes is not None : return self . _registered_python_classes registered_types = {} for name , v_type in self . value_types . items (): rel = v_type . candidate_python_types () if rel : for cls in rel : registered_types . setdefault ( cls , []) . append ( name ) self . _registered_python_classes = registered_types return self . _registered_python_classes def get_type_config_for_data_profile ( self , profile_name : str ) -> typing . Mapping [ str , typing . Any ]: type_name = TYPE_PROFILE_MAP [ profile_name ] return { \"type\" : type_name , \"type_config\" : {}} def determine_type ( self , data : typing . Any ) -> typing . Optional [ ValueType ]: if isinstance ( data , Value ): data = data . get_value_data () result : typing . List [ ValueType ] = [] registered_types = set ( self . registered_python_classes . get ( data . __class__ , [])) for cls in data . __class__ . __bases__ : reg = self . registered_python_classes . get ( cls ) if reg : registered_types . update ( reg ) if registered_types : for rt in registered_types : _cls : typing . Type [ ValueType ] = self . get_value_type_cls ( rt ) match = _cls . check_data ( data ) if match : result . append ( match ) # TODO: re-run all checks on all modules, not just the ones that registered interest in the class if len ( result ) == 0 : return None elif len ( result ) > 1 : result_str = [ x . _value_type_name for x in result ] # type: ignore raise Exception ( f \"Multiple value types found for value: { ', ' . join ( result_str ) } .\" ) else : return result [ 0 ] def get_value_type_cls ( self , type_name : str ) -> typing . Type [ ValueType ]: t = self . value_types . get ( type_name , None ) if t is None : raise Exception ( f \"No value type ' { type_name } ', available types: { ', ' . join ( self . value_types . keys ()) } \" ) return t # def get_value_type_transformations( # self, value_type_name: str # ) -> typing.Mapping[str, typing.Mapping[str, typing.Any]]: # \"\"\"Return available transform pipelines for value types.\"\"\" # # if value_type_name in self._value_type_transformations.keys(): # return self._value_type_transformations[value_type_name] # # type_cls = self.get_value_type_cls(type_name=value_type_name) # _configs = type_cls.conversions() # if _configs is None: # module_configs = {} # else: # module_configs = dict(_configs) # for base in type_cls.__bases__: # if hasattr(base, \"conversions\"): # _b_configs = base.conversions() # type: ignore # if not _b_configs: # continue # for k, v in _b_configs.items(): # if k not in module_configs.keys(): # module_configs[k] = v # # # TODO: check input type compatibility? # result: typing.Dict[str, typing.Dict[str, typing.Any]] = {} # for name, config in module_configs.items(): # config = dict(config) # module_type = config.pop(\"module_type\", None) # if not module_type: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}', no module type specified in config: {config}\" # ) # module_config = config.pop(\"module_config\", {}) # module = self._kiara.create_module( # id=f\"_transform_{value_type_name}_{name}\", # module_type=module_type, # module_config=module_config, # ) # # if \"input_name\" not in config.keys(): # # if len(module.input_schemas) == 1: # config[\"input_name\"] = next(iter(module.input_schemas.keys())) # else: # required_inputs = [ # inp # for inp, schema in module.input_schemas.items() # if schema.is_required() # ] # if len(required_inputs) == 1: # config[\"input_name\"] = required_inputs[0] # else: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}': can't determine input name between those options: '{', '.join(required_inputs)}'\" # ) # # if \"output_name\" not in config.keys(): # # if len(module.input_schemas) == 1: # config[\"output_name\"] = next(iter(module.output_schemas.keys())) # else: # required_outputs = [ # inp # for inp, schema in module.output_schemas.items() # if schema.is_required() # ] # if len(required_outputs) == 1: # config[\"output_name\"] = required_outputs[0] # else: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}': can't determine output name between those options: '{', '.join(required_outputs)}'\" # ) # # result[name] = { # \"module\": module, # \"module_type\": module_type, # \"module_config\": module_config, # \"transformation_config\": config, # } # # self._value_type_transformations[value_type_name] = result # return self._value_type_transformations[value_type_name] def find_value_types_for_package ( self , package_name : str ) -> typing . Dict [ str , typing . Type [ ValueType ]]: result = {} for value_type_name , value_type in self . value_types . items (): value_md = value_type . get_type_metadata () package = value_md . context . labels . get ( \"package\" ) if package == package_name : result [ value_type_name ] = value_type return result # def get_available_transformations_for_type( # self, value_type_name: str # ) -> typing.Iterable[str]: # # return self.get_value_type_transformations(value_type_name=value_type_name) # def transform_value( # self, # transformation_alias: str, # value: Value, # other_inputs: typing.Optional[typing.Mapping[str, typing.Any]] = None, # ) -> Value: # # transformations = self.get_value_type_transformations(value.value_schema.type) # # if transformation_alias not in transformations.keys(): # raise Exception( # f\"Can't transform value of type '{value.value_schema.type}', transformation '{transformation_alias}' not available for this type. Available: {', '.join(transformations.keys())}\" # ) # # config = transformations[transformation_alias] # # transformation_config = config[\"transformation_config\"] # input_name = transformation_config[\"input_name\"] # # module: KiaraModule = config[\"module\"] # # constants = module.get_config_value(\"constants\") # inputs = dict(constants) # # if other_inputs: # # for k, v in other_inputs.items(): # if k in constants.keys(): # raise Exception(f\"Invalid value '{k}' for 'other_inputs', this is a constant that can't be overwrittern.\") # inputs[k] = v # # defaults = transformation_config.get(\"defaults\", None) # if defaults: # for k, v in defaults.items(): # if k in constants.keys(): # raise Exception(f\"Invalid default value '{k}', this is a constant that can't be overwrittern.\") # if k not in inputs.keys(): # inputs[k] = v # # if input_name in inputs.keys(): # raise Exception( # f\"Invalid value for inputs in transform arguments, can't contain the main input key '{input_name}'.\" # ) # # inputs[input_name] = value # # result = module.run(**inputs) # output_name = transformation_config[\"output_name\"] # # result_value = result.get_value_obj(output_name) # return result_value get_type_lineage ( self , value_type ) \u00b6 Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type. Source code in kiara/data/types/type_mgmt.py def get_type_lineage ( self , value_type : str ) -> typing . Iterable [ str ]: \"\"\"Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type.\"\"\" if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx path = nx . shortest_path ( self . value_type_hierarchy , \"any\" , value_type ) return reversed ( path ) values special \u00b6 A module that contains value-related classes for Kiara . A value in Kiara-speak is a pointer to actual data (aka 'bytes'). It contains metadata about that data (like whether it's valid/set, what type/schema it has, when it was last modified, ...), but it does not contain the data itself. The reason for that is that such data can be fairly large, and in a lot of cases it is not necessary for the code involved to have access to it, access to the metadata is enough. Each Value has a unique id, which can be used to retrieve the data (whole, or parts of it) from a DataRegistry . In addition, that id can be used to subscribe to change events for a value (published whenever the data that is associated with a value was changed). Value ( BaseModel , JupyterMixin ) pydantic-model \u00b6 The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry. Source code in kiara/data/values/__init__.py class Value ( BaseModel , JupyterMixin ): \"\"\"The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry. \"\"\" class Config : extra = Extra . forbid use_enum_values = True def __init__ ( self , registry : \"BaseDataRegistry\" , value_schema : ValueSchema , type_obj : ValueType , is_set : bool , is_none : bool , hashes : typing . Optional [ typing . Iterable [ ValueHash ]] = None , metadata : typing . Optional [ typing . Mapping [ str , typing . Dict [ str , typing . Any ]]] = None , register_token : typing . Optional [ uuid . UUID ] = None ): # type: ignore if not register_token : raise Exception ( \"No register token provided.\" ) if not registry . _check_register_token ( register_token ): raise Exception ( f \"Value registration with token ' { register_token } ' not allowed.\" ) if value_schema is None : raise NotImplementedError () assert registry self . _registry = registry self . _kiara = self . _registry . _kiara kwargs : typing . Dict [ str , typing . Any ] = {} kwargs [ \"id\" ] = NO_ID_YET_MARKER kwargs [ \"value_schema\" ] = value_schema # if value_lineage is None: # value_lineage = ValueLineage() # # kwargs[\"value_lineage\"] = value_lineage # kwargs[\"is_streaming\"] = False # not used yet kwargs [ \"creation_date\" ] = datetime . now () kwargs [ \"is_set\" ] = is_set kwargs [ \"is_none\" ] = is_none if hashes : kwargs [ \"hashes\" ] = list ( hashes ) if metadata : kwargs [ \"metadata\" ] = dict ( metadata ) else : kwargs [ \"metadata\" ] = {} super () . __init__ ( ** kwargs ) self . _type_obj = type_obj _kiara : \"Kiara\" = PrivateAttr () _registry : \"BaseDataRegistry\" = PrivateAttr () _type_obj : ValueType = PrivateAttr () _value_info : \"ValueInfo\" = PrivateAttr ( default = None ) id : str = Field ( description = \"A unique id for this value.\" ) value_schema : ValueSchema = Field ( description = \"The schema of this value.\" ) creation_date : typing . Optional [ datetime ] = Field ( description = \"The time this value was created value happened.\" ) # is_streaming: bool = Field( # default=False, # description=\"Whether the value is currently streamed into this object.\", # ) is_set : bool = Field ( description = \"Whether the value was set (in some way: user input, default, constant...).\" , default = False , ) is_none : bool = Field ( description = \"Whether the value is 'None'.\" , default = True ) hashes : typing . List [ ValueHash ] = Field ( description = \"Available hashes relating to the actual value data. This attribute is not populated by default, use the 'get_hashes()' method to request one or several hash items, afterwards those hashes will be reflected in this attribute.\" , default_factory = list , ) metadata : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = Field ( description = \"Available metadata relating to the actual value data (size, no. of rows, etc. -- depending on data type). This attribute is not populated by default, use the 'get_metadata()' method to request one or several metadata items, afterwards those metadata items will be reflected in this attribute.\" , default_factory = dict , ) @property def type_name ( self ) -> str : return self . value_schema . type @property def type_obj ( self ): \"\"\"Return the object that contains all the type information for this value.\"\"\" return self . _type_obj def get_hash ( self , hash_type : str ) -> ValueHash : \"\"\"Calculate the hash of a specified type for this value. Hashes are cached.\"\"\" hashes = self . get_hashes ( hash_type ) return list ( hashes )[ 0 ] def get_hashes ( self , * hash_types : str ) -> typing . Iterable [ ValueHash ]: all_hash_types = self . type_obj . get_supported_hash_types () if not hash_types : try : hash_types = all_hash_types except Exception as e : log_message ( str ( e )) if not hash_types : return [] result = [] missing = list ( hash_types ) for hash_obj in self . hashes : if hash_obj . hash_type in hash_types : result . append ( hash_obj ) missing . remove ( hash_obj . hash_type ) for hash_type in missing : if hash_type not in all_hash_types : raise Exception ( f \"Hash type ' { hash_type } ' not supported for ' { self . type_name } '\" ) hash_str = self . type_obj . calculate_value_hash ( value = self . get_value_data (), hash_type = hash_type ) hash_obj = ValueHash ( hash_type = hash_type , hash = hash_str ) self . hashes . append ( hash_obj ) result . append ( hash_obj ) return result def item_is_valid ( self ) -> bool : \"\"\"Check whether the current value is valid\"\"\" if self . value_schema . optional : return True else : return self . is_set and not self . is_none def item_status ( self ) -> str : \"\"\"Print a human readable short description of this values status.\"\"\" if self . value_schema . optional : if self . is_set : if self . is_none : return \"no value (not required)\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set (not required)\" else : if self . is_set : if self . is_none : return \"no value\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set\" def get_value_data ( self ) -> typing . Any : return self . _registry . get_value_data ( self ) def get_lineage ( self ) -> typing . Optional [ ValueLineage ]: return self . _registry . get_lineage ( self ) def set_value_lineage ( self , value_lineage : ValueLineage ) -> None : if hasattr ( self . _registry , \"set_value_lineage\" ): return self . _registry . set_value_lineage ( self , value_lineage ) # type: ignore else : raise Exception ( \"Can't set value lineage: registry is read only\" ) def get_info ( self ) -> \"ValueInfo\" : if self . _value_info is None : self . _value_info = ValueInfo . from_value ( self ) return self . _value_info def create_info ( self , include_deserialization_config : bool = False ) -> \"ValueInfo\" : return ValueInfo . from_value ( self , include_deserialization_config = include_deserialization_config ) def get_metadata ( self , * metadata_keys : str , also_return_schema : bool = False ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: \"\"\"Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to `True`, the value will be a two-key map with the metadata under the ``metadata_item`` subkey, and the value schema under ``metadata_schema``. If no metadata key is specified, all available metadata for this value type will be returned. Arguments: metadata_keys: the metadata keys to retrieve metadata (empty for all available metadata) also_return_schema: whether to also return the schema for each metadata item \"\"\" if not metadata_keys : _metadata_keys = set ( self . _kiara . metadata_mgmt . get_metadata_keys_for_type ( self . type_name ) ) for key in self . metadata . keys (): _metadata_keys . add ( key ) else : _metadata_keys = set ( metadata_keys ) result = {} missing = set () for metadata_key in sorted ( _metadata_keys ): if metadata_key in self . metadata . keys (): if also_return_schema : result [ metadata_key ] = self . metadata [ metadata_key ] else : result [ metadata_key ] = self . metadata [ metadata_key ][ \"metadata_item\" ] else : missing . add ( metadata_key ) if not missing : return result _md = self . _kiara . metadata_mgmt . get_value_metadata ( self , * missing , also_return_schema = True ) for k , v in _md . items (): self . metadata [ k ] = v if also_return_schema : result [ k ] = v else : result [ k ] = v [ \"metadata_item\" ] return { k : result [ k ] for k in sorted ( result . keys ())} def save ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None , register_missing_aliases : bool = True , ) -> \"Value\" : \"\"\"Save this value, under the specified alias(es).\"\"\" # if self.get_value_lineage(): # for field_name, value in self.get_value_lineage().inputs.items(): # value_obj = self._registry.get_value_obj(value) # try: # value_obj.save() # except Exception as e: # print(e) value = self . _kiara . data_store . register_data ( self ) if aliases : self . _kiara . data_store . link_aliases ( value , * aliases , register_missing_aliases = register_missing_aliases ) return value def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , Value ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id ) def __repr__ ( self ): return f \"Value(id= { self . id } , type= { self . type_name } , is_set= { self . is_set } , is_none= { self . is_none } \" def __str__ ( self ): return self . __repr__ () def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : # table = self._create_value_table() title = \"Value\" yield Panel ( self . get_info (), box = box . ROUNDED , title_align = \"left\" , title = title ) creation_date : datetime pydantic-field \u00b6 The time this value was created value happened. hashes : List [ kiara . data . values . ValueHash ] pydantic-field \u00b6 Available hashes relating to the actual value data. This attribute is not populated by default, use the 'get_hashes()' method to request one or several hash items, afterwards those hashes will be reflected in this attribute. id : str pydantic-field required \u00b6 A unique id for this value. is_none : bool pydantic-field \u00b6 Whether the value is 'None'. is_set : bool pydantic-field \u00b6 Whether the value was set (in some way: user input, default, constant...). metadata : Dict [ str , Dict [ str , Any ]] pydantic-field \u00b6 Available metadata relating to the actual value data (size, no. of rows, etc. -- depending on data type). This attribute is not populated by default, use the 'get_metadata()' method to request one or several metadata items, afterwards those metadata items will be reflected in this attribute. type_obj property readonly \u00b6 Return the object that contains all the type information for this value. value_schema : ValueSchema pydantic-field required \u00b6 The schema of this value. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , Value ): return False return self . id == other . id __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash ( self . id ) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/data/values/__init__.py def __repr__ ( self ): return f \"Value(id= { self . id } , type= { self . type_name } , is_set= { self . is_set } , is_none= { self . is_none } \" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/data/values/__init__.py def __str__ ( self ): return self . __repr__ () get_hash ( self , hash_type ) \u00b6 Calculate the hash of a specified type for this value. Hashes are cached. Source code in kiara/data/values/__init__.py def get_hash ( self , hash_type : str ) -> ValueHash : \"\"\"Calculate the hash of a specified type for this value. Hashes are cached.\"\"\" hashes = self . get_hashes ( hash_type ) return list ( hashes )[ 0 ] get_metadata ( self , * metadata_keys , * , also_return_schema = False ) \u00b6 Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to True , the value will be a two-key map with the metadata under the metadata_item subkey, and the value schema under metadata_schema . If no metadata key is specified, all available metadata for this value type will be returned. Parameters: Name Type Description Default metadata_keys str the metadata keys to retrieve metadata (empty for all available metadata) () also_return_schema bool whether to also return the schema for each metadata item False Source code in kiara/data/values/__init__.py def get_metadata ( self , * metadata_keys : str , also_return_schema : bool = False ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: \"\"\"Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to `True`, the value will be a two-key map with the metadata under the ``metadata_item`` subkey, and the value schema under ``metadata_schema``. If no metadata key is specified, all available metadata for this value type will be returned. Arguments: metadata_keys: the metadata keys to retrieve metadata (empty for all available metadata) also_return_schema: whether to also return the schema for each metadata item \"\"\" if not metadata_keys : _metadata_keys = set ( self . _kiara . metadata_mgmt . get_metadata_keys_for_type ( self . type_name ) ) for key in self . metadata . keys (): _metadata_keys . add ( key ) else : _metadata_keys = set ( metadata_keys ) result = {} missing = set () for metadata_key in sorted ( _metadata_keys ): if metadata_key in self . metadata . keys (): if also_return_schema : result [ metadata_key ] = self . metadata [ metadata_key ] else : result [ metadata_key ] = self . metadata [ metadata_key ][ \"metadata_item\" ] else : missing . add ( metadata_key ) if not missing : return result _md = self . _kiara . metadata_mgmt . get_value_metadata ( self , * missing , also_return_schema = True ) for k , v in _md . items (): self . metadata [ k ] = v if also_return_schema : result [ k ] = v else : result [ k ] = v [ \"metadata_item\" ] return { k : result [ k ] for k in sorted ( result . keys ())} item_is_valid ( self ) \u00b6 Check whether the current value is valid Source code in kiara/data/values/__init__.py def item_is_valid ( self ) -> bool : \"\"\"Check whether the current value is valid\"\"\" if self . value_schema . optional : return True else : return self . is_set and not self . is_none item_status ( self ) \u00b6 Print a human readable short description of this values status. Source code in kiara/data/values/__init__.py def item_status ( self ) -> str : \"\"\"Print a human readable short description of this values status.\"\"\" if self . value_schema . optional : if self . is_set : if self . is_none : return \"no value (not required)\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set (not required)\" else : if self . is_set : if self . is_none : return \"no value\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set\" save ( self , aliases = None , register_missing_aliases = True ) \u00b6 Save this value, under the specified alias(es). Source code in kiara/data/values/__init__.py def save ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None , register_missing_aliases : bool = True , ) -> \"Value\" : \"\"\"Save this value, under the specified alias(es).\"\"\" # if self.get_value_lineage(): # for field_name, value in self.get_value_lineage().inputs.items(): # value_obj = self._registry.get_value_obj(value) # try: # value_obj.save() # except Exception as e: # print(e) value = self . _kiara . data_store . register_data ( self ) if aliases : self . _kiara . data_store . link_aliases ( value , * aliases , register_missing_aliases = register_missing_aliases ) return value ValueAlias ( BaseModel ) pydantic-model \u00b6 Source code in kiara/data/values/__init__.py class ValueAlias ( BaseModel ): @classmethod def from_string ( self , value_alias : str , default_repo_name : typing . Optional [ str ] = None ) -> \"ValueAlias\" : if not isinstance ( value_alias , str ): raise Exception ( \"Invalid id_or_alias: not a string.\" ) if not value_alias : raise Exception ( \"Invalid id_or_alias: can't be empty string.\" ) _repo_name : typing . Optional [ str ] = default_repo_name _version : typing . Optional [ int ] = None _tag : typing . Optional [ str ] = None if \"#\" in value_alias : _repo_name , _value_alias = value_alias . split ( \"#\" , maxsplit = 1 ) else : _value_alias = value_alias if \"@\" in _value_alias : _alias , _postfix = _value_alias . split ( \"@\" , maxsplit = 1 ) try : _version = int ( _postfix ) except ValueError : if not _postfix . isidentifier (): raise Exception ( f \"Invalid format for version/tag element of id_or_alias: { _tag } \" ) _tag = _postfix else : _alias = _value_alias return ValueAlias ( repo_name = _repo_name , alias = _alias , version = _version , tag = _tag ) @classmethod def from_strings ( cls , * value_aliases : typing . Union [ str , \"ValueAlias\" ] ) -> typing . List [ \"ValueAlias\" ]: result = [] for va in value_aliases : if isinstance ( va , str ): result . append ( ValueAlias . from_string ( va )) elif isinstance ( va , ValueAlias ): result . append ( va ) else : raise TypeError ( f \"Invalid type ' { type ( va ) } ' for type alias, expected 'str' or 'ValueAlias'.\" ) return result repo_name : typing . Optional [ str ] = Field ( description = \"The name of the data repo the value lives in.\" , default = None ) alias : str = Field ( \"The alias name.\" ) version : typing . Optional [ int ] = Field ( description = \"The version of this alias.\" , default = None ) tag : typing . Optional [ str ] = Field ( description = \"The tag for the alias.\" , default = None ) @property def full_alias ( self ): if self . tag is not None : return f \" { self . alias } @ { self . tag } \" elif self . version is not None : return f \" { self . alias } @ { self . version } \" else : return self . alias repo_name : str pydantic-field \u00b6 The name of the data repo the value lives in. tag : str pydantic-field \u00b6 The tag for the alias. version : int pydantic-field \u00b6 The version of this alias. ValueHash ( BaseModel ) pydantic-model \u00b6 Source code in kiara/data/values/__init__.py class ValueHash ( BaseModel ): hash : str = Field ( description = \"The value hash.\" ) hash_type : str = Field ( description = \"The value hash method.\" ) hash : str pydantic-field required \u00b6 The value hash. hash_type : str pydantic-field required \u00b6 The value hash method. ValueInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/data/values/__init__.py class ValueInfo ( KiaraInfoModel ): @classmethod def from_value ( cls , value : Value , include_deserialization_config : bool = False ): if value . id not in value . _registry . value_ids : raise Exception ( \"Value not registered (yet).\" ) # aliases = value._registry.find_aliases_for_value(value) hashes = value . get_hashes () metadata = value . get_metadata ( also_return_schema = True ) # metadata = value.metadata value_lineage = value . get_lineage () if include_deserialization_config : # serialize_operation: SerializeValueOperationType = ( # type: ignore # value._kiara.operation_mgmt.get_operation(\"serialize\") # type: ignore # ) raise NotImplementedError () return ValueInfo ( value_id = value . id , value_schema = value . value_schema , hashes = hashes , metadata = metadata , lineage = value_lineage , is_valid = value . item_is_valid (), ) value_id : str = Field ( description = \"The value id.\" ) value_schema : ValueSchema = Field ( description = \"The value schema.\" ) # aliases: typing.List[ValueAlias] = Field( # description=\"All aliases for this value.\", default_factory=list # ) # tags: typing.List[str] = Field( # description=\"All tags for this value.\", default_factory=list # ) # created: str = Field(description=\"The time the data was created.\") is_valid : bool = Field ( description = \"Whether the item is valid (in the context of its schema).\" ) hashes : typing . List [ ValueHash ] = Field ( description = \"All available hashes for this value.\" , default_factory = list ) metadata : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = Field ( description = \"The metadata associated with this value.\" ) lineage : typing . Optional [ ValueLineage ] = Field ( description = \"Information about how the value was created.\" , default = None ) deserialize_config : typing . Optional [ DeserializeConfig ] = Field ( description = \"The module config (incl. inputs) to deserialize the value.\" , default = None , ) def get_metadata_items ( self , * keys : str ) -> typing . Dict [ str , typing . Any ]: if not keys : _keys : typing . Iterable [ str ] = self . metadata . keys () else : _keys = keys result = {} for k in _keys : md = self . metadata . get ( k ) if md is None : raise Exception ( f \"No metadata for key ' { k } ' available.\" ) result [ k ] = md [ \"metadata_item\" ] return result def get_metadata_schemas ( self , * keys : str ) -> typing . Dict [ str , typing . Any ]: if not keys : _keys : typing . Iterable [ str ] = self . metadata . keys () else : _keys = keys result = {} for k in _keys : md = self . metadata . get ( k ) if md is None : raise Exception ( f \"No metadata for key ' { k } ' available.\" ) result [ k ] = md [ \"metadata_schema\" ] return result def create_renderable ( self , ** config : typing . Any ) -> RenderableType : padding = config . get ( \"padding\" , ( 0 , 1 )) skip_metadata = config . get ( \"skip_metadata\" , False ) skip_value_lineage = config . get ( \"skip_lineage\" , True ) include_ids = config . get ( \"include_ids\" , False ) table = Table ( box = box . SIMPLE , show_header = False , padding = padding ) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) table . add_row ( \"id\" , self . value_id ) # type: ignore table . add_row ( \"type\" , self . value_schema . type ) if self . value_schema . type_config : json_data = json . dumps ( self . value_schema . type_config ) tc_content = Syntax ( json_data , \"json\" ) table . add_row ( \"type config\" , tc_content ) table . add_row ( \"desc\" , self . value_schema . doc ) table . add_row ( \"is set\" , \"yes\" if self . is_valid else \"no\" ) # table.add_row(\"is constant\", \"yes\" if self.is_constant else \"no\") # if isinstance(self.value_hash, int): # vh = str(self.value_hash) # else: # vh = self.value_hash.value # table.add_row(\"hash\", vh) if self . hashes : hashes_dict = { hs . hash_type : hs . hash for hs in self . hashes } yaml_string = yaml . dump ( hashes_dict ) hases_str = Syntax ( yaml_string , \"yaml\" , background_color = \"default\" ) table . add_row ( \"\" , \"\" ) table . add_row ( \"hashes\" , hases_str ) if not skip_metadata : if self . metadata : yaml_string = yaml . dump ( data = self . get_metadata_items ()) # json_string = json.dumps(self.get_metadata_items(), indent=2) metadata = Syntax ( yaml_string , \"yaml\" , background_color = \"default\" ) table . add_row ( \"metadata\" , metadata ) else : table . add_row ( \"metadata\" , \"-- no metadata --\" ) if not skip_value_lineage and self . lineage : if self . metadata : table . add_row ( \"\" , \"\" ) # json_string = self.lineage.json(indent=2) # seed_content = Syntax(json_string, \"json\") table . add_row ( \"lineage\" , self . lineage . create_renderable ( include_ids = include_ids ) ) return table deserialize_config : DeserializeConfig pydantic-field \u00b6 The module config (incl. inputs) to deserialize the value. hashes : List [ kiara . data . values . ValueHash ] pydantic-field \u00b6 All available hashes for this value. is_valid : bool pydantic-field required \u00b6 Whether the item is valid (in the context of its schema). lineage : ValueLineage pydantic-field \u00b6 Information about how the value was created. metadata : Dict [ str , Dict [ str , Any ]] pydantic-field required \u00b6 The metadata associated with this value. value_id : str pydantic-field required \u00b6 The value id. value_schema : ValueSchema pydantic-field required \u00b6 The value schema. ValueLineage ( ModuleConfig ) pydantic-model \u00b6 Model containing the lineage of a value. Source code in kiara/data/values/__init__.py class ValueLineage ( ModuleConfig ): \"\"\"Model containing the lineage of a value.\"\"\" @classmethod def from_module_and_inputs ( cls , module : \"KiaraModule\" , output_name : str , inputs : typing . Mapping [ str , typing . Union [ \"Value\" , \"ValueInfo\" ]], ): module_type = module . _module_type_id # type: ignore module_config = module . config . dict () doc = module . get_type_metadata () . documentation _inputs = {} for field_name , value in inputs . items (): if isinstance ( value , Value ): _inputs [ field_name ] = value . get_info () else : _inputs [ field_name ] = value return ValueLineage . construct ( module_type = module_type , module_config = module_config , doc = doc , output_name = output_name , inputs = _inputs , ) @classmethod def create ( cls , module_type : str , module_config : typing . Mapping [ str , typing . Any ], module_doc : DocumentationMetadataModel , inputs : typing . Mapping [ str , typing . Union [ \"Value\" , \"ValueInfo\" ]], output_name : typing . Optional [ str ] = None , ): _inputs = {} for field_name , value in inputs . items (): if isinstance ( value , Value ): _inputs [ field_name ] = value . get_info () else : _inputs [ field_name ] = value return ValueLineage . construct ( module_type = module_type , module_config = dict ( module_config ), doc = module_doc , output_name = output_name , inputs = _inputs , ) output_name : typing . Optional [ str ] = Field ( description = \"The result field name for the value this refers to.\" ) inputs : typing . Dict [ str , \"ValueInfo\" ] = Field ( description = \"The inputs that were used to create the value this refers to.\" ) value_index : typing . Optional [ typing . Dict [ str , \"ValueInfo\" ]] = Field ( description = \"Index of all values that are associated with this value lineage.\" , default = None , ) def to_minimal_dict ( self , include_metadata : bool = False , include_module_doc : bool = False , include_module_config : bool = True , ) -> typing . Dict [ str , typing . Any ]: full_dict = self . dict ( exclude_none = True ) minimal_dict = filter_metadata_schema ( full_dict , include_metadata = include_metadata , include_module_doc = include_module_doc , include_module_config = include_module_config , ) return minimal_dict def create_renderable ( self , ** config : typing . Any ) -> RenderableType : all_ids = sorted ( find_all_ids_in_lineage ( self )) id_color_map = {} for idx , v_id in enumerate ( all_ids ): id_color_map [ v_id ] = COLOR_LIST [ idx % len ( COLOR_LIST )] show_ids = config . get ( \"include_ids\" , False ) tree = fill_lineage_tree ( self , include_ids = show_ids ) return tree def create_graph ( self ) -> nx . DiGraph : return create_lineage_graph ( self ) inputs : Dict [ str , ValueInfo ] pydantic-field required \u00b6 The inputs that were used to create the value this refers to. output_name : str pydantic-field \u00b6 The result field name for the value this refers to. value_index : Dict [ str , ValueInfo ] pydantic-field \u00b6 Index of all values that are associated with this value lineage. create_renderable ( self , ** config ) \u00b6 Create a renderable for this module configuration. Source code in kiara/data/values/__init__.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : all_ids = sorted ( find_all_ids_in_lineage ( self )) id_color_map = {} for idx , v_id in enumerate ( all_ids ): id_color_map [ v_id ] = COLOR_LIST [ idx % len ( COLOR_LIST )] show_ids = config . get ( \"include_ids\" , False ) tree = fill_lineage_tree ( self , include_ids = show_ids ) return tree ValueSchema ( BaseModel ) pydantic-model \u00b6 The schema of a value. The schema contains the ValueType of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the type_config field. Source code in kiara/data/values/__init__.py class ValueSchema ( BaseModel ): \"\"\"The schema of a value. The schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field. \"\"\" class Config : use_enum_values = True # extra = Extra.forbid type : str = Field ( description = \"The type of the value.\" ) type_config : typing . Dict [ str , typing . Any ] = Field ( description = \"Configuration for the type, in case it's complex.\" , default_factory = dict , ) default : typing . Any = Field ( description = \"A default value.\" , default = SpecialValue . NOT_SET ) optional : bool = Field ( description = \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , default = False , ) is_constant : bool = Field ( description = \"Whether the value is a constant.\" , default = False ) # required: typing.Any = Field( # description=\"Whether this value is required to be set.\", default=True # ) doc : str = Field ( default = \"-- n/a --\" , description = \"A description for the value of this input field.\" , ) def is_required ( self ): if self . optional : return False else : if self . default in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ]: return True else : return False def validate_types ( self , kiara : \"Kiara\" ): if self . type not in kiara . value_type_names : raise ValueError ( f \"Invalid value type ' { self . type } ', available types: { kiara . value_type_names } \" ) @property def desc ( self ): \"\"\"The first line of the 'doc' value.\"\"\" return self . doc . split ( \" \\n \" )[ 0 ] def __eq__ ( self , other ): if not isinstance ( other , ValueSchema ): return False return ( self . type , self . default ) == ( other . type , other . default ) def __hash__ ( self ): return hash (( self . type , self . default )) default : Any pydantic-field \u00b6 A default value. desc property readonly \u00b6 The first line of the 'doc' value. doc : str pydantic-field \u00b6 A description for the value of this input field. is_constant : bool pydantic-field \u00b6 Whether the value is a constant. optional : bool pydantic-field \u00b6 Whether this value is required (True), or whether 'None' value is allowed (False). type : str pydantic-field required \u00b6 The type of the value. type_config : Dict [ str , Any ] pydantic-field \u00b6 Configuration for the type, in case it's complex. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): if not isinstance ( other , ValueSchema ): return False return ( self . type , self . default ) == ( other . type , other . default ) __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash (( self . type , self . default )) ValueSlot ( BaseModel ) pydantic-model \u00b6 Source code in kiara/data/values/__init__.py class ValueSlot ( BaseModel ): @classmethod def from_value ( cls , id : str , value : Value ) -> \"ValueSlot\" : vs = ValueSlot . from_value_schema ( id = id , value_schema = value . value_schema , kiara = value . _kiara ) vs . add_value ( value ) return vs @classmethod def from_value_schema ( cls , id : str , value_schema : ValueSchema , kiara : \"Kiara\" ) -> \"ValueSlot\" : vs = ValueSlot ( id = id , value_schema = value_schema , kiara = kiara ) return vs def __init__ ( self , ** data ): # type: ignore _kiara = data . pop ( \"kiara\" , None ) if _kiara is None : raise Exception ( \"No kiara context provided.\" ) _registry = data . pop ( \"registry\" , None ) if _registry is None : _registry = _kiara . data_registry self . _kiara = _kiara self . _registry = _registry super () . __init__ ( ** data ) _kiara : \"Kiara\" = PrivateAttr () _registry : \"DataRegistry\" = PrivateAttr () _callbacks : typing . Dict [ str , \"ValueSlotUpdateHandler\" ] = PrivateAttr ( default_factory = dict ) id : str = Field ( description = \"The id for this slot.\" ) value_schema : ValueSchema = Field ( description = \"The schema for the values of this slot.\" ) values : typing . Dict [ int , Value ] = Field ( description = \"The values of this slot, with versions as key.\" , default_factory = dict , ) tags : typing . Dict [ str , int ] = Field ( description = \"The tags for this value slot (tag name as key, linked version as value.\" , default_factory = dict , ) @property def latest_version_nr ( self ) -> int : if not self . values : return 0 return max ( self . values . keys ()) def get_latest_value ( self ) -> Value : lv = self . latest_version_nr if lv == 0 : raise Exception ( \"No value added to value slot yet.\" ) return self . values [ self . latest_version_nr ] def register_callbacks ( self , * callbacks : \"ValueSlotUpdateHandler\" ): for cb in callbacks : cb_id : typing . Optional [ str ] = None if cb_id in self . _callbacks . keys (): raise Exception ( f \"Callback with id ' { cb_id } ' already registered.\" ) if hasattr ( cb , \"id\" ): if callable ( cb . id ): # type: ignore cb_id = cb . id () # type: ignore else : cb_id = cb . id # type: ignore elif hasattr ( cb , \"get_id\" ): cb_id = cb . get_id () # type: ignore if cb_id is None : cb_id = str ( uuid . uuid4 ()) assert isinstance ( cb_id , str ) self . _callbacks [ cb_id ] = cb def add_value ( self , value : Value , trigger_callbacks : bool = True , tags : typing . Optional [ typing . Iterable [ str ]] = None , ) -> int : \"\"\"Add a value to this slot.\"\"\" if self . latest_version_nr != 0 and value . id == self . get_latest_value () . id : return self . latest_version_nr # TODO: check value data equality if self . value_schema . is_constant and self . values : if is_debug (): import traceback traceback . print_stack () raise Exception ( \"Can't add value: value slot marked as 'constant'.\" ) version = self . latest_version_nr + 1 assert version not in self . values . keys () self . values [ version ] = value if tags : for tag in tags : self . tags [ tag ] = version if trigger_callbacks : for cb in self . _callbacks . values (): cb . values_updated ( self ) return version def is_latest_value ( self , value : Value ): return value . id == self . get_latest_value () . id def find_linked_aliases ( self , value_item : typing . Union [ Value , str ] ) -> typing . List [ \"ValueAlias\" ]: if isinstance ( value_item , Value ): value_item = value_item . id result = [] for _version , _value in self . values . items (): if _value . id == value_item : va = ValueAlias ( alias = self . id , version = _version ) result . append ( va ) if _version in self . tags . values (): for _tag , _tag_version in self . tags . items (): if _tag_version == _version : va = ValueAlias ( alias = self . id , tag = _tag ) result . append ( va ) return result def __eq__ ( self , other ): if not isinstance ( other , ValueSlot ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id ) id : str pydantic-field required \u00b6 The id for this slot. tags : Dict [ str , int ] pydantic-field \u00b6 The tags for this value slot (tag name as key, linked version as value. value_schema : ValueSchema pydantic-field required \u00b6 The schema for the values of this slot. values : Dict [ int , kiara . data . values . Value ] pydantic-field \u00b6 The values of this slot, with versions as key. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): if not isinstance ( other , ValueSlot ): return False return self . id == other . id __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash ( self . id ) add_value ( self , value , trigger_callbacks = True , tags = None ) \u00b6 Add a value to this slot. Source code in kiara/data/values/__init__.py def add_value ( self , value : Value , trigger_callbacks : bool = True , tags : typing . Optional [ typing . Iterable [ str ]] = None , ) -> int : \"\"\"Add a value to this slot.\"\"\" if self . latest_version_nr != 0 and value . id == self . get_latest_value () . id : return self . latest_version_nr # TODO: check value data equality if self . value_schema . is_constant and self . values : if is_debug (): import traceback traceback . print_stack () raise Exception ( \"Can't add value: value slot marked as 'constant'.\" ) version = self . latest_version_nr + 1 assert version not in self . values . keys () self . values [ version ] = value if tags : for tag in tags : self . tags [ tag ] = version if trigger_callbacks : for cb in self . _callbacks . values (): cb . values_updated ( self ) return version value_set \u00b6 SlottedValueSet ( ValueSet ) \u00b6 Source code in kiara/data/values/value_set.py class SlottedValueSet ( ValueSet ): @classmethod def from_schemas ( cls , schemas : typing . Mapping [ str , \"ValueSchema\" ], read_only : bool = True , check_for_sameness = True , initial_values : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , title : typing . Optional [ str ] = None , default_value : typing . Any = SpecialValue . NO_VALUE , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ) -> \"SlottedValueSet\" : if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if registry is None : registry = kiara . data_registry values = {} for field_name , schema in schemas . items (): def_val = default_value if callable ( default_value ): def_val = default_value () _init_value = def_val if initial_values and initial_values . get ( field_name , None ) is not None : _init_value = initial_values [ field_name ] if not isinstance ( _init_value , Value ): value : Value = registry . register_data ( value_data = _init_value , value_schema = schema ) # value: Value = Value(value_schema=schema, value_data=_init_value, registry=registry) # type: ignore else : value = _init_value values [ field_name ] = value return cls ( items = values , title = title , read_only = read_only , check_for_sameness = check_for_sameness , kiara = kiara , registry = registry , ) def __init__ ( self , items : typing . Mapping [ str , typing . Union [ \"ValueSlot\" , \"Value\" ]], read_only : bool , check_for_sameness : bool = False , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ): \"\"\"A `ValueSet` implementation that keeps a history of each fields value. Arguments: items: the value slots read_only: whether it is allowed to set new values to fields in this set check_for_sameness: whether a check should be performed that checks for equality of the new and old values, if equal, skip the update title: An optional title for this value set kiara: the kiara context registry: the registry to use to register the values to \"\"\" if not items : raise ValueError ( \"Can't create ValueSet: no values provided\" ) self . _check_for_sameness : bool = check_for_sameness super () . __init__ ( read_only = read_only , title = title , kiara = kiara ) if registry is None : registry = self . _kiara . data_registry self . _registry : DataRegistry = registry _value_slots : typing . Dict [ str , ValueSlot ] = {} for item , value in items . items (): if value is None : raise Exception ( f \"Can't create value set, item ' { item } ' does not have a value (yet).\" ) if item . startswith ( \"_\" ): raise ValueError ( f \"Value name can't start with '_': { item } \" ) if item in INVALID_VALUE_NAMES : raise ValueError ( f \"Invalid value name ' { item } '.\" ) if isinstance ( value , Value ): slot = self . _registry . register_alias ( value ) elif isinstance ( value , ValueSlot ): slot = value else : raise TypeError ( f \"Invalid type: ' { type ( value ) } '\" ) _value_slots [ item ] = slot self . _value_slots : typing . Dict [ str , ValueSlot ] = _value_slots def get_all_field_names ( self ) -> typing . Iterable [ str ]: return self . _value_slots . keys () def _get_value_obj ( self , field_name : str ) -> \"Value\" : slot = self . _value_slots [ field_name ] return slot . get_latest_value () def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: # we want to set all registry-type values seperately, in one go, because it's more efficient registries : typing . Dict [ DataRegistry , typing . Dict [ ValueSlot , typing . Union [ \"Value\" , typing . Any ]] ] = {} field_slot_map : typing . Dict [ ValueSlot , str ] = {} result : typing . Dict [ str , typing . Union [ bool , Exception ]] = {} for field_name , value_or_data in values . items (): value_slot : ValueSlot = self . _value_slots [ field_name ] if not metadata and self . _check_for_sameness : latest_val = value_slot . get_latest_value () if isinstance ( value_or_data , Value ): if latest_val . id == value_or_data . id : result [ field_name ] = False continue else : if ( latest_val . is_set and latest_val . get_value_data () == value_or_data ): result [ field_name ] = False continue registries . setdefault ( value_slot . _registry , {})[ value_slot ] = value_or_data field_slot_map [ value_slot ] = field_name for registry , value_slots_details in registries . items (): _r = registry . update_value_slots ( value_slots_details , metadata = metadata , lineage = lineage ) # type: ignore for value_slot , details in _r . items (): result [ field_slot_map [ value_slot ]] = details return result __init__ ( self , items , read_only , check_for_sameness = False , title = None , kiara = None , registry = None ) special \u00b6 A ValueSet implementation that keeps a history of each fields value. Parameters: Name Type Description Default items Mapping[str, Union[ValueSlot, Value]] the value slots required read_only bool whether it is allowed to set new values to fields in this set required check_for_sameness bool whether a check should be performed that checks for equality of the new and old values, if equal, skip the update False title Optional[str] An optional title for this value set None kiara Optional[Kiara] the kiara context None registry Optional[kiara.data.registry.DataRegistry] the registry to use to register the values to None Source code in kiara/data/values/value_set.py def __init__ ( self , items : typing . Mapping [ str , typing . Union [ \"ValueSlot\" , \"Value\" ]], read_only : bool , check_for_sameness : bool = False , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ): \"\"\"A `ValueSet` implementation that keeps a history of each fields value. Arguments: items: the value slots read_only: whether it is allowed to set new values to fields in this set check_for_sameness: whether a check should be performed that checks for equality of the new and old values, if equal, skip the update title: An optional title for this value set kiara: the kiara context registry: the registry to use to register the values to \"\"\" if not items : raise ValueError ( \"Can't create ValueSet: no values provided\" ) self . _check_for_sameness : bool = check_for_sameness super () . __init__ ( read_only = read_only , title = title , kiara = kiara ) if registry is None : registry = self . _kiara . data_registry self . _registry : DataRegistry = registry _value_slots : typing . Dict [ str , ValueSlot ] = {} for item , value in items . items (): if value is None : raise Exception ( f \"Can't create value set, item ' { item } ' does not have a value (yet).\" ) if item . startswith ( \"_\" ): raise ValueError ( f \"Value name can't start with '_': { item } \" ) if item in INVALID_VALUE_NAMES : raise ValueError ( f \"Invalid value name ' { item } '.\" ) if isinstance ( value , Value ): slot = self . _registry . register_alias ( value ) elif isinstance ( value , ValueSlot ): slot = value else : raise TypeError ( f \"Invalid type: ' { type ( value ) } '\" ) _value_slots [ item ] = slot self . _value_slots : typing . Dict [ str , ValueSlot ] = _value_slots ValueSet ( MutableMapping , Generic ) \u00b6 Source code in kiara/data/values/value_set.py class ValueSet ( typing . MutableMapping [ str , \"Value\" ]): def __init__ ( self , read_only : bool , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): # from kiara.data.values import ValueSchema # schema = ValueSchema(type=\"any\", default=None, doc=\"-- n/a --\") # self._schema = schema if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : \"Kiara\" = kiara self . _read_only : bool = read_only if title is None : title = \"-- n/a --\" self . _title = title @abc . abstractmethod def get_all_field_names ( self ) -> typing . Iterable [ str ]: pass @abc . abstractmethod def _get_value_obj ( self , field_name : str ): pass @abc . abstractmethod def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: pass def get_value_obj ( self , field_name : str , ensure_metadata : typing . Union [ bool , typing . Iterable [ str ], str ] = False , ) -> Value : if field_name not in list ( self . get_all_field_names ()): raise KeyError ( f \"Field ' { field_name } ' not available in value set. Available fields: { ', ' . join ( self . get_all_field_names ()) } \" ) obj : Value = self . _get_value_obj ( field_name ) if ensure_metadata : if isinstance ( ensure_metadata , bool ): obj . get_metadata () elif isinstance ( ensure_metadata , str ): obj . get_metadata ( ensure_metadata ) elif isinstance ( ensure_metadata , typing . Iterable ): obj . get_metadata ( * ensure_metadata ) else : raise ValueError ( f \"Invalid type ' { type ( ensure_metadata ) } ' for 'ensure_metadata' argument.\" ) return obj def set_value ( self , key : str , value : typing . Any , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> Value : result = self . set_values ( metadata = metadata , lineage = lineage , ** { key : value }) if isinstance ( result [ key ], Exception ): raise result [ key ] # type: ignore return result [ key ] # type: ignore def set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ Value , Exception ]]: \"\"\"Batch set several values. If metadata is provided, it is added to all values. \"\"\" if self . is_read_only (): raise Exception ( \"Can't set values: this value set is read-only.\" ) if not values : return {} invalid : typing . List [ str ] = [] for k in values . keys (): if k not in self . get_all_field_names (): invalid . append ( k ) if invalid : raise ValueError ( f \"No field(s) with name(s) { ', ' . join ( invalid ) } available, valid names: { ', ' . join ( self . get_all_field_names ()) } \" ) resolved_values = {} for field_name , data in values . items (): if isinstance ( data , str ) and data . startswith ( \"value:\" ): v = self . _kiara . get_value ( data ) resolved_values [ field_name ] = v else : resolved_values [ field_name ] = data try : value_set_result = self . _set_values ( metadata = metadata , lineage = lineage , ** resolved_values ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () raise e result : typing . Dict [ str , typing . Union [ Value , Exception ]] = {} for field in values . keys (): if isinstance ( value_set_result [ field ], Exception ): result [ field ] = value_set_result [ field ] # type: ignore else : result [ field ] = self . get_value_obj ( field ) return result def is_read_only ( self ): return self . _read_only def items_are_valid ( self ) -> bool : return check_valueset_valid ( self ) # for field_name in self.get_all_field_names(): # item = self.get_value_obj(field_name) # if not item.item_is_valid(): # return False # return True def check_invalid ( self ) -> typing . Optional [ typing . Dict [ str , str ]]: \"\"\"Check whether the value set is invalid, if it is, return a description of what's wrong.\"\"\" invalid = {} for field_name , item in self . items (): if not item . item_is_valid (): if item . value_schema . is_required (): if not item . is_set : msg = \"not set\" elif item . is_none : msg = \"no value\" else : msg = \"n/a\" else : msg = \"n/a\" invalid [ field_name ] = msg return invalid def get_all_value_objects ( self ) -> typing . Mapping [ str , typing . Any ]: return { fn : self . get_value_obj ( fn ) for fn in self . get_all_field_names ()} def get_value_data_for_fields ( self , * field_names : str , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). \"\"\" result : typing . Dict [ str , typing . Any ] = {} unset : typing . List [ str ] = [] for k in field_names : v = self . get_value_obj ( k ) if not v . is_set : if raise_exception_when_unset : unset . append ( k ) else : result [ k ] = None else : data = v . get_value_data () result [ k ] = data if unset : raise Exception ( f \"Can't get data for fields, one or several of the requested fields are not set yet: { ', ' . join ( unset ) } .\" ) return result def get_value_data ( self , field_name : str , raise_exception_when_unset : bool = False ) -> typing . Any : return self . get_value_data_for_fields ( field_name , raise_exception_when_unset = raise_exception_when_unset )[ field_name ] def get_all_value_data ( self , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: return self . get_value_data_for_fields ( * self . get_all_field_names (), raise_exception_when_unset = raise_exception_when_unset , ) def save_all ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None ): if aliases : aliases = set ( aliases ) for k , v in self . items (): field_aliases = None if aliases : field_aliases = [ f \" { a } __ { k } \" for a in aliases ] v . save ( aliases = field_aliases ) def __getitem__ ( self , item : str ) -> \"Value\" : return self . get_value_obj ( item ) def __setitem__ ( self , key : str , value ): self . set_value ( key , value ) def __delitem__ ( self , key : str ): raise Exception ( f \"Removing items not supported: { key } \" ) def __iter__ ( self ) -> typing . Iterator [ str ]: return iter ( self . get_all_field_names ()) def __len__ ( self ): return len ( list ( self . get_all_field_names ())) def to_details ( self , ensure_metadata : bool = False ) -> \"PipelineValuesInfo\" : from kiara.pipeline import PipelineValueInfo , PipelineValuesInfo result = {} for name in self . get_all_field_names (): item = self . get_value_obj ( name ) result [ name ] = PipelineValueInfo . from_value_obj ( item , ensure_metadata = ensure_metadata ) return PipelineValuesInfo ( values = result ) def _create_rich_table ( self , show_headers : bool = True , ensure_metadata : bool = False ) -> Table : table = Table ( box = box . SIMPLE , show_header = show_headers ) table . add_column ( \"Field name\" , style = \"i\" ) table . add_column ( \"Type\" ) table . add_column ( \"Description\" ) table . add_column ( \"Required\" ) table . add_column ( \"Is set\" ) for k in self . get_all_field_names (): v = self . get_value_obj ( k , ensure_metadata = ensure_metadata ) t = v . value_schema . type desc = v . value_schema . doc if not v . value_schema . is_required (): req = \"no\" else : if ( v . value_schema . default and v . value_schema . default != SpecialValue . NO_VALUE and v . value_schema . default != SpecialValue . NOT_SET ): req = \"no\" else : req = \"[red]yes[/red]\" is_set = \"yes\" if v . item_is_valid () else \"no\" table . add_row ( k , t , desc , req , is_set ) return table def __repr__ ( self ): title_str = \"\" if self . _title : title_str = f \" title=' { self . _title } '\" return f \" { self . __class__ . __name__ } (field_names= { list ( self . keys ()) }{ title_str } )\" def __str__ ( self ): return self . __repr__ () def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : title = self . _title if title : postfix = f \" [b] { title } [/b]\" else : postfix = \"\" yield Panel ( self . _create_rich_table ( show_headers = True ), box = box . ROUNDED , title_align = \"left\" , title = f \"Value-Set: { postfix } \" , ) check_invalid ( self ) \u00b6 Check whether the value set is invalid, if it is, return a description of what's wrong. Source code in kiara/data/values/value_set.py def check_invalid ( self ) -> typing . Optional [ typing . Dict [ str , str ]]: \"\"\"Check whether the value set is invalid, if it is, return a description of what's wrong.\"\"\" invalid = {} for field_name , item in self . items (): if not item . item_is_valid (): if item . value_schema . is_required (): if not item . is_set : msg = \"not set\" elif item . is_none : msg = \"no value\" else : msg = \"n/a\" else : msg = \"n/a\" invalid [ field_name ] = msg return invalid get_value_data_for_fields ( self , * field_names , * , raise_exception_when_unset = False ) \u00b6 Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). Source code in kiara/data/values/value_set.py def get_value_data_for_fields ( self , * field_names : str , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). \"\"\" result : typing . Dict [ str , typing . Any ] = {} unset : typing . List [ str ] = [] for k in field_names : v = self . get_value_obj ( k ) if not v . is_set : if raise_exception_when_unset : unset . append ( k ) else : result [ k ] = None else : data = v . get_value_data () result [ k ] = data if unset : raise Exception ( f \"Can't get data for fields, one or several of the requested fields are not set yet: { ', ' . join ( unset ) } .\" ) return result set_values ( self , metadata = None , lineage = None , ** values ) \u00b6 Batch set several values. If metadata is provided, it is added to all values. Source code in kiara/data/values/value_set.py def set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ Value , Exception ]]: \"\"\"Batch set several values. If metadata is provided, it is added to all values. \"\"\" if self . is_read_only (): raise Exception ( \"Can't set values: this value set is read-only.\" ) if not values : return {} invalid : typing . List [ str ] = [] for k in values . keys (): if k not in self . get_all_field_names (): invalid . append ( k ) if invalid : raise ValueError ( f \"No field(s) with name(s) { ', ' . join ( invalid ) } available, valid names: { ', ' . join ( self . get_all_field_names ()) } \" ) resolved_values = {} for field_name , data in values . items (): if isinstance ( data , str ) and data . startswith ( \"value:\" ): v = self . _kiara . get_value ( data ) resolved_values [ field_name ] = v else : resolved_values [ field_name ] = data try : value_set_result = self . _set_values ( metadata = metadata , lineage = lineage , ** resolved_values ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () raise e result : typing . Dict [ str , typing . Union [ Value , Exception ]] = {} for field in values . keys (): if isinstance ( value_set_result [ field ], Exception ): result [ field ] = value_set_result [ field ] # type: ignore else : result [ field ] = self . get_value_obj ( field ) return result defaults \u00b6 DEFAULT_EXCLUDE_DIRS \u00b6 List of directory names to exclude by default when walking a folder recursively. DEFAULT_EXCLUDE_FILES \u00b6 List of file names to exclude by default when reading folders. DEFAULT_PIPELINE_PARENT_ID \u00b6 Default parent id for pipeline objects that are not associated with a workflow. INVALID_VALUE_NAMES \u00b6 List of reserved names, inputs/outputs can't use those. KIARA_MODULE_BASE_FOLDER \u00b6 Marker to indicate the base folder for the kiara module. KIARA_RESOURCES_FOLDER \u00b6 Default resources folder for this package. MODULE_TYPE_KEY \u00b6 The key to specify the type of a module. MODULE_TYPE_NAME_KEY \u00b6 The string for the module type name in a module configuration dict. NO_HASH_MARKER \u00b6 Marker string to indicate no hash was calculated. NO_VALUE_ID_MARKER \u00b6 Marker string to indicate no value id exists. PIPELINE_PARENT_MARKER \u00b6 Marker string in the pipeline structure that indicates a parent pipeline element. STEP_ID_KEY \u00b6 The key to specify the step id. VALID_PIPELINE_FILE_EXTENSIONS \u00b6 File extensions a kiara pipeline/workflow file can have. SpecialValue ( Enum ) \u00b6 An enumeration. Source code in kiara/defaults.py class SpecialValue ( Enum ): NOT_SET = \"__not_set__\" NO_VALUE = \"__no_value__\" IGNORE = \"__ignore__\" doc special \u00b6 Main module for code that helps with documentation auto-generation in supported projects. FrklDocumentationPlugin ( BasePlugin ) \u00b6 mkdocs plugin to render API documentation for a project. To add to a project, add this to the 'plugins' section of a mkdocs config file: - frkl-docgen : main_module : \"module_name\" This will add an API reference navigation item to your page navigation, with auto-generated entries for every Python module in your package. Source code in kiara/doc/__init__.py class FrklDocumentationPlugin ( BasePlugin ): \"\"\"[mkdocs](https://www.mkdocs.org/) plugin to render API documentation for a project. To add to a project, add this to the 'plugins' section of a mkdocs config file: ```yaml - frkl-docgen: main_module: \"module_name\" ``` This will add an ``API reference`` navigation item to your page navigation, with auto-generated entries for every Python module in your package. \"\"\" config_scheme = (( \"main_module\" , mkdocs . config . config_options . Type ( str )),) def __init__ ( self ): self . _doc_paths = None self . _dir = tempfile . TemporaryDirectory ( prefix = \"frkl_doc_gen_\" ) self . _doc_files = None super () . __init__ () def on_files ( self , files : Files , config : Config ) -> Files : self . _doc_paths = gen_pages_for_module ( self . config [ \"main_module\" ]) self . _doc_files = {} for k in sorted ( self . _doc_paths , key = lambda x : os . path . splitext ( x )[ 0 ]): content = self . _doc_paths [ k ][ \"content\" ] _file = File ( k , src_dir = self . _dir . name , dest_dir = config [ \"site_dir\" ], use_directory_urls = config [ \"use_directory_urls\" ], ) os . makedirs ( os . path . dirname ( _file . abs_src_path ), exist_ok = True ) with open ( _file . abs_src_path , \"w\" ) as f : f . write ( content ) self . _doc_files [ k ] = _file files . append ( _file ) return files def on_page_content ( self , html , page : Page , config : Config , files : Files ): repo_url = config . get ( \"repo_url\" , None ) python_src = config . get ( \"edit_uri\" , None ) if page . file . src_path in self . _doc_paths . keys (): src_path = self . _doc_paths . get ( page . file . src_path )[ \"python_src\" ][ \"rel_path\" ] rel_base = urllib . parse . urljoin ( repo_url , f \" { python_src } /../src/ { src_path } \" ) page . edit_url = rel_base return html def on_nav ( self , nav : Navigation , config : Config , files : Files ): for item in nav . items : if item . title and \"Api reference\" in item . title : return nav pages = [] for _file in self . _doc_files . values (): pages . append ( _file . page ) section = Section ( title = \"API reference\" , children = pages ) nav . items . append ( section ) nav . pages . extend ( pages ) _add_previous_and_next_links ( nav . pages ) _add_parent_links ( nav . items ) return nav def on_post_build ( self , config : Config ): self . _dir . cleanup () gen_info_pages \u00b6 render_item_listing ( kiara , item_type , limit_to_package = None ) \u00b6 Render an item listing summary page, for: https://oprypin.github.io/mkdocs-literate-nav/ This code is a terrible, terrible mess, but I just don't care enough. If the output it produces is wrong, it'll be obvious in the documentation (hopefully). I'd have to spend considerable time cleaning this up, and at the moment it does not seem worth it. Source code in kiara/doc/gen_info_pages.py def render_item_listing ( kiara : Kiara , item_type : str , limit_to_package : typing . Optional [ str ] = None ) -> typing . Optional [ str ]: \"\"\"Render an item listing summary page, for: https://oprypin.github.io/mkdocs-literate-nav/ This code is a terrible, terrible mess, but I just don't care enough. If the output it produces is wrong, it'll be obvious in the documentation (hopefully). I'd have to spend considerable time cleaning this up, and at the moment it does not seem worth it. \"\"\" info : KiaraInfoModel = KiaraContext . get_info ( kiara = kiara ) tree = info . get_subcomponent_tree () if tree is None : raise Exception ( \"Can't render item listing, no subcomponent tree available.\" ) def extract_cls_from_kiara_module_type_metadata ( obj ): return obj . python_class . get_class () def extract_cls_from_operation ( obj ): return obj . module . __class__ def extract_cls_from_op_type ( obj ): return obj . python_class . get_class () def extract_cls_from_value_type ( obj ): return obj . python_class . get_class () item_type_map : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { \"module\" : { \"cls\" : KiaraModuleTypeMetadata , \"extract\" : extract_cls_from_kiara_module_type_metadata , }, \"value_type\" : { \"cls\" : ValueTypeMetadata , \"extract\" : extract_cls_from_value_type , }, \"operation\" : { \"cls\" : Operation , \"extract\" : extract_cls_from_operation }, \"operation_type\" : { \"cls\" : OperationsMetadata , \"extract\" : extract_cls_from_op_type , }, } item_cls = item_type_map [ item_type ][ \"cls\" ] plural = f \" { item_type } s\" item_summaries : typing . Dict [ typing . Tuple , typing . List ] = {} list_template = get_jina_env () . get_template ( f \" { item_type } _list.md.j2\" ) for node in tree . nodes (): tokens = node . split ( \".\" ) if len ( tokens ) < 3 : continue category = tokens [ 1 ] obj = tree . nodes [ node ][ \"obj\" ] path_tokens = tokens [ 2 :] if category == plural : if isinstance ( obj , item_cls ): # type: ignore if limit_to_package : cls = item_type_map [ item_type ][ \"extract\" ]( obj ) md = ContextMetadataModel . from_class ( cls ) if md . labels . get ( \"package\" , None ) != limit_to_package : continue item_summaries . setdefault ( tuple ( path_tokens ), []) . append ( obj ) elif isinstance ( obj , KiaraDynamicInfoModel ): item_summaries . setdefault ( tuple ( path_tokens ), []) new_summary : typing . Dict [ typing . Tuple , typing . List ] = {} no_childs = [] for path_tokens , items in item_summaries . items (): # if len(path_tokens) == 1: # new_summary[path_tokens] = item_summaries[path_tokens] # full_path = [path_tokens[0]] full_path : typing . List [ str ] = [] collect : typing . Optional [ typing . List [ str ]] = None for token in path_tokens : if collect is not None : collect . append ( token ) t = tuple ( full_path + collect ) if not item_summaries [ t ]: continue else : new_summary . setdefault ( tuple ( full_path ), []) . extend ( item_summaries [ t ] ) else : full_path . append ( token ) t = tuple ( full_path ) if item_summaries [ t ]: new_summary [ t ] = item_summaries [ t ] else : match = False any_childs = False for k in item_summaries . keys (): if len ( full_path ) == 1 and full_path [ 0 ] == k [ 0 ] and len ( k ) > 2 : match = True if ( len ( k ) > 1 and len ( t ) <= len ( k ) and k [ 0 : len ( t )] == t # noqa and item_summaries [ k ] ): any_childs = True if not any_childs : no_childs . append ( t ) if not match : new_summary . setdefault ( t , []) collect = [] else : new_summary . setdefault ( t , []) . extend ( item_summaries [ t ]) main_summary = [] for summary_path , items in new_summary . items (): if not summary_path : continue match = False for nc in no_childs : if len ( summary_path ) >= len ( nc ): if summary_path [ 0 : len ( nc )] == nc : # noqa match = True break if match : continue padding = \" \" * len ( summary_path ) path = os . path . join ( * summary_path ) rendered = list_template . render ( ** { \"path\" : path , plural : items }) p_write = os . path . join ( plural , path , \"index.md\" ) p_index = os . path . join ( path , \"index.md\" ) with mkdocs_gen_files . open ( p_write , \"w\" ) as f : f . write ( rendered ) main_summary . append ( f \" { padding } * [ { summary_path [ - 1 ] } ]( { p_index } )\" ) modules_content = \"xxxxxxx\" with mkdocs_gen_files . open ( f \" { plural } /index.md\" , \"w\" ) as f : f . write ( modules_content ) summary_content = \" \\n \" . join ( main_summary ) summary_page = f \" { plural } /SUMMARY.md\" with mkdocs_gen_files . open ( summary_page , \"w\" ) as f : f . write ( summary_content ) return f \" { plural } /\" generate_api_doc \u00b6 gen_pages_for_module ( module , prefix = 'api_reference' ) \u00b6 Generate modules for a set of modules (using the mkdocstring package. Source code in kiara/doc/generate_api_doc.py def gen_pages_for_module ( module : typing . Union [ str , ModuleType ], prefix : str = \"api_reference\" ): \"\"\"Generate modules for a set of modules (using the [mkdocstring](https://github.com/mkdocstrings/mkdocstrings) package.\"\"\" result = {} modules_info = get_source_tree ( module ) for module_name , path in modules_info . items (): page_name = module_name if page_name . endswith ( \"__init__\" ): page_name = page_name [ 0 : - 9 ] if page_name . endswith ( \"._frkl\" ): continue doc_path = f \" { prefix }{ os . path . sep }{ page_name } .md\" p = Path ( \"..\" , path [ \"abs_path\" ]) if not p . read_text () . strip (): continue main_module = path [ \"main_module\" ] if page_name == main_module : title = page_name else : title = page_name . replace ( f \" { main_module } .\" , \"\u279c\u2007\" ) result [ doc_path ] = { \"python_src\" : path , \"content\" : f \"--- \\n title: { title } \\n --- \\n # { page_name } \\n\\n ::: { module_name } \" , } return result get_source_tree ( module ) \u00b6 Find all python source files for a module. Source code in kiara/doc/generate_api_doc.py def get_source_tree ( module : typing . Union [ str , ModuleType ]): \"\"\"Find all python source files for a module.\"\"\" if isinstance ( module , str ): module = importlib . import_module ( module ) if not isinstance ( module , ModuleType ): raise TypeError ( f \"Invalid type ' { type ( module ) } ', input needs to be a string or module.\" ) module_file = module . __file__ assert module_file is not None module_root = os . path . dirname ( module_file ) module_name = module . __name__ src = {} for path in Path ( module_root ) . glob ( \"**/*.py\" ): rel = os . path . relpath ( path , module_root ) mod_name = f \" { module_name } . { rel [ 0 : - 3 ] . replace ( os . path . sep , '.' ) } \" rel_path = f \" { module_name }{ os . path . sep }{ rel } \" src [ mod_name ] = { \"rel_path\" : rel_path , \"abs_path\" : path , \"main_module\" : module_name , } return src mkdocs_macros_cli \u00b6 define_env ( env ) \u00b6 Helper macros for Python project documentation. Currently, those macros are available (check the source code for more details): cli \u00b6 Execute a command on the command-line, capture the output and return it to be used in a documentation page. inline_file_as_codeblock \u00b6 Read an external file, and return its content as a markdown code block. Source code in kiara/doc/mkdocs_macros_cli.py def define_env ( env ): \"\"\" Helper macros for Python project documentation. Currently, those macros are available (check the source code for more details): ## ``cli`` Execute a command on the command-line, capture the output and return it to be used in a documentation page. ## ``inline_file_as_codeblock`` Read an external file, and return its content as a markdown code block. \"\"\" # env.variables[\"baz\"] = \"John Doe\" @env . macro def cli ( * command , print_command : bool = True , code_block : bool = True , split_command_and_output : bool = True , max_height : Optional [ int ] = None , cache_key : Optional [ str ] = None , extra_env : Optional [ Dict [ str , str ]] = None , fake_command : Optional [ str ] = None , ): \"\"\"Execute the provided command, save the output and return it to be used in documentation modules.\"\"\" hashes = DeepHash ( command ) hash_str = hashes [ command ] hashes_env = DeepHash ( extra_env ) hashes_env_str = hashes_env [ extra_env ] hash_str = hash_str + \"_\" + hashes_env_str if cache_key : hash_str = hash_str + \"_\" + cache_key cache_file : Path = Path ( os . path . join ( CACHE_DIR , str ( hash_str ))) _run_env = dict ( os_env_vars ) if extra_env : _run_env . update ( extra_env ) if cache_file . is_file (): stdout = cache_file . read_text () else : try : print ( f \"RUNNING: { ' ' . join ( command ) } \" ) result = subprocess . check_output ( command , env = _run_env ) stdout = result . decode () cache_file . write_text ( stdout ) except subprocess . CalledProcessError as e : stdout = f \"Error: { e } \\n\\n Stdout: { e . stdout } \\n\\n Stderr: { e . stderr } \" print ( \"stdout:\" ) print ( e . stdout ) print ( \"stderr:\" ) print ( e . stderr ) if os . getenv ( \"FAIL_DOC_BUILD_ON_ERROR\" ) == \"true\" : sys . exit ( 1 ) if fake_command : command_str = fake_command else : command_str = \" \" . join ( command ) if split_command_and_output and print_command : _c = f \" \\n ``` console \\n { command_str } \\n ``` \\n \" _output = \"``` console \\n \" + stdout + \" \\n ``` \\n \" if max_height is not None and max_height > 0 : _output = f \"<div style='max-height: { max_height } px;overflow:auto'> \\n { _output } \\n </div>\" _stdout = _c + _output else : if print_command : _stdout = f \"> { command_str } \\n { stdout } \" if code_block : _stdout = \"``` console \\n \" + _stdout + \" \\n ``` \\n \" if max_height is not None and max_height > 0 : _stdout = f \"<div style='max-height: { max_height } px;overflow:auto'> \\n { _stdout } \\n </div>\" return _stdout @env . macro def inline_file_as_codeblock ( path , format : str = \"\" ): \"\"\"Import external file and return its content as a markdown code block.\"\"\" f = Path ( path ) return f \"``` { format } \\n { f . read_text () } \\n ```\" mkdocs_macros_kiara \u00b6 define_env ( env ) \u00b6 This is the hook for defining variables, macros and filters variables: the dictionary that contains the environment variables macro: a decorator function, to declare a macro. Source code in kiara/doc/mkdocs_macros_kiara.py def define_env ( env ): \"\"\" This is the hook for defining variables, macros and filters - variables: the dictionary that contains the environment variables - macro: a decorator function, to declare a macro. \"\"\" # env.variables[\"baz\"] = \"John Doe\" @env . macro def get_schema_for_model ( model_class : typing . Union [ str , typing . Type [ BaseModel ]]): if isinstance ( model_class , str ): _class : typing . Type [ BaseModel ] = locate ( model_class ) # type: ignore else : _class = model_class schema_json = _class . schema_json ( indent = 2 ) return schema_json @env . macro def get_src_of_object ( obj : typing . Union [ str , typing . Any ]): try : if isinstance ( obj , str ): _obj : typing . Type [ BaseModel ] = locate ( obj ) # type: ignore else : _obj = obj src = inspect . getsource ( _obj ) return src except Exception as e : return f \"Can't render object source: { str ( e ) } \" @env . macro def get_pipeline_config ( pipeline_name : str ): pmm = PipelineModuleManager () desc = pmm . pipeline_descs [ pipeline_name ][ \"data\" ] desc_str = yaml . dump ( desc ) return desc_str @env . macro def get_module_info ( module_type : str ): try : m_cls = Kiara . instance () . get_module_class ( module_type ) info = KiaraModuleTypeMetadata . from_module_class ( m_cls ) from rich.console import Console console = Console ( record = True ) console . print ( info ) html = console . export_text () return html except Exception as e : return f \"Can't render module info: { str ( e ) } \" @env . macro def get_module_list_for_package ( package_name : str , include_core_modules : bool = True , include_pipelines : bool = True , ): modules = kiara_obj . module_mgmt . find_modules_for_package ( package_name , include_core_modules = include_core_modules , include_pipelines = include_pipelines , ) result = [] for name , info in modules . items (): type_md = info . get_type_metadata () result . append ( f \"- [`` { name } ``][kiara_info.modules. { name } ]\" ) result . append ( f \" - [`` { name } ``]( { type_md . context . get_url_for_reference ( 'module_doc' ) } ): { type_md . documentation . description } \" ) print ( result ) return \" \\n \" . join ( result ) @env . macro def get_value_types_for_package ( package_name : str ): value_types = kiara_obj . type_mgmt . find_value_types_for_package ( package_name ) result = [] for name , info in value_types . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result ) @env . macro def get_metadata_models_for_package ( package_name : str ): metadata_schemas = kiara_obj . metadata_mgmt . find_all_models_for_package ( package_name ) result = [] for name , info in metadata_schemas . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result ) @env . macro def get_kiara_context () -> KiaraContext : return kiara_context on_post_build ( env ) \u00b6 Post-build actions Source code in kiara/doc/mkdocs_macros_kiara.py def on_post_build ( env ): \"Post-build actions\" site_dir = env . conf [ \"site_dir\" ] for category , classes in KIARA_MODEL_CLASSES . items (): for cls in classes : schema_json = cls . schema_json ( indent = 2 ) file_path = os . path . join ( site_dir , \"development\" , \"entities\" , category , f \" { cls . __name__ } .json\" ) os . makedirs ( os . path . dirname ( file_path ), exist_ok = True ) with open ( file_path , \"w\" ) as f : f . write ( schema_json ) mkdocstrings special \u00b6 collector \u00b6 KiaraCollector ( BaseCollector ) \u00b6 The class responsible for loading Jinja templates and rendering them. It defines some configuration options, implements the render method, and overrides the update_env method of the BaseRenderer class . Source code in kiara/doc/mkdocstrings/collector.py class KiaraCollector ( BaseCollector ): \"\"\"The class responsible for loading Jinja templates and rendering them. It defines some configuration options, implements the `render` method, and overrides the `update_env` method of the [`BaseRenderer` class][mkdocstrings.handlers.base.BaseRenderer]. \"\"\" default_config : dict = { \"docstring_style\" : \"google\" , \"docstring_options\" : {}} \"\"\"The default selection options. Option | Type | Description | Default ------ | ---- | ----------- | ------- **`docstring_style`** | `\"google\" | \"numpy\" | \"sphinx\" | None` | The docstring style to use. | `\"google\"` **`docstring_options`** | `dict[str, Any]` | The options for the docstring parser. | `{}` \"\"\" fallback_config : dict = { \"fallback\" : True } def __init__ ( self ) -> None : \"\"\"Initialize the collector.\"\"\" self . _kiara : Kiara = Kiara . instance () self . _context : KiaraContext = KiaraContext . get_info ( kiara = self . _kiara , ignore_errors = False ) self . _component_tree : nx . DiGraph = self . _context . get_subcomponent_tree () # type: ignore def collect ( self , identifier : str , config : dict ) -> CollectorItem : # noqa: WPS231 \"\"\"Collect the documentation tree given an identifier and selection options. Arguments: identifier: The dotted-path of a Python object available in the Python path. config: Selection options, used to alter the data collection done by `pytkdocs`. Raises: CollectionError: When there was a problem collecting the object documentation. Returns: The collected object-tree. \"\"\" tokens = identifier . split ( \".\" ) kiara_id = \".\" . join ( tokens [ 1 :]) print ( f \"REQUESTED: { identifier } \" ) if tokens [ 0 ] != \"kiara_info\" : return None # raise Exception( # f\"Handler 'kiara' can only be used with identifiers that start with 'kiara_info.', the provided id is invalid: {identifier}\" # ) try : info = self . _component_tree . nodes [ f \"__self__. { kiara_id } \" ][ \"obj\" ] except Exception as e : import traceback traceback . print_exc () raise e print ( f \"DONE: { identifier } \" ) return { \"obj\" : info , \"identifier\" : identifier , \"kiara_id\" : kiara_id } default_config : dict \u00b6 The default selection options. Option | Type | Description | Default ------ | ---- | ----------- | ------- docstring_style | \"google\" | \"numpy\" | \"sphinx\" | None | The docstring style to use. | \"google\" docstring_options | dict[str, Any] | The options for the docstring parser. | {} __init__ ( self ) special \u00b6 Initialize the collector. Source code in kiara/doc/mkdocstrings/collector.py def __init__ ( self ) -> None : \"\"\"Initialize the collector.\"\"\" self . _kiara : Kiara = Kiara . instance () self . _context : KiaraContext = KiaraContext . get_info ( kiara = self . _kiara , ignore_errors = False ) self . _component_tree : nx . DiGraph = self . _context . get_subcomponent_tree () # type: ignore collect ( self , identifier , config ) \u00b6 Collect the documentation tree given an identifier and selection options. Parameters: Name Type Description Default identifier str The dotted-path of a Python object available in the Python path. required config dict Selection options, used to alter the data collection done by pytkdocs . required Exceptions: Type Description CollectionError When there was a problem collecting the object documentation. Returns: Type Description CollectorItem The collected object-tree. Source code in kiara/doc/mkdocstrings/collector.py def collect ( self , identifier : str , config : dict ) -> CollectorItem : # noqa: WPS231 \"\"\"Collect the documentation tree given an identifier and selection options. Arguments: identifier: The dotted-path of a Python object available in the Python path. config: Selection options, used to alter the data collection done by `pytkdocs`. Raises: CollectionError: When there was a problem collecting the object documentation. Returns: The collected object-tree. \"\"\" tokens = identifier . split ( \".\" ) kiara_id = \".\" . join ( tokens [ 1 :]) print ( f \"REQUESTED: { identifier } \" ) if tokens [ 0 ] != \"kiara_info\" : return None # raise Exception( # f\"Handler 'kiara' can only be used with identifiers that start with 'kiara_info.', the provided id is invalid: {identifier}\" # ) try : info = self . _component_tree . nodes [ f \"__self__. { kiara_id } \" ][ \"obj\" ] except Exception as e : import traceback traceback . print_exc () raise e print ( f \"DONE: { identifier } \" ) return { \"obj\" : info , \"identifier\" : identifier , \"kiara_id\" : kiara_id } handler \u00b6 KiaraHandler ( BaseHandler ) \u00b6 The kiara handler class. Attributes: Name Type Description domain str The cross-documentation domain/language for this handler. enable_inventory bool Whether this handler is interested in enabling the creation of the objects.inv Sphinx inventory file. Source code in kiara/doc/mkdocstrings/handler.py class KiaraHandler ( BaseHandler ): \"\"\"The kiara handler class. Attributes: domain: The cross-documentation domain/language for this handler. enable_inventory: Whether this handler is interested in enabling the creation of the `objects.inv` Sphinx inventory file. \"\"\" domain : str = \"kiara\" enable_inventory : bool = True # load_inventory = staticmethod(inventory.list_object_urls) # # @classmethod # def load_inventory( # cls, # in_file: typing.BinaryIO, # url: str, # base_url: typing.Optional[str] = None, # **kwargs: typing.Any, # ) -> typing.Iterator[typing.Tuple[str, str]]: # \"\"\"Yield items and their URLs from an inventory file streamed from `in_file`. # This implements mkdocstrings' `load_inventory` \"protocol\" (see plugin.py). # Arguments: # in_file: The binary file-like object to read the inventory from. # url: The URL that this file is being streamed from (used to guess `base_url`). # base_url: The URL that this inventory's sub-paths are relative to. # **kwargs: Ignore additional arguments passed from the config. # Yields: # Tuples of (item identifier, item URL). # \"\"\" # # print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\") # # if base_url is None: # base_url = posixpath.dirname(url) # # for item in Inventory.parse_sphinx( # in_file, domain_filter=(\"py\",) # ).values(): # noqa: WPS526 # yield item.name, posixpath.join(base_url, item.uri) get_handler ( theme , custom_templates = None , ** config ) \u00b6 Simply return an instance of PythonHandler . Parameters: Name Type Description Default theme str The theme to use when rendering contents. required custom_templates Optional[str] Directory containing custom templates. None **config Any Configuration passed to the handler. {} Returns: Type Description KiaraHandler An instance of PythonHandler . Source code in kiara/doc/mkdocstrings/handler.py def get_handler ( theme : str , # noqa: W0613 (unused argument config) custom_templates : typing . Optional [ str ] = None , ** config : typing . Any , ) -> KiaraHandler : \"\"\"Simply return an instance of `PythonHandler`. Arguments: theme: The theme to use when rendering contents. custom_templates: Directory containing custom templates. **config: Configuration passed to the handler. Returns: An instance of `PythonHandler`. \"\"\" if custom_templates is not None : raise Exception ( \"Custom templates are not supported for the kiara renderer.\" ) custom_templates = os . path . join ( KIARA_RESOURCES_FOLDER , \"templates\" , \"info_templates\" ) return KiaraHandler ( collector = KiaraCollector (), renderer = KiaraInfoRenderer ( \"kiara\" , theme , custom_templates ), ) renderer \u00b6 KiaraInfoRenderer ( BaseRenderer ) \u00b6 Source code in kiara/doc/mkdocstrings/renderer.py class KiaraInfoRenderer ( BaseRenderer ): default_config : dict = {} def render ( self , data : typing . Dict [ str , typing . Any ], config : dict ) -> str : final_config = ChainMap ( config , self . default_config ) obj = data [ \"obj\" ] data_type = obj . __class__ . __name__ . lower () func_name = f \"render_ { data_type } \" if hasattr ( self , func_name ): func = getattr ( self , func_name ) return func ( payload = data , config = final_config ) else : html = obj . create_html () print ( f \"Rendered: { data [ 'identifier' ] } \" ) return html render ( self , data , config ) \u00b6 Render a template using provided data and configuration options. Parameters: Name Type Description Default data Dict[str, Any] The collected data to render. required config dict The rendering options. required Returns: Type Description str The rendered template as HTML. Source code in kiara/doc/mkdocstrings/renderer.py def render ( self , data : typing . Dict [ str , typing . Any ], config : dict ) -> str : final_config = ChainMap ( config , self . default_config ) obj = data [ \"obj\" ] data_type = obj . __class__ . __name__ . lower () func_name = f \"render_ { data_type } \" if hasattr ( self , func_name ): func = getattr ( self , func_name ) return func ( payload = data , config = final_config ) else : html = obj . create_html () print ( f \"Rendered: { data [ 'identifier' ] } \" ) return html environment special \u00b6 RuntimeEnvironmentConfig ( BaseModel ) pydantic-model \u00b6 Source code in kiara/environment/__init__.py class RuntimeEnvironmentConfig ( BaseModel ): class Config : allow_mutation = False include_all_info : bool = Field ( default = False , description = \"Whether to include all properties, even if it might include potentially private/sensitive information. This is mainly used for debug purposes.\" , ) include_all_info : bool pydantic-field \u00b6 Whether to include all properties, even if it might include potentially private/sensitive information. This is mainly used for debug purposes. operating_system \u00b6 OSRuntimeEnvironment ( BaseRuntimeEnvironment ) pydantic-model \u00b6 Manages information about the OS this kiara instance is running in. TODO: details for other OS's (mainly BSDs) \u00b6 Source code in kiara/environment/operating_system.py class OSRuntimeEnvironment ( BaseRuntimeEnvironment ): \"\"\"Manages information about the OS this kiara instance is running in. # TODO: details for other OS's (mainly BSDs) \"\"\" environment_type : typing . Literal [ \"operating_system\" ] operation_system : str = Field ( description = \"The operation system name.\" ) platform : str = Field ( description = \"The platform name.\" ) release : str = Field ( description = \"The platform release name.\" ) version : str = Field ( description = \"The platform version name.\" ) machine : str = Field ( description = \"The architecture.\" ) os_specific : typing . Dict [ str , typing . Any ] = Field ( description = \"OS specific platform metadata.\" , default_factory = dict ) uname : typing . Optional [ typing . Dict [ str , str ]] = Field ( description = \"Platform uname information.\" ) @classmethod def retrieve_environment_data ( self , config : RuntimeEnvironmentConfig ) -> typing . Dict [ str , typing . Any ]: os_specific : typing . Dict [ str , typing . Any ] = {} platform_system = platform . system () if platform_system == \"Linux\" : import distro data = distro . linux_distribution () os_specific [ \"distribution\" ] = { \"name\" : data [ 0 ], \"version\" : data [ 1 ], \"codename\" : data [ 2 ], } elif platform_system == \"Darwin\" : mac_version = platform . mac_ver () os_specific [ \"mac_ver_release\" ] = mac_version [ 0 ] os_specific [ \"mac_ver_machine\" ] = mac_version [ 2 ] result = { \"operation_system\" : os . name , \"platform\" : platform_system , \"release\" : platform . release (), \"version\" : platform . version (), \"machine\" : platform . machine (), \"os_specific\" : os_specific , } if config . include_all_info : result [ \"uname\" ] = platform . uname () . _asdict () return result machine : str pydantic-field required \u00b6 The architecture. operation_system : str pydantic-field required \u00b6 The operation system name. os_specific : Dict [ str , Any ] pydantic-field \u00b6 OS specific platform metadata. platform : str pydantic-field required \u00b6 The platform name. release : str pydantic-field required \u00b6 The platform release name. uname : Dict [ str , str ] pydantic-field \u00b6 Platform uname information. version : str pydantic-field required \u00b6 The platform version name. python \u00b6 PythonPackage ( BaseModel ) pydantic-model \u00b6 Source code in kiara/environment/python.py class PythonPackage ( BaseModel ): name : str = Field ( description = \"The name of the Python package.\" ) version : str = Field ( description = \"The version of the package.\" ) name : str pydantic-field required \u00b6 The name of the Python package. version : str pydantic-field required \u00b6 The version of the package. PythonRuntimeEnvironment ( BaseRuntimeEnvironment ) pydantic-model \u00b6 Source code in kiara/environment/python.py class PythonRuntimeEnvironment ( BaseRuntimeEnvironment ): environment_type : typing . Literal [ \"python\" ] python_version : str = Field ( description = \"The version of Python.\" ) packages : typing . List [ PythonPackage ] = Field ( description = \"The packages installed in the Python (virtual) environment.\" ) python_config : typing . Dict [ str , str ] = Field ( description = \"Configuration details about the Python installation.\" ) @classmethod def retrieve_environment_data ( cls , config : RuntimeEnvironmentConfig ) -> typing . Dict [ str , typing . Any ]: packages = [] all_packages = packages_distributions () for name , pkgs in all_packages . items (): for pkg in pkgs : dist = distribution ( pkg ) packages . append ({ \"name\" : name , \"version\" : dist . version }) result : typing . Dict [ str , typing . Any ] = { \"python_version\" : sys . version , \"packages\" : sorted ( packages , key = lambda x : x [ \"name\" ]), } if config . include_all_info : import sysconfig result [ \"python_config\" ] = sysconfig . get_config_vars () return result packages : List [ kiara . environment . python . PythonPackage ] pydantic-field required \u00b6 The packages installed in the Python (virtual) environment. python_config : Dict [ str , str ] pydantic-field required \u00b6 Configuration details about the Python installation. python_version : str pydantic-field required \u00b6 The version of Python. events \u00b6 PipelineInputEvent ( StepEvent ) pydantic-model \u00b6 Event that gets fired when one or several inputs for the pipeline itself have changed. Source code in kiara/events.py class PipelineInputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several inputs for the pipeline itself have changed.\"\"\" type : Literal [ \"pipeline_input\" ] = \"pipeline_input\" updated_pipeline_inputs : typing . List [ str ] = Field ( description = \"list of pipeline input names that where changed\" ) updated_pipeline_inputs : List [ str ] pydantic-field required \u00b6 list of pipeline input names that where changed PipelineOutputEvent ( StepEvent ) pydantic-model \u00b6 Event that gets fired when one or several outputs for the pipeline itself have changed. Source code in kiara/events.py class PipelineOutputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several outputs for the pipeline itself have changed.\"\"\" type : Literal [ \"pipeline_output\" ] = \"pipeline_output\" updated_pipeline_outputs : typing . List [ str ] = Field ( description = \"list of pipeline output names that where changed\" ) updated_pipeline_outputs : List [ str ] pydantic-field required \u00b6 list of pipeline output names that where changed StepEvent ( BaseModel ) pydantic-model \u00b6 Source code in kiara/events.py class StepEvent ( BaseModel ): class Config : allow_mutation = False pipeline_id : str def __repr__ ( self ): d = self . dict () d . pop ( \"pipeline_id\" ) return f \" { self . __class__ . __name__ } (pipeline_id= { self . pipeline_id } data= { d } \" def __str__ ( self ): return self . __repr__ () __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/events.py def __repr__ ( self ): d = self . dict () d . pop ( \"pipeline_id\" ) return f \" { self . __class__ . __name__ } (pipeline_id= { self . pipeline_id } data= { d } \" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/events.py def __str__ ( self ): return self . __repr__ () StepInputEvent ( StepEvent ) pydantic-model \u00b6 Event that gets fired when one or several inputs for steps within a pipeline have changed. Source code in kiara/events.py class StepInputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several inputs for steps within a pipeline have changed.\"\"\" type : Literal [ \"step_input\" ] = \"step_input\" updated_step_inputs : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"steps (keys) with updated inputs which need re-processing (value is list of updated input names)\" ) @property def newly_stale_steps ( self ) -> typing . List [ str ]: \"\"\"Convenience method to display the steps that have been rendered 'stale' by this event.\"\"\" return list ( self . updated_step_inputs . keys ()) newly_stale_steps : List [ str ] property readonly \u00b6 Convenience method to display the steps that have been rendered 'stale' by this event. updated_step_inputs : Dict [ str , List [ str ]] pydantic-field required \u00b6 steps (keys) with updated inputs which need re-processing (value is list of updated input names) StepOutputEvent ( StepEvent ) pydantic-model \u00b6 Event that gets fired when one or several outputs for steps within a pipeline have changed. Source code in kiara/events.py class StepOutputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several outputs for steps within a pipeline have changed.\"\"\" type : Literal [ \"step_output\" ] = \"step_output\" updated_step_outputs : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"steps (keys) that finished processing of one, several or all outputs (values are list of 'finished' output fields)\" ) updated_step_outputs : Dict [ str , List [ str ]] pydantic-field required \u00b6 steps (keys) that finished processing of one, several or all outputs (values are list of 'finished' output fields) examples special \u00b6 example_controller \u00b6 ExampleController ( PipelineController ) \u00b6 Source code in kiara/examples/example_controller.py class ExampleController ( PipelineController ): def pipeline_inputs_changed ( self , event : PipelineInputEvent ): print ( f \"Pipeline inputs changed: { event . updated_pipeline_inputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): print ( f \"Pipeline outputs changed: { event . updated_pipeline_outputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) def step_inputs_changed ( self , event : StepInputEvent ): print ( \"Step inputs changed, new values:\" ) for step_id , input_names in event . updated_step_inputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in input_names : new_value = self . get_step_inputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) def step_outputs_changed ( self , event : StepOutputEvent ): print ( \"Step outputs changed, new values:\" ) for step_id , output_names in event . updated_step_outputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in output_names : new_value = self . get_step_outputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) def execute ( self ): print ( \"Executing steps: 'and', 'not'...\" ) and_job_id = self . process_step ( \"and\" ) self . wait_for_jobs ( and_job_id ) not_job_id = self . process_step ( \"not\" ) self . wait_for_jobs ( not_job_id ) pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/examples/example_controller.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): print ( f \"Pipeline inputs changed: { event . updated_pipeline_inputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/examples/example_controller.py def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): print ( f \"Pipeline outputs changed: { event . updated_pipeline_outputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/examples/example_controller.py def step_inputs_changed ( self , event : StepInputEvent ): print ( \"Step inputs changed, new values:\" ) for step_id , input_names in event . updated_step_inputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in input_names : new_value = self . get_step_inputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) step_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Parameters: Name Type Description Default event StepOutputEvent the step output event required Source code in kiara/examples/example_controller.py def step_outputs_changed ( self , event : StepOutputEvent ): print ( \"Step outputs changed, new values:\" ) for step_id , output_names in event . updated_step_outputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in output_names : new_value = self . get_step_outputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) info special \u00b6 kiara \u00b6 KiaraContext ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/kiara.py class KiaraContext ( KiaraInfoModel ): available_categories : typing . ClassVar = [ \"value_types\" , \"modules\" , \"pipelines\" , \"operations\" , \"operation_types\" , ] _info_cache : typing . ClassVar = {} @classmethod def get_info_for_category ( cls , kiara : \"Kiara\" , category_name : str , ignore_errors : bool = False ) -> KiaraInfoModel : if category_name not in cls . available_categories : raise Exception ( f \"Can't provide information for category ' { category_name } ': invalid category name. Valid names: { ', ' . join ( cls . available_categories ) } \" ) cache = ( cls . _info_cache . get ( kiara . _id , {}) . get ( category_name , {}) . get ( ignore_errors , None ) ) if cache is not None : return cache if category_name == \"value_types\" : all_types = ValueTypeMetadata . create_all ( kiara = kiara ) info = KiaraDynamicInfoModel . create_from_child_models ( ** all_types ) elif category_name == \"modules\" : info = ModuleTypesGroupInfo . from_type_names ( kiara = kiara , ignore_pipeline_modules = True ) elif category_name == \"pipelines\" : info = PipelineTypesGroupInfo . create ( kiara = kiara , ignore_errors = ignore_errors ) elif category_name == \"operation_types\" : all_op_types = OperationsMetadata . create_all ( kiara = kiara ) info = KiaraDynamicInfoModel . create_from_child_models ( ** all_op_types ) elif category_name == \"operations\" : ops_infos = KiaraDynamicInfoModel . create_from_child_models ( ** OperationsInfo . create_all ( kiara = kiara ) ) operation_configs = {} for v in ops_infos . __root__ . values (): configs = v . operation_configs operation_configs . update ( configs ) info = KiaraDynamicInfoModel . create_from_child_models ( ** operation_configs ) else : raise NotImplementedError ( f \"Category not available: { category_name } \" ) cls . _info_cache . setdefault ( kiara . _id , {}) . setdefault ( ignore_errors , {})[ category_name ] = info return info @classmethod def get_info ( cls , kiara : \"Kiara\" , sub_type : typing . Optional [ str ] = None , ignore_errors : bool = False , ): if sub_type is None : module_types = cls . get_info_for_category ( kiara = kiara , category_name = \"modules\" , ignore_errors = ignore_errors ) value_types = cls . get_info_for_category ( kiara = kiara , category_name = \"value_types\" , ignore_errors = ignore_errors ) pipeline_types = cls . get_info_for_category ( kiara = kiara , category_name = \"pipelines\" , ignore_errors = ignore_errors ) op_group = cls . get_info_for_category ( kiara = kiara , category_name = \"operations\" , ignore_errors = ignore_errors ) op_types = cls . get_info_for_category ( kiara = kiara , category_name = \"operation_types\" , ignore_errors = ignore_errors , ) return KiaraContext ( value_types = value_types , modules = module_types , pipelines = pipeline_types , operations = op_group , operation_types = op_types , ) else : tokens = sub_type . split ( \".\" ) current = cls . get_info_for_category ( kiara = kiara , category_name = tokens [ 0 ], ignore_errors = ignore_errors ) if len ( tokens ) == 1 : return current path = \".\" . join ( tokens [ 1 :]) try : current = current . get_subcomponent ( path ) except Exception as e : raise Exception ( f \"Can't get ' { path } ' information for ' { tokens [ 0 ] } ': { e } \" ) return current value_types : KiaraDynamicInfoModel = Field ( description = \"Information about available value types.\" ) modules : ModuleTypesGroupInfo = Field ( description = \"Information about the available modules.\" ) pipelines : PipelineTypesGroupInfo = Field ( description = \"Information about available pipelines.\" ) operation_types : KiaraDynamicInfoModel = Field ( description = \"Information about operation types contained in the current kiara context.\" ) operations : KiaraDynamicInfoModel = Field ( description = \"Available operations.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) value_type_table = self . value_types . create_renderable () table . add_row ( \"value_types\" , value_type_table ) module_table = self . modules . create_renderable () table . add_row ( \"modules\" , module_table ) pipelines_table = self . pipelines . create_renderable () table . add_row ( \"pipelines\" , pipelines_table ) operation_types_table = self . operation_types . create_renderable () table . add_row ( \"operation_types\" , operation_types_table ) operations_table = self . operations . create_renderable () table . add_row ( \"operations\" , operations_table ) return table modules : ModuleTypesGroupInfo pydantic-field required \u00b6 Information about the available modules. operation_types : KiaraDynamicInfoModel pydantic-field required \u00b6 Information about operation types contained in the current kiara context. operations : KiaraDynamicInfoModel pydantic-field required \u00b6 Available operations. pipelines : PipelineTypesGroupInfo pydantic-field required \u00b6 Information about available pipelines. value_types : KiaraDynamicInfoModel pydantic-field required \u00b6 Information about available value types. modules \u00b6 ModuleTypesGroupInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/modules.py class ModuleTypesGroupInfo ( KiaraInfoModel ): __root__ : typing . Dict [ str , KiaraModuleTypeMetadata ] @classmethod def from_type_names ( cls , kiara : \"Kiara\" , type_names : typing . Optional [ typing . Iterable [ str ]] = None , ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if type_names is None : type_names = kiara . available_module_types classes = {} for tn in type_names : _cls = kiara . get_module_class ( tn ) if ignore_pipeline_modules and _cls . is_pipeline (): continue if ignore_non_pipeline_modules and not _cls . is_pipeline (): continue classes [ tn ] = KiaraModuleTypeMetadata . from_module_class ( _cls ) return ModuleTypesGroupInfo ( __root__ = classes ) @classmethod def create_renderable_from_type_names ( cls , kiara : \"Kiara\" , type_names : typing . Iterable [ str ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): classes = {} for tn in type_names : _cls = kiara . get_module_class ( tn ) classes [ tn ] = _cls return cls . create_renderable_from_module_type_map ( module_types = classes , ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config ) @classmethod def create_renderable_from_module_type_map ( cls , module_types : typing . Mapping [ str , typing . Type [ \"KiaraModule\" ]], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" return cls . create_renderable_from_module_info_map ( { k : KiaraModuleTypeMetadata . from_module_class ( v ) for k , v in module_types . items () }, ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config ) @classmethod def create_renderable_from_module_info_map ( cls , module_types : typing . Mapping [ str , KiaraModuleTypeMetadata ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if ignore_pipeline_modules : module_types = { k : v for k , v in module_types . items () if not v . is_pipeline } elif ignore_non_pipeline_modules : module_types = { k : v for k , v in module_types . items () if v . is_pipeline } show_lines = False table = Table ( show_header = False , box = box . SIMPLE , show_lines = show_lines ) table . add_column ( \"name\" , style = \"b\" ) table . add_column ( \"desc\" , style = \"i\" ) for name , details in module_types . items (): if config . get ( \"include_full_doc\" , False ): table . add_row ( name , details . documentation . full_doc ) else : table . add_row ( name , details . documentation . description ) return table def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return ModuleTypesGroupInfo . create_renderable_from_module_info_map ( self . __root__ ) create_renderable_from_module_info_map ( module_types , ignore_pipeline_modules = False , ignore_non_pipeline_modules = False , ** config ) classmethod \u00b6 Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description Source code in kiara/info/modules.py @classmethod def create_renderable_from_module_info_map ( cls , module_types : typing . Mapping [ str , KiaraModuleTypeMetadata ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if ignore_pipeline_modules : module_types = { k : v for k , v in module_types . items () if not v . is_pipeline } elif ignore_non_pipeline_modules : module_types = { k : v for k , v in module_types . items () if v . is_pipeline } show_lines = False table = Table ( show_header = False , box = box . SIMPLE , show_lines = show_lines ) table . add_column ( \"name\" , style = \"b\" ) table . add_column ( \"desc\" , style = \"i\" ) for name , details in module_types . items (): if config . get ( \"include_full_doc\" , False ): table . add_row ( name , details . documentation . full_doc ) else : table . add_row ( name , details . documentation . description ) return table create_renderable_from_module_type_map ( module_types , ignore_pipeline_modules = False , ignore_non_pipeline_modules = False , ** config ) classmethod \u00b6 Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description Source code in kiara/info/modules.py @classmethod def create_renderable_from_module_type_map ( cls , module_types : typing . Mapping [ str , typing . Type [ \"KiaraModule\" ]], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" return cls . create_renderable_from_module_info_map ( { k : KiaraModuleTypeMetadata . from_module_class ( v ) for k , v in module_types . items () }, ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config ) operations \u00b6 OperationsGroupInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/operations.py class OperationsGroupInfo ( KiaraInfoModel ): @classmethod def create ( cls , kiara : \"Kiara\" , ignore_errors : bool = False ): operation_types = OperationsInfo . create_all ( kiara = kiara ) operation_configs = operation_types . pop ( \"all\" ) return OperationsGroupInfo ( operation_types = operation_types , operation_configs = operation_configs . operation_configs , ) operation_types : typing . Dict [ str , OperationsInfo ] = Field ( description = \"The available operation types and their details.\" ) operation_configs : typing . Dict [ str , Operation ] = Field ( description = \"The available operation ids and module_configs.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) op_map_table = OperationsInfo . create_renderable_from_operations_map ( self . operation_types ) table . add_row ( \"operation types\" , op_map_table ) configs = ModuleConfig . create_renderable_from_module_instance_configs ( self . operation_configs ) table . add_row ( \"operations\" , configs ) return table operation_configs : Dict [ str , kiara . operations . Operation ] pydantic-field required \u00b6 The available operation ids and module_configs. operation_types : Dict [ str , kiara . info . operations . OperationsInfo ] pydantic-field required \u00b6 The available operation types and their details. OperationsInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/operations.py class OperationsInfo ( KiaraInfoModel ): @classmethod def create_all ( self , kiara : Kiara ) -> typing . Dict [ str , \"OperationsInfo\" ]: op_types = kiara . operation_mgmt . operation_types return { op_name : OperationsInfo . create ( operations = op_types [ op_name ]) for op_name in sorted ( op_types . keys ()) } @classmethod def create ( cls , operations : OperationType ): info = OperationsMetadata . from_operations_class ( operations . __class__ ) return OperationsInfo ( info = info , operation_configs = dict ( operations . operations )) @classmethod def create_renderable_from_operations_map ( cls , op_map : typing . Mapping [ str , \"OperationsInfo\" ] ): table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) for op_name , ops in op_map . items (): table . add_row ( op_name , ops ) return table info : OperationsMetadata = Field ( description = \"Details about the type of operations contained in this collection.\" ) operation_configs : typing . Dict [ str , Operation ] = Field ( description = \"All available operation ids and their configurations.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return self . info . create_renderable ( operations = self . operation_configs , omit_type_name = True ) info : OperationsMetadata pydantic-field required \u00b6 Details about the type of operations contained in this collection. operation_configs : Dict [ str , kiara . operations . Operation ] pydantic-field required \u00b6 All available operation ids and their configurations. pipelines \u00b6 PipelineModuleInfo ( KiaraModuleTypeMetadata ) pydantic-model \u00b6 Source code in kiara/info/pipelines.py class PipelineModuleInfo ( KiaraModuleTypeMetadata ): class Config : extra = Extra . forbid @classmethod def from_type_name ( cls , module_type_name : str , kiara : \"Kiara\" ): m = kiara . get_module_class ( module_type = module_type_name ) base_conf : \"PipelineConfig\" = m . _base_pipeline_config # type: ignore structure = base_conf . create_pipeline_structure ( kiara = kiara ) struc_desc = PipelineStructureDesc . create_pipeline_structure_desc ( pipeline = structure ) attrs = PipelineModuleInfo . extract_module_attributes ( module_cls = m ) attrs [ \"structure\" ] = struc_desc pmi = PipelineModuleInfo ( ** attrs ) pmi . _kiara = kiara pmi . _structure = structure return pmi _kiara : \"Kiara\" = PrivateAttr () _structure : \"PipelineStructure\" = PrivateAttr () structure : PipelineStructureDesc = Field ( description = \"The pipeline structure.\" ) def print_data_flow_graph ( self , simplified : bool = True ) -> None : structure = self . _structure if simplified : graph = structure . data_flow_graph_simple else : graph = structure . data_flow_graph print_ascii_graph ( graph ) def print_execution_graph ( self ) -> None : structure = self . _structure print_ascii_graph ( structure . execution_graph ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : my_table = Table ( box = box . SIMPLE , show_lines = True , show_header = False ) my_table . add_column ( \"Property\" , style = \"i\" ) my_table . add_column ( \"Value\" ) my_table . add_row ( \"class\" , self . python_class . full_name ) my_table . add_row ( \"is pipeline\" , \"yes\" ) my_table . add_row ( \"doc\" , self . documentation . full_doc ) my_table . add_row ( \"config class\" , self . config . python_class . full_name ) my_table . add_row ( \"config\" , create_table_from_config_class ( self . config . python_class . get_class (), # type: ignore remove_pipeline_config = True , ), ) structure = self . _structure p_inputs = {} for input_name , schema in structure . pipeline_input_schema . items (): p_inputs [ input_name ] = { \"type\" : schema . type , \"doc\" : schema . doc } inputs_str = yaml . dump ( p_inputs ) _inputs_txt = Syntax ( inputs_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"pipeline inputs\" , _inputs_txt ) outputs = {} for output_name , schema in structure . pipeline_output_schema . items (): outputs [ output_name ] = { \"type\" : schema . type , \"doc\" : schema . doc } outputs_str = yaml . dump ( outputs ) _outputs_txt = Syntax ( outputs_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"pipeline outputs\" , _outputs_txt ) stages : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} for nr , stage in enumerate ( structure . processing_stages ): for s_id in stage : step = structure . get_step ( s_id ) mc = self . _kiara . get_module_class ( step . module_type ) desc = mc . get_type_metadata () . documentation . full_doc inputs : typing . Dict [ \"ValueRef\" , typing . List [ str ]] = {} for inp in structure . steps_inputs . values (): if inp . step_id != s_id : continue if inp . connected_outputs : for co in inp . connected_outputs : inputs . setdefault ( inp , []) . append ( co . alias ) else : inputs . setdefault ( inp , []) . append ( f \"__pipeline__. { inp . connected_pipeline_input } \" ) inp_str = [] for k , v in inputs . items (): s = f \" { k . value_name } \u2190 { ', ' . join ( v ) } \" inp_str . append ( s ) outp_str = [] for outp in structure . steps_outputs . values (): if outp . step_id != s_id : continue if outp . pipeline_output : outp_str . append ( f \" { outp . value_name } \u2192 __pipeline__. { outp . pipeline_output } \" ) else : outp_str . append ( outp . value_name ) stages . setdefault ( f \"stage { nr } \" , {})[ s_id ] = { \"module\" : step . module_type , \"desc\" : desc , \"inputs\" : inp_str , \"outputs\" : outp_str , } stages_str = yaml . dump ( stages ) _stages_txt = Syntax ( stages_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"processing stages\" , _stages_txt ) return my_table structure : PipelineStructureDesc pydantic-field required \u00b6 The pipeline structure. PipelineState ( KiaraInfoModel ) pydantic-model \u00b6 Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the dict or json methods to convert this object into a generic data structure. Source code in kiara/info/pipelines.py class PipelineState ( KiaraInfoModel ): \"\"\"Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the ``dict`` or ``json`` methods to convert this object into a generic data structure. \"\"\" structure : PipelineStructureDesc = Field ( description = \"The structure (interconnections of modules/steps) of the pipeline.\" ) pipeline_inputs : PipelineValuesInfo = Field ( description = \"The current (externally facing) input values of this pipeline.\" ) pipeline_outputs : PipelineValuesInfo = Field ( description = \"The current (externally facing) output values of this pipeline.\" ) step_states : typing . Dict [ str , StepStatus ] = Field ( description = \"The status of each step.\" ) step_inputs : typing . Dict [ str , PipelineValuesInfo ] = Field ( description = \"The current (internal) input values of each step of this pipeline.\" ) step_outputs : typing . Dict [ str , PipelineValuesInfo ] = Field ( description = \"The current (internal) output values of each step of this pipeline.\" ) status : StepStatus = Field ( description = \"The current overal status of the pipeline.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : from kiara.pipeline.pipeline import StepStatus all : typing . List [ RenderableType ] = [] all . append ( \"[b]Pipeline state[/b]\" ) all . append ( \"\" ) if self . status == StepStatus . RESULTS_READY : c = \"green\" elif self . status == StepStatus . INPUTS_READY : c = \"yellow\" else : c = \"red\" all . append ( f \"[b]Status[/b]: [b i { c } ] { self . status . name } [/b i { c } ]\" ) all . append ( \"\" ) all . append ( \"[b]Inputs / Outputs[/b]\" ) r_gro = [] inp_table = Table ( show_header = True , box = box . SIMPLE ) inp_table . add_column ( \"Field name\" , style = \"i\" ) inp_table . add_column ( \"Type\" ) inp_table . add_column ( \"Description\" ) inp_table . add_column ( \"Required\" ) inp_table . add_column ( \"Status\" , justify = \"center\" ) inp_table . add_column ( \"Ready\" , justify = \"center\" ) for field_name , value in self . pipeline_inputs . values . items (): req = value . value_schema . is_required () if not req : req_string = \"no\" else : req_string = \"[bold]yes[/bold]\" if value . is_set : status = \"-- set --\" valid = \"[green]yes[/green]\" else : valid = \"[green]yes[/green]\" if value . is_valid else \"[red]no[/red]\" if value . is_valid : status = \"-- not set --\" else : status = \"-- not set --\" inp_table . add_row ( field_name , value . value_schema . type , value . value_schema . doc , req_string , status , valid , ) r_gro . append ( Panel ( inp_table , box = box . ROUNDED , title_align = \"left\" , title = \"Inputs\" ) ) out_table = Table ( show_header = True , box = box . SIMPLE ) out_table . add_column ( \"Field name\" , style = \"i\" ) out_table . add_column ( \"Type\" ) out_table . add_column ( \"Description\" ) out_table . add_column ( \"Required\" ) out_table . add_column ( \"Status\" , justify = \"center\" ) out_table . add_column ( \"Ready\" , justify = \"center\" ) for field_name , value in self . pipeline_outputs . values . items (): req = value . value_schema . is_required () if not req : req_string = \"no\" else : req_string = \"[bold]yes[/bold]\" if value . is_set : status = \"-- set --\" valid = \"[green]yes[/green]\" else : valid = \"[green]yes[/green]\" if value . is_valid else \"[red]no[/red]\" status = \"-- not set --\" out_table . add_row ( field_name , value . value_schema . type , value . value_schema . doc , req_string , status , valid , ) r_gro . append ( Panel ( out_table , box = box . ROUNDED , title_align = \"left\" , title = \"Outputs\" ) ) all . append ( Panel ( RenderGroup ( * r_gro ), box = box . SIMPLE )) rg = [] for nr , stage in enumerate ( self . structure . processing_stages ): render_group = [] for s in self . structure . steps . values (): if s . step . step_id not in stage : continue step_table = create_pipeline_step_table ( self , s ) render_group . append ( step_table ) panel = Panel ( RenderGroup ( * render_group ), box = box . ROUNDED , title = f \"Processing stage: { nr + 1 } \" , title_align = \"left\" , ) rg . append ( panel ) all . append ( \"[b]Steps[/b]\" ) r_panel = Panel ( RenderGroup ( * rg ), box = box . SIMPLE ) all . append ( r_panel ) return RenderGroup ( * all ) pipeline_inputs : PipelineValuesInfo pydantic-field required \u00b6 The current (externally facing) input values of this pipeline. pipeline_outputs : PipelineValuesInfo pydantic-field required \u00b6 The current (externally facing) output values of this pipeline. status : StepStatus pydantic-field required \u00b6 The current overal status of the pipeline. step_inputs : Dict [ str , kiara . pipeline . PipelineValuesInfo ] pydantic-field required \u00b6 The current (internal) input values of each step of this pipeline. step_outputs : Dict [ str , kiara . pipeline . PipelineValuesInfo ] pydantic-field required \u00b6 The current (internal) output values of each step of this pipeline. step_states : Dict [ str , kiara . pipeline . StepStatus ] pydantic-field required \u00b6 The status of each step. structure : PipelineStructureDesc pydantic-field required \u00b6 The structure (interconnections of modules/steps) of the pipeline. PipelineStructureDesc ( BaseModel ) pydantic-model \u00b6 Outlines the internal structure of a Pipeline . Source code in kiara/info/pipelines.py class PipelineStructureDesc ( BaseModel ): \"\"\"Outlines the internal structure of a [Pipeline][kiara.pipeline.pipeline.Pipeline].\"\"\" @classmethod def create_pipeline_structure_desc ( cls , pipeline : typing . Union [ \"Pipeline\" , \"PipelineStructure\" ] ) -> \"PipelineStructureDesc\" : from kiara.pipeline.pipeline import Pipeline from kiara.pipeline.structure import PipelineStructure if isinstance ( pipeline , Pipeline ): structure : PipelineStructure = pipeline . structure elif isinstance ( pipeline , PipelineStructure ): structure = pipeline else : raise TypeError ( f \"Invalid type ' { type ( pipeline ) } ' for pipeline.\" ) steps = {} workflow_inputs : typing . Dict [ str , typing . List [ str ]] = {} workflow_outputs : typing . Dict [ str , str ] = {} for m_id , details in structure . steps_details . items (): step = details [ \"step\" ] input_connections : typing . Dict [ str , typing . List [ str ]] = {} for k , v in details [ \"inputs\" ] . items (): if v . connected_pipeline_input is not None : connected_item = v . connected_pipeline_input input_connections [ k ] = [ generate_step_alias ( PIPELINE_PARENT_MARKER , connected_item ) ] workflow_inputs . setdefault ( f \" { connected_item } \" , []) . append ( v . alias ) elif v . connected_outputs is not None : assert len ( v . connected_outputs ) > 0 for co in v . connected_outputs : input_connections . setdefault ( k , []) . append ( co . alias ) else : raise TypeError ( f \"Invalid connection type: { v } \" ) output_connections : typing . Dict [ str , typing . Any ] = {} for k , v in details [ \"outputs\" ] . items (): for connected_item in v . connected_inputs : output_connections . setdefault ( k , []) . append ( generate_step_alias ( connected_item . step_id , connected_item . value_name ) ) if v . pipeline_output : output_connections . setdefault ( k , []) . append ( generate_step_alias ( PIPELINE_PARENT_MARKER , v . pipeline_output ) ) workflow_outputs [ v . pipeline_output ] = v . alias steps [ step . step_id ] = StepDesc ( step = step , processing_stage = details [ \"processing_stage\" ], input_connections = input_connections , output_connections = output_connections , required = step . required , ) return PipelineStructureDesc ( steps = steps , processing_stages = structure . processing_stages , pipeline_input_connections = workflow_inputs , pipeline_output_connections = workflow_outputs , pipeline_inputs = structure . pipeline_inputs , pipeline_outputs = structure . pipeline_outputs , ) class Config : allow_mutation = False extra = Extra . forbid steps : typing . Dict [ str , StepDesc ] = Field ( description = \"The steps contained in this pipeline, with the 'step_id' as key.\" ) processing_stages : typing . List [ typing . List [ str ]] = Field ( description = \"The order in which this pipeline has to be processed (basically the dependencies of each step on other steps, if any).\" ) pipeline_input_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"The connections of this pipelines input fields. One input field can be connected to one or several step input fields.\" ) pipeline_output_connections : typing . Dict [ str , str ] = Field ( description = \"The connections of this pipelines output fields. Each pipeline output is connected to exactly one step output field.\" ) pipeline_inputs : typing . Dict [ str , PipelineInputRef ] = Field ( description = \"The pipeline inputs.\" ) pipeline_outputs : typing . Dict [ str , PipelineOutputRef ] = Field ( description = \"The pipeline outputs.\" ) @property def steps_info ( self ) -> StepsInfo : return StepsInfo ( processing_stages = self . processing_stages , steps = self . steps , ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield \"[b]Pipeline structure[/b] \\n \" yield \"[b]Inputs / Outputs[/b]\" data_panel : typing . List [ typing . Any ] = [] inp_table = Table ( show_header = True , box = box . SIMPLE , show_lines = True ) inp_table . add_column ( \"Name\" , style = \"i\" ) inp_table . add_column ( \"Type\" ) inp_table . add_column ( \"Description\" ) inp_table . add_column ( \"Required\" , justify = \"center\" ) inp_table . add_column ( \"Default\" , justify = \"center\" ) for inp , details in self . pipeline_inputs . items (): req = details . value_schema . is_required () if not req : req_str = \"no\" else : d = details . value_schema . default if d in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ]: req_str = \"[b]yes[/b]\" else : req_str = \"no\" default = details . value_schema . default if default in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ]: default = \"-- no default --\" else : default = str ( default ) inp_table . add_row ( inp , details . value_schema . type , details . value_schema . doc , req_str , default , ) p_inp = Panel ( inp_table , box = box . ROUNDED , title = \"Input fields\" , title_align = \"left\" ) data_panel . append ( p_inp ) # yield \"[b]Pipeline outputs[/b]\" out_table = Table ( show_header = True , box = box . SIMPLE , show_lines = True ) out_table . add_column ( \"Name\" , style = \"i\" ) out_table . add_column ( \"Type\" ) out_table . add_column ( \"Description\" ) for inp , details_o in self . pipeline_outputs . items (): out_table . add_row ( inp , details_o . value_schema . type , details_o . value_schema . doc , ) outp = Panel ( out_table , box = box . ROUNDED , title = \"Output fields\" , title_align = \"left\" ) data_panel . append ( outp ) yield Panel ( RenderGroup ( * data_panel ), box = box . SIMPLE ) step_color_map = {} for i , s in enumerate ( self . steps . values ()): step_color_map [ s . step . step_id ] = COLOR_LIST [ i % len ( COLOR_LIST )] rg = [] for nr , stage in enumerate ( self . processing_stages ): render_group = [] for s in self . steps . values (): if s . step . step_id not in stage : continue step_table = create_step_table ( s , step_color_map ) render_group . append ( step_table ) panel = Panel ( RenderGroup ( * render_group ), box = box . ROUNDED , title = f \"Processing stage: { nr + 1 } \" , title_align = \"left\" , ) rg . append ( panel ) yield \"[b]Steps[/b]\" r_panel = Panel ( RenderGroup ( * rg ), box = box . SIMPLE ) yield r_panel pipeline_input_connections : Dict [ str , List [ str ]] pydantic-field required \u00b6 The connections of this pipelines input fields. One input field can be connected to one or several step input fields. pipeline_inputs : Dict [ str , kiara . pipeline . values . PipelineInputRef ] pydantic-field required \u00b6 The pipeline inputs. pipeline_output_connections : Dict [ str , str ] pydantic-field required \u00b6 The connections of this pipelines output fields. Each pipeline output is connected to exactly one step output field. pipeline_outputs : Dict [ str , kiara . pipeline . values . PipelineOutputRef ] pydantic-field required \u00b6 The pipeline outputs. processing_stages : List [ List [ str ]] pydantic-field required \u00b6 The order in which this pipeline has to be processed (basically the dependencies of each step on other steps, if any). steps : Dict [ str , kiara . pipeline . config . StepDesc ] pydantic-field required \u00b6 The steps contained in this pipeline, with the 'step_id' as key. StepsInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/pipelines.py class StepsInfo ( KiaraInfoModel ): steps : typing . Dict [ str , StepDesc ] = Field ( description = \"A list of step details.\" ) processing_stages : typing . List [ typing . List [ str ]] = Field ( description = \"The stages in which the steps are processed.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( box = box . SIMPLE , show_lines = True ) table . add_column ( \"Stage\" , justify = \"center\" ) table . add_column ( \"Step Id\" ) table . add_column ( \"Module type\" , style = \"i\" ) table . add_column ( \"Description\" ) for nr , stage in enumerate ( self . processing_stages ): for i , step_id in enumerate ( stage ): step : StepDesc = self . steps [ step_id ] if step . required : title = f \"[b] { step_id } [/b]\" else : title = f \"[b] { step_id } [/b] [i](optional)[/i]\" if hasattr ( step . step . module , \"instance_doc\" ): doc = step . step . module . module_instance_doc else : doc = step . step . module . get_type_metadata () . documentation . full_doc row : typing . List [ typing . Any ] = [] if i == 0 : row . append ( str ( nr )) else : row . append ( \"\" ) row . append ( title ) # TODO; generate the right link here module_link = ( step . step . module . get_type_metadata () . context . references . get ( \"sources\" , None ) ) if module_link : module_str = f \"[link= { module_link } ] { step . step . module_type } [/link]\" else : module_str = step . step . module_type row . append ( module_str ) if doc and doc != \"-- n/a --\" : m = Markdown ( doc + \" \\n\\n --- \\n \" ) row . append ( m ) else : row . append ( \"\" ) table . add_row ( * row ) return table # yield Panel(table, title_align=\"left\", title=\"Processing stages\") # def __rich_console_old__( # self, console: Console, options: ConsoleOptions # ) -> RenderResult: # # explanation = {} # # for nr, stage in enumerate(self.processing_stages): # # stage_details = {} # for step_id in stage: # step: StepDesc = self.steps[step_id] # if step.required: # title = step_id # else: # title = f\"{step_id} (optional)\" # stage_details[title] = step.step.module.get_type_metadata().model_doc() # # explanation[nr + 1] = stage_details # # lines = [] # for stage_nr, stage_steps in explanation.items(): # lines.append(f\"[bold]Processing stage {stage_nr}[/bold]:\") # lines.append(\"\") # for step_id, desc in stage_steps.items(): # if desc == DEFAULT_NO_DESC_VALUE: # lines.append(f\" - {step_id}\") # else: # lines.append(f\" - {step_id}: [i]{desc}[/i]\") # lines.append(\"\") # # padding = (1, 2, 0, 2) # yield Panel( # \"\\n\".join(lines), # box=box.ROUNDED, # title_align=\"left\", # title=f\"Stages for pipeline: [b]{self.pipeline_id}[/b]\", # padding=padding, # ) processing_stages : List [ List [ str ]] pydantic-field required \u00b6 The stages in which the steps are processed. steps : Dict [ str , kiara . pipeline . config . StepDesc ] pydantic-field required \u00b6 A list of step details. types \u00b6 ValueTypeInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/types.py class ValueTypeInfo ( KiaraInfoModel ): @classmethod def from_type_class ( cls , type_cls : typing . Type [ \"ValueType\" ], kiara : typing . Optional [ Kiara ] = None ): type_attrs = cls . extract_type_attributes ( type_cls = type_cls , kiara = kiara ) return cls ( ** type_attrs ) @classmethod def extract_type_attributes ( self , type_cls : typing . Type [ \"ValueType\" ], kiara : typing . Optional [ Kiara ] = None ) -> typing . Dict [ str , typing . Any ]: if kiara is None : kiara = Kiara . instance () origin_md = OriginMetadataModel . from_class ( type_cls ) doc = DocumentationMetadataModel . from_class_doc ( type_cls ) python_class = PythonClassMetadata . from_class ( type_cls ) properties_md = ContextMetadataModel . from_class ( type_cls ) value_type : str = type_cls . _value_type_name # type: ignore config = ValueTypeConfigMetadata . from_config_class ( type_cls . type_config_cls ()) metadata_keys = kiara . metadata_mgmt . get_metadata_keys_for_type ( value_type = value_type ) metadata_models : typing . Dict [ str , typing . Type [ MetadataModel ]] = {} for metadata_key in metadata_keys : schema = kiara . metadata_mgmt . all_schemas . get ( metadata_key ) if schema is not None : metadata_models [ metadata_key ] = MetadataModelMetadata . from_model_class ( schema ) return { \"type_name\" : type_cls . _value_type_name , # type: ignore \"documentation\" : doc , \"origin\" : origin_md , \"context\" : properties_md , \"python_class\" : python_class , \"config\" : config , \"metadata_types\" : metadata_models , } type_name : str = Field ( description = \"The name under which the type is registered.\" ) documentation : DocumentationMetadataModel = Field ( description = \"The documentation for this value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about authorship for the value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type (description, tags, labels, references, ...).\" ) python_class : PythonClassMetadata = Field ( description = \"Information about the Python class for this value type.\" ) config : ValueTypeConfigMetadata = Field ( description = \"Details on how this value type can be configured.\" ) metadata_types : typing . Dict [ str , MetadataModelMetadata ] = Field ( description = \"The available metadata types for this value type.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_config_schema = config . get ( \"include_config_schema\" , True ) include_doc = config . get ( \"include_doc\" , True ) include_full_metadata = config . get ( \"include_full_metadata\" , True ) table = Table ( box = box . SIMPLE , show_header = False , padding = ( 0 , 0 , 0 , 0 )) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) if include_config_schema : if config : config_cls = self . config . python_class . get_class () table . add_row ( \"Type config\" , create_table_from_base_model ( config_cls )) else : table . add_row ( \"Type config\" , \"-- no config --\" ) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) if include_full_metadata and self . metadata_types : md_table = Table ( show_header = False , box = box . SIMPLE ) for key , md in self . metadata_types . items (): fields_table = md . create_fields_table ( show_header = False , show_required = False ) md_table . add_row ( f \"[i] { key } [/i]\" , fields_table ) table . add_row ( \"Type metadata\" , md_table ) return table config : ValueTypeConfigMetadata pydantic-field required \u00b6 Details on how this value type can be configured. context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type (description, tags, labels, references, ...). documentation : DocumentationMetadataModel pydantic-field required \u00b6 The documentation for this value type. metadata_types : Dict [ str , kiara . metadata . core_models . MetadataModelMetadata ] pydantic-field required \u00b6 The available metadata types for this value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about authorship for the value type. python_class : PythonClassMetadata pydantic-field required \u00b6 Information about the Python class for this value type. type_name : str pydantic-field required \u00b6 The name under which the type is registered. interfaces special \u00b6 Implementation of interfaces for Kiara . get_console () \u00b6 Get a global Console instance. Returns: Type Description Console A console instance. Source code in kiara/interfaces/__init__.py def get_console () -> Console : \"\"\"Get a global Console instance. Returns: Console: A console instance. \"\"\" global _console if _console is None or True : console_width = os . environ . get ( \"CONSOLE_WIDTH\" , None ) width = None if console_width : try : width = int ( console_width ) except Exception : pass _console = Console ( width = width ) return _console cli special \u00b6 A command-line interface for Kiara . data special \u00b6 commands \u00b6 Data-related sub-commands for the cli. explain \u00b6 The 'run' subcommand for the cli. KiaraEntityMatches ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/interfaces/cli/explain.py class KiaraEntityMatches ( KiaraInfoModel ): @classmethod def search ( self , search_term : str , search_ids : bool = True , search_aliases : bool = True , search_desc : bool = True , search_full_doc : bool = False , kiara : typing . Optional [ Kiara ] = None , ): if kiara is None : kiara = Kiara . instance () module_types = find_module_types ( item = search_term , search_desc = search_desc , search_full_doc = search_full_doc , kiara = kiara , ) operation_types = find_operation_types ( item = search_term , kiara = kiara ) operations = find_operations ( item = search_term , search_desc = search_desc , search_full_doc = search_full_doc , kiara = kiara , ) values = find_values ( item = search_term , search_ids = search_ids , search_aliases = search_aliases , kiara = kiara , ) return KiaraEntityMatches ( module_types = module_types , operation_types = operation_types , operations = operations , values = values , kiara = kiara , ) def __init__ ( self , module_types : typing . Optional [ typing . Dict [ str , typing . Type [ KiaraModule ]] ] = None , operation_types : typing . Optional [ typing . Dict [ str , OperationsMetadata ]] = None , operations : typing . Optional [ typing . Dict [ str , Operation ]] = None , values : typing . Optional [ typing . Dict [ str , Value ]] = None , kiara : Kiara = None , ): if kiara is None : kiara = Kiara . instance () init_dict : typing . Dict [ str , typing . Any ] = {} if module_types : init_dict [ \"module_types\" ] = module_types if operation_types : init_dict [ \"operation_types\" ] = operation_types if operations : init_dict [ \"operations\" ] = operations if values : init_dict [ \"values\" ] = values super () . __init__ ( ** init_dict ) self . _kiara = kiara _kiara : Kiara = PrivateAttr () module_types : typing . Dict [ str , KiaraModuleTypeMetadata ] = Field ( description = \"Matching module types.\" , default_factory = dict ) operation_types : typing . Dict [ str , OperationsMetadata ] = Field ( description = \"Matching operation types.\" , default_factory = dict ) operations : typing . Dict [ str , Operation ] = Field ( description = \"Matching operations.\" , default_factory = dict ) values : typing . Dict [ str , ValueInfo ] = Field ( description = \"Matching values.\" , default_factory = dict ) @property def no_module_types ( self ) -> int : return len ( self . module_types ) @property def no_operation_types ( self ) -> int : return len ( self . operation_types ) @property def no_operations ( self ) -> int : return len ( self . operations ) @property def no_values ( self ) -> int : return len ( self . values ) @property def no_results ( self ) -> int : return ( self . no_module_types + self . no_operation_types + self . no_operations + self . no_values ) def get_single_result ( self , ) -> typing . Optional [ typing . Tuple [ typing . Type , str , typing . Any ]]: if self . no_results != 1 : return None if self . module_types : match_key = next ( iter ( self . module_types . keys ())) match_value : typing . Any = self . module_types [ match_key ] return ( KiaraModuleTypeMetadata , match_key , match_value ) if self . operation_types : match_key = next ( iter ( self . operation_types . keys ())) match_value = self . operation_types [ match_key ] return ( OperationsMetadata , match_key , match_value ) if self . operations : match_key = next ( iter ( self . operations . keys ())) match_value = self . operations [ match_key ] return ( Operation , match_key , match_value ) if self . values : match_key = next ( iter ( self . values . keys ())) match_value = self . values [ match_key ] return ( ValueInfo , match_key , match_value ) raise Exception ( \"No match found. This is a bug.\" ) def merge ( self , other : \"KiaraEntityMatches\" ) -> \"KiaraEntityMatches\" : module_types = copy . copy ( self . module_types ) module_types . update ( other . module_types ) operation_types = copy . copy ( self . operation_types ) operation_types . update ( other . operation_types ) operations = copy . copy ( self . operations ) operations . update ( other . operations ) values = copy . copy ( self . values ) values . update ( other . values ) return KiaraEntityMatches ( module_types = module_types , operation_types = operation_types , operations = operations , values = values , kiara = self . _kiara , ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" ) table . add_column ( \"Description\" ) empty_line = False if self . module_types : table . add_row () table . add_row ( \"[b]Module types[/b]\" , \"\" ) table . add_row () for module_type , module_type_info in self . module_types . items (): table . add_row ( f \" [i] { module_type } [/i]\" , module_type_info . documentation . description , ) empty_line = True if self . operation_types : if empty_line : table . add_row () table . add_row ( \"[b]Operation types[/b]\" , \"\" ) table . add_row () for op_type , op_type_info in self . operation_types . items (): table . add_row ( f \" [i] { op_type } [/i]\" , op_type_info . documentation . description ) empty_line = True if self . operations : if empty_line : table . add_row () table . add_row ( \"[b]Operations[/b]\" , \"\" ) table . add_row () for op_id , op_info in self . operations . items (): table . add_row ( f \" [i] { op_id } [/i]\" , op_info . doc . description ) empty_line = True if self . values : if empty_line : table . add_row () table . add_row ( \"[b]Values[/b]\" , \"\" ) table . add_row () for value_id , value in self . values . items (): table . add_row ( f \" [i] { value_id } [/i]\" , value . value_schema . type ) return table module_types : Dict [ str , kiara . metadata . module_models . KiaraModuleTypeMetadata ] pydantic-field \u00b6 Matching module types. operation_types : Dict [ str , kiara . metadata . operation_models . OperationsMetadata ] pydantic-field \u00b6 Matching operation types. operations : Dict [ str , kiara . operations . Operation ] pydantic-field \u00b6 Matching operations. values : Dict [ str , kiara . data . values . ValueInfo ] pydantic-field \u00b6 Matching values. metadata special \u00b6 Metadata-related sub-commands for the cli. module special \u00b6 commands \u00b6 Module related subcommands for the cli. pipeline special \u00b6 commands \u00b6 Pipeline-related subcommands for the cli. run \u00b6 The 'run' subcommand for the cli. type special \u00b6 commands \u00b6 Type-related subcommands for the cli. utils \u00b6 Shared utilties to be used by cli sub-commands. python_api special \u00b6 A Python API for creating workflow sessions and dynamic pipelines in kiara . StepInfo ( JupyterMixin , BaseModel ) pydantic-model \u00b6 Source code in kiara/interfaces/python_api/__init__.py class StepInfo ( JupyterMixin , BaseModel ): step_id : str = Field ( description = \"The step id.\" ) module_metadata : KiaraModuleInstanceMetadata = Field ( description = \"The module metadata.\" ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : doc = self . module_metadata . type_metadata . documentation . create_renderable () metadata = self . module_metadata . create_renderable ( include_doc = False ) panel = Panel ( RenderGroup ( \"\" , Panel ( doc , box = box . SIMPLE ), \"\" , metadata ), title = f \"Step info: [b] { self . step_id } [/b] (type: [i] { self . module_metadata . type_metadata . type_id } [/i])\" , title_align = \"left\" , ) yield panel module_metadata : KiaraModuleInstanceMetadata pydantic-field required \u00b6 The module metadata. step_id : str pydantic-field required \u00b6 The step id. controller \u00b6 ApiController ( PipelineController ) \u00b6 A pipeline controller for pipelines generated by the kiara Python API. Source code in kiara/interfaces/python_api/controller.py class ApiController ( PipelineController ): \"\"\"A pipeline controller for pipelines generated by the *kiara* Python API.\"\"\" def __init__ ( self , pipeline : typing . Optional [ Pipeline ] = None , processor : typing . Optional [ ModuleProcessor ] = None , ): self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor ) def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : # from kiara.utils.output import rich_print # rich_print(self.pipeline.get_current_state()) raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False def step_inputs_changed ( self , event : StepInputEvent ): for step_id in event . updated_step_inputs . keys (): # outputs = self.get_step_outputs(step_id) raise NotImplementedError () # outputs.invalidate() def pipeline_inputs_changed ( self , event : PipelineInputEvent ): if self . _is_running : raise NotImplementedError () def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False def get_pipeline_input ( self , step_id : str , field_name : str ) -> Value : self . ensure_step ( step_id ) outputs = self . get_step_inputs ( step_id ) return outputs . get_value_obj ( field_name = field_name ) def get_pipeline_output ( self , step_id : str , field_name : str ) -> Value : self . ensure_step ( step_id ) outputs = self . get_step_outputs ( step_id ) return outputs . get_value_obj ( field_name = field_name ) def _process ( self , step_id : str ) -> typing . Union [ None , bool , str ]: \"\"\"Process the specified step. Returns: 'None', if no processing was needed, but the step output is valid, 'False' if it's not possible to process the step, or a string which represents the job id of the processing \"\"\" if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): return None else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) return job_id else : return None except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False def ensure_step ( self , step_id : str ) -> bool : \"\"\"Ensure a step has valid outputs. Returns: 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs \"\"\" if step_id not in self . pipeline . step_ids : raise Exception ( f \"No step with id: { step_id } \" ) outputs = self . get_step_outputs ( step_id = step_id ) if outputs . items_are_valid (): return True if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids : typing . List [ str ] = [] finished = False if step_id in stage : finished = True job_id = self . _process ( step_id = step_id ) if job_id is None : return True if not job_id : return False job_ids . append ( job_id ) # type: ignore else : for s_id in stage : job_id = self . _process ( step_id = s_id ) if job_id is None : continue elif job_id is False : return False job_ids . append ( job_id ) # type: ignore self . _processor . wait_for ( * job_ids ) if finished : break finally : self . _is_running = False return True ensure_step ( self , step_id ) \u00b6 Ensure a step has valid outputs. Returns: Type Description bool 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs Source code in kiara/interfaces/python_api/controller.py def ensure_step ( self , step_id : str ) -> bool : \"\"\"Ensure a step has valid outputs. Returns: 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs \"\"\" if step_id not in self . pipeline . step_ids : raise Exception ( f \"No step with id: { step_id } \" ) outputs = self . get_step_outputs ( step_id = step_id ) if outputs . items_are_valid (): return True if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids : typing . List [ str ] = [] finished = False if step_id in stage : finished = True job_id = self . _process ( step_id = step_id ) if job_id is None : return True if not job_id : return False job_ids . append ( job_id ) # type: ignore else : for s_id in stage : job_id = self . _process ( step_id = s_id ) if job_id is None : continue elif job_id is False : return False job_ids . append ( job_id ) # type: ignore self . _processor . wait_for ( * job_ids ) if finished : break finally : self . _is_running = False return True pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/interfaces/python_api/controller.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): if self . _is_running : raise NotImplementedError () pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/interfaces/python_api/controller.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/interfaces/python_api/controller.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : # from kiara.utils.output import rich_print # rich_print(self.pipeline.get_current_state()) raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/interfaces/python_api/controller.py def step_inputs_changed ( self , event : StepInputEvent ): for step_id in event . updated_step_inputs . keys (): # outputs = self.get_step_outputs(step_id) raise NotImplementedError () # outputs.invalidate() kiara \u00b6 Main module. Kiara \u00b6 The core context of a kiara session. The Kiara object holds all information related to the current environment the user does works in. This includes: available modules, operations & pipelines available value types available metadata schemas available data items available controller and processor types misc. configuration options It's possible to use kiara without ever manually touching the 'Kiara' class, by default all relevant classes and functions will use a default instance of this class (available via the Kiara.instance() method. The Kiara class is highly dependent on the Python environment it lives in, because it auto-discovers available sub-classes of its building blocks (modules, value types, etc.). So, you can't assume that, for example, a pipeline you create will work the same way (or at all) in a different environment. kiara will always be able to tell you all the details of this environment, though, and it will attach those details to things like data, so there is always a record of how something was created, and in which environment. Source code in kiara/kiara.py class Kiara ( object ): \"\"\"The core context of a kiara session. The `Kiara` object holds all information related to the current environment the user does works in. This includes: - available modules, operations & pipelines - available value types - available metadata schemas - available data items - available controller and processor types - misc. configuration options It's possible to use *kiara* without ever manually touching the 'Kiara' class, by default all relevant classes and functions will use a default instance of this class (available via the `Kiara.instance()` method. The Kiara class is highly dependent on the Python environment it lives in, because it auto-discovers available sub-classes of its building blocks (modules, value types, etc.). So, you can't assume that, for example, a pipeline you create will work the same way (or at all) in a different environment. *kiara* will always be able to tell you all the details of this environment, though, and it will attach those details to things like data, so there is always a record of how something was created, and in which environment. \"\"\" _instance = None @classmethod def instance ( cls ): \"\"\"The default *kiara* context. In most cases, it's recommended you create and manage your own, though.\"\"\" if cls . _instance is None : cls . _instance = Kiara () return cls . _instance def __init__ ( self , config : typing . Optional [ KiaraConfig ] = None ): if not config : config = KiaraConfig () self . _id : str = str ( uuid . uuid4 ()) self . _config : KiaraConfig = config # self._zmq_context: Context = Context.instance() self . _operation_mgmt = OperationMgmt ( kiara = self ) self . _metadata_mgmt = MetadataMgmt ( kiara = self ) self . _data_store = LocalDataStore ( kiara = self , base_path = config . data_store ) # self.start_zmq_device() # self.start_log_thread() self . _default_processor : ModuleProcessor = ModuleProcessor . from_config ( config . default_processor , kiara = self ) self . _type_mgmt_obj : TypeMgmt = TypeMgmt ( self ) self . _data_registry : InMemoryDataRegistry = InMemoryDataRegistry ( self ) self . _module_mgr : MergedModuleManager = MergedModuleManager ( config . module_managers , extra_pipeline_folders = self . _config . extra_pipeline_folders , ignore_errors = self . _config . ignore_errors , ) self . _template_mgmt : TemplateRenderingMgmt = TemplateRenderingMgmt . create ( kiara = self ) @property def config ( self ) -> KiaraConfig : \"\"\"The configuration of this *kiara* environment.\"\"\" return self . _config @property def default_processor ( self ) -> \"ModuleProcessor\" : \"\"\"The default module processor that will be used in this environment, unless otherwise specified.\"\"\" return self . _default_processor def start_zmq_device ( self ): pd = ThreadDevice ( zmq . QUEUE , zmq . SUB , zmq . PUB ) pd . bind_in ( \"inproc://kiara_in\" ) pd . bind_out ( \"inproc://kiara_out\" ) pd . setsockopt_in ( zmq . SUBSCRIBE , b \"\" ) pd . setsockopt_in ( zmq . IDENTITY , b \"SUB\" ) pd . setsockopt_out ( zmq . IDENTITY , b \"PUB\" ) pd . start () def start_log_thread ( self ): def log_messages (): socket = self . _zmq_context . socket ( zmq . SUB ) socket . setsockopt_string ( zmq . SUBSCRIBE , \"\" ) socket . connect ( \"inproc://kiara_out\" ) debug = is_debug () while True : message = socket . recv () topic , details = message . decode () . split ( \" \" , maxsplit = 1 ) try : job = Job . parse_raw ( details ) if debug : print ( f \" { topic } : { job . pipeline_name } . { job . step_id } \" ) else : log . debug ( f \" { topic } : { job . pipeline_name } . { job . step_id } \" ) except Exception as e : if debug : import traceback traceback . print_exception () else : log . debug ( e ) t = Thread ( target = log_messages , daemon = True ) t . start () def explain ( self , item : typing . Any ): explain ( item ) @property def type_mgmt ( self ) -> TypeMgmt : return self . _type_mgmt_obj @property def data_store ( self ) -> LocalDataStore : return self . _data_store @property def module_mgmt ( self ) -> MergedModuleManager : return self . _module_mgr @property def metadata_mgmt ( self ) -> MetadataMgmt : return self . _metadata_mgmt @property def value_types ( self ) -> typing . Mapping [ str , typing . Type [ ValueType ]]: return self . type_mgmt . value_types @property def value_type_names ( self ) -> typing . List [ str ]: return self . type_mgmt . value_type_names def determine_type ( self , data : typing . Any ) -> typing . Optional [ ValueType ]: raise NotImplementedError () # return self.type_mgmt.determine_type(data) def get_value_type_cls ( self , type_name : str ) -> typing . Type [ ValueType ]: return self . type_mgmt . get_value_type_cls ( type_name = type_name ) def get_value ( self , value_id : str ) -> Value : if not isinstance ( value_id , str ): raise TypeError ( f \"Invalid type ' { type ( value_id ) } ' for value id, must be a string.\" ) if value_id . startswith ( \"value:\" ): value_id = value_id [ 6 :] if not value_id : raise Exception ( \"No value id provided.\" ) try : value = self . data_registry . get_value_obj ( value_id ) if value is None : raise Exception ( f \"No value with id: { value_id } \" ) return value except Exception : pass try : value = self . data_store . get_value_obj ( value_id ) if value is None : raise Exception ( f \"No value with id: { value_id } \" ) return value except Exception : pass raise Exception ( f \"No value registered for id: { value_id } \" ) def add_module_manager ( self , module_manager : ModuleManager ): self . _module_mgr . add_module_manager ( module_manager ) self . _type_mgmt_obj . invalidate_types () @property def data_registry ( self ) -> DataRegistry : return self . _data_registry @property def operation_mgmt ( self ) -> OperationMgmt : return self . _operation_mgmt @property def template_mgmt ( self ) -> TemplateRenderingMgmt : return self . _template_mgmt def get_module_class ( self , module_type : str ) -> typing . Type [ \"KiaraModule\" ]: return self . _module_mgr . get_module_class ( module_type = module_type ) # def get_module_info(self, module_type: str) -> \"ModuleInfo\": # # if module_type not in self.available_module_types: # raise ValueError(f\"Module type '{module_type}' not available.\") # # if module_type in self.available_pipeline_module_types: # from kiara.pipeline.module import PipelineModuleInfo # # info = PipelineModuleInfo(module_type=module_type, _kiara=self) # type: ignore # return info # else: # from kiara.module import ModuleInfo # # info = ModuleInfo.from_module_cls(module_cls=module_type) # return info @property def available_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available modules\"\"\" return self . _module_mgr . available_module_types @property def available_non_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return self . _module_mgr . available_non_pipeline_module_types @property def available_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return self . _module_mgr . available_pipeline_module_types @property def available_operation_ids ( self ) -> typing . List [ str ]: return self . _operation_mgmt . operation_ids def is_pipeline_module ( self , module_type : str ): return self . _module_mgr . is_pipeline_module ( module_type = module_type ) def register_pipeline_description ( self , data : typing . Union [ Path , str , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , raise_exception : bool = False , ) -> typing . Optional [ str ]: return self . _module_mgr . register_pipeline_description ( data = data , module_type_name = module_type_name , namespace = namespace , raise_exception = raise_exception , ) def create_module ( self , module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Arguments: module_type: the module type- or operation-id module_config: the module instance configuration (must be empty in case of the module_type being an operation id id: the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline parent_id: a reference to the pipeline that contains this module (if applicable) Returns: The instantiated module object. \"\"\" if module_type == \"pipeline\" : from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = module_config , kiara = self ) return module elif ( module_type not in self . available_module_types and module_type in self . operation_mgmt . operation_ids ): op = self . operation_mgmt . get_operation ( module_type ) assert op is not None module_type = op . module_type op_config = op . module_config if not module_config : _module_config : typing . Optional [ typing . Mapping [ str , typing . Any ] ] = op_config else : _module_config = dict ( op_config ) _module_config . update ( module_config ) elif ( module_type not in self . available_module_types and isinstance ( module_type , str ) and os . path . isfile ( os . path . realpath ( os . path . expanduser ( module_type ))) ): if module_config : raise Exception ( \"Creating pipeline module from file with extra module_config not supported (yet).\" ) path = os . path . expanduser ( module_type ) pipeline_config_data = get_data_from_file ( path ) pipeline_config = PipelineConfig ( ** pipeline_config_data ) from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = pipeline_config , kiara = self ) return module else : _module_config = module_config return self . _module_mgr . create_module ( kiara = self , id = id , module_type = module_type , module_config = _module_config , parent_id = parent_id , ) def get_module_doc ( self , module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ): m = self . create_module ( module_type = module_type , module_config = module_config ) return m . module_instance_doc def get_operation ( self , operation_id : str ) -> Operation : op = self . operation_mgmt . profiles . get ( operation_id , None ) if op is None : raise Exception ( f \"No operation with id ' { operation_id } ' available.\" ) return op def run ( self , module_type : typing . Union [ str , Operation ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , inputs : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , output_name : typing . Optional [ str ] = None , resolve_result : bool = False , ) -> typing . Union [ ValueSet , Value , typing . Any ]: if isinstance ( module_type , str ): if module_type in self . available_module_types : module = self . create_module ( module_type = module_type , module_config = module_config ) elif module_type in self . operation_mgmt . profiles . keys (): if module_config : raise NotImplementedError () op = self . operation_mgmt . profiles [ module_type ] module = op . module elif os . path . isfile ( os . path . realpath ( os . path . expanduser ( module_type ))): path = os . path . expanduser ( module_type ) pipeline_config_data = get_data_from_file ( path ) # pipeline_config = PipelineConfig(**pipeline_config_data) module = self . create_module ( \"pipeline\" , module_config = pipeline_config_data ) else : raise Exception ( f \"Can't run operation: invalid module type ' { module_type } '\" ) elif isinstance ( module_type , Operation ): if module_config : raise NotImplementedError () module = module_type . module else : raise Exception ( f \"Invalid class for module_type: { type ( module_type ) } \" ) return self . run_module ( module = module , inputs = inputs , output_name = output_name , resolve_result = resolve_result , ) def run_module ( self , module : \"KiaraModule\" , inputs : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , output_name : typing . Optional [ str ] = None , resolve_result : bool = False , ): if inputs is None : inputs = {} result = module . run ( ** inputs ) if output_name is not None : v = result . get_value_obj ( output_name ) if resolve_result : return v . get_value_data () else : return v else : if resolve_result : return result . get_all_value_data () else : return result def create_pipeline ( self , config : typing . Union [ PipelineConfig , typing . Mapping [ str , typing . Any ], str ], controller : typing . Optional [ PipelineController ] = None , ) -> Pipeline : if isinstance ( config , typing . Mapping ): pipeline_config : PipelineConfig = PipelineConfig ( ** config ) elif isinstance ( config , str ): if config == \"pipeline\" : raise Exception ( \"Can't create pipeline from 'pipeline' module type without further configuration.\" ) # TODO: if already a pipeline, don't wrap # if config in self.available_pipeline_module_types: # pass if config in self . available_module_types : config_data = { \"steps\" : [ { \"module_type\" : config , \"step_id\" : create_valid_identifier ( config ), } ] } pipeline_config = PipelineConfig ( ** config_data ) elif os . path . isfile ( os . path . realpath ( os . path . expanduser ( config ))): path = os . path . expanduser ( config ) pipeline_config_data = get_data_from_file ( path ) pipeline_config = PipelineConfig ( ** pipeline_config_data ) else : raise Exception ( f \"Can't create pipeline config from string ' { config } '. Value must be path to a file, or one of: { ', ' . join ( self . available_module_types ) } \" ) elif isinstance ( config , PipelineConfig ): pipeline_config = config else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for pipeline configuration.\" ) pipeline = pipeline_config . create_pipeline ( controller = controller , kiara = self ) return pipeline def create_workflow_from_operation_config ( self , config : \"ModuleConfig\" , workflow_id : typing . Optional [ str ] = None , controller : typing . Optional [ PipelineController ] = None , ): if not workflow_id : workflow_id = get_auto_workflow_alias ( config . module_type , use_incremental_ids = True ) workflow = KiaraWorkflow ( workflow_id = workflow_id , config = config , controller = controller , kiara = self , ) return workflow def create_workflow ( self , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], workflow_id : typing . Optional [ str ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , controller : typing . Optional [ PipelineController ] = None , ) -> KiaraWorkflow : _config = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = self ) return self . create_workflow_from_operation_config ( config = _config , workflow_id = workflow_id , controller = controller ) def pretty_print ( self , value : Value , target_type : str = \"renderables\" , print_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ) -> typing . Any : pretty_print_ops : PrettyPrintOperationType = self . operation_mgmt . get_operations ( \"pretty_print\" ) # type: ignore return pretty_print_ops . pretty_print ( value = value , target_type = target_type , print_config = print_config ) available_module_types : List [ str ] property readonly \u00b6 Return the names of all available modules available_non_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. available_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. config : KiaraConfig property readonly \u00b6 The configuration of this kiara environment. default_processor : ModuleProcessor property readonly \u00b6 The default module processor that will be used in this environment, unless otherwise specified. create_module ( self , module_type , module_config = None , id = None , parent_id = None ) \u00b6 Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Parameters: Name Type Description Default module_type str the module type- or operation-id required module_config Optional[Mapping[str, Any]] the module instance configuration (must be empty in case of the module_type being an operation id None id Optional[str] the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline None parent_id Optional[str] a reference to the pipeline that contains this module (if applicable) None Returns: Type Description KiaraModule The instantiated module object. Source code in kiara/kiara.py def create_module ( self , module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Arguments: module_type: the module type- or operation-id module_config: the module instance configuration (must be empty in case of the module_type being an operation id id: the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline parent_id: a reference to the pipeline that contains this module (if applicable) Returns: The instantiated module object. \"\"\" if module_type == \"pipeline\" : from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = module_config , kiara = self ) return module elif ( module_type not in self . available_module_types and module_type in self . operation_mgmt . operation_ids ): op = self . operation_mgmt . get_operation ( module_type ) assert op is not None module_type = op . module_type op_config = op . module_config if not module_config : _module_config : typing . Optional [ typing . Mapping [ str , typing . Any ] ] = op_config else : _module_config = dict ( op_config ) _module_config . update ( module_config ) elif ( module_type not in self . available_module_types and isinstance ( module_type , str ) and os . path . isfile ( os . path . realpath ( os . path . expanduser ( module_type ))) ): if module_config : raise Exception ( \"Creating pipeline module from file with extra module_config not supported (yet).\" ) path = os . path . expanduser ( module_type ) pipeline_config_data = get_data_from_file ( path ) pipeline_config = PipelineConfig ( ** pipeline_config_data ) from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = pipeline_config , kiara = self ) return module else : _module_config = module_config return self . _module_mgr . create_module ( kiara = self , id = id , module_type = module_type , module_config = _module_config , parent_id = parent_id , ) instance () classmethod \u00b6 The default kiara context. In most cases, it's recommended you create and manage your own, though. Source code in kiara/kiara.py @classmethod def instance ( cls ): \"\"\"The default *kiara* context. In most cases, it's recommended you create and manage your own, though.\"\"\" if cls . _instance is None : cls . _instance = Kiara () return cls . _instance explain ( item ) \u00b6 Pretty print information about an item on the terminal. Source code in kiara/kiara.py def explain ( item : typing . Any ): \"\"\"Pretty print information about an item on the terminal.\"\"\" if isinstance ( item , type ): from kiara.module import KiaraModule if issubclass ( item , KiaraModule ): item = KiaraModuleTypeMetadata . from_module_class ( module_cls = item ) elif isinstance ( item , Value ): item . get_metadata () console = get_console () console . print ( item ) metadata special \u00b6 MetadataModel ( KiaraInfoModel ) pydantic-model \u00b6 Base class for classes that represent value metadata in kiara. Source code in kiara/metadata/__init__.py class MetadataModel ( KiaraInfoModel ): \"\"\"Base class for classes that represent value metadata in kiara.\"\"\" @classmethod def get_type_metadata ( cls ) -> \"MetadataModelMetadata\" : \"\"\"Return all metadata associated with this module type.\"\"\" from kiara.metadata.core_models import MetadataModelMetadata return MetadataModelMetadata . from_model_class ( model_cls = cls ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) for k in self . __fields__ . keys (): if k == \"type_name\" and config . get ( \"omit_type_name\" , False ): continue attr = getattr ( self , k ) v = extract_renderable ( attr ) table . add_row ( k , v ) if \"operations\" in config . keys (): ids = list ( config [ \"operations\" ] . keys ()) table . add_row ( \"operations\" , \" \\n \" . join ( ids )) return table get_type_metadata () classmethod \u00b6 Return all metadata associated with this module type. Source code in kiara/metadata/__init__.py @classmethod def get_type_metadata ( cls ) -> \"MetadataModelMetadata\" : \"\"\"Return all metadata associated with this module type.\"\"\" from kiara.metadata.core_models import MetadataModelMetadata return MetadataModelMetadata . from_model_class ( model_cls = cls ) ValueTypeAndDescription ( BaseModel ) pydantic-model \u00b6 Source code in kiara/metadata/__init__.py class ValueTypeAndDescription ( BaseModel ): description : str = Field ( description = \"The description for the value.\" ) type : str = Field ( description = \"The value type.\" ) value_default : typing . Any = Field ( description = \"Default for the value.\" , default = None ) required : bool = Field ( description = \"Whether this value is required\" ) description : str pydantic-field required \u00b6 The description for the value. required : bool pydantic-field required \u00b6 Whether this value is required type : str pydantic-field required \u00b6 The value type. value_default : Any pydantic-field \u00b6 Default for the value. core_models \u00b6 AuthorModel ( BaseModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class AuthorModel ( BaseModel ): name : str = Field ( description = \"The full name of the author.\" ) email : typing . Optional [ EmailStr ] = Field ( description = \"The email address of the author\" , default = None ) email : EmailStr pydantic-field \u00b6 The email address of the author name : str pydantic-field required \u00b6 The full name of the author. ContextMetadataModel ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class ContextMetadataModel ( MetadataModel ): @classmethod def from_class ( cls , item_cls : typing . Type ): data = get_metadata_for_python_module_or_class ( item_cls ) # type: ignore merged = merge_dicts ( * data ) return cls . parse_obj ( merged ) _metadata_key = \"properties\" references : typing . Dict [ str , LinkModel ] = Field ( description = \"References for the item.\" , default_factory = dict ) tags : typing . Set [ str ] = Field ( description = \"A list of tags for the item.\" , default_factory = set ) labels : typing . Dict [ str , str ] = Field ( description = \"A list of labels for the item.\" , default_factory = list ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . tags : table . add_row ( \"Tags\" , \", \" . join ( self . tags )) if self . labels : labels = [] for k , v in self . labels . items (): labels . append ( f \"[i] { k } [/i]: { v } \" ) table . add_row ( \"Labels\" , \" \\n \" . join ( labels )) if self . references : references = [] for _k , _v in self . references . items (): link = f \"[link= { _v . url } ] { _v . url } [/link]\" references . append ( f \"[i] { _k } [/i]: { link } \" ) table . add_row ( \"References\" , \" \\n \" . join ( references )) return table def add_reference ( self , ref_type : str , url : str , desc : typing . Optional [ str ] = None , force : bool = False , ): if ref_type in self . references . keys () and not force : raise Exception ( f \"Reference of type ' { ref_type } ' already present.\" ) link = LinkModel ( url = url , desc = desc ) self . references [ ref_type ] = link def get_url_for_reference ( self , ref : str ) -> typing . Optional [ str ]: link = self . references . get ( ref , None ) if not link : return None return link . url labels : Dict [ str , str ] pydantic-field \u00b6 A list of labels for the item. references : Dict [ str , kiara . metadata . core_models . LinkModel ] pydantic-field \u00b6 References for the item. tags : Set [ str ] pydantic-field \u00b6 A list of tags for the item. DocumentationMetadataModel ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class DocumentationMetadataModel ( MetadataModel ): _metadata_key = \"documentation\" @classmethod def from_class_doc ( cls , item_cls : typing . Type ): doc = item_cls . __doc__ if not doc : doc = DEFAULT_NO_DESC_VALUE doc = inspect . cleandoc ( doc ) return cls . from_string ( doc ) @classmethod def from_string ( cls , doc : typing . Optional [ str ]): if not doc : doc = DEFAULT_NO_DESC_VALUE if \" \\n \" in doc : desc , doc = doc . split ( \" \\n \" , maxsplit = 1 ) else : desc = doc doc = None if doc : doc = doc . strip () return cls ( description = desc . strip (), doc = doc ) @classmethod def from_dict ( cls , data : typing . Mapping ): doc = data . get ( \"doc\" , None ) desc = data . get ( \"description\" , None ) if desc is None : desc = data . get ( \"desc\" , None ) if not doc and not desc : return cls . from_string ( DEFAULT_NO_DESC_VALUE ) elif doc and not desc : return cls . from_string ( doc ) elif desc and not doc : return cls . from_string ( desc ) else : return DocumentationMetadataModel ( description = desc , doc = doc ) @classmethod def create ( cls , item : typing . Any ): if not item : return cls . from_string ( DEFAULT_NO_DESC_VALUE ) elif isinstance ( item , DocumentationMetadataModel ): return item elif isinstance ( item , typing . Mapping ): return cls . from_dict ( item ) if isinstance ( item , type ): return cls . from_class_doc ( item ) elif isinstance ( item , str ): return cls . from_string ( item ) else : raise TypeError ( f \"Can't create documentation from type ' { type ( item ) } '.\" ) description : str = Field ( description = \"Short description of the item.\" , default = DEFAULT_NO_DESC_VALUE ) doc : typing . Optional [ str ] = Field ( description = \"Detailed documentation of the item (in markdown).\" , default = None ) @property def full_doc ( self ): if self . doc : return f \" { self . description } \\n\\n { self . doc } \" else : return self . description def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return Markdown ( self . full_doc ) description : str pydantic-field \u00b6 Short description of the item. doc : str pydantic-field \u00b6 Detailed documentation of the item (in markdown). LinkModel ( BaseModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class LinkModel ( BaseModel ): url : AnyUrl = Field ( description = \"The url.\" ) desc : typing . Optional [ str ] = Field ( description = \"A short description of the link content.\" , default = DEFAULT_NO_DESC_VALUE , ) desc : str pydantic-field \u00b6 A short description of the link content. url : AnyUrl pydantic-field required \u00b6 The url. MetadataModelMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class MetadataModelMetadata ( MetadataModel ): @classmethod def from_model_class ( cls , model_cls : typing . Type [ MetadataModel ]): origin_md = OriginMetadataModel . from_class ( model_cls ) doc = DocumentationMetadataModel . from_class_doc ( model_cls ) python_class = PythonClassMetadata . from_class ( model_cls ) properties_md = ContextMetadataModel . from_class ( model_cls ) return MetadataModelMetadata ( type_name = model_cls . _metadata_key , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" ) def create_fields_table ( self , show_header : bool = True , show_required : bool = True ) -> Table : type_cls = self . python_class . get_class () fields_table = Table ( show_header = show_header , box = box . SIMPLE ) fields_table . add_column ( \"Field name\" , style = \"i\" ) fields_table . add_column ( \"Type\" ) if show_required : fields_table . add_column ( \"Required\" ) fields_table . add_column ( \"Description\" ) for field_name , details in type_cls . __fields__ . items (): field_type = type_cls . schema ()[ \"properties\" ][ field_name ][ \"type\" ] info = details . field_info . description if show_required : req = \"yes\" if details . required else \"no\" fields_table . add_row ( field_name , field_type , req , info ) else : fields_table . add_row ( field_name , field_type , info ) return fields_table def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_schema = config . get ( \"display_schema\" , False ) include_doc = config . get ( \"include_doc\" , True ) table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) fields_table = self . create_fields_table () table . add_row ( \"Fields\" , fields_table ) if include_schema : json_str = Syntax ( self . python_class . get_class () . schema_json ( indent = 2 ), \"json\" , background_color = \"default\" , ) table . add_row ( \"Json Schema\" , json_str ) return table context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type. documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about the creator of this value type. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this value type. type_name : str pydantic-field required \u00b6 The registered name for this value type. OriginMetadataModel ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class OriginMetadataModel ( MetadataModel ): _metadata_key = \"origin\" @classmethod def from_class ( cls , item_cls : typing . Type ): data = get_metadata_for_python_module_or_class ( item_cls ) # type: ignore merged = merge_dicts ( * data ) return cls . parse_obj ( merged ) authors : typing . List [ AuthorModel ] = Field ( description = \"The authors/creators of this item.\" , default_factory = list ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) authors = [] for author in reversed ( self . authors ): if author . email : authors . append ( f \" { author . name } ( { author . email } )\" ) else : authors . append ( author . name ) table . add_row ( \"Authors\" , \" \\n \" . join ( authors )) return table authors : List [ kiara . metadata . core_models . AuthorModel ] pydantic-field \u00b6 The authors/creators of this item. PythonClassMetadata ( MetadataModel ) pydantic-model \u00b6 Python class and module information. Source code in kiara/metadata/core_models.py class PythonClassMetadata ( MetadataModel ): \"\"\"Python class and module information.\"\"\" _metadata_key : typing . ClassVar [ str ] = \"python_class\" @classmethod def from_class ( cls , item_cls : typing . Type ): conf : typing . Dict [ str , typing . Any ] = { \"class_name\" : item_cls . __name__ , \"module_name\" : item_cls . __module__ , \"full_name\" : f \" { item_cls . __module__ } . { item_cls . __name__ } \" , } return PythonClassMetadata ( ** conf ) class_name : str = Field ( description = \"The name of the Python class.\" ) module_name : str = Field ( description = \"The name of the Python module this class lives in.\" ) full_name : str = Field ( description = \"The full class namespace.\" ) def get_class ( self ) -> typing . Type : m = self . get_module () return getattr ( m , self . class_name ) def get_module ( self ) -> ModuleType : m = importlib . import_module ( self . module_name ) return m class_name : str pydantic-field required \u00b6 The name of the Python class. full_name : str pydantic-field required \u00b6 The full class namespace. module_name : str pydantic-field required \u00b6 The name of the Python module this class lives in. data \u00b6 DeserializeConfig ( ModuleConfig ) pydantic-model \u00b6 Source code in kiara/metadata/data.py class DeserializeConfig ( ModuleConfig ): # value_id: str = Field(description=\"The id of the value.\") serialization_type : str = Field ( description = \"The serialization type.\" ) input : typing . Any = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) output_name : str = Field ( description = \"The name of the output field for the value.\" ) input : Any pydantic-field \u00b6 The inputs to use when running this module. output_name : str pydantic-field required \u00b6 The name of the output field for the value. serialization_type : str pydantic-field required \u00b6 The serialization type. LoadConfig ( ModuleConfig ) pydantic-model \u00b6 Source code in kiara/metadata/data.py class LoadConfig ( ModuleConfig ): value_id : str = Field ( description = \"The id of the value.\" ) base_path_input_name : typing . Optional [ str ] = Field ( description = \"The name of the input that stores the base_path where the value is saved.\" , default = None , ) inputs : typing . Dict [ str , typing . Any ] = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) output_name : str = Field ( description = \"The name of the output field for the value.\" ) base_path_input_name : str pydantic-field \u00b6 The name of the input that stores the base_path where the value is saved. inputs : Dict [ str , Any ] pydantic-field \u00b6 The inputs to use when running this module. output_name : str pydantic-field required \u00b6 The name of the output field for the value. value_id : str pydantic-field required \u00b6 The id of the value. SaveConfig ( ModuleConfig ) pydantic-model \u00b6 Source code in kiara/metadata/data.py class SaveConfig ( ModuleConfig ): inputs : typing . Dict [ str , typing . Any ] = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) load_config_output : str = Field ( description = \"The output name that will contain the load config output value.\" ) inputs : Dict [ str , Any ] pydantic-field \u00b6 The inputs to use when running this module. load_config_output : str pydantic-field required \u00b6 The output name that will contain the load config output value. module_models \u00b6 KiaraModuleConfigMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/module_models.py class KiaraModuleConfigMetadata ( MetadataModel ): @classmethod def from_config_class ( cls , config_cls : typing . Type [ ModuleTypeConfigSchema ], remove_pipeline_config : bool = False , ): flat_models = get_flat_models_from_model ( config_cls ) model_name_map = get_model_name_map ( flat_models ) m_schema , _ , _ = model_process_schema ( config_cls , model_name_map = model_name_map ) fields = m_schema [ \"properties\" ] config_values = {} for field_name , details in fields . items (): if remove_pipeline_config and field_name in [ \"steps\" , \"input_aliases\" , \"output_aliases\" , \"doc\" , ]: continue type_str = \"-- n/a --\" if \"type\" in details . keys (): type_str = details [ \"type\" ] desc = details . get ( \"description\" , DEFAULT_NO_DESC_VALUE ) default = config_cls . __fields__ [ field_name ] . default default = config_cls . __fields__ [ field_name ] . default if default is None : if callable ( config_cls . __fields__ [ field_name ] . default_factory ): default = config_cls . __fields__ [ field_name ] . default_factory () # type: ignore req = config_cls . __fields__ [ field_name ] . required config_values [ field_name ] = ValueTypeAndDescription ( description = desc , type = type_str , value_default = default , required = req ) python_cls = PythonClassMetadata . from_class ( config_cls ) return KiaraModuleConfigMetadata ( python_class = python_cls , config_values = config_values ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this configuration.\" ) config_values : typing . Dict [ str , ValueTypeAndDescription ] = Field ( description = \"The available configuration values.\" ) config_values : Dict [ str , kiara . metadata . ValueTypeAndDescription ] pydantic-field required \u00b6 The available configuration values. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this configuration. KiaraModuleInstanceMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/module_models.py class KiaraModuleInstanceMetadata ( MetadataModel ): @classmethod def from_module_obj ( cls , obj : \"KiaraModule\" ): config = obj . config . dict () for x in [ \"steps\" , \"input_aliases\" , \"output_aliases\" , \"module_type_name\" , \"doc\" , \"metadata\" , ]: config . pop ( x , None ) type_metadata = KiaraModuleTypeMetadata . from_module_class ( obj . __class__ ) result = KiaraModuleInstanceMetadata ( type_metadata = type_metadata , config = config , inputs_schema = obj . input_schemas , outputs_schema = obj . output_schemas , ) return result type_metadata : KiaraModuleTypeMetadata = Field ( description = \"Metadata for the module type of this instance.\" ) config : typing . Dict [ str , typing . Any ] = Field ( description = \"Configuration that was used to create this module instance.\" ) inputs_schema : typing . Dict [ str , ValueSchema ] = Field ( description = \"The schema for the module inputs.\" ) outputs_schema : typing . Dict [ str , ValueSchema ] = Field ( description = \"The schema for the module outputs.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_desc = config . get ( \"include_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if include_desc : table . add_row ( \"Description\" , self . type_metadata . documentation . description ) table . add_row ( \"Origin\" , self . type_metadata . origin . create_renderable ()) table . add_row ( \"Type context\" , self . type_metadata . context . create_renderable ()) table . add_row ( \"Python class\" , self . type_metadata . python_class . create_renderable () ) conf = Syntax ( json . dumps ( self . config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Configuration\" , conf ) constants = self . config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . inputs_schema , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . outputs_schema , ) table . add_row ( \"Outputs\" , outputs_table ) # table.add_row(\"Source code\", self.type_metadata.process_src) return table config : Dict [ str , Any ] pydantic-field required \u00b6 Configuration that was used to create this module instance. inputs_schema : Dict [ str , kiara . data . values . ValueSchema ] pydantic-field required \u00b6 The schema for the module inputs. outputs_schema : Dict [ str , kiara . data . values . ValueSchema ] pydantic-field required \u00b6 The schema for the module outputs. type_metadata : KiaraModuleTypeMetadata pydantic-field required \u00b6 Metadata for the module type of this instance. KiaraModuleTypeMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/module_models.py class KiaraModuleTypeMetadata ( MetadataModel ): @classmethod def from_module_class ( cls , module_cls : typing . Type [ \"KiaraModule\" ]): module_attrs = cls . extract_module_attributes ( module_cls = module_cls ) return cls ( ** module_attrs ) @classmethod def extract_module_attributes ( self , module_cls : typing . Type [ \"KiaraModule\" ] ) -> typing . Dict [ str , typing . Any ]: if not hasattr ( module_cls , \"process\" ): raise Exception ( f \"Module class ' { module_cls } ' misses 'process' method.\" ) proc_src = textwrap . dedent ( inspect . getsource ( module_cls . process )) # type: ignore origin_md = OriginMetadataModel . from_class ( module_cls ) doc = DocumentationMetadataModel . from_class_doc ( module_cls ) python_class = PythonClassMetadata . from_class ( module_cls ) properties_md = ContextMetadataModel . from_class ( module_cls ) is_pipeline = module_cls . is_pipeline () doc_url = properties_md . get_url_for_reference ( \"documentation\" ) if doc_url : class_doc = calculate_class_doc_url ( doc_url , module_cls . _module_type_id , pipeline = is_pipeline ) # type: ignore properties_md . add_reference ( \"module_doc\" , class_doc , \"A link to the published, auto-generated module documentation.\" , ) if not is_pipeline : repo_url = properties_md . get_url_for_reference ( \"source_repo\" ) if repo_url is not None : src_url = calculate_class_source_url ( repo_url , python_class ) properties_md . add_reference ( \"source_url\" , src_url , \"A link to the published source file that contains this module.\" , ) config = KiaraModuleConfigMetadata . from_config_class ( module_cls . _config_cls ) pipeline_config = None if module_cls . _module_type_id != \"pipeline\" and is_pipeline : # type: ignore pipeline_config = module_cls . _base_pipeline_config # type: ignore return { \"type_name\" : module_cls . _module_type_name , # type: ignore \"type_id\" : module_cls . _module_type_id , # type: ignore \"documentation\" : doc , \"origin\" : origin_md , \"context\" : properties_md , \"python_class\" : python_class , \"config\" : config , \"is_pipeline\" : is_pipeline , \"pipeline_config\" : pipeline_config , \"process_src\" : proc_src , } type_name : str = Field ( description = \"The registered name for this module type.\" ) type_id : str = Field ( description = \"The full type id.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the module.\" ) origin : OriginMetadataModel = Field ( description = \"Information about authorship for the module type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this module (description, tags, labels, references, ...).\" ) python_class : PythonClassMetadata = Field ( description = \"Information about the Python class for this module type.\" ) config : KiaraModuleConfigMetadata = Field ( description = \"Details on how this module type can be configured.\" ) is_pipeline : bool = Field ( description = \"Whether the module type is a pipeline, or a core module.\" ) pipeline_config : typing . Optional [ PipelineConfig ] = Field ( description = \"If this module is a pipeline, this field contains the pipeline configuration.\" , default_factory = None , ) process_src : str = Field ( description = \"The source code of the process method of the module.\" ) @validator ( \"documentation\" , pre = True ) def validate_doc ( cls , value ): return DocumentationMetadataModel . create ( value ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_config_schema = config . get ( \"include_config_schema\" , True ) include_src = config . get ( \"include_src\" , True ) include_doc = config . get ( \"include_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , padding = ( 0 , 0 , 0 , 0 )) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) if include_config_schema : if self . config : config_cls = self . config . python_class . get_class () table . add_row ( \"Module config\" , create_table_from_base_model ( config_cls )) table . add_row ( \"Module config\" , \"-- no config --\" ) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) if include_src : if self . is_pipeline : json_str = self . pipeline_config . json ( indent = 2 ) # type: ignore _config : Syntax = Syntax ( json_str , \"json\" , background_color = \"default\" ) table . add_row ( \"Pipeline config\" , Panel ( _config , box = box . HORIZONTALS )) else : _config = Syntax ( self . process_src , \"python\" , background_color = \"default\" ) table . add_row ( \"Processing source code\" , Panel ( _config , box = box . HORIZONTALS ) ) return table config : KiaraModuleConfigMetadata pydantic-field required \u00b6 Details on how this module type can be configured. context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this module (description, tags, labels, references, ...). documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the module. is_pipeline : bool pydantic-field required \u00b6 Whether the module type is a pipeline, or a core module. origin : OriginMetadataModel pydantic-field required \u00b6 Information about authorship for the module type. pipeline_config : PipelineConfig pydantic-field \u00b6 If this module is a pipeline, this field contains the pipeline configuration. process_src : str pydantic-field required \u00b6 The source code of the process method of the module. python_class : PythonClassMetadata pydantic-field required \u00b6 Information about the Python class for this module type. type_id : str pydantic-field required \u00b6 The full type id. type_name : str pydantic-field required \u00b6 The registered name for this module type. operation_models \u00b6 OperationsMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/operation_models.py class OperationsMetadata ( MetadataModel ): @classmethod def create_all ( cls , kiara : \"Kiara\" ) -> typing . Dict [ str , \"OperationsMetadata\" ]: op_types = kiara . operation_mgmt . operation_types result = {} for op_type in op_types : op_type_cls = kiara . operation_mgmt . get_operations ( op_type ) result [ op_type ] = cls . from_operations_class ( op_type_cls . __class__ ) return result @classmethod def from_operations_class ( cls , operation_type_cls : typing . Type [ \"OperationType\" ] ) -> \"OperationsMetadata\" : origin_md = OriginMetadataModel . from_class ( operation_type_cls ) doc = DocumentationMetadataModel . from_class_doc ( operation_type_cls ) python_class = PythonClassMetadata . from_class ( operation_type_cls ) properties_md = ContextMetadataModel . from_class ( operation_type_cls ) return OperationsMetadata . construct ( type_name = operation_type_cls . _operation_type_name , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" ) context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type. documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about the creator of this value type. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this value type. type_name : str pydantic-field required \u00b6 The registered name for this value type. type_models \u00b6 ValueTypeMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/type_models.py class ValueTypeMetadata ( MetadataModel ): @classmethod def create_all ( cls , kiara : \"Kiara\" ) -> typing . Dict [ str , \"ValueTypeMetadata\" ]: result = {} for vt in kiara . value_types : t_cls = kiara . get_value_type_cls ( vt ) result [ vt ] = cls . from_value_type_class ( t_cls ) return result @classmethod def from_value_type_class ( cls , value_type_cls : typing . Type [ \"ValueType\" ]): origin_md = OriginMetadataModel . from_class ( value_type_cls ) doc = DocumentationMetadataModel . from_class_doc ( value_type_cls ) python_class = PythonClassMetadata . from_class ( value_type_cls ) properties_md = ContextMetadataModel . from_class ( value_type_cls ) return ValueTypeMetadata ( type_name = value_type_cls . _value_type_name , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" ) context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type. documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about the creator of this value type. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this value type. type_name : str pydantic-field required \u00b6 The registered name for this value type. module \u00b6 KiaraModule ( Generic , ABC ) \u00b6 The base class that every custom module in Kiara needs to inherit from. The core of every KiaraModule is a process method, which should be a 'pure', idempotent function that creates one or several output values from the given input(s), and its purpose is to transfor a set of inputs into a set of outputs. Every module can be configured. The module configuration schema can differ, but every one such configuration needs to subclass the ModuleTypeConfigSchema class and set as the value to the _config_cls attribute of the module class. This is useful, because it allows for some modules to serve a much larger variety of use-cases than non-configurable modules would be, which would mean more code duplication because of very simlilar, but slightly different module types. Each module class (type) has a unique -- within a kiara context -- module type id which can be accessed via the _module_type_id class attribute. Examples: A simple example would be an 'addition' module, with a and b configured as inputs, and z as the output field name. An implementing class would look something like this: TODO Parameters: Name Type Description Default id str the id for this module (needs to be unique within a pipeline) None parent_id Optional[str] the id of the parent, in case this module is part of a pipeline None module_config Any the configuation for this module None metadata Mapping[str, Any] metadata for this module (not implemented yet) required Source code in kiara/module.py class KiaraModule ( typing . Generic [ KIARA_CONFIG ], abc . ABC ): \"\"\"The base class that every custom module in *Kiara* needs to inherit from. The core of every ``KiaraModule`` is a ``process`` method, which should be a 'pure', idempotent function that creates one or several output values from the given input(s), and its purpose is to transfor a set of inputs into a set of outputs. Every module can be configured. The module configuration schema can differ, but every one such configuration needs to subclass the [ModuleTypeConfigSchema][kiara.module_config.ModuleTypeConfigSchema] class and set as the value to the ``_config_cls`` attribute of the module class. This is useful, because it allows for some modules to serve a much larger variety of use-cases than non-configurable modules would be, which would mean more code duplication because of very simlilar, but slightly different module types. Each module class (type) has a unique -- within a *kiara* context -- module type id which can be accessed via the ``_module_type_id`` class attribute. Examples: A simple example would be an 'addition' module, with ``a`` and ``b`` configured as inputs, and ``z`` as the output field name. An implementing class would look something like this: TODO Arguments: id (str): the id for this module (needs to be unique within a pipeline) parent_id (typing.Optional[str]): the id of the parent, in case this module is part of a pipeline module_config (typing.Any): the configuation for this module metadata (typing.Mapping[str, typing.Any]): metadata for this module (not implemented yet) \"\"\" # TODO: not quite sure about this generic type here, mypy doesn't seem to like it _config_cls : typing . Type [ KIARA_CONFIG ] = ModuleTypeConfigSchema # type: ignore @classmethod def create_instance ( cls , module_type : typing . Optional [ str ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"KiaraModule\" : \"\"\"Create an instance of a *kiara* module. This class method is overloaded in a way that you can either provide the `module_type` argument, in which case the relevant sub-class will be queried from the *kiara* context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Arguments: module_type: must be None if called on the ``KiaraModule`` base class, otherwise the module or operation id module_config: the configuration of the module instance kiara: the *kiara* context \"\"\" if cls == KiaraModule : if not module_type : raise Exception ( \"This method must be either called on a subclass of KiaraModule, not KiaraModule itself, or it needs the 'module_type' argument specified.\" ) else : if module_type : raise Exception ( \"This method must be either called without the 'module_type' argument specified, or on a subclass of the KiaraModule class, but not both.\" ) if cls == KiaraModule : assert module_type is not None module_conf = ModuleConfig . create_module_config ( config = module_type , module_config = module_config , kiara = kiara ) else : module_conf = ModuleConfig . create_module_config ( config = cls , module_config = module_config , kiara = kiara ) return module_conf . create_module ( kiara = kiara ) @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: \"\"\"Retrieve a collection of profiles (pre-set module configs) for this *kiara* module type. This is used to automatically create generally useful operations (incl. their ids). \"\"\" @classmethod def get_type_metadata ( cls ) -> KiaraModuleTypeMetadata : \"\"\"Return all metadata associated with this module type.\"\"\" return KiaraModuleTypeMetadata . from_module_class ( cls ) # @classmethod # def profiles( # cls, # ) -> typing.Optional[typing.Mapping[str, typing.Mapping[str, typing.Any]]]: # \"\"\"Retrieve a collection of profiles (pre-set module configs) for this *kiara* module type. # # This is used to automatically create generally useful operations (incl. their ids). # \"\"\" # return None @classmethod def is_pipeline ( cls ) -> bool : \"\"\"Check whether this module type is a pipeline, or not.\"\"\" return False def __init__ ( self , id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , module_config : typing . Union [ None , KIARA_CONFIG , typing . Mapping [ str , typing . Any ] ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if id is None : id = str ( uuid . uuid4 ()) self . _id : str = id self . _parent_id = parent_id if kiara is None : from kiara import Kiara kiara = Kiara . instance () self . _kiara = kiara if isinstance ( module_config , ModuleTypeConfigSchema ): self . _config : KIARA_CONFIG = module_config # type: ignore elif module_config is None : self . _config = self . __class__ . _config_cls () elif isinstance ( module_config , typing . Mapping ): try : self . _config = self . __class__ . _config_cls ( ** module_config ) except ValidationError as ve : raise KiaraModuleConfigException ( f \"Error creating module ' { id } '. { ve } \" , self . __class__ , module_config , ve , ) else : raise TypeError ( f \"Invalid type for module config: { type ( module_config ) } \" ) self . _module_hash : typing . Optional [ int ] = None self . _info : typing . Optional [ KiaraModuleInstanceMetadata ] = None self . _input_schemas : typing . Mapping [ str , ValueSchema ] = None # type: ignore self . _constants : typing . Mapping [ str , ValueSchema ] = None # type: ignore self . _merged_input_schemas : typing . Mapping [ str , ValueSchema ] = None # type: ignore self . _output_schemas : typing . Mapping [ str , ValueSchema ] = None # type: ignore @property def id ( self ) -> str : \"\"\"The id of this module. This is only unique within a pipeline. \"\"\" return self . _id @property def parent_id ( self ) -> typing . Optional [ str ]: \"\"\"The id of the parent of this module (if part of a pipeline).\"\"\" return self . _parent_id @property def full_id ( self ) -> str : \"\"\"The full id for this module.\"\"\" if self . parent_id : return f \" { self . parent_id } . { self . id } \" else : return self . id @property def config ( self ) -> KIARA_CONFIG : \"\"\"Retrieve the configuration object for this module. Returns: the module-class-specific config object \"\"\" return self . _config def input_required ( self , input_name : str ): if input_name not in self . _input_schemas . keys (): raise Exception ( f \"No input ' { input_name } ' for module ' { self . id } '.\" ) if not self . _input_schemas [ input_name ] . is_required (): return False if input_name in self . constants . keys (): return False else : return True def get_config_value ( self , key : str ) -> typing . Any : \"\"\"Retrieve the value for a specific configuration option. Arguments: key: the config key Returns: the value for the provided key \"\"\" try : return self . config . get ( key ) except Exception : raise Exception ( f \"Error accessing config value ' { key } ' in module { self . __class__ . _module_type_id } .\" # type: ignore ) @abstractmethod def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\" @abstractmethod def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\" @property def input_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: \"\"\"The input schema for this module.\"\"\" if self . _input_schemas is None : self . _create_input_schemas () return self . _input_schemas # type: ignore @property def full_input_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: if self . _merged_input_schemas is not None : return self . _merged_input_schemas self . _merged_input_schemas = dict ( self . input_schemas ) self . _merged_input_schemas . update ( self . constants ) return self . _merged_input_schemas @property def constants ( self ) -> typing . Mapping [ str , ValueSchema ]: if self . _constants is None : self . _create_input_schemas () return self . _constants # type: ignore def _create_input_schemas ( self ) -> None : try : _input_schemas_data = self . create_input_schema () if not _input_schemas_data : raise Exception ( f \"Invalid module implementation for ' { self . __class__ . __name__ } ': empty input schema\" ) try : _input_schemas = create_schemas ( schema_config = _input_schemas_data , kiara = self . _kiara ) except Exception as e : raise Exception ( f \"Can't create input schemas for module { self . full_id } : { e } \" ) defaults = self . config . defaults constants = self . config . constants for k , v in defaults . items (): if k not in _input_schemas . keys (): raise Exception ( f \"Can't create inputs for module ' { self . _module_type_id } ', invalid default field name ' { k } '. Available field names: ' { ', ' . join ( _input_schemas . keys ()) } '\" # type: ignore ) for k , v in constants . items (): if k not in _input_schemas . keys (): raise Exception ( f \"Can't create inputs for module ' { self . _module_type_id } ', invalid constant field name ' { k } '. Available field names: ' { ', ' . join ( _input_schemas . keys ()) } '\" # type: ignore ) self . _input_schemas , self . _constants = overlay_constants_and_defaults ( _input_schemas , defaults = defaults , constants = constants ) except Exception as e : raise Exception ( f \"Can't create input schemas for module of type ' { self . __class__ . _module_type_id } ': { e } \" ) # type: ignore @property def output_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: \"\"\"The output schema for this module.\"\"\" if self . _output_schemas is not None : return self . _output_schemas try : _output_schema = self . create_output_schema () if not _output_schema : raise Exception ( f \"Invalid module implementation for ' { self . __class__ . __name__ } ': empty output schema\" ) try : self . _output_schemas = create_schemas ( schema_config = _output_schema , kiara = self . _kiara ) except Exception as e : raise Exception ( f \"Can't create output schemas for module { self . full_id } : { e } \" ) return self . _output_schemas except Exception as e : raise Exception ( f \"Can't create output schemas for module of type ' { self . __class__ . _module_type_id } ': { e } \" ) # type: ignore @property def input_names ( self ) -> typing . Iterable [ str ]: \"\"\"A list of input field names for this module.\"\"\" return self . input_schemas . keys () @property def output_names ( self ) -> typing . Iterable [ str ]: \"\"\"A list of output field names for this module.\"\"\" return self . output_schemas . keys () def process_step ( self , inputs : ValueSet , outputs : ValueSet , job_log : JobLog ) -> None : \"\"\"Kick off processing for a specific set of input/outputs. This method calls the implemented [process][kiara.module.KiaraModule.process] method of the inheriting class, as well as wrapping input/output-data related functionality. Arguments: inputs: the input value set outputs: the output value set \"\"\" signature = inspect . signature ( self . process ) # type: ignore if \"job_log\" not in signature . parameters . keys (): try : self . process ( inputs = inputs , outputs = outputs ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e else : try : self . process ( inputs = inputs , outputs = outputs , job_log = job_log ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e # @typing.overload # def process(self, inputs: ValueSet, outputs: ValueSet) -> None: # \"\"\"Abstract method to implement by child classes, should be a pure, idempotent function that uses the values from ``inputs``, and stores results in the provided ``outputs`` object. # # Arguments: # inputs: the input value set # outputs: the output value set # \"\"\" # pass # # @typing.overload # def process(self, inputs: ValueSet, outputs: ValueSet, job_log: typing.Optional[JobLog]=None) -> None: # pass # # def process(self, inputs, outputs, job_log=None) -> None: # pass def create_full_inputs ( self , ** inputs : typing . Any ) -> typing . Mapping [ str , Value ]: # TODO: find a generic way to do this kind of stuff def clean_value ( v : typing . Any ) -> typing . Any : if hasattr ( v , \"as_py\" ): return v . as_py () # type: ignore else : return v resolved_inputs : typing . Dict [ str , Value ] = {} for k , v in self . constants . items (): if k in inputs . keys (): raise Exception ( f \"Invalid input: value provided for constant ' { k } '\" ) inputs [ k ] = v for k , value in inputs . items (): value = clean_value ( value ) if not isinstance ( value , Value ): if ( k not in self . input_schemas . keys () and k not in self . constants . keys () ): raise Exception ( f \"Invalid input name ' { k } for module { self . _module_type_id } . Not part of the schema, allowed input names: { ', ' . join ( self . input_names ) } \" # type: ignore ) if k in self . input_schemas . keys (): schema = self . input_schemas [ k ] value = self . _kiara . data_registry . register_data ( value_data = value , value_schema = schema ) # value = Value( # value_data=value, # type: ignore # value_schema=schema, # is_constant=False, # registry=self._kiara.data_registry, # type: ignore # ) else : schema = self . constants [ k ] value = self . _kiara . data_registry . register_data ( value_data = SpecialValue . NOT_SET , value_schema = schema , ) # value = Value( # value_schema=schema, # is_constant=False, # kiara=self._kiara, # type: ignore # registry=self._kiara.data_registry, # type: ignore # ) resolved_inputs [ k ] = value return resolved_inputs def run ( self , _attach_lineage : bool = True , ** inputs : typing . Any ) -> ValueSet : \"\"\"Execute the module with the provided inputs directly. Arguments: inputs: a map of the input values (as described by the input schema Returns: a map of the output values (as described by the output schema) \"\"\" resolved_inputs = self . create_full_inputs ( ** inputs ) # TODO: introduce a 'temp' value set implementation and use that here input_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . full_input_schemas , read_only = True , initial_values = resolved_inputs , title = f \"module_inputs_ { self . id } \" , ) if not input_value_set . items_are_valid (): invalid_details = input_value_set . check_invalid () raise Exception ( f \"Can't process module ' { self . _module_type_name } ', input field(s) not valid: { ', ' . join ( invalid_details . keys ()) } \" # type: ignore ) output_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = False , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore default_value = SpecialValue . NOT_SET , ) self . process ( inputs = input_value_set , outputs = output_value_set ) # type: ignore result_outputs : typing . MutableMapping [ str , Value ] = {} if _attach_lineage : input_infos = { k : v . get_info () for k , v in resolved_inputs . items ()} for field_name , output in output_value_set . items (): value_lineage = ValueLineage . from_module_and_inputs ( module = self , output_name = field_name , inputs = input_infos ) # value_lineage = None output_val = self . _kiara . data_registry . register_data ( value_data = output , lineage = value_lineage ) result_outputs [ field_name ] = output_val else : result_outputs = output_value_set result_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = True , initial_values = result_outputs , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore ) return result_set # result = output_value_set.get_all_value_objects() # return output_value_set # return ValueSetImpl(items=result, read_only=True) @property def module_instance_doc ( self ) -> str : \"\"\"Return documentation for this instance of the module. If not overwritten, will return this class' method ``doc()``. \"\"\" # TODO: auto create instance doc? return self . get_type_metadata () . documentation . full_doc @property def module_instance_hash ( self ) -> int : \"\"\"Return this modules 'module_hash'. If two module instances ``module_instance_hash`` values are the same, it is guaranteed that their ``process`` methods will return the same output, given the same inputs (except if that processing step uses randomness). It can also be assumed that the two instances have the same input and output fields, with the same schemas. !!! note This implementation is preliminary, since it's not yet 100% clear to me how much that will be needed, and in which situations. Also, module versioning needs to be implemented before this can work reliably. Also, for now it is assumed that a module configuration is not changed once set, this also might change in the future Returns: this modules 'module_instance_hash' \"\"\" # TODO: if self . _module_hash is None : _d = { \"module_cls\" : f \" { self . __class__ . __module__ } . { self . __class__ . __name__ } \" , \"version\" : \"0.0.0\" , # TODO: implement module versioning, package name might also need to be included here \"config_hash\" : self . config . config_hash , } hashes = deepdiff . DeepHash ( _d ) self . _module_hash = hashes [ _d ] return self . _module_hash @property def info ( self ) -> KiaraModuleInstanceMetadata : \"\"\"Return an info wrapper class for this module.\"\"\" if self . _info is None : self . _info = KiaraModuleInstanceMetadata . from_module_obj ( self ) return self . _info def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return ( self . full_id , self . config ) == ( self . full_id , other . config ) def __hash__ ( self ): return hash (( self . __class__ , self . full_id , self . config )) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (id= { self . id } input_names= { list ( self . input_names ) } output_names= { list ( self . output_names ) } )\" def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : if not hasattr ( self . __class__ , \"_module_type_id\" ): raise Exception ( \"Invalid model class, no '_module_type_id' attribute added. This is a bug\" ) r_gro : typing . List [ typing . Any ] = [] md = self . info table = md . create_renderable () r_gro . append ( table ) yield Panel ( RenderGroup ( * r_gro ), box = box . ROUNDED , title_align = \"left\" , title = f \"Module: [b] { self . id } [/b]\" , ) config : ~ KIARA_CONFIG property readonly \u00b6 Retrieve the configuration object for this module. Returns: Type Description ~KIARA_CONFIG the module-class-specific config object full_id : str property readonly \u00b6 The full id for this module. id : str property readonly \u00b6 The id of this module. This is only unique within a pipeline. info : KiaraModuleInstanceMetadata property readonly \u00b6 Return an info wrapper class for this module. input_names : Iterable [ str ] property readonly \u00b6 A list of input field names for this module. input_schemas : Mapping [ str , kiara . data . values . ValueSchema ] property readonly \u00b6 The input schema for this module. module_instance_doc : str property readonly \u00b6 Return documentation for this instance of the module. If not overwritten, will return this class' method doc() . module_instance_hash : int property readonly \u00b6 Return this modules 'module_hash'. If two module instances module_instance_hash values are the same, it is guaranteed that their process methods will return the same output, given the same inputs (except if that processing step uses randomness). It can also be assumed that the two instances have the same input and output fields, with the same schemas. Note This implementation is preliminary, since it's not yet 100% clear to me how much that will be needed, and in which situations. Also, module versioning needs to be implemented before this can work reliably. Also, for now it is assumed that a module configuration is not changed once set, this also might change in the future Returns: Type Description int this modules 'module_instance_hash' output_names : Iterable [ str ] property readonly \u00b6 A list of output field names for this module. output_schemas : Mapping [ str , kiara . data . values . ValueSchema ] property readonly \u00b6 The output schema for this module. parent_id : Optional [ str ] property readonly \u00b6 The id of the parent of this module (if part of a pipeline). create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/module.py @abstractmethod def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\" create_instance ( module_type = None , module_config = None , kiara = None ) classmethod \u00b6 Create an instance of a kiara module. This class method is overloaded in a way that you can either provide the module_type argument, in which case the relevant sub-class will be queried from the kiara context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Parameters: Name Type Description Default module_type Optional[str] must be None if called on the KiaraModule base class, otherwise the module or operation id None module_config Optional[Mapping[str, Any]] the configuration of the module instance None kiara Optional[Kiara] the kiara context None Source code in kiara/module.py @classmethod def create_instance ( cls , module_type : typing . Optional [ str ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"KiaraModule\" : \"\"\"Create an instance of a *kiara* module. This class method is overloaded in a way that you can either provide the `module_type` argument, in which case the relevant sub-class will be queried from the *kiara* context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Arguments: module_type: must be None if called on the ``KiaraModule`` base class, otherwise the module or operation id module_config: the configuration of the module instance kiara: the *kiara* context \"\"\" if cls == KiaraModule : if not module_type : raise Exception ( \"This method must be either called on a subclass of KiaraModule, not KiaraModule itself, or it needs the 'module_type' argument specified.\" ) else : if module_type : raise Exception ( \"This method must be either called without the 'module_type' argument specified, or on a subclass of the KiaraModule class, but not both.\" ) if cls == KiaraModule : assert module_type is not None module_conf = ModuleConfig . create_module_config ( config = module_type , module_config = module_config , kiara = kiara ) else : module_conf = ModuleConfig . create_module_config ( config = cls , module_config = module_config , kiara = kiara ) return module_conf . create_module ( kiara = kiara ) create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/module.py @abstractmethod def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\" get_config_value ( self , key ) \u00b6 Retrieve the value for a specific configuration option. Parameters: Name Type Description Default key str the config key required Returns: Type Description Any the value for the provided key Source code in kiara/module.py def get_config_value ( self , key : str ) -> typing . Any : \"\"\"Retrieve the value for a specific configuration option. Arguments: key: the config key Returns: the value for the provided key \"\"\" try : return self . config . get ( key ) except Exception : raise Exception ( f \"Error accessing config value ' { key } ' in module { self . __class__ . _module_type_id } .\" # type: ignore ) get_type_metadata () classmethod \u00b6 Return all metadata associated with this module type. Source code in kiara/module.py @classmethod def get_type_metadata ( cls ) -> KiaraModuleTypeMetadata : \"\"\"Return all metadata associated with this module type.\"\"\" return KiaraModuleTypeMetadata . from_module_class ( cls ) is_pipeline () classmethod \u00b6 Check whether this module type is a pipeline, or not. Source code in kiara/module.py @classmethod def is_pipeline ( cls ) -> bool : \"\"\"Check whether this module type is a pipeline, or not.\"\"\" return False process_step ( self , inputs , outputs , job_log ) \u00b6 Kick off processing for a specific set of input/outputs. This method calls the implemented process method of the inheriting class, as well as wrapping input/output-data related functionality. Parameters: Name Type Description Default inputs ValueSet the input value set required outputs ValueSet the output value set required Source code in kiara/module.py def process_step ( self , inputs : ValueSet , outputs : ValueSet , job_log : JobLog ) -> None : \"\"\"Kick off processing for a specific set of input/outputs. This method calls the implemented [process][kiara.module.KiaraModule.process] method of the inheriting class, as well as wrapping input/output-data related functionality. Arguments: inputs: the input value set outputs: the output value set \"\"\" signature = inspect . signature ( self . process ) # type: ignore if \"job_log\" not in signature . parameters . keys (): try : self . process ( inputs = inputs , outputs = outputs ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e else : try : self . process ( inputs = inputs , outputs = outputs , job_log = job_log ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/module.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: \"\"\"Retrieve a collection of profiles (pre-set module configs) for this *kiara* module type. This is used to automatically create generally useful operations (incl. their ids). \"\"\" run ( self , _attach_lineage = True , ** inputs ) \u00b6 Execute the module with the provided inputs directly. Parameters: Name Type Description Default inputs Any a map of the input values (as described by the input schema {} Returns: Type Description ValueSet a map of the output values (as described by the output schema) Source code in kiara/module.py def run ( self , _attach_lineage : bool = True , ** inputs : typing . Any ) -> ValueSet : \"\"\"Execute the module with the provided inputs directly. Arguments: inputs: a map of the input values (as described by the input schema Returns: a map of the output values (as described by the output schema) \"\"\" resolved_inputs = self . create_full_inputs ( ** inputs ) # TODO: introduce a 'temp' value set implementation and use that here input_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . full_input_schemas , read_only = True , initial_values = resolved_inputs , title = f \"module_inputs_ { self . id } \" , ) if not input_value_set . items_are_valid (): invalid_details = input_value_set . check_invalid () raise Exception ( f \"Can't process module ' { self . _module_type_name } ', input field(s) not valid: { ', ' . join ( invalid_details . keys ()) } \" # type: ignore ) output_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = False , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore default_value = SpecialValue . NOT_SET , ) self . process ( inputs = input_value_set , outputs = output_value_set ) # type: ignore result_outputs : typing . MutableMapping [ str , Value ] = {} if _attach_lineage : input_infos = { k : v . get_info () for k , v in resolved_inputs . items ()} for field_name , output in output_value_set . items (): value_lineage = ValueLineage . from_module_and_inputs ( module = self , output_name = field_name , inputs = input_infos ) # value_lineage = None output_val = self . _kiara . data_registry . register_data ( value_data = output , lineage = value_lineage ) result_outputs [ field_name ] = output_val else : result_outputs = output_value_set result_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = True , initial_values = result_outputs , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore ) return result_set # result = output_value_set.get_all_value_objects() # return output_value_set # return ValueSetImpl(items=result, read_only=True) StepInputs ( ValueSet ) \u00b6 Wrapper class to hold a set of inputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Parameters: Name Type Description Default inputs ValueSet the input values of a pipeline step required Source code in kiara/module.py class StepInputs ( ValueSet ): \"\"\"Wrapper class to hold a set of inputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Arguments: inputs (ValueSet): the input values of a pipeline step \"\"\" def __init__ ( self , inputs : typing . Mapping [ str , Value ], title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _inputs : typing . Mapping [ str , Value ] = inputs super () . __init__ ( read_only = True , title = title , kiara = kiara ) def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _inputs . keys () def _get_value_obj ( self , field_name : str , ensure_metadata : typing . Union [ bool , typing . Iterable [ str ], str ] = False , ) -> Value : \"\"\"Retrieve the value object for the specified field.\"\"\" value = self . _inputs [ field_name ] if ensure_metadata : if isinstance ( ensure_metadata , bool ): value . get_metadata () elif isinstance ( ensure_metadata , str ): value . get_metadata ( ensure_metadata ) elif isinstance ( ensure_metadata , typing . Iterable ): value . get_metadata ( * ensure_metadata ) else : raise ValueError ( f \"Invalid type ' { type ( ensure_metadata ) } ' for 'ensure_metadata' argument.\" ) return value def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: raise Exception ( \"Inputs are read-only.\" ) get_all_field_names ( self ) \u00b6 All field names included in this ValueSet. Source code in kiara/module.py def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _inputs . keys () StepOutputs ( ValueSet ) \u00b6 Wrapper class to hold a set of outputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Internally, this class stores two sets of its values: the 'actual', up-to-date values, and the referenced (original) ones that were used when creating an object of this class. It's not a good idea to keep both synced all the time, because that could potentially involve unnecessary data transfer and I/O. Also, in some cases a developer might want to avoid events that could be triggered by a changed value. Both value sets can be synced manually using the 'sync()' method. Parameters: Name Type Description Default outputs ValueSet the output values of a pipeline step required Source code in kiara/module.py class StepOutputs ( ValueSet ): \"\"\"Wrapper class to hold a set of outputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Internally, this class stores two sets of its values: the 'actual', up-to-date values, and the referenced (original) ones that were used when creating an object of this class. It's not a good idea to keep both synced all the time, because that could potentially involve unnecessary data transfer and I/O. Also, in some cases a developer might want to avoid events that could be triggered by a changed value. Both value sets can be synced manually using the 'sync()' method. Arguments: outputs (ValueSet): the output values of a pipeline step \"\"\" def __init__ ( self , outputs : ValueSet , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _outputs_staging : typing . Dict [ str , typing . Any ] = {} self . _outputs : ValueSet = outputs super () . __init__ ( read_only = False , title = title , kiara = kiara ) def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _outputs . get_all_field_names () def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: wrong = [] for key in values . keys (): if key not in self . _outputs . keys (): # type: ignore wrong . append ( key ) if wrong : av = \", \" . join ( self . _outputs . keys ()) # type: ignore raise Exception ( f \"Can't set output value(s), invalid key name(s): { ', ' . join ( wrong ) } . Available: { av } \" ) if metadata : raise NotImplementedError () if lineage : raise NotImplementedError () result = {} for output_name , value in values . items (): # value_obj = self._outputs.get_value_obj(output_name) if ( output_name not in self . _outputs_staging . keys () # type: ignore or value != self . _outputs_staging [ output_name ] # type: ignore ): self . _outputs_staging [ output_name ] = value # type: ignore result [ output_name ] = True else : result [ output_name ] = False return result def _get_value_obj ( self , output_name ): \"\"\"Retrieve the value object for the specified field.\"\"\" # self.sync() return self . _outputs . get_value_obj ( output_name ) def sync ( self , lineage : typing . Optional [ ValueLineage ] = None , ** metadata : MetadataModel ): \"\"\"Sync this value sets 'shadow' values with the ones a user would retrieve.\"\"\" self . _outputs . set_values ( lineage = lineage , metadata = metadata , ** self . _outputs_staging ) # type: ignore self . _outputs_staging . clear () # type: ignore get_all_field_names ( self ) \u00b6 All field names included in this ValueSet. Source code in kiara/module.py def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _outputs . get_all_field_names () sync ( self , lineage = None , ** metadata ) \u00b6 Sync this value sets 'shadow' values with the ones a user would retrieve. Source code in kiara/module.py def sync ( self , lineage : typing . Optional [ ValueLineage ] = None , ** metadata : MetadataModel ): \"\"\"Sync this value sets 'shadow' values with the ones a user would retrieve.\"\"\" self . _outputs . set_values ( lineage = lineage , metadata = metadata , ** self . _outputs_staging ) # type: ignore self . _outputs_staging . clear () # type: ignore module_config \u00b6 Module-related configuration models for the Kiara package. ModuleConfig ( KiaraInfoModel ) pydantic-model \u00b6 A class to hold the type and configuration for a module instance. Source code in kiara/module_config.py class ModuleConfig ( KiaraInfoModel ): \"\"\"A class to hold the type and configuration for a module instance.\"\"\" @classmethod def create_module_config ( cls , config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str , typing . Type [ \"KiaraModule\" ], \"PipelineConfig\" , ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleConfig\" : conf = parse_and_create_module_config ( config = config , module_config = module_config , kiara = kiara ) return conf @classmethod def create_renderable_from_module_instance_configs ( cls , configs : typing . Mapping [ str , \"ModuleConfig\" ], ** render_config : typing . Any , ): \"\"\"Convenience method to create a renderable for this module configuration, to be printed to terminal.\"\"\" table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" , style = \"i\" , no_wrap = True ) table . add_column ( \"Description\" ) if not render_config . get ( \"omit_config\" , False ): table . add_column ( \"Configuration\" ) for id , config in configs . items (): if config . doc : doc = config . doc . description else : doc = DEFAULT_NO_DESC_VALUE row : typing . List [ RenderableType ] = [ id , doc ] if not render_config . get ( \"omit_config\" , False ): conf = config . json ( exclude = { \"doc\" }, indent = 2 ) row . append ( Syntax ( conf , \"json\" , background_color = \"default\" )) table . add_row ( * row ) return table class Config : extra = Extra . forbid validate_all = True _module : typing . Optional [ \"KiaraModule\" ] = PrivateAttr ( default = None ) module_type : str = Field ( description = \"The module type.\" ) module_config : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"The configuration for the module.\" ) doc : DocumentationMetadataModel = Field ( description = \"Documentation for this operation.\" , default = None ) @validator ( \"doc\" , pre = True ) def create_doc ( cls , value ): return DocumentationMetadataModel . create ( value ) def create_module ( self , kiara : typing . Optional [ \"Kiara\" ] = None , module_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance from this configuration.\"\"\" if module_id and not isinstance ( module_id , str ): raise TypeError ( f \"Invalid type, module_id must be a string, not: { type ( module_id ) } \" ) if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if self . _module is None : self . _module = kiara . create_module ( id = module_id , module_type = self . module_type , module_config = self . module_config , ) return self . _module def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a renderable for this module configuration.\"\"\" conf = Syntax ( self . json ( exclude_none = True , indent = 2 ), \"json\" , background_color = \"default\" , ) return conf doc : DocumentationMetadataModel pydantic-field \u00b6 Documentation for this operation. module_config : Dict [ str , Any ] pydantic-field \u00b6 The configuration for the module. module_type : str pydantic-field required \u00b6 The module type. create_module ( self , kiara = None , module_id = None ) \u00b6 Create a module instance from this configuration. Source code in kiara/module_config.py def create_module ( self , kiara : typing . Optional [ \"Kiara\" ] = None , module_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance from this configuration.\"\"\" if module_id and not isinstance ( module_id , str ): raise TypeError ( f \"Invalid type, module_id must be a string, not: { type ( module_id ) } \" ) if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if self . _module is None : self . _module = kiara . create_module ( id = module_id , module_type = self . module_type , module_config = self . module_config , ) return self . _module create_renderable ( self , ** config ) \u00b6 Create a renderable for this module configuration. Source code in kiara/module_config.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a renderable for this module configuration.\"\"\" conf = Syntax ( self . json ( exclude_none = True , indent = 2 ), \"json\" , background_color = \"default\" , ) return conf create_renderable_from_module_instance_configs ( configs , ** render_config ) classmethod \u00b6 Convenience method to create a renderable for this module configuration, to be printed to terminal. Source code in kiara/module_config.py @classmethod def create_renderable_from_module_instance_configs ( cls , configs : typing . Mapping [ str , \"ModuleConfig\" ], ** render_config : typing . Any , ): \"\"\"Convenience method to create a renderable for this module configuration, to be printed to terminal.\"\"\" table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" , style = \"i\" , no_wrap = True ) table . add_column ( \"Description\" ) if not render_config . get ( \"omit_config\" , False ): table . add_column ( \"Configuration\" ) for id , config in configs . items (): if config . doc : doc = config . doc . description else : doc = DEFAULT_NO_DESC_VALUE row : typing . List [ RenderableType ] = [ id , doc ] if not render_config . get ( \"omit_config\" , False ): conf = config . json ( exclude = { \"doc\" }, indent = 2 ) row . append ( Syntax ( conf , \"json\" , background_color = \"default\" )) table . add_row ( * row ) return table ModuleTypeConfigSchema ( BaseModel ) pydantic-model \u00b6 Base class that describes the configuration a KiaraModule class accepts. This is stored in the _config_cls class attribute in each KiaraModule class. There are two config options every KiaraModule supports: constants , and defaults Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default values that override the schema defaults, and those can be overwritten by users. If both a constant and a default value is set for an input field, an error is thrown. Source code in kiara/module_config.py class ModuleTypeConfigSchema ( BaseModel ): \"\"\"Base class that describes the configuration a [``KiaraModule``][kiara.module.KiaraModule] class accepts. This is stored in the ``_config_cls`` class attribute in each ``KiaraModule`` class. There are two config options every ``KiaraModule`` supports: - ``constants``, and - ``defaults`` Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default values that override the schema defaults, and those can be overwritten by users. If both a constant and a default value is set for an input field, an error is thrown. \"\"\" @classmethod def requires_config ( cls , config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : if config : if config . get ( field_name , None ) is None : return True else : return True return False _config_hash : str = PrivateAttr ( default = None ) constants : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"Value constants for this module.\" ) defaults : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"Value defaults for this module.\" ) class Config : extra = Extra . forbid validate_assignment = True def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) @property def config_hash ( self ): if self . _config_hash is None : _d = self . dict () hashes = deepdiff . DeepHash ( _d ) self . _config_hash = hashes [ _d ] return self . _config_hash def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () def __hash__ ( self ): return hash ( self . config_hash ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : my_table = Table ( box = box . MINIMAL , show_header = False ) my_table . add_column ( \"Field name\" , style = \"i\" ) my_table . add_column ( \"Value\" ) for field in self . __fields__ : my_table . add_row ( field , getattr ( self , field )) yield my_table constants : Dict [ str , Any ] pydantic-field \u00b6 Value constants for this module. defaults : Dict [ str , Any ] pydantic-field \u00b6 Value defaults for this module. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/module_config.py def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/module_config.py def __hash__ ( self ): return hash ( self . config_hash ) get ( self , key ) \u00b6 Get the value for the specified configuation key. Source code in kiara/module_config.py def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) requires_config ( config = None ) classmethod \u00b6 Return whether this class can be used as-is, or requires configuration before an instance can be created. Source code in kiara/module_config.py @classmethod def requires_config ( cls , config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : if config : if config . get ( field_name , None ) is None : return True else : return True return False parse_and_create_module_config ( config , module_config = None , kiara = None ) \u00b6 Utility method to create a ModuleConfig object from different input types. Parameters: Name Type Description Default config Union[ModuleConfig, Mapping, str, Type[KiaraModule], PipelineConfig] the 'main' configuation required module_config Optional[Mapping[str, Any]] if the 'main' configuration value is an (unconfigured) module type, this argument can contain the configuration for the module instance None kiara Optional[Kiara] the kiara context None Source code in kiara/module_config.py def parse_and_create_module_config ( config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str , typing . Type [ \"KiaraModule\" ], \"PipelineConfig\" , ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleConfig\" : \"\"\"Utility method to create a `ModuleConfig` object from different input types. Arguments: config: the 'main' configuation module_config: if the 'main' configuration value is an (unconfigured) module type, this argument can contain the configuration for the module instance kiara: the *kiara* context \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if isinstance ( config , type ): from kiara.module import KiaraModule if issubclass ( config , KiaraModule ): # this basically makes sure the class is augmented with the '_module_type_id` attribute if not hasattr ( config , \"_module_type_id\" ): match = False for mod_name in kiara . available_module_types : mod_cls = kiara . get_module_class ( mod_name ) if mod_cls == config : match = True break if not match : raise Exception ( f \"Class ' { config } ' not a valid kiara module.\" ) config = config . _module_type_id # type: ignore else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for module configuration.\" ) if isinstance ( config , typing . Mapping ): if module_config : raise NotImplementedError () module_instance_config : ModuleConfig = ModuleConfig ( ** config ) elif isinstance ( config , str ): if config == \"pipeline\" : if not module_config : raise Exception ( \"Can't create module from 'pipeline' module type without further configuration.\" ) elif isinstance ( module_config , typing . Mapping ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = module_config ) else : from kiara.pipeline.config import PipelineConfig if isinstance ( config , PipelineConfig ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = config . dict () ) else : raise Exception ( f \"Can't create module config, invalid type for 'module_config' parameter: { type ( module_config ) } \" ) elif config in kiara . available_module_types : if module_config : module_instance_config = ModuleConfig ( module_type = config , module_config = module_config ) else : module_instance_config = ModuleConfig ( module_type = config ) elif config in kiara . operation_mgmt . profiles . keys (): module_instance_config = kiara . operation_mgmt . profiles [ config ] elif os . path . isfile ( os . path . expanduser ( config )): path = os . path . expanduser ( config ) module_config_data = get_data_from_file ( path ) if module_config : raise NotImplementedError () if not isinstance ( module_config_data , typing . Mapping ): raise Exception ( f \"Invalid module/pipeline config, must be a mapping type: { module_config_data } \" ) if \"steps\" in module_config_data . keys (): module_type = \"pipeline\" module_instance_config = ModuleConfig ( module_type = module_type , module_config = module_config_data ) elif \"module_type\" in module_config_data . keys (): module_instance_config = ModuleConfig ( ** module_config_data ) else : raise Exception ( f \"Can't create module config from string ' { config } '. Value must be path to a file, or one of: { ', ' . join ( kiara . available_module_types ) } \" ) elif isinstance ( config , ModuleConfig ): module_instance_config = config if module_config : raise NotImplementedError () else : from kiara.pipeline.config import PipelineConfig if isinstance ( config , PipelineConfig ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = config . dict () ) else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for module configuration.\" ) return module_instance_config module_mgmt special \u00b6 Base module for code that handles the import and management of KiaraModule sub-classes. merged \u00b6 MergedModuleManager ( ModuleManager ) \u00b6 Source code in kiara/module_mgmt/merged.py class MergedModuleManager ( ModuleManager ): def __init__ ( self , module_managers : typing . Optional [ typing . List [ typing . Union [ PythonModuleManagerConfig , PipelineModuleManagerConfig ] ] ] = None , extra_pipeline_folders : typing . Iterable [ str ] = None , ignore_errors : bool = False , ): self . _modules : typing . Optional [ typing . Dict [ str , ModuleManager ]] = None self . _module_cls_cache : typing . Dict [ str , typing . Type [ KiaraModule ]] = {} self . _default_python_mgr : typing . Optional [ PythonModuleManager ] = None self . _default_pipeline_mgr : typing . Optional [ PipelineModuleManager ] = None self . _custom_pipelines_mgr : typing . Optional [ PipelineModuleManager ] = None if extra_pipeline_folders is None : extra_pipeline_folders = [] self . _extra_pipeline_folders : typing . Iterable [ str ] = extra_pipeline_folders self . _ignore_errors : bool = ignore_errors if module_managers : raise NotImplementedError () # for mmc in module_managers: # mm = ModuleManager.from_config(mmc) # _mms.append(mm) self . _module_mgrs : typing . Optional [ typing . List [ ModuleManager ]] = None @property def module_managers ( self ) -> typing . Iterable [ ModuleManager ]: if self . _module_mgrs is not None : return self . _module_mgrs _mms = [ self . default_python_module_manager , self . default_pipeline_module_manager , self . default_custom_pipelines_manager , ] self . _module_mgrs = [] self . _modules = {} for mm in _mms : self . add_module_manager ( mm ) return self . _module_mgrs @property def module_map ( self ) -> typing . MutableMapping [ str , ModuleManager ]: if self . _modules is not None : return self . _modules # make sure module managers are initialized before this # this will also initialize the _modules attribute self . module_managers # noqa return self . _modules # type: ignore @property def default_python_module_manager ( self ) -> PythonModuleManager : if self . _default_python_mgr is None : self . _default_python_mgr = PythonModuleManager () return self . _default_python_mgr @property def default_pipeline_module_manager ( self ) -> PipelineModuleManager : if self . _default_pipeline_mgr is None : self . _default_pipeline_mgr = PipelineModuleManager ( folders = None , ignore_errors = self . _ignore_errors ) return self . _default_pipeline_mgr @property def default_custom_pipelines_manager ( self ) -> PipelineModuleManager : if self . _custom_pipelines_mgr is None : if self . _extra_pipeline_folders : folders = { \"extra\" : self . _extra_pipeline_folders } else : folders = {} self . _custom_pipelines_mgr = PipelineModuleManager ( folders = folders , ignore_errors = self . _ignore_errors ) return self . _custom_pipelines_mgr @property def available_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available modules\"\"\" return sorted ( set ( self . module_map . keys ())) @property def available_non_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return [ module_type for module_type in self . available_module_types if module_type != \"pipeline\" and not self . get_module_class ( module_type ) . is_pipeline () ] @property def available_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return [ module_type for module_type in self . available_module_types if module_type != \"pipeline\" and self . get_module_class ( module_type ) . is_pipeline () ] def get_module_types ( self ) -> typing . Iterable [ str ]: return self . available_module_types def is_pipeline_module ( self , module_type : str ): cls = self . get_module_class ( module_type = module_type ) return cls . is_pipeline () def add_module_manager ( self , module_manager : ModuleManager ): if self . _module_mgrs is None : self . module_managers # noqa for module_type in module_manager . get_module_types (): if module_type in self . module_map . keys (): log . warning ( f \"Duplicate module name ' { module_type } '. Ignoring all but the first.\" ) continue self . module_map [ module_type ] = module_manager self . _module_mgrs . append ( module_manager ) # type: ignore self . _value_types = None def register_pipeline_description ( self , data : typing . Union [ Path , str , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , raise_exception : bool = False , ) -> typing . Optional [ str ]: # making sure the module_map attribute is populated self . module_map # noqa name = self . default_custom_pipelines_manager . register_pipeline ( data = data , module_type_name = module_type_name , namespace = namespace ) if name in self . module_map . keys (): if raise_exception : raise Exception ( f \"Duplicate module name: { name } \" ) log . warning ( f \"Duplicate module name ' { name } '. Ignoring all but the first.\" ) return None else : self . module_map [ name ] = self . default_custom_pipelines_manager return name def get_module_class ( self , module_type : str ) -> typing . Type [ \"KiaraModule\" ]: if module_type == \"pipeline\" : from kiara import PipelineModule return PipelineModule mm = self . module_map . get ( module_type , None ) if mm is None : raise Exception ( f \"No module ' { module_type } ' available.\" ) if module_type in self . _module_cls_cache . keys (): return self . _module_cls_cache [ module_type ] cls = mm . get_module_class ( module_type ) if not hasattr ( cls , \"_module_type_name\" ): raise Exception ( f \"Class does not have a '_module_type_name' attribute: { cls } \" ) assert module_type . endswith ( cls . _module_type_name ) # type: ignore if hasattr ( cls , \"_module_type_id\" ) and cls . _module_type_id != \"pipeline\" and cls . _module_type_id != module_type : # type: ignore raise Exception ( f \"Can't create module class ' { cls } ', it already has a _module_type_id attribute and it's different to the module name ' { module_type } ': { cls . _module_type_id } \" # type: ignore ) self . _module_cls_cache [ module_type ] = cls setattr ( cls , \"_module_type_id\" , module_type ) return cls def create_module ( self , kiara : \"Kiara\" , id : typing . Optional [ str ], module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : mm = self . module_map . get ( module_type , None ) if mm is None : raise Exception ( f \"No module ' { module_type } ' registered. Available modules: { ', ' . join ( self . available_module_types ) } \" ) _ = self . get_module_class ( module_type ) # just to make sure the _module_type_id attribute is added return mm . create_module ( id = id , parent_id = parent_id , module_type = module_type , module_config = module_config , kiara = kiara , ) def find_modules_for_package ( self , package_name : str , include_core_modules : bool = True , include_pipelines : bool = True , ) -> typing . Dict [ str , typing . Type [ \"KiaraModule\" ]]: result = {} for module_type in self . available_module_types : if module_type == \"pipeline\" : continue module_cls = self . get_module_class ( module_type ) module_package = module_cls . get_type_metadata () . context . labels . get ( \"package\" , None ) if module_package != package_name : continue if module_cls . is_pipeline (): if include_pipelines : result [ module_type ] = module_cls else : if include_core_modules : result [ module_type ] = module_cls return result available_module_types : List [ str ] property readonly \u00b6 Return the names of all available modules available_non_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. available_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. MergedModuleManagerConfig ( BaseModel ) pydantic-model \u00b6 Source code in kiara/module_mgmt/merged.py class MergedModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"pipeline\" ] folders : typing . List [ str ] = Field ( description = \"A list of folders that contain pipeline descriptions.\" , default_factory = list , ) @validator ( \"folders\" , pre = True ) def _validate_folders ( cls , v ): if isinstance ( v , str ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( v , Path ): item = v . as_posix () assert isinstance ( item , str ) result . append ( item ) return result folders : List [ str ] pydantic-field \u00b6 A list of folders that contain pipeline descriptions. pipelines \u00b6 PipelineModuleManager ( ModuleManager ) \u00b6 Module manager that discovers pipeline descriptions, and create modules out of them. Parameters: Name Type Description Default folders Optional[Mapping[str, Union[str, pathlib.Path, Iterable[Union[str, pathlib.Path]]]]] a list of folders to search for pipeline descriptions, if 'None', 'kiara.pipelines' entrypoints are searched, as well as the user config None ignore_errors bool ignore any errors that occur during pipeline discovery (as much as that is possible) False Source code in kiara/module_mgmt/pipelines.py class PipelineModuleManager ( ModuleManager ): \"\"\"Module manager that discovers pipeline descriptions, and create modules out of them. Arguments: folders: a list of folders to search for pipeline descriptions, if 'None', 'kiara.pipelines' entrypoints are searched, as well as the user config ignore_errors: ignore any errors that occur during pipeline discovery (as much as that is possible) \"\"\" def __init__ ( self , folders : typing . Optional [ typing . Mapping [ str , typing . Union [ str , Path , typing . Iterable [ typing . Union [ str , Path ]]] ] ] = None , ignore_errors : bool = False , ): if folders is None : from kiara.utils.class_loading import find_all_kiara_pipeline_paths folders_map : typing . Dict [ str , typing . List [ typing . Tuple [ typing . Optional [ str ], str ]] ] = find_all_kiara_pipeline_paths ( skip_errors = ignore_errors ) if os . path . exists ( USER_PIPELINES_FOLDER ): folders_map [ \"user\" ] = [( None , USER_PIPELINES_FOLDER )] elif not folders : folders_map = {} else : assert isinstance ( folders , typing . Mapping ) assert \"user\" not in folders . keys () folders_map = {} for k , _folders in folders . items (): if isinstance ( _folders , str ) or not isinstance ( _folders , typing . Iterable ): raise NotImplementedError ( f \"Invalid folder configuration (must be an iterable): { _folders } \" ) for _f in _folders : # type: ignore if isinstance ( _f , Path ): _f = _f . as_posix () folders_map . setdefault ( k , []) . append (( None , _f )) self . _pipeline_desc_folders : typing . List [ Path ] = [] self . _pipeline_descs : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} self . _cached_classes : typing . Dict [ str , typing . Type [ PipelineModule ]] = {} for ns , paths in folders_map . items (): for path in paths : self . add_pipelines_path ( ns , path [ 1 ], path [ 0 ]) def register_pipeline ( self , data : typing . Union [ str , Path , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , ) -> str : \"\"\"Register a pipeline description to the pipeline pool. Arguments: data: the pipeline data (a dict, or a path to a file) module_type_name: the type name this pipeline should be registered as \"\"\" # TODO: verify that there is no conflict with module_type_name if isinstance ( data , str ): data = Path ( os . path . expanduser ( data )) if isinstance ( data , Path ): _name , _data = get_pipeline_details_from_path ( data ) _data = check_doc_sidecar ( data , _data ) if module_type_name : _name = module_type_name elif isinstance ( data , typing . Mapping ): _data = dict ( data ) if module_type_name : _data [ MODULE_TYPE_NAME_KEY ] = module_type_name _name = _data . get ( MODULE_TYPE_NAME_KEY , None ) if not _name : raise Exception ( f \"Can't register pipeline, no module type name available: { data } \" ) _data = { \"data\" : _data , \"source\" : data , \"source_type\" : \"dict\" } else : raise Exception ( f \"Can't register pipeline, must be dict-like data, not { type ( data ) } \" ) if not _name : raise Exception ( \"No pipeline name set.\" ) if not namespace : full_name = _name else : full_name = f \" { namespace } . { _name } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Can't register pipeline: duplicate workflow name ' { _name } '\" ) self . _pipeline_descs [ full_name ] = _data return full_name def add_pipelines_path ( self , namespace : str , path : typing . Union [ str , Path ], base_module : typing . Optional [ str ], ) -> typing . Iterable [ str ]: \"\"\"Add a pipeline description file or folder containing some to this manager. Arguments: namespace: the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted path: the path to a pipeline description file, or folder which contains some base_module: the base module the assembled pipeline modules under this path will be located at in the Python namespace Returns: a list of module type names that were added \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) elif isinstance ( path , typing . Iterable ): raise TypeError ( f \"Invalid type for path: { path } \" ) if not path . exists (): log . warning ( f \"Can't add pipeline path ' { path } ': path does not exist\" ) return [] elif path . is_dir (): files : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} for root , dirnames , filenames in os . walk ( path , topdown = True ): dirnames [:] = [ d for d in dirnames if d not in DEFAULT_EXCLUDE_DIRS ] for filename in [ f for f in filenames if os . path . isfile ( os . path . join ( root , f )) and any ( f . endswith ( ext ) for ext in VALID_PIPELINE_FILE_EXTENSIONS ) ]: try : full_path = os . path . join ( root , filename ) name , data = get_pipeline_details_from_path ( path = full_path , base_module = base_module ) data = check_doc_sidecar ( full_path , data ) rel_path = os . path . relpath ( os . path . dirname ( full_path ), path ) if not rel_path or rel_path == \".\" : ns_name = name else : _rel_path = rel_path . replace ( os . path . sep , \".\" ) ns_name = f \" { _rel_path } . { name } \" if not ns_name : raise Exception ( f \"Could not determine namespace for pipeline file ' { filename } '.\" ) if ns_name in files . keys (): raise Exception ( f \"Duplicate workflow name in namespace ' { namespace } ': { ns_name } \" ) files [ ns_name ] = data except Exception as e : log . warning ( f \"Ignoring invalid pipeline file ' { full_path } ': { e } \" ) elif path . is_file (): name , data = get_pipeline_details_from_path ( path = path , base_module = base_module ) data = check_doc_sidecar ( path , data ) if not name : raise Exception ( f \"Could not determine pipeline name for: { path } \" ) files = { name : data } result = {} for k , v in files . items (): if namespace . startswith ( \"_\" ): tokens = namespace . split ( \".\" ) if len ( tokens ) == 1 : _namespace = \"\" else : _namespace = \".\" . join ( tokens [ 1 :]) else : _namespace = namespace if not _namespace : full_name = k else : full_name = f \" { _namespace } . { k } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Duplicate workflow name: { name } \" ) result [ full_name ] = v self . _pipeline_descs . update ( result ) return result . keys () @property def pipeline_descs ( self ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: return self . _pipeline_descs def get_module_class ( self , module_type : str ) -> typing . Type [ \"PipelineModule\" ]: if module_type in self . _cached_classes . keys (): return self . _cached_classes [ module_type ] desc = self . _pipeline_descs . get ( module_type , None ) if desc is None : raise Exception ( f \"No pipeline with name ' { module_type } ' available.\" ) tokens = re . split ( r \"\\.|_\" , module_type ) cls_name = \"\" . join ( x . capitalize () or \"_\" for x in tokens ) if len ( tokens ) != 1 : full_name = \".\" . join ( tokens [ 0 : - 1 ] + [ cls_name ]) else : full_name = cls_name base_module = desc . get ( \"base_module\" , None ) cls = create_pipeline_class ( f \" { cls_name } PipelineModule\" , full_name , desc [ \"data\" ], base_module = base_module , ) setattr ( cls , \"_module_type_name\" , module_type ) self . _cached_classes [ module_type ] = cls return self . _cached_classes [ module_type ] def get_module_types ( self ) -> typing . Iterable [ str ]: return self . _pipeline_descs . keys () add_pipelines_path ( self , namespace , path , base_module ) \u00b6 Add a pipeline description file or folder containing some to this manager. Parameters: Name Type Description Default namespace str the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted required path Union[str, pathlib.Path] the path to a pipeline description file, or folder which contains some required base_module Optional[str] the base module the assembled pipeline modules under this path will be located at in the Python namespace required Returns: Type Description Iterable[str] a list of module type names that were added Source code in kiara/module_mgmt/pipelines.py def add_pipelines_path ( self , namespace : str , path : typing . Union [ str , Path ], base_module : typing . Optional [ str ], ) -> typing . Iterable [ str ]: \"\"\"Add a pipeline description file or folder containing some to this manager. Arguments: namespace: the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted path: the path to a pipeline description file, or folder which contains some base_module: the base module the assembled pipeline modules under this path will be located at in the Python namespace Returns: a list of module type names that were added \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) elif isinstance ( path , typing . Iterable ): raise TypeError ( f \"Invalid type for path: { path } \" ) if not path . exists (): log . warning ( f \"Can't add pipeline path ' { path } ': path does not exist\" ) return [] elif path . is_dir (): files : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} for root , dirnames , filenames in os . walk ( path , topdown = True ): dirnames [:] = [ d for d in dirnames if d not in DEFAULT_EXCLUDE_DIRS ] for filename in [ f for f in filenames if os . path . isfile ( os . path . join ( root , f )) and any ( f . endswith ( ext ) for ext in VALID_PIPELINE_FILE_EXTENSIONS ) ]: try : full_path = os . path . join ( root , filename ) name , data = get_pipeline_details_from_path ( path = full_path , base_module = base_module ) data = check_doc_sidecar ( full_path , data ) rel_path = os . path . relpath ( os . path . dirname ( full_path ), path ) if not rel_path or rel_path == \".\" : ns_name = name else : _rel_path = rel_path . replace ( os . path . sep , \".\" ) ns_name = f \" { _rel_path } . { name } \" if not ns_name : raise Exception ( f \"Could not determine namespace for pipeline file ' { filename } '.\" ) if ns_name in files . keys (): raise Exception ( f \"Duplicate workflow name in namespace ' { namespace } ': { ns_name } \" ) files [ ns_name ] = data except Exception as e : log . warning ( f \"Ignoring invalid pipeline file ' { full_path } ': { e } \" ) elif path . is_file (): name , data = get_pipeline_details_from_path ( path = path , base_module = base_module ) data = check_doc_sidecar ( path , data ) if not name : raise Exception ( f \"Could not determine pipeline name for: { path } \" ) files = { name : data } result = {} for k , v in files . items (): if namespace . startswith ( \"_\" ): tokens = namespace . split ( \".\" ) if len ( tokens ) == 1 : _namespace = \"\" else : _namespace = \".\" . join ( tokens [ 1 :]) else : _namespace = namespace if not _namespace : full_name = k else : full_name = f \" { _namespace } . { k } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Duplicate workflow name: { name } \" ) result [ full_name ] = v self . _pipeline_descs . update ( result ) return result . keys () register_pipeline ( self , data , module_type_name = None , namespace = None ) \u00b6 Register a pipeline description to the pipeline pool. Parameters: Name Type Description Default data Union[str, pathlib.Path, Mapping[str, Any]] the pipeline data (a dict, or a path to a file) required module_type_name Optional[str] the type name this pipeline should be registered as None Source code in kiara/module_mgmt/pipelines.py def register_pipeline ( self , data : typing . Union [ str , Path , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , ) -> str : \"\"\"Register a pipeline description to the pipeline pool. Arguments: data: the pipeline data (a dict, or a path to a file) module_type_name: the type name this pipeline should be registered as \"\"\" # TODO: verify that there is no conflict with module_type_name if isinstance ( data , str ): data = Path ( os . path . expanduser ( data )) if isinstance ( data , Path ): _name , _data = get_pipeline_details_from_path ( data ) _data = check_doc_sidecar ( data , _data ) if module_type_name : _name = module_type_name elif isinstance ( data , typing . Mapping ): _data = dict ( data ) if module_type_name : _data [ MODULE_TYPE_NAME_KEY ] = module_type_name _name = _data . get ( MODULE_TYPE_NAME_KEY , None ) if not _name : raise Exception ( f \"Can't register pipeline, no module type name available: { data } \" ) _data = { \"data\" : _data , \"source\" : data , \"source_type\" : \"dict\" } else : raise Exception ( f \"Can't register pipeline, must be dict-like data, not { type ( data ) } \" ) if not _name : raise Exception ( \"No pipeline name set.\" ) if not namespace : full_name = _name else : full_name = f \" { namespace } . { _name } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Can't register pipeline: duplicate workflow name ' { _name } '\" ) self . _pipeline_descs [ full_name ] = _data return full_name PipelineModuleManagerConfig ( BaseModel ) pydantic-model \u00b6 Source code in kiara/module_mgmt/pipelines.py class PipelineModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"pipeline\" ] folders : typing . List [ str ] = Field ( description = \"A list of folders that contain pipeline descriptions.\" , default_factory = list , ) @validator ( \"folders\" , pre = True ) def _validate_folders ( cls , v ): if isinstance ( v , str ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( v , Path ): item = v . as_posix () assert isinstance ( item , str ) result . append ( item ) return result folders : List [ str ] pydantic-field \u00b6 A list of folders that contain pipeline descriptions. get_pipeline_details_from_path ( path , module_type_name = None , base_module = None ) \u00b6 Load a pipeline description, save it's content, and determine it the pipeline base name. Parameters: Name Type Description Default path Union[str, pathlib.Path] the path to the pipeline file required module_type_name Optional[str] if specifies, overwrites any auto-detected or assigned pipeline name None base_module Optional[str] overrides the base module the assembled pipeline module will be located in the python hierarchy None Source code in kiara/module_mgmt/pipelines.py def get_pipeline_details_from_path ( path : typing . Union [ str , Path ], module_type_name : typing . Optional [ str ] = None , base_module : typing . Optional [ str ] = None , ) -> typing . Tuple [ typing . Optional [ str ], typing . Mapping [ str , typing . Any ]]: \"\"\"Load a pipeline description, save it's content, and determine it the pipeline base name. Arguments: path: the path to the pipeline file module_type_name: if specifies, overwrites any auto-detected or assigned pipeline name base_module: overrides the base module the assembled pipeline module will be located in the python hierarchy \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) if not path . is_file (): raise Exception ( f \"Can't add pipeline description ' { path . as_posix () } ': not a file\" ) data = get_data_from_file ( path ) if not data : raise Exception ( f \"Can't register pipeline file ' { path . as_posix () } ': no content.\" ) if module_type_name : data [ MODULE_TYPE_NAME_KEY ] = module_type_name filename = path . name if not isinstance ( data , typing . Mapping ): raise Exception ( \"Not a dictionary type.\" ) name = data . get ( MODULE_TYPE_NAME_KEY , None ) if name is None : name = filename . split ( \".\" , maxsplit = 1 )[ 0 ] result = { \"data\" : data , \"source\" : path . as_posix (), \"source_type\" : \"file\" } if base_module : result [ \"base_module\" ] = base_module return ( name , result ) python_classes \u00b6 PythonModuleManagerConfig ( BaseModel ) pydantic-model \u00b6 Source code in kiara/module_mgmt/python_classes.py class PythonModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"python\" ] module_classes : typing . Dict [ str , typing . Type [ \"KiaraModule\" ]] = Field ( description = \"The module classes this manager should hold.\" ) @validator ( \"module_classes\" , pre = True ) def _ensure_module_class_types ( cls , v ): _classes = [] if v : for _cls in v : if isinstance ( _cls , str ): try : module_name , cls_name = _cls . rsplit ( \".\" , maxsplit = 1 ) module = __import__ ( module_name ) _cls = getattr ( module , cls_name ) except Exception : raise ValueError ( f \"Can't parse value ' { _cls } ' into KiaraModule class.\" ) from kiara import KiaraModule # noqa if not issubclass ( _cls , KiaraModule ): raise ValueError ( f \"Not a KiaraModule sub-class: { _cls } \" ) _classes . append ( _cls ) return _classes module_classes : Dict [ str , Type [ KiaraModule ]] pydantic-field required \u00b6 The module classes this manager should hold. modules special \u00b6 Base module under which the 'official' KiaraModule implementations live. metadata \u00b6 ExtractPythonClass ( ExtractMetadataModule ) \u00b6 Extract metadata about the Python type of a value. Source code in kiara/modules/metadata.py class ExtractPythonClass ( ExtractMetadataModule ): \"\"\"Extract metadata about the Python type of a value.\"\"\" _module_type_name = \"metadata.python_class\" @classmethod def _get_supported_types ( cls ) -> typing . Union [ str , typing . Iterable [ str ]]: return \"*\" @classmethod def get_metadata_key ( cls ) -> str : return \"python_class\" def _get_metadata_schema ( self , type : str ) -> typing . Union [ str , typing . Type [ BaseModel ]]: return PythonClassMetadata def extract_metadata ( self , value : Value ) -> typing . Mapping [ str , typing . Any ]: item = value . get_value_data () cls = item . __class__ return { \"class_name\" : cls . __name__ , \"module_name\" : cls . __module__ , \"full_name\" : f \" { cls . __module__ } . { cls . __name__ } \" , } pipelines special \u00b6 Base module that holds PipelineModule classes that are auto-generated from pipeline descriptions in the pipelines folder. type_conversion \u00b6 OldTypeConversionModule ( KiaraModule ) \u00b6 Source code in kiara/modules/type_conversion.py class OldTypeConversionModule ( KiaraModule ): _config_cls = TypeConversionModuleConfig @classmethod @abc . abstractmethod def _get_supported_source_types ( self ) -> typing . Union [ typing . Iterable [ str ], str ]: pass @classmethod @abc . abstractmethod def _get_target_types ( self ) -> typing . Union [ typing . Iterable [ str ], str ]: pass @classmethod def get_supported_source_types ( self ) -> typing . Set [ str ]: _types : typing . Iterable [ str ] = self . _get_supported_source_types () if isinstance ( _types , str ): _types = [ _types ] if \"config\" in _types : raise Exception ( \"Invalid source type, type name 'config' is invalid.\" ) return set ( _types ) @classmethod def get_supported_target_types ( self ) -> typing . Set [ str ]: _types : typing . Iterable [ str ] = self . _get_target_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @property def source_type ( self ) -> str : data_type = self . get_config_value ( \"source_type\" ) supported = self . get_supported_source_types () if \"*\" not in supported and data_type not in supported : raise ValueError ( f \"Invalid module configuration, source type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_source_types ()) } .\" ) return data_type @property def target_type ( self ) -> str : data_type = self . get_config_value ( \"target_type\" ) if data_type not in self . get_supported_target_types (): raise ValueError ( f \"Invalid module configuration, target type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_target_types ()) } .\" ) return data_type def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"source_value\" : { \"type\" : self . source_type , \"doc\" : f \"A value of type ' { self . source_type } '.\" , }, \"config\" : { \"type\" : \"dict\" , \"doc\" : \"The configuration for the transformation.\" , \"optional\" : True , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"target_value\" : { \"type\" : self . target_type , \"doc\" : f \"A value of type ' { self . target_type } '.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value = inputs . get_value_obj ( \"source_value\" ) if value . value_schema . type != self . source_type : raise KiaraProcessingException ( f \"Can't convert value of source type ' { value . value_schema . type } '. Expected type ' { self . source_type } '.\" ) config = inputs . get_value_data ( \"config\" ) if config is None : config = {} target_value = self . convert ( value = value , config = config ) # TODO: validate value? outputs . set_value ( \"target_value\" , target_value ) @abc . abstractmethod def convert ( self , value : Value , config : typing . Mapping [ str , typing . Any ] ) -> typing . Any : pass create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/type_conversion.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"source_value\" : { \"type\" : self . source_type , \"doc\" : f \"A value of type ' { self . source_type } '.\" , }, \"config\" : { \"type\" : \"dict\" , \"doc\" : \"The configuration for the transformation.\" , \"optional\" : True , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/type_conversion.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"target_value\" : { \"type\" : self . target_type , \"doc\" : f \"A value of type ' { self . target_type } '.\" , } } return outputs TypeConversionModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/modules/type_conversion.py class TypeConversionModuleConfig ( ModuleTypeConfigSchema ): source_type : str = Field ( description = \"The source type.\" ) target_type : str = Field ( description = \"The target type.\" ) source_type : str pydantic-field required \u00b6 The source type. target_type : str pydantic-field required \u00b6 The target type. operations special \u00b6 Operation ( ModuleConfig ) pydantic-model \u00b6 Source code in kiara/operations/__init__.py class Operation ( ModuleConfig ): @classmethod def create_operation ( cls , kiara : \"Kiara\" , operation_id : str , config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , ) -> \"Operation\" : _config = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) _config_dict = _config . dict () _config_dict [ \"id\" ] = operation_id op_config = cls ( ** _config_dict ) op_config . _kiara = kiara return op_config _kiara : typing . Optional [ \"Kiara\" ] = PrivateAttr ( default = None ) _module : typing . Optional [ \"KiaraModule\" ] id : str = Field ( description = \"The operation id.\" ) type_category : typing . Optional [ str ] = Field ( description = \"The operation category this belongs to.\" , default = None ) @property def kiara ( self ) -> \"Kiara\" : if self . _kiara is None : raise Exception ( \"Kiara context not set for operation.\" ) return self . _kiara @property def module ( self ) -> \"KiaraModule\" : if self . _module is None : self . _module = self . create_module ( kiara = self . kiara ) return self . _module @property def input_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . module . input_schemas @property def output_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . module . output_schemas @property def module_cls ( self ) -> typing . Type [ \"KiaraModule\" ]: return self . kiara . get_module_class ( self . module_type ) def run ( self , _attach_lineage : bool = True , ** inputs : typing . Any ) -> ValueSet : return self . module . run ( _attach_lineage = _attach_lineage , ** inputs ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code \"\"\" include_full_doc = config . get ( \"include_full_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . doc : if include_full_doc : table . add_row ( \"Documentation\" , self . doc . full_doc ) else : table . add_row ( \"Description\" , self . doc . description ) module_type_md = self . module . get_type_metadata () table . add_row ( \"Module type\" , self . module_type ) conf = Syntax ( json . dumps ( self . module_config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Module config\" , conf ) constants = self . module_config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . module . input_schemas , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . module . output_schemas , ) table . add_row ( \"Outputs\" , outputs_table ) m_md_o = module_type_md . origin . create_renderable () m_md_c = module_type_md . context . create_renderable () m_md = RenderGroup ( m_md_o , m_md_c ) table . add_row ( \"Module metadata\" , m_md ) if config . get ( \"include_src\" , False ): table . add_row ( \"Source code\" , module_type_md . process_src ) return table id : str pydantic-field required \u00b6 The operation id. type_category : str pydantic-field \u00b6 The operation category this belongs to. create_renderable ( self , ** config ) \u00b6 Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code Source code in kiara/operations/__init__.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code \"\"\" include_full_doc = config . get ( \"include_full_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . doc : if include_full_doc : table . add_row ( \"Documentation\" , self . doc . full_doc ) else : table . add_row ( \"Description\" , self . doc . description ) module_type_md = self . module . get_type_metadata () table . add_row ( \"Module type\" , self . module_type ) conf = Syntax ( json . dumps ( self . module_config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Module config\" , conf ) constants = self . module_config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . module . input_schemas , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . module . output_schemas , ) table . add_row ( \"Outputs\" , outputs_table ) m_md_o = module_type_md . origin . create_renderable () m_md_c = module_type_md . context . create_renderable () m_md = RenderGroup ( m_md_o , m_md_c ) table . add_row ( \"Module metadata\" , m_md ) if config . get ( \"include_src\" , False ): table . add_row ( \"Source code\" , module_type_md . process_src ) return table calculate_hash \u00b6 CalculateHashOperationType ( OperationType ) \u00b6 Operation type to calculate hash(es) for data. This is mostly used internally, so users usually won't be exposed to operations of this type. Source code in kiara/operations/calculate_hash.py class CalculateHashOperationType ( OperationType ): \"\"\"Operation type to calculate hash(es) for data. This is mostly used internally, so users usually won't be exposed to operations of this type. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return op_config . module_cls == CalculateValueHashModule def get_hash_operations_for_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: result = {} for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] == value_type : result [ op_config . module_config [ \"hash_type\" ]] = op_config return result CalculateValueHashModule ( KiaraModule ) \u00b6 Calculate the hash of a value. Source code in kiara/operations/calculate_hash.py class CalculateValueHashModule ( KiaraModule ): \"\"\"Calculate the hash of a value.\"\"\" _module_type_name = \"value.hash\" _config_cls = CalculateValueHashesConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type in kiara . type_mgmt . value_types . items (): hash_types = value_type . get_supported_hash_types () for ht in hash_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"hash_type\" : ht }, \"doc\" : f \"Calculate ' { ht } ' hash for value type ' { value_type_name } '.\" , } all_metadata_profiles [ f \"calculate_hash. { ht } .for. { value_type_name } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , } } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"hash\" : { \"type\" : \"string\" , \"doc\" : \"The hash string.\" }} def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" value : Value = inputs . get_value_obj ( input_name ) value_hash = value . get_hash ( hash_type = self . get_config_value ( \"hash_type\" )) outputs . set_value ( \"hash\" , value_hash . hash ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/calculate_hash.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , } } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/calculate_hash.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"hash\" : { \"type\" : \"string\" , \"doc\" : \"The hash string.\" }} retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/calculate_hash.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type in kiara . type_mgmt . value_types . items (): hash_types = value_type . get_supported_hash_types () for ht in hash_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"hash_type\" : ht }, \"doc\" : f \"Calculate ' { ht } ' hash for value type ' { value_type_name } '.\" , } all_metadata_profiles [ f \"calculate_hash. { ht } .for. { value_type_name } \" ] = op_config return all_metadata_profiles CalculateValueHashesConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/calculate_hash.py class CalculateValueHashesConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to calculate the hash for.\" ) hash_type : str = Field ( description = \"The hash type.\" ) hash_type : str pydantic-field required \u00b6 The hash type. value_type : str pydantic-field required \u00b6 The type of the value to calculate the hash for. create_value \u00b6 CreateValueModule ( KiaraModule ) \u00b6 Base class for 'create' value type operations. Source code in kiara/operations/create_value.py class CreateValueModule ( KiaraModule ): \"\"\"Base class for 'create' value type operations.\"\"\" _config_cls = CreateValueModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} # value_types: typing.Iterable[str] = cls.get_supported_value_types() # if \"*\" in value_types: # value_types = kiara.type_mgmt.value_type_names # # for value_type in value_types: target_type = cls . get_target_value_type () if target_type not in kiara . type_mgmt . value_type_names : raise Exception ( f \"Can't assemble type convert operations for target type ' { target_type } ': type not available\" ) for source_profile in cls . get_source_value_profiles (): mod_conf = { \"source_profile\" : source_profile , \"target_type\" : target_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Create a ' { target_type } ' value from a ' { source_profile } '.\" , } # key = f\"{value_type}.convert_from.{target_type}\" # type: ignore # if key in all_metadata_profiles.keys(): # raise Exception(f\"Duplicate profile key: {key}\") # all_metadata_profiles[key] = op_config key = f \"create. { target_type } .from. { source_profile } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles # @classmethod # def get_supported_value_types(cls) -> typing.Set[str]: # # _types = cls._get_supported_value_types() # if isinstance(_types, str): # _types = [_types] # # return set(_types) # @classmethod @abc . abstractmethod def get_target_value_type ( cls ) -> str : pass @classmethod def get_source_value_profiles ( cls ) -> typing . Iterable [ str ]: # supported = cls.get_supported_value_types() types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"from_\" ) and callable ( attr ): v_type = attr_name [ 5 :] # if v_type in supported: # raise Exception( # f\"Invalid configuration for type conversion module class {cls.__name__}: conversion source type can't be '{v_type}' because this is already registered as a supported type of the class.\" # ) types . append ( v_type ) return types # @classmethod # def get_target_value_type(cls) -> str: # # supported = cls.get_supported_value_types() # # types = [] # for attr_name, attr in cls.__dict__.items(): # # if attr_name.startswith(\"to_\") and callable(attr): # v_type = attr_name[3:] # if v_type in supported: # raise Exception( # f\"Invalid configuration for type conversion module class {cls.__name__}: conversion target type can't be '{v_type}' because this is already registered as a supported type of the class.\" # ) # types.append(attr_name[3:]) # # return types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) source_config = dict ( self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) ) source_config [ \"doc\" ] = f \"The ' { source_profile } ' value to be converted.\" schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { source_profile : source_config } if self . get_config_value ( \"allow_none_input\" ): schema [ source_config [ \"type\" ]][ \"optional\" ] = True return schema def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) target_type = self . get_config_value ( \"target_type\" ) schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { target_type : { \"type\" : target_type , \"doc\" : f \"The converted ' { source_profile } ' value as ' { target_type } '.\" , } } if self . get_config_value ( \"allow_none_input\" ): schema [ target_type ][ \"optional\" ] = True return schema def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : source_profile : str = self . get_config_value ( \"source_profile\" ) source_config : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) source_type = source_config [ \"type\" ] target_type : str = self . get_config_value ( \"target_type\" ) allow_none : bool = self . get_config_value ( \"allow_none_input\" ) source : Value = inputs . get_value_obj ( source_profile ) if source_type != source . type_name : raise KiaraProcessingException ( f \"Invalid type ( { source . type_name } ) of source value: expected ' { source_type } ' (source profile name: { source_profile } ).\" ) if not source . is_set or source . is_none : if allow_none : outputs . set_value ( \"value_item\" , None ) return else : raise KiaraProcessingException ( \"No source value set.\" ) if not hasattr ( self , f \"from_ { source_profile } \" ): raise Exception ( f \"Module ' { self . _module_type_id } ' can't convert ' { source_type } ' into ' { target_type } ': missing method 'from_ { source_type } '. This is a bug.\" # type: ignore ) func = getattr ( self , f \"from_ { source_profile } \" ) converted = func ( source ) outputs . set_value ( target_type , converted ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/create_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) source_config = dict ( self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) ) source_config [ \"doc\" ] = f \"The ' { source_profile } ' value to be converted.\" schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { source_profile : source_config } if self . get_config_value ( \"allow_none_input\" ): schema [ source_config [ \"type\" ]][ \"optional\" ] = True return schema create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/create_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) target_type = self . get_config_value ( \"target_type\" ) schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { target_type : { \"type\" : target_type , \"doc\" : f \"The converted ' { source_profile } ' value as ' { target_type } '.\" , } } if self . get_config_value ( \"allow_none_input\" ): schema [ target_type ][ \"optional\" ] = True return schema retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/create_value.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} # value_types: typing.Iterable[str] = cls.get_supported_value_types() # if \"*\" in value_types: # value_types = kiara.type_mgmt.value_type_names # # for value_type in value_types: target_type = cls . get_target_value_type () if target_type not in kiara . type_mgmt . value_type_names : raise Exception ( f \"Can't assemble type convert operations for target type ' { target_type } ': type not available\" ) for source_profile in cls . get_source_value_profiles (): mod_conf = { \"source_profile\" : source_profile , \"target_type\" : target_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Create a ' { target_type } ' value from a ' { source_profile } '.\" , } # key = f\"{value_type}.convert_from.{target_type}\" # type: ignore # if key in all_metadata_profiles.keys(): # raise Exception(f\"Duplicate profile key: {key}\") # all_metadata_profiles[key] = op_config key = f \"create. { target_type } .from. { source_profile } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles CreateValueModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/create_value.py class CreateValueModuleConfig ( ModuleTypeConfigSchema ): source_profile : str = Field ( description = \"The profile of the source value.\" ) target_type : str = Field ( description = \"The type of the value to convert to.\" ) allow_none_input : bool = Field ( description = \"Whether to allow 'none' source values, if one is encountered 'none' is returned.\" , default = False , ) allow_none_input : bool pydantic-field \u00b6 Whether to allow 'none' source values, if one is encountered 'none' is returned. source_profile : str pydantic-field required \u00b6 The profile of the source value. target_type : str pydantic-field required \u00b6 The type of the value to convert to. CreateValueOperationType ( OperationType ) \u00b6 Operations that create values of specific types from values of certain value profiles. In most cases, source profiles will be 'file' or 'file_bundle' values of some sort, and the created values will exhibit more inherent structure and stricter specs than their sources. The 'create' operation type differs from 'import' in that it expects values that are already onboarded and are all information in the value is stored in a kiara data registry (in most cases the kiara data store ). Source code in kiara/operations/create_value.py class CreateValueOperationType ( OperationType ): \"\"\"Operations that create values of specific types from values of certain value profiles. In most cases, source profiles will be 'file' or 'file_bundle' values of some sort, and the created values will exhibit more inherent structure and stricter specs than their sources. The 'create' operation type differs from 'import' in that it expects values that are already onboarded and are all information in the value is stored in a *kiara* data registry (in most cases the *kiara data store*). \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , CreateValueModule ) # def get_operations_for_source_type( # self, value_type: str # ) -> typing.Dict[str, Operation]: # \"\"\"Find all operations that transform from the specified type. # # The result dict uses the target type of the conversion as key, and the operation itself as value. # \"\"\" # # result: typing.Dict[str, Operation] = {} # for o_id, op in self.operations.items(): # source_type = op.module_config[\"source_type\"] # if source_type == value_type: # target_type = op.module_config[\"target_type\"] # if target_type in result.keys(): # raise Exception( # f\"Multiple operations to transform from '{source_type}' to {target_type}\" # ) # result[target_type] = op # # return result def get_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): target_type = op . module_config [ \"target_type\" ] if target_type == value_type : source_type = op . module_config [ \"source_type\" ] if source_type in result . keys (): raise Exception ( f \"Multiple operations to transform from ' { source_type } ' to { target_type } \" ) result [ source_type ] = op return result get_operations_for_target_type ( self , value_type ) \u00b6 Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. Source code in kiara/operations/create_value.py def get_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): target_type = op . module_config [ \"target_type\" ] if target_type == value_type : source_type = op . module_config [ \"source_type\" ] if source_type in result . keys (): raise Exception ( f \"Multiple operations to transform from ' { source_type } ' to { target_type } \" ) result [ source_type ] = op return result data_export \u00b6 DataExportModule ( KiaraModule ) \u00b6 Source code in kiara/operations/data_export.py class DataExportModule ( KiaraModule ): _config_cls = DataExportModuleConfig @classmethod @abc . abstractmethod def get_source_value_type ( cls ) -> str : pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_source_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data export operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"export_as__\" ): continue target_profile = attr [ 11 :] op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_type\" : sup_type , \"target_profile\" : target_profile , }, \"doc\" : f \"Export data of type ' { sup_type } ' as: { target_profile } .\" , } all_metadata_profiles [ f \"export. { sup_type } .as. { target_profile } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_type = self . get_config_value ( \"source_type\" ) inputs : typing . Dict [ str , typing . Any ] = { source_type : { \"type\" : source_type , \"doc\" : f \"A value of type ' { source_type } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The directory to export the file(s) to.\" , \"optional\" : True , }, \"name\" : { \"type\" : \"string\" , \"doc\" : \"The (base) name of the exported file(s).\" , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"export_details\" : { \"type\" : \"dict\" , \"doc\" : \"Details about the exported files/folders.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : target_profile : str = self . get_config_value ( \"target_profile\" ) source_type : str = self . get_config_value ( \"source_type\" ) source = inputs . get_value_data ( source_type ) func_name = f \"export_as__ { target_profile } \" if not hasattr ( self , func_name ): raise Exception ( f \"Can't export ' { source_type } ' value: missing function ' { func_name } ' in class ' { self . __class__ . __name__ } '. Please check this modules documentation or source code to determine which source types and profiles are supported.\" ) base_path = inputs . get_value_data ( \"base_path\" ) if base_path is None : base_path = os . getcwd () name = inputs . get_value_data ( \"name\" ) func = getattr ( self , func_name ) # TODO: check signature? base_path = os . path . abspath ( base_path ) os . makedirs ( base_path , exist_ok = True ) result = func ( value = source , base_path = base_path , name = name ) # schema = ValueSchema(type=self.get_target_value_type(), doc=\"Imported dataset.\") # value_lineage = ValueLineage.from_module_and_inputs( # module=self, output_name=output_key, inputs=inputs # ) # value: Value = self._kiara.data_registry.register_data( # value_data=result, value_schema=schema, lineage=None # ) outputs . set_value ( \"export_details\" , result ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_export.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_type = self . get_config_value ( \"source_type\" ) inputs : typing . Dict [ str , typing . Any ] = { source_type : { \"type\" : source_type , \"doc\" : f \"A value of type ' { source_type } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The directory to export the file(s) to.\" , \"optional\" : True , }, \"name\" : { \"type\" : \"string\" , \"doc\" : \"The (base) name of the exported file(s).\" , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_export.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"export_details\" : { \"type\" : \"dict\" , \"doc\" : \"Details about the exported files/folders.\" , } } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/data_export.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_source_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data export operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"export_as__\" ): continue target_profile = attr [ 11 :] op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_type\" : sup_type , \"target_profile\" : target_profile , }, \"doc\" : f \"Export data of type ' { sup_type } ' as: { target_profile } .\" , } all_metadata_profiles [ f \"export. { sup_type } .as. { target_profile } \" ] = op_config return all_metadata_profiles DataExportModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/data_export.py class DataExportModuleConfig ( ModuleTypeConfigSchema ): target_profile : str = Field ( description = \"The name of the target profile. Used to distinguish different target formats for the same data type.\" ) source_type : str = Field ( description = \"The type of the source data that is going to be exported.\" ) source_type : str pydantic-field required \u00b6 The type of the source data that is going to be exported. target_profile : str pydantic-field required \u00b6 The name of the target profile. Used to distinguish different target formats for the same data type. ExportDataOperationType ( OperationType ) \u00b6 Export data from kiara . Operations of this type use internally handled datasets, and export it to the local file system. Export operations are created by implementing a class that inherits from DataExportModule , kiara will register it under an operation id following this template: <SOURCE_DATA_TYPE>.export_as.<EXPORT_PROFILE> The meaning of the templated fields is: EXPORTED_DATA_TYPE : the data type of the value to export EXPORT_PROFILE : a short, free-form description of the format the data will be exported as Source code in kiara/operations/data_export.py class ExportDataOperationType ( OperationType ): \"\"\"Export data from *kiara*. Operations of this type use internally handled datasets, and export it to the local file system. Export operations are created by implementing a class that inherits from [DataExportModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.data_export/#kiara.operations.data_export.DataExportModule), *kiara* will register it under an operation id following this template: ``` <SOURCE_DATA_TYPE>.export_as.<EXPORT_PROFILE> ``` The meaning of the templated fields is: - `EXPORTED_DATA_TYPE`: the data type of the value to export - `EXPORT_PROFILE`: a short, free-form description of the format the data will be exported as \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : match = issubclass ( op_config . module_cls , DataExportModule ) return match def get_export_operations_per_source_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_source_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] target_profile = op_config . module_config [ \"target_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ target_profile ] = op_config return result def get_export_operations_for_source_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_export_operations_per_source_type () . get ( value_type , {}) get_export_operations_for_source_type ( self , value_type ) \u00b6 Return all available import operations that produce data of the specified type. Source code in kiara/operations/data_export.py def get_export_operations_for_source_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_export_operations_per_source_type () . get ( value_type , {}) get_export_operations_per_source_type ( self ) \u00b6 Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. Source code in kiara/operations/data_export.py def get_export_operations_per_source_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_source_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] target_profile = op_config . module_config [ \"target_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ target_profile ] = op_config return result FileBundleImportModule ( DataExportModule ) \u00b6 Import a file, optionally saving it to the data store. Source code in kiara/operations/data_export.py class FileBundleImportModule ( DataExportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file_bundle\" FileExportModule ( DataExportModule ) \u00b6 Import a file, optionally saving it to the data store. Source code in kiara/operations/data_export.py class FileExportModule ( DataExportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file\" data_import \u00b6 DataImportModule ( KiaraModule ) \u00b6 Source code in kiara/operations/data_import.py class DataImportModule ( KiaraModule ): _config_cls = DataImportModuleConfig @classmethod @abc . abstractmethod def get_target_value_type ( cls ) -> str : pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_target_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data import operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"import_from__\" ): continue tokens = attr [ 13 :] . rsplit ( \"__\" , maxsplit = 1 ) if len ( tokens ) != 2 : log_message ( f \"Can't determine source name and type from string in module { cls . _module_type_id } , ignoring method: { attr } \" # type: ignore ) source_profile , source_type = tokens op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_profile\" : source_profile , \"source_type\" : source_type , }, \"doc\" : f \"Import data of type ' { sup_type } ' from a { source_profile } { source_type } and save it to the kiara data store.\" , } all_metadata_profiles [ f \"import. { sup_type } .from. { source_profile } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"source_profile\" ) inputs : typing . Dict [ str , typing . Any ] = { input_name : { \"type\" : self . get_config_value ( \"source_type\" ), \"doc\" : f \"A { self . get_config_value ( 'source_profile' ) } ' { self . get_config_value ( 'source_type' ) } ' value.\" , }, } # allow_save = self.get_config_value(\"allow_save_input\") # save_default = self.get_config_value(\"save_default\") # if allow_save: # inputs[\"save\"] = { # \"type\": \"boolean\", # \"doc\": \"Whether to save the imported value, or not.\", # \"default\": save_default, # } # # allow_aliases: typing.Optional[bool] = self.get_config_value( # \"allow_aliases_input\" # ) # if allow_aliases is None: # allow_aliases = allow_save # # if allow_aliases and not allow_save and not save_default: # raise Exception( # \"Invalid module configuration: allowing aliases input does not make sense if save is disabled.\" # ) # # if allow_aliases: # default_aliases = self.get_config_value(\"aliases_default\") # inputs[\"aliases\"] = { # \"type\": \"list\", # \"doc\": \"A list of aliases to use when storing the value (only applicable if 'save' is set).\", # \"default\": default_aliases, # } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: output_name = self . get_target_value_type () if output_name == \"any\" : output_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { output_name : { \"type\" : self . get_target_value_type (), \"doc\" : f \"The imported { self . get_target_value_type () } value.\" , }, } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : source_profile : str = self . get_config_value ( \"source_profile\" ) source_type : str = self . get_config_value ( \"source_type\" ) source = inputs . get_value_data ( source_profile ) if self . get_target_value_type () == \"any\" : output_key : str = \"value_item\" else : output_key = self . get_target_value_type () func_name = f \"import_from__ { source_profile } __ { source_type } \" if not hasattr ( self , func_name ): raise Exception ( f \"Can't import ' { source_type } ' value: missing function ' { func_name } ' in class ' { self . __class__ . __name__ } '. Please check this modules documentation or source code to determine which source types and profiles are supported.\" ) func = getattr ( self , func_name ) # TODO: check signature? result = func ( source ) # schema = ValueSchema(type=self.get_target_value_type(), doc=\"Imported dataset.\") # value_lineage = ValueLineage.from_module_and_inputs( # module=self, output_name=output_key, inputs=inputs # ) # value: Value = self._kiara.data_registry.register_data( # value_data=result, value_schema=schema, lineage=None # ) outputs . set_value ( output_key , result ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_import.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"source_profile\" ) inputs : typing . Dict [ str , typing . Any ] = { input_name : { \"type\" : self . get_config_value ( \"source_type\" ), \"doc\" : f \"A { self . get_config_value ( 'source_profile' ) } ' { self . get_config_value ( 'source_type' ) } ' value.\" , }, } # allow_save = self.get_config_value(\"allow_save_input\") # save_default = self.get_config_value(\"save_default\") # if allow_save: # inputs[\"save\"] = { # \"type\": \"boolean\", # \"doc\": \"Whether to save the imported value, or not.\", # \"default\": save_default, # } # # allow_aliases: typing.Optional[bool] = self.get_config_value( # \"allow_aliases_input\" # ) # if allow_aliases is None: # allow_aliases = allow_save # # if allow_aliases and not allow_save and not save_default: # raise Exception( # \"Invalid module configuration: allowing aliases input does not make sense if save is disabled.\" # ) # # if allow_aliases: # default_aliases = self.get_config_value(\"aliases_default\") # inputs[\"aliases\"] = { # \"type\": \"list\", # \"doc\": \"A list of aliases to use when storing the value (only applicable if 'save' is set).\", # \"default\": default_aliases, # } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_import.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: output_name = self . get_target_value_type () if output_name == \"any\" : output_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { output_name : { \"type\" : self . get_target_value_type (), \"doc\" : f \"The imported { self . get_target_value_type () } value.\" , }, } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/data_import.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_target_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data import operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"import_from__\" ): continue tokens = attr [ 13 :] . rsplit ( \"__\" , maxsplit = 1 ) if len ( tokens ) != 2 : log_message ( f \"Can't determine source name and type from string in module { cls . _module_type_id } , ignoring method: { attr } \" # type: ignore ) source_profile , source_type = tokens op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_profile\" : source_profile , \"source_type\" : source_type , }, \"doc\" : f \"Import data of type ' { sup_type } ' from a { source_profile } { source_type } and save it to the kiara data store.\" , } all_metadata_profiles [ f \"import. { sup_type } .from. { source_profile } \" ] = op_config return all_metadata_profiles DataImportModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/data_import.py class DataImportModuleConfig ( ModuleTypeConfigSchema ): # value_type: str = Field(description=\"The type of the value to be imported.\") source_profile : str = Field ( description = \"The name of the source profile. Used to distinguish different input categories for the same input type.\" ) source_type : str = Field ( description = \"The type of the source to import from.\" ) # allow_save_input: bool = Field( # description=\"Allow the user to choose whether to save the imported item or not.\", # default=True, # ) # save_default: bool = Field( # description=\"The default of the 'save' input if not specified by the user.\", # default=False, # ) # allow_aliases_input: typing.Optional[bool] = Field( # description=\"Allow the user to choose aliases for the saved value.\", # default=None, # ) # aliases_default: typing.List[str] = Field( # description=\"Default value for aliases.\", default_factory=list # ) source_profile : str pydantic-field required \u00b6 The name of the source profile. Used to distinguish different input categories for the same input type. source_type : str pydantic-field required \u00b6 The type of the source to import from. FileBundleImportModule ( DataImportModule ) \u00b6 Import a file, optionally saving it to the data store. Source code in kiara/operations/data_import.py class FileBundleImportModule ( DataImportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file_bundle\" FileImportModule ( DataImportModule ) \u00b6 Import a file, optionally saving it to the data store. Source code in kiara/operations/data_import.py class FileImportModule ( DataImportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file\" ImportDataOperationType ( OperationType ) \u00b6 Import data into kiara . Operations of this type take external data, and register it into kiara . External data is different in that it usually does not come with any metadata on how it was created, who created it, when, etc. Import operations are created by implementing a class that inherits from DataImportModule , kiara will register it under an operation id following this template: <IMPORTED_DATA_TYPE>.import_from.<IMPORT_PROFILE>.<INPUT_TYPE> The meaning of the templated fields is: IMPORTED_DATA_TYPE : the data type of the imported value IMPORT_PROFILE : a short, free-form description of where from (or how) the data is imported INPUT_TYPE : the data type of the user input that points to the data (like a file path, url, query, etc.) -- in most cases this will be some form of a string or uri There are two main scenarios when an operation of this type is used: 'onboard' data that was created by a 3rd party, or using external processes 're-import' data that as created in kiara , then exported to be transformed in an external process, and then imported again into kiara In both of those scenarios, we'll need to have a way to add metadata to fill out 'holes' in the metadata 'chold chain'. We don't have a concept yet as to how to do that, but that is planned for the future. Source code in kiara/operations/data_import.py class ImportDataOperationType ( OperationType ): \"\"\"Import data into *kiara*. Operations of this type take external data, and register it into *kiara*. External data is different in that it usually does not come with any metadata on how it was created, who created it, when, etc. Import operations are created by implementing a class that inherits from [DataImportModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModule), *kiara* will register it under an operation id following this template: ``` <IMPORTED_DATA_TYPE>.import_from.<IMPORT_PROFILE>.<INPUT_TYPE> ``` The meaning of the templated fields is: - `IMPORTED_DATA_TYPE`: the data type of the imported value - `IMPORT_PROFILE`: a short, free-form description of where from (or how) the data is imported - `INPUT_TYPE`: the data type of the user input that points to the data (like a file path, url, query, etc.) -- in most cases this will be some form of a string or uri There are two main scenarios when an operation of this type is used: - 'onboard' data that was created by a 3rd party, or using external processes - 're-import' data that as created in *kiara*, then exported to be transformed in an external process, and then imported again into *kiara* In both of those scenarios, we'll need to have a way to add metadata to fill out 'holes' in the metadata 'chold chain'. We don't have a concept yet as to how to do that, but that is planned for the future. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , DataImportModule ) def get_import_operations_per_target_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_target_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] source_profile = op_config . module_config [ \"source_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ source_profile ] = op_config return result def get_import_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_import_operations_per_target_type () . get ( value_type , {}) get_import_operations_for_target_type ( self , value_type ) \u00b6 Return all available import operations that produce data of the specified type. Source code in kiara/operations/data_import.py def get_import_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_import_operations_per_target_type () . get ( value_type , {}) get_import_operations_per_target_type ( self ) \u00b6 Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. Source code in kiara/operations/data_import.py def get_import_operations_per_target_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_target_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] source_profile = op_config . module_config [ \"source_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ source_profile ] = op_config return result extract_metadata \u00b6 ExtractMetadataModule ( KiaraModule ) \u00b6 Base class to use when writing a module to extract metadata from a file. It's possible to use any arbitrary kiara module for this purpose, but sub-classing this makes it easier. Source code in kiara/operations/extract_metadata.py class ExtractMetadataModule ( KiaraModule ): \"\"\"Base class to use when writing a module to extract metadata from a file. It's possible to use any arbitrary *kiara* module for this purpose, but sub-classing this makes it easier. \"\"\" _config_cls = MetadataModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_types : typing . Iterable = cls . get_supported_value_types () if \"*\" in value_types : value_types = kiara . type_mgmt . value_type_names metadata_key = cls . get_metadata_key () all_value_types = set () for value_type in value_types : if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring metadata-extract operation (metadata key: { metadata_key } ) for type ' { value_type } ': type not available\" ) continue all_value_types . add ( value_type ) sub_types = kiara . type_mgmt . get_sub_types ( value_type ) all_value_types . update ( sub_types ) for value_type in all_value_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type }, \"doc\" : f \"Extract ' { metadata_key } ' metadata for value of type ' { value_type } '.\" , } all_metadata_profiles [ f \"extract. { metadata_key } .metadata.from. { value_type } \" ] = op_config return all_metadata_profiles @classmethod @abc . abstractmethod def _get_supported_types ( self ) -> typing . Union [ str , typing . Iterable [ str ]]: pass @classmethod def get_metadata_key ( cls ) -> str : return cls . _module_type_name # type: ignore @classmethod def get_supported_value_types ( cls ) -> typing . Set [ str ]: _types = cls . _get_supported_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) def __init__ ( self , * args , ** kwargs ): self . _metadata_schema : typing . Optional [ str ] = None super () . __init__ ( * args , ** kwargs ) @property def value_type ( self ) -> str : data_type = self . get_config_value ( \"value_type\" ) sup_types = self . get_supported_value_types () if \"*\" not in sup_types and data_type not in sup_types : match = False for sup_type in sup_types : for sub_type in self . _kiara . type_mgmt . get_sub_types ( sup_type ): if sub_type == data_type : match = True break if not match : raise ValueError ( f \"Invalid module configuration, type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_value_types ()) } .\" ) return data_type @property def metadata_schema ( self ) -> str : if self . _metadata_schema is not None : return self . _metadata_schema schema = self . _get_metadata_schema ( type = self . value_type ) if isinstance ( schema , type ) and issubclass ( schema , BaseModel ): schema = schema . schema_json () elif not isinstance ( schema , str ): raise TypeError ( f \"Invalid type for metadata schema: { type ( schema ) } \" ) self . _metadata_schema = schema return self . _metadata_schema @abc . abstractmethod def _get_metadata_schema ( self , type : str ) -> typing . Union [ str , typing . Type [ BaseModel ]]: \"\"\"Create the metadata schema for the configured type.\"\"\" @abc . abstractmethod def extract_metadata ( self , value : Value ) -> typing . Union [ typing . Mapping [ str , typing . Any ], BaseModel ]: pass def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : { \"type\" : self . value_type , \"doc\" : f \"A value of type ' { self . value_type } '\" , \"optional\" : False , } } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"metadata_item\" : { \"type\" : \"dict\" , \"doc\" : \"The metadata for the provided value.\" , }, \"metadata_item_schema\" : { \"type\" : \"string\" , \"doc\" : \"The (json) schema for the metadata.\" , }, } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" value = inputs . get_value_obj ( input_name ) if self . value_type != \"any\" and value . type_name != self . value_type : raise KiaraProcessingException ( f \"Can't extract metadata for value of type ' { value . value_schema . type } '. Expected type ' { self . value_type } '.\" ) # TODO: if type 'any', validate that the data is actually of the right type? outputs . set_value ( \"metadata_item_schema\" , self . metadata_schema ) metadata = self . extract_metadata ( value ) if isinstance ( metadata , BaseModel ): metadata = metadata . dict ( exclude_none = True ) # TODO: validate metadata? outputs . set_value ( \"metadata_item\" , metadata ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/extract_metadata.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : { \"type\" : self . value_type , \"doc\" : f \"A value of type ' { self . value_type } '\" , \"optional\" : False , } } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/extract_metadata.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"metadata_item\" : { \"type\" : \"dict\" , \"doc\" : \"The metadata for the provided value.\" , }, \"metadata_item_schema\" : { \"type\" : \"string\" , \"doc\" : \"The (json) schema for the metadata.\" , }, } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/extract_metadata.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_types : typing . Iterable = cls . get_supported_value_types () if \"*\" in value_types : value_types = kiara . type_mgmt . value_type_names metadata_key = cls . get_metadata_key () all_value_types = set () for value_type in value_types : if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring metadata-extract operation (metadata key: { metadata_key } ) for type ' { value_type } ': type not available\" ) continue all_value_types . add ( value_type ) sub_types = kiara . type_mgmt . get_sub_types ( value_type ) all_value_types . update ( sub_types ) for value_type in all_value_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type }, \"doc\" : f \"Extract ' { metadata_key } ' metadata for value of type ' { value_type } '.\" , } all_metadata_profiles [ f \"extract. { metadata_key } .metadata.from. { value_type } \" ] = op_config return all_metadata_profiles ExtractMetadataOperationType ( OperationType ) \u00b6 Extract metadata from a dataset. The purpose of this operation type is to be able to extract arbitrary, type-specific metadata from value data. In general, kiara wants to collect (and store along the value) as much metadata related to data as possible, but the extraction process should not take a lot of processing time (since this is done whenever a value is registered into a data registry). As its hard to predict all the types of metadata of a specific type that could be interesting in specific scenarios, kiara supports a pluggable mechanism to add new metadata extraction processes by extending the base class ExtractMetadataModule and adding that implementation somewhere kiara can find it. Once that is done, kiara will automatically add a new operation with an id that follows this template: <VALUE_TYPE>.extract_metadata.<METADATA_KEY> , where METADATA_KEY is a name under which the metadata will be stored within the value object. By default, every value type should have at least one metadata extraction module where the METADATA_KEY is the same as the value type name, and which contains basic, type-specific metadata (e.g. for a 'table', that would be number of rows, column names, column types, etc.). Source code in kiara/operations/extract_metadata.py class ExtractMetadataOperationType ( OperationType ): \"\"\"Extract metadata from a dataset. The purpose of this operation type is to be able to extract arbitrary, type-specific metadata from value data. In general, *kiara* wants to collect (and store along the value) as much metadata related to data as possible, but the extraction process should not take a lot of processing time (since this is done whenever a value is registered into a data registry). As its hard to predict all the types of metadata of a specific type that could be interesting in specific scenarios, *kiara* supports a pluggable mechanism to add new metadata extraction processes by extending the base class [`ExtractMetadataModule`](http://dharpa.org/kiara/latest/api_reference/kiara.operations.extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataModule) and adding that implementation somewhere *kiara* can find it. Once that is done, *kiara* will automatically add a new operation with an id that follows this template: `<VALUE_TYPE>.extract_metadata.<METADATA_KEY>`, where `METADATA_KEY` is a name under which the metadata will be stored within the value object. By default, every value type should have at least one metadata extraction module where the `METADATA_KEY` is the same as the value type name, and which contains basic, type-specific metadata (e.g. for a 'table', that would be number of rows, column names, column types, etc.). \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , ExtractMetadataModule ) def get_all_operations_for_type ( self , value_type : str ) -> typing . Mapping [ str , Operation ]: result = {} for op_config in self . operations . values (): v_t = op_config . module_config [ \"value_type\" ] if v_t != value_type : continue module_cls : ExtractMetadataModule = op_config . module_cls # type: ignore md_key = module_cls . get_metadata_key () result [ md_key ] = op_config return result MetadataModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/extract_metadata.py class MetadataModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The data type this module will be used for.\" ) value_type : str pydantic-field required \u00b6 The data type this module will be used for. merge_values \u00b6 ValueMergeModule ( KiaraModule ) \u00b6 Base class for operations that merge several values into one. NOT USED YET. Source code in kiara/operations/merge_values.py class ValueMergeModule ( KiaraModule ): \"\"\"Base class for operations that merge several values into one. NOT USED YET. \"\"\" def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_schema_dicts : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . get_config_value ( \"input_schemas\" ) if not input_schema_dicts : raise Exception ( \"No input schemas provided.\" ) input_schemas : typing . Dict [ str , ValueSchema ] = {} for k , v in input_schema_dicts . items (): input_schemas [ k ] = ValueSchema ( ** v ) return input_schemas def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"merged_value\" : { \"type\" : self . get_config_value ( \"output_type\" )}} create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/merge_values.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_schema_dicts : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . get_config_value ( \"input_schemas\" ) if not input_schema_dicts : raise Exception ( \"No input schemas provided.\" ) input_schemas : typing . Dict [ str , ValueSchema ] = {} for k , v in input_schema_dicts . items (): input_schemas [ k ] = ValueSchema ( ** v ) return input_schemas create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/merge_values.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"merged_value\" : { \"type\" : self . get_config_value ( \"output_type\" )}} ValueMergeModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/merge_values.py class ValueMergeModuleConfig ( ModuleTypeConfigSchema ): input_schemas : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = Field ( description = \"The schemas for all of the expected inputs.\" ) output_type : str = Field ( description = \"The result type of the merged value.\" ) input_schemas : Dict [ str , Mapping [ str , Any ]] pydantic-field required \u00b6 The schemas for all of the expected inputs. output_type : str pydantic-field required \u00b6 The result type of the merged value. pretty_print \u00b6 PrettyPrintModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/pretty_print.py class PrettyPrintModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to save.\" ) target_type : str = Field ( description = \"The target to print the value to.\" , default = \"string\" ) target_type : str pydantic-field \u00b6 The target to print the value to. value_type : str pydantic-field required \u00b6 The type of the value to save. PrettyPrintOperationType ( OperationType ) \u00b6 This operation type renders values of any type into human readable output for different targets (e.g. terminal, html, ...). The main purpose of this operation type is to show the user as much content of the data as possible for the specific rendering target, without giving any guarantees that all information contained in the data is shown. It may print out the whole content (if the content is small, or the available space large enough), or just a few bits and pieces from the start and/or end of the data, whatever makes most sense for the data itself. For example, for large tables it might be a good idea to print out the first and last 30 rows, so users can see which columns are available, and get a rough overview of the content itself. For network graphs it might make sense to print out some graph properties (number of nodes, edges, available attributes, ...) on the terminal, but render the graph itself when using html as target. Currently, it is only possible to implement a pretty_print renderer if you have access to the source code of the value type you want to render. To add a new render method, add a new instance method to the ValueType class in question, in the format: def pretty_print_as_<TARGET_TYPE>(value: Value, print_config: typing.Mapping[str, typing.Any]) -> typing.Any: ... ... kiara will look at all available ValueType classes for methods that match this signature, and auto-generate operations following this naming template: <SOURCE_TYPE>.pretty_print_as.<TARGET_TYPE> . Currently, only the type renderables is implemented as target type (for printing out to the terminal). It is planned to add output suitable for use in Jupyter notebooks in the near future. Source code in kiara/operations/pretty_print.py class PrettyPrintOperationType ( OperationType ): \"\"\"This operation type renders values of any type into human readable output for different targets (e.g. terminal, html, ...). The main purpose of this operation type is to show the user as much content of the data as possible for the specific rendering target, without giving any guarantees that all information contained in the data is shown. It may print out the whole content (if the content is small, or the available space large enough), or just a few bits and pieces from the start and/or end of the data, whatever makes most sense for the data itself. For example, for large tables it might be a good idea to print out the first and last 30 rows, so users can see which columns are available, and get a rough overview of the content itself. For network graphs it might make sense to print out some graph properties (number of nodes, edges, available attributes, ...) on the terminal, but render the graph itself when using html as target. Currently, it is only possible to implement a `pretty_print` renderer if you have access to the source code of the value type you want to render. To add a new render method, add a new instance method to the `ValueType` class in question, in the format: ``` def pretty_print_as_<TARGET_TYPE>(value: Value, print_config: typing.Mapping[str, typing.Any]) -> typing.Any: ... ... ``` *kiara* will look at all available `ValueType` classes for methods that match this signature, and auto-generate operations following this naming template: `<SOURCE_TYPE>.pretty_print_as.<TARGET_TYPE>`. Currently, only the type `renderables` is implemented as target type (for printing out to the terminal). It is planned to add output suitable for use in Jupyter notebooks in the near future. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , PrettyPrintValueModule ) def get_pretty_print_operation ( self , value_type : str , target_type : str ) -> Operation : result = [] for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] != value_type : continue if op_config . module_config [ \"target_type\" ] != target_type : continue result . append ( op_config ) if not result : raise Exception ( f \"No pretty print operation for value type ' { value_type } ' and output ' { target_type } ' registered.\" ) elif len ( result ) != 1 : raise Exception ( f \"Multiple pretty print operations for value type ' { value_type } ' and output ' { target_type } ' registered.\" ) return result [ 0 ] def pretty_print ( self , value : Value , target_type : str , print_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ) -> typing . Any : ops_config = self . get_pretty_print_operation ( value_type = value . type_name , target_type = target_type ) inputs : typing . Mapping [ str , typing . Any ] = { value . type_name : value , \"print_config\" : print_config , } result = ops_config . module . run ( ** inputs ) printed = result . get_value_data ( \"printed\" ) return printed PrettyPrintValueModule ( KiaraModule ) \u00b6 Source code in kiara/operations/pretty_print.py class PrettyPrintValueModule ( KiaraModule ): _config_cls = PrettyPrintModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type_cls in kiara . type_mgmt . value_types . items (): for attr in dir ( value_type_cls ): if not attr . startswith ( \"pretty_print_as_\" ): continue target_type = attr [ 16 :] if target_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Pretty print target type ' { target_type } ' for source type ' { value_type_name } ' not valid, ignoring.\" ) continue op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"target_type\" : target_type , }, \"doc\" : f \"Pretty print a value of type ' { value_type_name } ' as ' { target_type } '.\" , } all_metadata_profiles [ f \"pretty_print. { value_type_name } .as. { target_type } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type = self . get_config_value ( \"value_type\" ) if value_type == \"all\" : input_name = \"value_item\" doc = \"A value of any type.\" else : input_name = value_type doc = f \"A value of type ' { value_type } '.\" inputs : typing . Mapping [ str , typing . Any ] = { input_name : { \"type\" : value_type , \"doc\" : doc }, \"print_config\" : { \"type\" : \"dict\" , \"doc\" : \"Optional print configuration.\" , \"optional\" : True , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"printed\" : { \"type\" : self . get_config_value ( \"target_type\" ), \"doc\" : f \"The printed value as ' { self . get_config_value ( 'target_type' ) } '.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_type : str = self . get_config_value ( \"value_type\" ) target_type : str = self . get_config_value ( \"target_type\" ) if value_type == \"all\" : input_name = \"value_item\" else : input_name = value_type value_obj : Value = inputs . get_value_obj ( input_name ) print_config = dict ( DEFAULT_PRETTY_PRINT_CONFIG ) config : typing . Mapping = inputs . get_value_data ( \"print_config\" ) if config : print_config . update ( config ) func_name = f \"pretty_print_as_ { target_type } \" if not hasattr ( value_obj . type_obj , func_name ): raise Exception ( f \"Type ' { value_type } ' can't be pretty printed as ' { target_type } '. This is most likely a bug.\" ) if not value_obj . is_set : printed = \"-- not set --\" else : func = getattr ( value_obj . type_obj , func_name ) # TODO: check signature try : printed = func ( value = value_obj , print_config = print_config ) except Exception as e : if is_debug (): import traceback traceback . print_exc () raise KiaraProcessingException ( str ( e )) outputs . set_value ( \"printed\" , printed ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/pretty_print.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type = self . get_config_value ( \"value_type\" ) if value_type == \"all\" : input_name = \"value_item\" doc = \"A value of any type.\" else : input_name = value_type doc = f \"A value of type ' { value_type } '.\" inputs : typing . Mapping [ str , typing . Any ] = { input_name : { \"type\" : value_type , \"doc\" : doc }, \"print_config\" : { \"type\" : \"dict\" , \"doc\" : \"Optional print configuration.\" , \"optional\" : True , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/pretty_print.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"printed\" : { \"type\" : self . get_config_value ( \"target_type\" ), \"doc\" : f \"The printed value as ' { self . get_config_value ( 'target_type' ) } '.\" , } } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/pretty_print.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type_cls in kiara . type_mgmt . value_types . items (): for attr in dir ( value_type_cls ): if not attr . startswith ( \"pretty_print_as_\" ): continue target_type = attr [ 16 :] if target_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Pretty print target type ' { target_type } ' for source type ' { value_type_name } ' not valid, ignoring.\" ) continue op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"target_type\" : target_type , }, \"doc\" : f \"Pretty print a value of type ' { value_type_name } ' as ' { target_type } '.\" , } all_metadata_profiles [ f \"pretty_print. { value_type_name } .as. { target_type } \" ] = op_config return all_metadata_profiles sample \u00b6 SampleValueModule ( KiaraModule ) \u00b6 Base class for operations that take samples of data. Source code in kiara/operations/sample.py class SampleValueModule ( KiaraModule ): \"\"\"Base class for operations that take samples of data.\"\"\" _config_cls = SampleValueModuleConfig @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : \"\"\"Return the value type for this sample module.\"\"\" @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring sample operation for source type ' { value_type } ': type not available\" ) return {} for sample_type in cls . get_supported_sample_types (): op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"sample_type\" : sample_type }, \"doc\" : f \"Sample value of type ' { value_type } ' using method: { sample_type } .\" , } key = f \"sample. { value_type } . { sample_type } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles @classmethod def get_supported_sample_types ( cls ) -> typing . Iterable [ str ]: types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"sample_\" ) and callable ( attr ): sample_type = attr_name [ 7 :] if sample_type in types : raise Exception ( f \"Error in sample module ' { cls . __name__ } ': multiple sample methods for type ' { sample_type } '.\" ) types . append ( sample_type ) return types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_value_type (), \"doc\" : \"The value to sample.\" , }, \"sample_size\" : { \"type\" : \"integer\" , \"doc\" : \"The sample size.\" , \"default\" : 10 , }, } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"sampled_value\" : { \"type\" : self . get_value_type (), \"doc\" : \"The sampled value\" } } def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : sample_size : int = inputs . get_value_data ( \"sample_size\" ) sample_type : str = self . get_config_value ( \"sample_type\" ) if sample_size < 0 : raise KiaraProcessingException ( f \"Invalid sample size ' { sample_size } ': can't be negative.\" ) input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" value : Value = inputs . get_value_obj ( input_name ) func = getattr ( self , f \"sample_ { sample_type } \" ) result = func ( value = value , sample_size = sample_size ) outputs . set_value ( \"sampled_value\" , result ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/sample.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_value_type (), \"doc\" : \"The value to sample.\" , }, \"sample_size\" : { \"type\" : \"integer\" , \"doc\" : \"The sample size.\" , \"default\" : 10 , }, } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/sample.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"sampled_value\" : { \"type\" : self . get_value_type (), \"doc\" : \"The sampled value\" } } get_value_type () classmethod \u00b6 Return the value type for this sample module. Source code in kiara/operations/sample.py @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : \"\"\"Return the value type for this sample module.\"\"\" retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/sample.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring sample operation for source type ' { value_type } ': type not available\" ) return {} for sample_type in cls . get_supported_sample_types (): op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"sample_type\" : sample_type }, \"doc\" : f \"Sample value of type ' { value_type } ' using method: { sample_type } .\" , } key = f \"sample. { value_type } . { sample_type } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles SampleValueModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/sample.py class SampleValueModuleConfig ( ModuleTypeConfigSchema ): sample_type : str = Field ( description = \"The sample method.\" ) sample_type : str pydantic-field required \u00b6 The sample method. SampleValueOperationType ( OperationType ) \u00b6 Operation type for sampling data of different types. This is useful to reduce the size of some datasets in previews and test-runs, while adjusting parameters and the like. Operations of this type can implement very simple or complex ways to take samples of the data they are fed. The most important ones are sampling operations relating to tables and arrays, but it might also make sense to sample texts, image-sets and so on. Operations of this type take the value to be sampled as input, as well as a sample size (integer), which can be, for example, a number of rows, or percentage of the whole dataset, depending on sample type. Modules that implement sampling should inherit from SampleValueModule , and will get auto-registered with operation ids following this template: <VALUE_TYPE>.sample.<SAMPLE_TYPE_NAME> , where SAMPLE_TYPE_NAME is a descriptive name what will be sampled, or how sampling will be done. Source code in kiara/operations/sample.py class SampleValueOperationType ( OperationType ): \"\"\"Operation type for sampling data of different types. This is useful to reduce the size of some datasets in previews and test-runs, while adjusting parameters and the like. Operations of this type can implement very simple or complex ways to take samples of the data they are fed. The most important ones are sampling operations relating to tables and arrays, but it might also make sense to sample texts, image-sets and so on. Operations of this type take the value to be sampled as input, as well as a sample size (integer), which can be, for example, a number of rows, or percentage of the whole dataset, depending on sample type. Modules that implement sampling should inherit from [SampleValueModule](https://dharpa.org/kiara/latest/api_reference/kiara.operations.sample/#kiara.operations.sample.SampleValueModule), and will get auto-registered with operation ids following this template: `<VALUE_TYPE>.sample.<SAMPLE_TYPE_NAME>`, where `SAMPLE_TYPE_NAME` is a descriptive name what will be sampled, or how sampling will be done. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , SampleValueModule ) def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): sample_op_module_cls : typing . Type [ SampleValueModule ] = op . module_cls # type: ignore source_type = sample_op_module_cls . get_value_type () if source_type == value_type : target_type = op . module_config [ \"sample_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to sample ' { source_type } ' using { target_type } \" ) result [ target_type ] = op return result get_operations_for_value_type ( self , value_type ) \u00b6 Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. Source code in kiara/operations/sample.py def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): sample_op_module_cls : typing . Type [ SampleValueModule ] = op . module_cls # type: ignore source_type = sample_op_module_cls . get_value_type () if source_type == value_type : target_type = op . module_config [ \"sample_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to sample ' { source_type } ' using { target_type } \" ) result [ target_type ] = op return result serialize \u00b6 SerializeValueModule ( KiaraModule ) \u00b6 Base class for 'serialize' operations. Source code in kiara/operations/serialize.py class SerializeValueModule ( KiaraModule ): \"\"\"Base class for 'serialize' operations.\"\"\" _config_cls = SerializeValueModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring serialization operation for source type ' { value_type } ': type not available\" ) return {} for serialization_type in cls . get_supported_serialization_types (): mod_conf = { \"value_type\" : value_type , \"serialization_type\" : serialization_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Serialize value of type ' { value_type } ' to ' { serialization_type } '.\" , } key = f \"serialize. { value_type } .as. { serialization_type } \" if key in all_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_profiles [ key ] = op_config return all_profiles @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : pass @classmethod def get_supported_serialization_types ( cls ) -> typing . Iterable [ str ]: serialize_types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"to_\" ) and callable ( attr ): serialize_types . append ( attr_name [ 3 :]) return serialize_types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) # serialization_type: str = self.get_config_value(\"serialization_type\") return { \"value_item\" : { \"type\" : value_type , \"doc\" : f \"The ' { value_type } ' value to be serialized.\" , } } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) return { \"value_info\" : { \"type\" : \"value_info\" , \"doc\" : \"Information about the (original) serialized value (can be used to re-constitute the value, incl. its original id).\" , }, \"deserialize_config\" : { \"type\" : \"deserialize_config\" , \"doc\" : f \"The config to use to deserialize the value of type ' { value_type } '.\" , }, } def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_type : str = self . get_config_value ( \"value_type\" ) value_obj = inputs . get_value_obj ( \"value_item\" ) serialization_type = self . get_config_value ( \"serialization_type\" ) if value_type != value_obj . type_name : raise KiaraProcessingException ( f \"Invalid type ( { value_obj . type_name } ) of source value: expected ' { value_type } '.\" ) if not hasattr ( self , f \"to_ { serialization_type } \" ): # this can never happen, I think raise Exception ( f \"Module ' { self . _module_type_id } ' can't serialize ' { value_type } ' to ' { serialization_type } ': missing method 'to_ { serialization_type } '. This is a bug.\" # type: ignore ) func = getattr ( self , f \"to_ { serialization_type } \" ) serialized = func ( value_obj ) if isinstance ( serialized , typing . Mapping ): serialized = DeserializeConfig ( ** serialized ) if not isinstance ( serialized , DeserializeConfig ): raise KiaraProcessingException ( f \"Invalid serialization result type: { type ( serialized ) } \" ) outputs . set_values ( deserialize_config = serialized , value_info = value_obj . get_info () ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/serialize.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) # serialization_type: str = self.get_config_value(\"serialization_type\") return { \"value_item\" : { \"type\" : value_type , \"doc\" : f \"The ' { value_type } ' value to be serialized.\" , } } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/serialize.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) return { \"value_info\" : { \"type\" : \"value_info\" , \"doc\" : \"Information about the (original) serialized value (can be used to re-constitute the value, incl. its original id).\" , }, \"deserialize_config\" : { \"type\" : \"deserialize_config\" , \"doc\" : f \"The config to use to deserialize the value of type ' { value_type } '.\" , }, } retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/serialize.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring serialization operation for source type ' { value_type } ': type not available\" ) return {} for serialization_type in cls . get_supported_serialization_types (): mod_conf = { \"value_type\" : value_type , \"serialization_type\" : serialization_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Serialize value of type ' { value_type } ' to ' { serialization_type } '.\" , } key = f \"serialize. { value_type } .as. { serialization_type } \" if key in all_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_profiles [ key ] = op_config return all_profiles SerializeValueModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/serialize.py class SerializeValueModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the source value.\" ) serialization_type : str = Field ( description = \"The type of the converted value.\" ) serialization_type : str pydantic-field required \u00b6 The type of the converted value. value_type : str pydantic-field required \u00b6 The type of the source value. SerializeValueOperationType ( OperationType ) \u00b6 Operations that serialize data into formats that can be used for data exchange. NOT USED YET Source code in kiara/operations/serialize.py class SerializeValueOperationType ( OperationType ): \"\"\"Operations that serialize data into formats that can be used for data exchange. NOT USED YET \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , SerializeValueModule ) def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): source_type = op . module_config [ \"value_type\" ] if source_type == value_type : target_type = op . module_config [ \"serialization_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to serialize ' { source_type } ' to { target_type } \" ) result [ target_type ] = op return result get_operations_for_value_type ( self , value_type ) \u00b6 Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. Source code in kiara/operations/serialize.py def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): source_type = op . module_config [ \"value_type\" ] if source_type == value_type : target_type = op . module_config [ \"serialization_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to serialize ' { source_type } ' to { target_type } \" ) result [ target_type ] = op return result store_value \u00b6 StoreOperationType ( OperationType ) \u00b6 Store a value into a local data store. This is a special operation type, that is used internally by the [LocalDataStore](http://dharpa.org/kiara/latest/api_reference/kiara.data.registry.store/#kiara.data.registry.store.LocalDataStore] data registry implementation. For each value type that should be supported by the persistent kiara data store, there must be an implementation of the StoreValueTypeModule class, which handles the actual persisting on disk. In most cases, end users won't need to interact with this type of operation. Source code in kiara/operations/store_value.py class StoreOperationType ( OperationType ): \"\"\"Store a value into a local data store. This is a special operation type, that is used internally by the [LocalDataStore](http://dharpa.org/kiara/latest/api_reference/kiara.data.registry.store/#kiara.data.registry.store.LocalDataStore] data registry implementation. For each value type that should be supported by the persistent *kiara* data store, there must be an implementation of the [StoreValueTypeModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.store_value/#kiara.operations.store_value.StoreValueTypeModule) class, which handles the actual persisting on disk. In most cases, end users won't need to interact with this type of operation. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , StoreValueTypeModule ) def get_store_operation_for_type ( self , value_type : str ) -> Operation : result = [] for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] == value_type : result . append ( op_config ) if not result : raise Exception ( f \"No 'store_value' operation for type ' { value_type } ' registered.\" ) elif len ( result ) != 1 : pass for r in result : print ( r . json ( indent = 2 )) raise Exception ( f \"Multiple 'store_value' operations for type ' { value_type } ' registered.\" ) return result [ 0 ] StoreValueModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/store_value.py class StoreValueModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to save.\" ) value_type : str pydantic-field required \u00b6 The type of the value to save. StoreValueTypeModule ( KiaraModule ) \u00b6 Store a specific value type. This is used internally. Source code in kiara/operations/store_value.py class StoreValueTypeModule ( KiaraModule ): \"\"\"Store a specific value type. This is used internally. \"\"\" _config_cls = StoreValueModuleConfig @classmethod def get_supported_value_types ( cls ) -> typing . Set [ str ]: _types = cls . retrieve_supported_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) @classmethod @abc . abstractmethod def retrieve_supported_types ( cls ) -> typing . Union [ str , typing . Iterable [ str ]]: pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} all_types_and_subtypes = set () for sup_type in cls . get_supported_value_types (): if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring save operation for type ' { sup_type } ': type not available\" ) continue all_types_and_subtypes . add ( sup_type ) sub_types = kiara . type_mgmt . get_sub_types ( sup_type ) all_types_and_subtypes . update ( sub_types ) for sup_type in all_types_and_subtypes : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : sup_type }, \"doc\" : f \"Store a value of type ' { sup_type } '.\" , } all_metadata_profiles [ f \"store. { sup_type } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" inputs : typing . Mapping [ str , typing . Any ] = { \"value_id\" : { \"type\" : \"string\" , \"doc\" : \"The id to use when saving the value.\" , }, field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The base path to save the value to.\" , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : \"The original or cloned (if applicable) value that was saved.\" , }, \"load_config\" : { \"type\" : \"load_config\" , \"doc\" : \"The configuration to use with kiara to load the saved value.\" , }, } return outputs @abc . abstractmethod def store_value ( self , value : Value , base_path : str ) -> typing . Union [ typing . Tuple [ typing . Dict [ str , typing . Any ], typing . Any ], typing . Dict [ str , typing . Any ], ]: \"\"\"Save the value, and return the load config needed to load it again.\"\"\" def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_id : str = inputs . get_value_data ( \"value_id\" ) if not value_id : raise KiaraProcessingException ( \"No value id provided.\" ) field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" value_obj : Value = inputs . get_value_obj ( field_name ) base_path : str = inputs . get_value_data ( \"base_path\" ) result = self . store_value ( value = value_obj , base_path = base_path ) if isinstance ( result , typing . Mapping ): load_config = result result_value = value_obj elif isinstance ( result , tuple ): load_config = result [ 0 ] if result [ 1 ]: result_value = result [ 1 ] else : result_value = value_obj else : raise KiaraProcessingException ( f \"Invalid result type for 'store_value' method in class ' { self . __class__ . __name__ } '. This is a bug.\" ) load_config [ \"value_id\" ] = value_id lc = LoadConfig ( ** load_config ) if lc . base_path_input_name and lc . base_path_input_name not in lc . inputs . keys (): raise KiaraProcessingException ( f \"Invalid load config: base path ' { lc . base_path_input_name } ' not part of inputs.\" ) outputs . set_values ( metadata = None , lineage = None , ** { \"load_config\" : lc , field_name : result_value } ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/store_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" inputs : typing . Mapping [ str , typing . Any ] = { \"value_id\" : { \"type\" : \"string\" , \"doc\" : \"The id to use when saving the value.\" , }, field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The base path to save the value to.\" , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/store_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : \"The original or cloned (if applicable) value that was saved.\" , }, \"load_config\" : { \"type\" : \"load_config\" , \"doc\" : \"The configuration to use with kiara to load the saved value.\" , }, } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/store_value.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} all_types_and_subtypes = set () for sup_type in cls . get_supported_value_types (): if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring save operation for type ' { sup_type } ': type not available\" ) continue all_types_and_subtypes . add ( sup_type ) sub_types = kiara . type_mgmt . get_sub_types ( sup_type ) all_types_and_subtypes . update ( sub_types ) for sup_type in all_types_and_subtypes : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : sup_type }, \"doc\" : f \"Store a value of type ' { sup_type } '.\" , } all_metadata_profiles [ f \"store. { sup_type } \" ] = op_config return all_metadata_profiles store_value ( self , value , base_path ) \u00b6 Save the value, and return the load config needed to load it again. Source code in kiara/operations/store_value.py @abc . abstractmethod def store_value ( self , value : Value , base_path : str ) -> typing . Union [ typing . Tuple [ typing . Dict [ str , typing . Any ], typing . Any ], typing . Dict [ str , typing . Any ], ]: \"\"\"Save the value, and return the load config needed to load it again.\"\"\" pipeline special \u00b6 PipelineValueInfo ( BaseModel ) pydantic-model \u00b6 Convenience wrapper to make the PipelineState json/dict export prettier. Source code in kiara/pipeline/__init__.py class PipelineValueInfo ( BaseModel ): \"\"\"Convenience wrapper to make the [PipelineState][kiara.pipeline.pipeline.PipelineState] json/dict export prettier.\"\"\" @classmethod def from_value_obj ( cls , value : Value , ensure_metadata : bool = False ): if ensure_metadata : value . get_metadata () return PipelineValueInfo ( id = value . id , value_schema = value . value_schema , is_valid = value . item_is_valid (), is_set = value . is_set , status = value . item_status (), # value_metadata=value.value_metadata, # last_update=value.last_update, # value_hash=value.value_hash, # is_streaming=value.is_streaming, metadata = value . metadata , ) class Config : extra = Extra . forbid allow_mutation = False id : str = Field ( description = \"A unique id for this value.\" ) is_valid : bool = Field ( description = \"Whether the value is set and valid.\" , default = False ) status : str = Field ( description = \"The value status string\" ) is_set : bool = Field ( description = \"Whether the value is set.\" ) value_schema : ValueSchema = Field ( description = \"The schema of this value.\" ) # is_constant: bool = Field( # description=\"Whether this value is a constant.\", default=False # ) # value_metadata: ValueMetadata = Field( # description=\"The metadata of the value itself (not the actual data).\" # ) # last_update: datetime = Field( # default=None, description=\"The time the last update to this value happened.\" # ) # value_hash: typing.Union[ValueHashMarker, int] = Field( # description=\"The hash of the current value.\" # ) # is_streaming: bool = Field( # default=False, # description=\"Whether the value is currently streamed into this object.\", # ) metadata : typing . Dict [ str , typing . Any ] = Field ( description = \"Metadata relating to the actual data (size, no. of rows, etc. -- depending on data type).\" , default_factory = dict , ) id : str pydantic-field required \u00b6 A unique id for this value. is_set : bool pydantic-field required \u00b6 Whether the value is set. is_valid : bool pydantic-field \u00b6 Whether the value is set and valid. metadata : Dict [ str , Any ] pydantic-field \u00b6 Metadata relating to the actual data (size, no. of rows, etc. -- depending on data type). status : str pydantic-field required \u00b6 The value status string value_schema : ValueSchema pydantic-field required \u00b6 The schema of this value. PipelineValuesInfo ( BaseModel ) pydantic-model \u00b6 Convenience wrapper to make the PipelineState json/dict export prettier. This is basically just a simplified version of the ValueSet class that is using pydantic, in order to make it easy to export to json. Source code in kiara/pipeline/__init__.py class PipelineValuesInfo ( BaseModel ): \"\"\"Convenience wrapper to make the [PipelineState][kiara.pipeline.pipeline.PipelineState] json/dict export prettier. This is basically just a simplified version of the [ValueSet][kiara.data.values.ValueSet] class that is using pydantic, in order to make it easy to export to json. \"\"\" @classmethod def from_value_set ( cls , value_set : ValueSet , ensure_metadata : bool = False ): values : typing . Dict [ str , PipelineValueInfo ] = {} for k in value_set . get_all_field_names (): v = value_set . get_value_obj ( k , ensure_metadata = ensure_metadata ) values [ k ] = PipelineValueInfo . from_value_obj ( v , ensure_metadata = ensure_metadata ) return PipelineValuesInfo ( values = values ) values : typing . Dict [ str , PipelineValueInfo ] = Field ( description = \"Field names are keys, and the data as values.\" ) class Config : use_enum_values = True values : Dict [ str , kiara . pipeline . PipelineValueInfo ] pydantic-field required \u00b6 Field names are keys, and the data as values. StepStatus ( Enum ) \u00b6 Enum to describe the state of a workflow. Source code in kiara/pipeline/__init__.py class StepStatus ( Enum ): \"\"\"Enum to describe the state of a workflow.\"\"\" STALE = \"stale\" INPUTS_READY = \"inputs_ready\" RESULTS_INCOMING = \"processing\" RESULTS_READY = \"results_ready\" StepValueAddress ( BaseModel ) pydantic-model \u00b6 Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure. Source code in kiara/pipeline/__init__.py class StepValueAddress ( BaseModel ): \"\"\"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.\"\"\" class Config : extra = Extra . forbid step_id : str = Field ( description = \"The id of a step within a pipeline.\" ) value_name : str = Field ( description = \"The name of the value (output name or pipeline input name).\" ) sub_value : typing . Optional [ typing . Dict [ str , typing . Any ]] = Field ( default = None , description = \"A reference to a subitem of a value (e.g. column, list item)\" , ) @property def alias ( self ): \"\"\"An alias string for this address (in the form ``[step_id].[value_name]``).\"\"\" return generate_step_alias ( self . step_id , self . value_name ) def __eq__ ( self , other ): if not isinstance ( other , StepValueAddress ): return False return ( self . step_id , self . value_name , self . sub_value ) == ( other . step_id , other . value_name , other . sub_value , ) def __hash__ ( self ): return hash (( self . step_id , self . value_name , self . sub_value )) def __repr__ ( self ): if self . sub_value : sub_value = f \" sub_value= { self . sub_value } \" else : sub_value = \"\" return f \"StepValueAddres(step_id= { self . step_id } , value_name= { self . value_name }{ sub_value } )\" def __str__ ( self ): return self . __repr__ () alias property readonly \u00b6 An alias string for this address (in the form [step_id].[value_name] ). step_id : str pydantic-field required \u00b6 The id of a step within a pipeline. sub_value : Dict [ str , Any ] pydantic-field \u00b6 A reference to a subitem of a value (e.g. column, list item) value_name : str pydantic-field required \u00b6 The name of the value (output name or pipeline input name). __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/pipeline/__init__.py def __eq__ ( self , other ): if not isinstance ( other , StepValueAddress ): return False return ( self . step_id , self . value_name , self . sub_value ) == ( other . step_id , other . value_name , other . sub_value , ) __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/pipeline/__init__.py def __hash__ ( self ): return hash (( self . step_id , self . value_name , self . sub_value )) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/pipeline/__init__.py def __repr__ ( self ): if self . sub_value : sub_value = f \" sub_value= { self . sub_value } \" else : sub_value = \"\" return f \"StepValueAddres(step_id= { self . step_id } , value_name= { self . value_name }{ sub_value } )\" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/pipeline/__init__.py def __str__ ( self ): return self . __repr__ () config \u00b6 PipelineConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 A class to hold the configuration for a PipelineModule . If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify auto , in which case Kiara will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a nand logic gate (in Python): and_step = PipelineStepConfig ( module_type = \"and\" , step_id = \"and\" ) not_step = PipelineStepConfig ( module_type = \"not\" , step_id = \"not\" , input_links = { \"a\" : [ \"and.y\" ]} nand_p_conf = PipelineConfig ( doc = \"Returns 'False' if both inputs are 'True'.\" , steps = [ and_step , not_step ], input_aliases = { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, output_aliases = { \"not__y\" : \"y\" }} Or, the same thing in json: { \"module_type_name\" : \"nand\" , \"doc\" : \"Returns 'False' if both inputs are 'True'.\" , \"steps\" : [ { \"module_type\" : \"and\" , \"step_id\" : \"and\" }, { \"module_type\" : \"not\" , \"step_id\" : \"not\" , \"input_links\" : { \"a\" : \"and.y\" } } ], \"input_aliases\" : { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, \"output_aliases\" : { \"not__y\" : \"y\" } } Source code in kiara/pipeline/config.py class PipelineConfig ( ModuleTypeConfigSchema ): \"\"\"A class to hold the configuration for a [PipelineModule][kiara.pipeline.module.PipelineModule]. If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify ``auto``, in which case *Kiara* will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a ``nand`` logic gate (in Python): ``` python and_step = PipelineStepConfig(module_type=\"and\", step_id=\"and\") not_step = PipelineStepConfig(module_type=\"not\", step_id=\"not\", input_links={\"a\": [\"and.y\"]} nand_p_conf = PipelineConfig(doc=\"Returns 'False' if both inputs are 'True'.\", steps=[and_step, not_step], input_aliases={ \"and__a\": \"a\", \"and__b\": \"b\" }, output_aliases={ \"not__y\": \"y\" }} ``` Or, the same thing in json: ``` json { \"module_type_name\": \"nand\", \"doc\": \"Returns 'False' if both inputs are 'True'.\", \"steps\": [ { \"module_type\": \"and\", \"step_id\": \"and\" }, { \"module_type\": \"not\", \"step_id\": \"not\", \"input_links\": { \"a\": \"and.y\" } } ], \"input_aliases\": { \"and__a\": \"a\", \"and__b\": \"b\" }, \"output_aliases\": { \"not__y\": \"y\" } } ``` \"\"\" # @classmethod # def from_file(cls, path: typing.Union[str, Path]): # # content = get_data_from_file(path) # return PipelineConfig(**content) @classmethod def create_pipeline_config ( cls , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineConfig\" : \"\"\"Create a PipelineModule instance. The main 'config' argument here can be either: - a string: in which case it needs to be (in that order): - a module id - an operation id - a path to a local file - a [ModuleConfig][kiara.module_config.ModuleConfig] object - a dict (to create a `ModuleInstsanceConfig` from Arguments: kiara: the kiara context config: the 'main' config object module_config: in case the 'main' config object was a module id, this argument is used to instantiate the module kiara: the kiara context (will use default context instance if not provided) \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () module_config_obj = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) if not module_config_obj . module_type == \"pipeline\" : raise Exception ( f \"Not a valid pipeline configuration: { config } \" ) # TODO: this is a bit round-about, to create a module config first, but it probably doesn't matter pipeline_config_data = module_config_obj . module_config module : PipelineConfig = PipelineConfig ( ** pipeline_config_data ) return module class Config : extra = Extra . allow validate_assignment = True steps : typing . List [ PipelineStepConfig ] = Field ( default_factory = list , description = \"A list of steps/modules of this pipeline, and their connections.\" , ) input_aliases : typing . Union [ str , typing . Dict [ str , str ]] = Field ( default_factory = dict , description = \"A map of input aliases, with the calculated (<step_id>__<input_name> -- double underscore!) name as key, and a string (the resulting workflow input alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.\" , ) output_aliases : typing . Union [ str , typing . Dict [ str , str ]] = Field ( default_factory = dict , description = \"A map of output aliases, with the calculated (<step_id>__<output_name> -- double underscore!) name as key, and a string (the resulting workflow output alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.\" , ) documentation : str = Field ( default = \"-- n/a --\" , description = \"Documentation about what the pipeline does.\" ) context : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"Metadata for this workflow.\" ) @validator ( \"steps\" , pre = True ) def _validate_steps ( cls , v ): steps = [] for step in v : if isinstance ( step , PipelineStepConfig ): steps . append ( step ) elif isinstance ( step , typing . Mapping ): steps . append ( PipelineStepConfig ( ** step )) else : raise TypeError ( step ) return steps def create_pipeline_structure ( self , kiara : typing . Optional [ \"Kiara\" ] = None ) -> \"PipelineStructure\" : from kiara import Kiara , PipelineStructure if kiara is None : kiara = Kiara . instance () ps = PipelineStructure ( config = self , kiara = kiara , ) return ps def create_pipeline ( self , controller : typing . Optional [ \"PipelineController\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): # if parent_id is None: # parent_id = DEFAULT_PIPELINE_PARENT_ID structure = self . create_pipeline_structure ( kiara = kiara ) from kiara import Pipeline pipeline = Pipeline ( structure = structure , controller = controller , ) return pipeline def create_pipeline_module ( self , module_id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineModule\" : if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () from kiara.pipeline.module import PipelineModule module = PipelineModule ( id = module_id , parent_id = parent_id , module_config = self , kiara = kiara , ) return module # def __rich_console__( # self, console: Console, options: ConsoleOptions # ) -> RenderResult: # # table = Table(show_header=False, box=box.SIMPLE) context : Dict [ str , Any ] pydantic-field \u00b6 Metadata for this workflow. documentation : str pydantic-field \u00b6 Documentation about what the pipeline does. input_aliases : Union [ str , Dict [ str , str ]] pydantic-field \u00b6 A map of input aliases, with the calculated ( __ -- double underscore!) name as key, and a string (the resulting workflow input alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible. output_aliases : Union [ str , Dict [ str , str ]] pydantic-field \u00b6 A map of output aliases, with the calculated ( __ -- double underscore!) name as key, and a string (the resulting workflow output alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible. steps : List [ kiara . pipeline . config . PipelineStepConfig ] pydantic-field \u00b6 A list of steps/modules of this pipeline, and their connections. create_pipeline_config ( config , module_config = None , kiara = None ) classmethod \u00b6 Create a PipelineModule instance. The main 'config' argument here can be either: a string: in which case it needs to be (in that order): a module id an operation id a path to a local file a ModuleConfig object a dict (to create a ModuleInstsanceConfig from Parameters: Name Type Description Default kiara Optional[Kiara] the kiara context None config Union[kiara.module_config.ModuleConfig, Mapping[str, Any], str] the 'main' config object required module_config Optional[Mapping[str, Any]] in case the 'main' config object was a module id, this argument is used to instantiate the module None kiara Optional[Kiara] the kiara context (will use default context instance if not provided) None Source code in kiara/pipeline/config.py @classmethod def create_pipeline_config ( cls , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineConfig\" : \"\"\"Create a PipelineModule instance. The main 'config' argument here can be either: - a string: in which case it needs to be (in that order): - a module id - an operation id - a path to a local file - a [ModuleConfig][kiara.module_config.ModuleConfig] object - a dict (to create a `ModuleInstsanceConfig` from Arguments: kiara: the kiara context config: the 'main' config object module_config: in case the 'main' config object was a module id, this argument is used to instantiate the module kiara: the kiara context (will use default context instance if not provided) \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () module_config_obj = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) if not module_config_obj . module_type == \"pipeline\" : raise Exception ( f \"Not a valid pipeline configuration: { config } \" ) # TODO: this is a bit round-about, to create a module config first, but it probably doesn't matter pipeline_config_data = module_config_obj . module_config module : PipelineConfig = PipelineConfig ( ** pipeline_config_data ) return module PipelineStepConfig ( ModuleConfig ) pydantic-model \u00b6 A class to hold the configuration of one module within a PipelineModule . Source code in kiara/pipeline/config.py class PipelineStepConfig ( ModuleConfig ): \"\"\"A class to hold the configuration of one module within a [PipelineModule][kiara.pipeline.module.PipelineModule].\"\"\" class Config : extra = Extra . forbid validate_assignment = True step_id : str = Field ( description = \"The id of the step.\" ) input_links : typing . Dict [ str , typing . List [ StepValueAddress ]] = Field ( default_factory = dict , description = \"The map with the name of an input link as key, and the connected module output name(s) as value.\" , ) @root_validator ( pre = True ) def create_step_id ( cls , values ): if \"module_type\" not in values : raise ValueError ( \"No 'module_type' specified.\" ) if \"step_id\" not in values or not values [ \"step_id\" ]: values [ \"step_id\" ] = slugify ( values [ \"module_type\" ]) return values @validator ( \"step_id\" ) def ensure_valid_id ( cls , v ): # TODO: check with regex if \".\" in v or \" \" in v : raise ValueError ( f \"Step id can't contain special characters or whitespaces: { v } \" ) return v @validator ( \"module_config\" , pre = True ) def ensure_dict ( cls , v ): if v is None : v = {} return v @validator ( \"input_links\" , pre = True ) def ensure_input_links_valid ( cls , v ): if v is None : v = {} result = {} for input_name , output in v . items (): input_links = ensure_step_value_addresses ( default_field_name = input_name , link = output ) result [ input_name ] = input_links return result input_links : Dict [ str , List [ kiara . pipeline . StepValueAddress ]] pydantic-field \u00b6 The map with the name of an input link as key, and the connected module output name(s) as value. step_id : str pydantic-field required \u00b6 The id of the step. StepDesc ( BaseModel ) pydantic-model \u00b6 Details of a single PipelineStep (which lives within a Pipeline Source code in kiara/pipeline/config.py class StepDesc ( BaseModel ): \"\"\"Details of a single [PipelineStep][kiara.pipeline.structure.PipelineStep] (which lives within a [Pipeline][kiara.pipeline.pipeline.Pipeline]\"\"\" class Config : allow_mutation = False extra = Extra . forbid step : PipelineStep = Field ( description = \"Attributes of the step itself.\" ) processing_stage : int = Field ( description = \"The processing stage of this step within a Pipeline.\" ) input_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"\"\"A map that explains what elements connect to this steps inputs. A connection could either be a Pipeline input (indicated by the ``__pipeline__`` token), or another steps output. Example: ``` json input_connections: { \"a\": [\"__pipeline__.a\"], \"b\": [\"step_one.a\"] } ``` \"\"\" ) output_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"A map that explains what elemnts connect to this steps outputs. A connection could be either a Pipeline output, or another steps input.\" ) required : bool = Field ( description = \"Whether this step is always required, or potentially could be skipped in case some inputs are not available.\" ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield f \"[b]Step: { self . step . step_id } [ \\b ]\" input_connections : Dict [ str , List [ str ]] pydantic-field required \u00b6 A map that explains what elements connect to this steps inputs. A connection could either be a Pipeline input (indicated by the __pipeline__ token), or another steps output. Examples: i n pu t _co nne c t io ns : { \"a\" : [ \"__pipeline__.a\" ], \"b\" : [ \"step_one.a\" ] } output_connections : Dict [ str , List [ str ]] pydantic-field required \u00b6 A map that explains what elemnts connect to this steps outputs. A connection could be either a Pipeline output, or another steps input. processing_stage : int pydantic-field required \u00b6 The processing stage of this step within a Pipeline. required : bool pydantic-field required \u00b6 Whether this step is always required, or potentially could be skipped in case some inputs are not available. step : PipelineStep pydantic-field required \u00b6 Attributes of the step itself. controller special \u00b6 PipelineController ( PipelineListener ) \u00b6 An object that controls how a Pipeline should react to events related to it's inputs/outputs. This is the base for the central controller class that needs to be implemented by a Kiara frontend. The default implementation that is used if no PipelineController is provided in a Pipeline constructor is the BatchController , which basically waits until all required inputs are set, and then processes all pipeline steps in one go (in the right order). The pipeline object to control can be set either in the constructor, or via the set_pipeline method. But only once, every subsequent attempt to set a pipeline will raise an Exception. If you want to implement your own controller, you have to override at least one of the (empty) event hook methods: pipeline_inputs_changed pipeline_outputs_changed step_inputs_changed step_outputs_changed Parameters: Name Type Description Default pipeline Pipeline the pipeline object to control None Source code in kiara/pipeline/controller/__init__.py class PipelineController ( PipelineListener ): \"\"\"An object that controls how a [Pipeline][kiara.pipeline.pipeline.Pipeline] should react to events related to it's inputs/outputs. This is the base for the central controller class that needs to be implemented by a *Kiara* frontend. The default implementation that is used if no ``PipelineController`` is provided in a [Pipeline][kiara.pipeline.pipeline.Pipeline] constructor is the [BatchController][kiara.pipeline.controller.BatchController], which basically waits until all required inputs are set, and then processes all pipeline steps in one go (in the right order). The pipeline object to control can be set either in the constructor, or via the ``set_pipeline`` method. But only once, every subsequent attempt to set a pipeline will raise an Exception. If you want to implement your own controller, you have to override at least one of the (empty) event hook methods: - [``pipeline_inputs_changed``][kiara.pipeline.controller.PipelineController.pipeline_inputs_changed] - [``pipeline_outputs_changed``][kiara.pipeline.controller.PipelineController.pipeline_outputs_changed] - [``step_inputs_changed``][kiara.pipeline.controller.PipelineController.step_inputs_changed] - [``step_outputs_changed``][kiara.pipeline.controller.PipelineController.step_outputs_changed] Arguments: pipeline (Pipeline): the pipeline object to control \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : Kiara = kiara self . _pipeline : typing . Optional [ Pipeline ] = None if processor is None : from kiara.processing.synchronous import SynchronousProcessor processor = SynchronousProcessor ( kiara = kiara ) self . _processor : ModuleProcessor = processor self . _job_ids : typing . Dict [ str , str ] = {} \"\"\"A map of the last or current job ids per step_id.\"\"\" if pipeline is not None : self . set_pipeline ( pipeline ) @property def pipeline ( self ) -> \"Pipeline\" : \"\"\"Return the pipeline this controller, well, ...controls...\"\"\" if self . _pipeline is None : raise Exception ( \"Pipeline not set yet.\" ) return self . _pipeline @property def pipeline_status ( self ) -> \"StepStatus\" : return self . pipeline . status def set_pipeline ( self , pipeline : \"Pipeline\" ): \"\"\"Set the pipeline object for this controller. Only one pipeline can be set, once. Arguments: pipeline: the pipeline object \"\"\" if self . _pipeline is not None : raise Exception ( \"Pipeline already set.\" ) self . _pipeline = pipeline @property def processing_stages ( self ) -> typing . List [ typing . List [ str ]]: \"\"\"Return the processing stage order of the pipeline. Returns: a list of lists of step ids \"\"\" return self . pipeline . structure . processing_stages def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the step object for the provided id. Arguments: step_id: the step id Returns: the step object \"\"\" return self . pipeline . get_step ( step_id ) def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the inputs object for the pipeline.\"\"\" return self . pipeline . get_step_inputs ( step_id ) def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the outputs object for the pipeline.\"\"\" return self . pipeline . get_step_outputs ( step_id ) def get_step_input ( self , step_id : str , input_name : str ) -> Value : \"\"\"Get the (current) input value for a specified step and input field name.\"\"\" item = self . get_step_inputs ( step_id ) . get_value_obj ( input_name ) assert item is not None return item def get_step_output ( self , step_id : str , output_name : str ) -> Value : \"\"\"Get the (current) output value for a specified step and output field name.\"\"\" item = self . get_step_outputs ( step_id ) . get_value_obj ( output_name ) assert item is not None return item def get_current_pipeline_state ( self ) -> \"PipelineState\" : \"\"\"Return a description of the current pipeline state. This methods creates a new [PipelineState][kiara.pipeline.pipeline.PipelineState] object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: an object outlining the current pipeline state \"\"\" return self . pipeline . get_current_state () @property def pipeline_inputs ( self ) -> ValueSet : \"\"\"Return the inputs object for this pipeline.\"\"\" return self . pipeline . _pipeline_inputs @pipeline_inputs . setter def pipeline_inputs ( self , inputs : typing . Mapping [ str , typing . Any ]) -> None : \"\"\"Set one, several or all inputs for this pipeline.\"\"\" self . set_pipeline_inputs ( ** inputs ) @property def pipeline_outputs ( self ) -> ValueSet : \"\"\"Return the (current) pipeline outputs object for this pipeline.\"\"\" return self . pipeline . _pipeline_outputs def wait_for_jobs ( self , * job_ids : str , sync_outputs : bool = True ): self . _processor . wait_for ( * job_ids , sync_outputs = sync_outputs ) def can_be_processed ( self , step_id : str ) -> bool : \"\"\"Check whether the step with the provided id is ready to be processed.\"\"\" result = True step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): result = False break return result def check_inputs_status ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]]: \"\"\"Check the status of all stages/steps, and whether they are ready to be processed. The result dictionary uses stage numbers as keys, and a secondary dictionary as values which in turn uses the step_id as key, and a dict with details (keys: 'ready', 'invalid_inputs', 'required') as values. \"\"\" result : typing . Dict [ int , typing . Dict [ str , typing . Dict [ str , typing . Any ]]] = {} for idx , stage in enumerate ( self . processing_stages , start = 1 ): for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], } else : result . setdefault ( idx , {})[ step_id ] = { \"ready\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], \"required\" : True , } else : if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : True , \"invalid_inputs\" : [], } return result def can_be_skipped ( self , step_id : str ) -> bool : \"\"\"Check whether the processing of a step can be skipped.\"\"\" result = True step = self . get_step ( step_id ) if step . required : result = self . can_be_processed ( step_id ) return result def invalid_inputs ( self , step_id : str ) -> typing . List [ str ]: invalid = [] step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): invalid . append ( input_name ) return invalid def process_step ( self , step_id : str , raise_exception : bool = False , wait : bool = False ) -> str : \"\"\"Kick off processing for the step with the provided id. Arguments: step_id: the id of the step that should be started \"\"\" step_inputs = self . get_step_inputs ( step_id ) # if the inputs are not valid, ignore this step if not step_inputs . items_are_valid (): status = step_inputs . check_invalid () assert status is not None raise Exception ( f \"Can't execute step ' { step_id } ', invalid inputs: { ', ' . join ( status . keys ()) } \" ) # get the output 'holder' objects, which we'll need to pass to the module step_outputs = self . get_step_outputs ( step_id ) # get the module object that holds the code that will do the processing step = self . get_step ( step_id ) job_id = self . _processor . start ( pipeline_id = self . pipeline . id , pipeline_name = self . pipeline . title , step_id = step_id , module = step . module , inputs = step_inputs , outputs = step_outputs , ) self . _job_ids [ step_id ] = job_id if wait : self . wait_for_jobs ( job_id , sync_outputs = True ) return job_id def get_job_details ( self , step_or_job_id : str ) -> typing . Optional [ Job ]: \"\"\"Returns job details for a job id, or in case a step_id was provided, the last execution of this step.\"\"\" if self . _job_ids . get ( step_or_job_id , None ) is None : return self . _processor . get_job_details ( step_or_job_id ) else : return self . _processor . get_job_details ( self . _job_ids [ step_or_job_id ]) def step_is_ready ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id is ready to be processed. A ``True`` result means that all input fields are currently set with valid values. Arguments: step_id: the id of the step to check Returns: whether the step is ready (``True``) or not (``False``) \"\"\" return self . get_step_inputs ( step_id ) . items_are_valid () def step_is_finished ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id has been processed successfully. A ``True`` result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Arguments: step_id: the id of the step to check Returns: whether the step result is valid (``True``) or not (``False``) \"\"\" return self . get_step_outputs ( step_id ) . items_are_valid () def pipeline_is_ready ( self ) -> bool : \"\"\"Return whether the pipeline is ready to be processed. A ``True`` result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: whether the pipeline can be processed as a whole (``True``) or not (``False``) \"\"\" return self . pipeline . inputs . items_are_valid () def pipeline_is_finished ( self ) -> bool : \"\"\"Return whether the pipeline has been processed successfully. A ``True`` result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: whether the pipeline was processed successfully (``True``) or not (``False``) \"\"\" return self . pipeline . outputs . items_are_valid () def set_pipeline_inputs ( self , ** inputs : typing . Any ): \"\"\"Set one, several or all inputs for this pipeline. Arguments: **inputs: the input values to set \"\"\" _inputs = self . _pipeline_input_hook ( ** inputs ) self . pipeline_inputs . set_values ( ** _inputs ) def _pipeline_input_hook ( self , ** inputs : typing . Any ): \"\"\"Hook before setting input. Can be implemented by child controller classes, to prevent, transform, validate or queue inputs. \"\"\" log . debug ( f \"Inputs for pipeline ' { self . pipeline . id } ' set: { inputs } \" ) return inputs def process_pipeline ( self ): \"\"\"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Returns: \"\"\" raise NotImplementedError () pipeline : Pipeline property readonly \u00b6 Return the pipeline this controller, well, ...controls... pipeline_inputs : ValueSet property writable \u00b6 Return the inputs object for this pipeline. pipeline_outputs : ValueSet property readonly \u00b6 Return the (current) pipeline outputs object for this pipeline. processing_stages : List [ List [ str ]] property readonly \u00b6 Return the processing stage order of the pipeline. Returns: Type Description List[List[str]] a list of lists of step ids can_be_processed ( self , step_id ) \u00b6 Check whether the step with the provided id is ready to be processed. Source code in kiara/pipeline/controller/__init__.py def can_be_processed ( self , step_id : str ) -> bool : \"\"\"Check whether the step with the provided id is ready to be processed.\"\"\" result = True step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): result = False break return result can_be_skipped ( self , step_id ) \u00b6 Check whether the processing of a step can be skipped. Source code in kiara/pipeline/controller/__init__.py def can_be_skipped ( self , step_id : str ) -> bool : \"\"\"Check whether the processing of a step can be skipped.\"\"\" result = True step = self . get_step ( step_id ) if step . required : result = self . can_be_processed ( step_id ) return result check_inputs_status ( self ) \u00b6 Check the status of all stages/steps, and whether they are ready to be processed. The result dictionary uses stage numbers as keys, and a secondary dictionary as values which in turn uses the step_id as key, and a dict with details (keys: 'ready', 'invalid_inputs', 'required') as values. Source code in kiara/pipeline/controller/__init__.py def check_inputs_status ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]]: \"\"\"Check the status of all stages/steps, and whether they are ready to be processed. The result dictionary uses stage numbers as keys, and a secondary dictionary as values which in turn uses the step_id as key, and a dict with details (keys: 'ready', 'invalid_inputs', 'required') as values. \"\"\" result : typing . Dict [ int , typing . Dict [ str , typing . Dict [ str , typing . Any ]]] = {} for idx , stage in enumerate ( self . processing_stages , start = 1 ): for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], } else : result . setdefault ( idx , {})[ step_id ] = { \"ready\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], \"required\" : True , } else : if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : True , \"invalid_inputs\" : [], } return result get_current_pipeline_state ( self ) \u00b6 Return a description of the current pipeline state. This methods creates a new PipelineState object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: Type Description PipelineState an object outlining the current pipeline state Source code in kiara/pipeline/controller/__init__.py def get_current_pipeline_state ( self ) -> \"PipelineState\" : \"\"\"Return a description of the current pipeline state. This methods creates a new [PipelineState][kiara.pipeline.pipeline.PipelineState] object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: an object outlining the current pipeline state \"\"\" return self . pipeline . get_current_state () get_job_details ( self , step_or_job_id ) \u00b6 Returns job details for a job id, or in case a step_id was provided, the last execution of this step. Source code in kiara/pipeline/controller/__init__.py def get_job_details ( self , step_or_job_id : str ) -> typing . Optional [ Job ]: \"\"\"Returns job details for a job id, or in case a step_id was provided, the last execution of this step.\"\"\" if self . _job_ids . get ( step_or_job_id , None ) is None : return self . _processor . get_job_details ( step_or_job_id ) else : return self . _processor . get_job_details ( self . _job_ids [ step_or_job_id ]) get_step ( self , step_id ) \u00b6 Return the step object for the provided id. Parameters: Name Type Description Default step_id str the step id required Returns: Type Description PipelineStep the step object Source code in kiara/pipeline/controller/__init__.py def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the step object for the provided id. Arguments: step_id: the step id Returns: the step object \"\"\" return self . pipeline . get_step ( step_id ) get_step_input ( self , step_id , input_name ) \u00b6 Get the (current) input value for a specified step and input field name. Source code in kiara/pipeline/controller/__init__.py def get_step_input ( self , step_id : str , input_name : str ) -> Value : \"\"\"Get the (current) input value for a specified step and input field name.\"\"\" item = self . get_step_inputs ( step_id ) . get_value_obj ( input_name ) assert item is not None return item get_step_inputs ( self , step_id ) \u00b6 Return the inputs object for the pipeline. Source code in kiara/pipeline/controller/__init__.py def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the inputs object for the pipeline.\"\"\" return self . pipeline . get_step_inputs ( step_id ) get_step_output ( self , step_id , output_name ) \u00b6 Get the (current) output value for a specified step and output field name. Source code in kiara/pipeline/controller/__init__.py def get_step_output ( self , step_id : str , output_name : str ) -> Value : \"\"\"Get the (current) output value for a specified step and output field name.\"\"\" item = self . get_step_outputs ( step_id ) . get_value_obj ( output_name ) assert item is not None return item get_step_outputs ( self , step_id ) \u00b6 Return the outputs object for the pipeline. Source code in kiara/pipeline/controller/__init__.py def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the outputs object for the pipeline.\"\"\" return self . pipeline . get_step_outputs ( step_id ) pipeline_is_finished ( self ) \u00b6 Return whether the pipeline has been processed successfully. A True result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: Type Description bool whether the pipeline was processed successfully ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def pipeline_is_finished ( self ) -> bool : \"\"\"Return whether the pipeline has been processed successfully. A ``True`` result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: whether the pipeline was processed successfully (``True``) or not (``False``) \"\"\" return self . pipeline . outputs . items_are_valid () pipeline_is_ready ( self ) \u00b6 Return whether the pipeline is ready to be processed. A True result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: Type Description bool whether the pipeline can be processed as a whole ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def pipeline_is_ready ( self ) -> bool : \"\"\"Return whether the pipeline is ready to be processed. A ``True`` result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: whether the pipeline can be processed as a whole (``True``) or not (``False``) \"\"\" return self . pipeline . inputs . items_are_valid () process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/__init__.py def process_pipeline ( self ): \"\"\"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Returns: \"\"\" raise NotImplementedError () process_step ( self , step_id , raise_exception = False , wait = False ) \u00b6 Kick off processing for the step with the provided id. Parameters: Name Type Description Default step_id str the id of the step that should be started required Source code in kiara/pipeline/controller/__init__.py def process_step ( self , step_id : str , raise_exception : bool = False , wait : bool = False ) -> str : \"\"\"Kick off processing for the step with the provided id. Arguments: step_id: the id of the step that should be started \"\"\" step_inputs = self . get_step_inputs ( step_id ) # if the inputs are not valid, ignore this step if not step_inputs . items_are_valid (): status = step_inputs . check_invalid () assert status is not None raise Exception ( f \"Can't execute step ' { step_id } ', invalid inputs: { ', ' . join ( status . keys ()) } \" ) # get the output 'holder' objects, which we'll need to pass to the module step_outputs = self . get_step_outputs ( step_id ) # get the module object that holds the code that will do the processing step = self . get_step ( step_id ) job_id = self . _processor . start ( pipeline_id = self . pipeline . id , pipeline_name = self . pipeline . title , step_id = step_id , module = step . module , inputs = step_inputs , outputs = step_outputs , ) self . _job_ids [ step_id ] = job_id if wait : self . wait_for_jobs ( job_id , sync_outputs = True ) return job_id set_pipeline ( self , pipeline ) \u00b6 Set the pipeline object for this controller. Only one pipeline can be set, once. Parameters: Name Type Description Default pipeline Pipeline the pipeline object required Source code in kiara/pipeline/controller/__init__.py def set_pipeline ( self , pipeline : \"Pipeline\" ): \"\"\"Set the pipeline object for this controller. Only one pipeline can be set, once. Arguments: pipeline: the pipeline object \"\"\" if self . _pipeline is not None : raise Exception ( \"Pipeline already set.\" ) self . _pipeline = pipeline set_pipeline_inputs ( self , ** inputs ) \u00b6 Set one, several or all inputs for this pipeline. Parameters: Name Type Description Default **inputs Any the input values to set {} Source code in kiara/pipeline/controller/__init__.py def set_pipeline_inputs ( self , ** inputs : typing . Any ): \"\"\"Set one, several or all inputs for this pipeline. Arguments: **inputs: the input values to set \"\"\" _inputs = self . _pipeline_input_hook ( ** inputs ) self . pipeline_inputs . set_values ( ** _inputs ) step_is_finished ( self , step_id ) \u00b6 Return whether the step with the provided id has been processed successfully. A True result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Parameters: Name Type Description Default step_id str the id of the step to check required Returns: Type Description bool whether the step result is valid ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def step_is_finished ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id has been processed successfully. A ``True`` result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Arguments: step_id: the id of the step to check Returns: whether the step result is valid (``True``) or not (``False``) \"\"\" return self . get_step_outputs ( step_id ) . items_are_valid () step_is_ready ( self , step_id ) \u00b6 Return whether the step with the provided id is ready to be processed. A True result means that all input fields are currently set with valid values. Parameters: Name Type Description Default step_id str the id of the step to check required Returns: Type Description bool whether the step is ready ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def step_is_ready ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id is ready to be processed. A ``True`` result means that all input fields are currently set with valid values. Arguments: step_id: the id of the step to check Returns: whether the step is ready (``True``) or not (``False``) \"\"\" return self . get_step_inputs ( step_id ) . items_are_valid () batch \u00b6 BatchController ( PipelineController ) \u00b6 A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline Optional[Pipeline] the pipeline to control None auto_process bool whether to automatically start processing the pipeline as soon as the input set is valid True Source code in kiara/pipeline/controller/batch.py class BatchController ( PipelineController ): \"\"\"A [PipelineController][kiara.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively. This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Arguments: pipeline: the pipeline to control auto_process: whether to automatically start processing the pipeline as soon as the input set is valid \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , auto_process : bool = True , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _auto_process : bool = auto_process self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor , kiara = kiara ) @property def auto_process ( self ) -> bool : return self . _auto_process @auto_process . setter def auto_process ( self , auto_process : bool ): self . _auto_process = auto_process def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_exc () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) # for j_id in job_ids: # job = self._processor.get_job_details(j_id) # assert job is not None # if job.error: # print(job.error) finally : self . _is_running = False def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return if self . _auto_process : self . process_pipeline () def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_exc () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) # for j_id in job_ids: # job = self._processor.get_job_details(j_id) # assert job is not None # if job.error: # print(job.error) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return if self . _auto_process : self . process_pipeline () BatchControllerManual ( PipelineController ) \u00b6 A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline Optional[Pipeline] the pipeline to control None auto_process whether to automatically start processing the pipeline as soon as the input set is valid required Source code in kiara/pipeline/controller/batch.py class BatchControllerManual ( PipelineController ): \"\"\"A [PipelineController][kiara.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively. This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Arguments: pipeline: the pipeline to control auto_process: whether to automatically start processing the pipeline as soon as the input set is valid \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _finished_until : typing . Optional [ int ] = None self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor , kiara = kiara ) @property def last_finished_stage ( self ) -> int : if self . _finished_until is None : return 0 else : return self . _finished_until def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return def pipeline_inputs_changed ( self , event : \"PipelineInputEvent\" ): self . _finished_until = None # print(\"============================\") # min_stage_to_clear = sys.maxsize # for inp in event.updated_pipeline_inputs: # stage = self.pipeline.get_stage_for_pipeline_input(inp) # if stage < min_stage_to_clear: # min_stage_to_clear = stage # # for stage, steps in self.pipeline.get_steps_by_stage().items(): # if stage < min_stage_to_clear: # continue # for step_id, step in steps.items(): # step_inputs = self.get_step_inputs(step_id) # empty = { k: None for k in step_inputs.keys() } # step_inputs.set_values(**empty) # step_outputs = self.get_step_outputs(step_id) # empty = { k: None for k in step_outputs.keys() } # step_outputs.set_values(**empty) def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False def process_stage ( self , stage_nr : int ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Union [ None , str , Exception ]]]: if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True result : typing . Dict [ int , typing . Dict [ str , typing . Union [ None , str , Exception ]] ] = {} try : for idx , stage in enumerate ( self . processing_stages ): current_stage = idx + 1 if current_stage > stage_nr : break if self . _finished_until is not None and idx <= self . _finished_until : continue job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( current_stage , {})[ step_id ] = None continue else : exc = Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) result . setdefault ( current_stage , {})[ step_id ] = exc try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) result . setdefault ( current_stage , {})[ step_id ] = job_id except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) result . setdefault ( current_stage , {})[ step_id ] = e self . _processor . wait_for ( * job_ids ) # for job_id in job_ids: # job = self.get_job_details(job_id) # dbg(job.dict()) self . _finished_until = idx finally : self . _is_running = False print ( \"BATCH FINISHED\" ) return result pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/pipeline/controller/batch.py def pipeline_inputs_changed ( self , event : \"PipelineInputEvent\" ): self . _finished_until = None # print(\"============================\") # min_stage_to_clear = sys.maxsize # for inp in event.updated_pipeline_inputs: # stage = self.pipeline.get_stage_for_pipeline_input(inp) # if stage < min_stage_to_clear: # min_stage_to_clear = stage # # for stage, steps in self.pipeline.get_steps_by_stage().items(): # if stage < min_stage_to_clear: # continue # for step_id, step in steps.items(): # step_inputs = self.get_step_inputs(step_id) # empty = { k: None for k in step_inputs.keys() } # step_inputs.set_values(**empty) # step_outputs = self.get_step_outputs(step_id) # empty = { k: None for k in step_outputs.keys() } # step_outputs.set_values(**empty) pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return listeners \u00b6 PipelineListener ( ABC ) \u00b6 Source code in kiara/pipeline/listeners.py class PipelineListener ( abc . ABC ): def step_inputs_changed ( self , event : StepInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Arguments: event: the step input event \"\"\" def step_outputs_changed ( self , event : StepOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Arguments: event: the step output event \"\"\" def pipeline_inputs_changed ( self , event : PipelineInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. !!! note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the ``step_inputs_changed`` method if you want to. This behaviour might change in the future. Arguments: event: the pipeline input event \"\"\" def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Arguments: event: the pipeline output event \"\"\" pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/pipeline/listeners.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. !!! note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the ``step_inputs_changed`` method if you want to. This behaviour might change in the future. Arguments: event: the pipeline input event \"\"\" pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/listeners.py def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Arguments: event: the pipeline output event \"\"\" step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/listeners.py def step_inputs_changed ( self , event : StepInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Arguments: event: the step input event \"\"\" step_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Parameters: Name Type Description Default event StepOutputEvent the step output event required Source code in kiara/pipeline/listeners.py def step_outputs_changed ( self , event : StepOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Arguments: event: the step output event \"\"\" module \u00b6 PipelineModule ( KiaraModule ) \u00b6 A KiaraModule that contains a collection of interconnected other modules. Source code in kiara/pipeline/module.py class PipelineModule ( KiaraModule [ PipelineConfig ]): \"\"\"A [KiaraModule][kiara.module.KiaraModule] that contains a collection of interconnected other modules.\"\"\" _config_cls : typing . Type [ PipelineConfig ] = PipelineConfig # type: ignore _module_type_id = \"pipeline\" @classmethod def is_pipeline ( cls ) -> bool : return True def __init__ ( self , id : typing . Optional [ str ], parent_id : typing . Optional [ str ] = None , module_config : typing . Union [ None , PipelineConfig , typing . Mapping [ str , typing . Any ] ] = None , # controller: typing.Union[ # None, PipelineController, str, typing.Type[PipelineController] # ] = None, kiara : typing . Optional [ \"Kiara\" ] = None , ): # if controller is not None and not isinstance(controller, PipelineController): # raise NotImplementedError() # if controller is None: super () . __init__ ( id = id , parent_id = parent_id , module_config = module_config , kiara = kiara , ) self . _pipeline_structure : PipelineStructure = self . _create_structure () assert not self . _config . constants self . _config . constants = dict ( self . _pipeline_structure . constants ) @property def structure ( self ) -> PipelineStructure : \"\"\"The ``PipelineStructure`` of this module.\"\"\" return self . _pipeline_structure def _create_structure ( self ) -> PipelineStructure : pipeline_structure = PipelineStructure ( config = self . config , kiara = self . _kiara ) return pipeline_structure def create_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_input_schema def create_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_output_schema def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : from kiara import Pipeline # controller = BatchController(auto_process=False, kiara=self._kiara) pipeline = Pipeline ( structure = self . structure ) pipeline . inputs . set_values ( ** inputs ) if not pipeline . inputs . items_are_valid (): raise KiaraProcessingException ( f \"Can't start processing of { self . _module_type_id } pipeline: one or several inputs missing or invalid.\" ) # type: ignore if not pipeline . status == StepStatus . RESULTS_READY : # TODO: error details raise KiaraProcessingException ( f \"Error when running pipeline of type ' { self . _module_type_id } '.\" ) # type: ignore outputs . set_values ( ** pipeline . outputs ) structure : PipelineStructure property readonly \u00b6 The PipelineStructure of this module. create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/pipeline/module.py def create_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_input_schema create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/pipeline/module.py def create_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_output_schema is_pipeline () classmethod \u00b6 Check whether this module type is a pipeline, or not. Source code in kiara/pipeline/module.py @classmethod def is_pipeline ( cls ) -> bool : return True pipeline \u00b6 Pipeline \u00b6 An instance of a PipelineStructure that holds state for all of the inputs/outputs of the steps within. Source code in kiara/pipeline/pipeline.py class Pipeline ( object ): \"\"\"An instance of a [PipelineStructure][kiara.pipeline.structure.PipelineStructure] that holds state for all of the inputs/outputs of the steps within.\"\"\" def __init__ ( self , structure : PipelineStructure , # constants: typing.Optional[typing.Mapping[str, typing.Any]] = None, controller : typing . Optional [ PipelineController ] = None , title : typing . Optional [ str ] = None , ): self . _id : str = str ( uuid . uuid4 ()) if title is None : title = self . _id self . _title : str = title self . _structure : PipelineStructure = structure self . _pipeline_inputs : SlottedValueSet = None # type: ignore self . _pipeline_outputs : SlottedValueSet = None # type: ignore self . _step_inputs : typing . Mapping [ str , ValueSet ] = None # type: ignore self . _step_outputs : typing . Mapping [ str , ValueSet ] = None # type: ignore self . _value_refs : typing . Mapping [ ValueSlot , typing . Iterable [ ValueRef ]] = None # type: ignore self . _status : StepStatus = StepStatus . STALE self . _steps_by_stage : typing . Optional [ typing . Dict [ int , typing . Dict [ str , PipelineStep ]] ] = None self . _inputs_by_stage : typing . Optional [ typing . Dict [ int , typing . List [ str ]] ] = None self . _outputs_by_stage : typing . Optional [ typing . Dict [ int , typing . List [ str ]] ] = None self . _kiara : \"Kiara\" = self . _structure . _kiara self . _data_registry : DataRegistry = self . _kiara . data_registry self . _init_values () if controller is None : controller = BatchController ( self , kiara = self . _kiara ) else : controller . set_pipeline ( self ) self . _controller : PipelineController = controller self . _listeners : typing . List [ PipelineListener ] = [] self . _update_status () def __eq__ ( self , other ): if not isinstance ( other , Pipeline ): return False return self . _id == other . _id def __hash__ ( self ): return hash ( self . _id ) @property def id ( self ) -> str : return self . _id @property def title ( self ) -> str : return self . _title @property def structure ( self ) -> PipelineStructure : return self . _structure @property def controller ( self ) -> PipelineController : if self . _controller is None : raise Exception ( \"No controller set (yet).\" ) return self . _controller @property def inputs ( self ) -> SlottedValueSet : \"\"\"All (pipeline) input values of this pipeline.\"\"\" return self . _pipeline_inputs @property def outputs ( self ) -> SlottedValueSet : \"\"\"All (pipeline) output values of this pipeline.\"\"\" return self . _pipeline_outputs # def set_pipeline_inputs(self, **inputs: typing.Any): # self._controller.set_pipeline_inputs(**inputs) @property def step_ids ( self ) -> typing . Iterable [ str ]: \"\"\"Return all ids of the steps of this pipeline.\"\"\" return self . _structure . step_ids def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the object representing a step in this workflow, identified by the step id.\"\"\" return self . _structure . get_step ( step_id ) def get_steps_by_stage ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , PipelineStep ]]: \"\"\"Return a all pipeline steps, ordered by stage they belong to.\"\"\" if self . _steps_by_stage is not None : return self . _steps_by_stage result : typing . Dict [ int , typing . Dict [ str , PipelineStep ]] = {} for step_id in self . step_ids : step = self . get_step ( step_id ) stage = step . processing_stage assert stage is not None result . setdefault ( stage , {})[ step_id ] = step self . _steps_by_stage = result return self . _steps_by_stage def get_pipeline_inputs_by_stage ( self ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by stage they are first required.\"\"\" if self . _inputs_by_stage is not None : return self . _inputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . inputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepInputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _inputs_by_stage = result return self . _inputs_by_stage def get_pipeline_outputs_by_stage ( self , ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by the stage that needs to be executed before they are available.\"\"\" if self . _outputs_by_stage is not None : return self . _outputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . outputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepOutputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _outputs_by_stage = result return self . _outputs_by_stage def get_pipeline_inputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline inputs that are required for a stage to be processed. The result of this method does not include inputs that were required earlier already. \"\"\" return self . get_pipeline_inputs_by_stage () . get ( stage , []) def get_stage_for_pipeline_input ( self , input_name : str ) -> int : for stage , input_names in self . get_pipeline_inputs_by_stage () . items (): if input_name in input_names : return stage raise Exception ( f \"No input name ' { input_name } '. Available inputs: { ', ' . join ( self . inputs . keys ()) } \" ) def stage_for_pipeline_output ( self , output_name : str ) -> int : for stage , output_names in self . get_pipeline_outputs_by_stage () . items (): if output_name in output_names : return stage raise Exception ( f \"No output name ' { output_name } '. Available outputs: { ', ' . join ( self . outputs . keys ()) } \" ) def get_pipeline_outputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline outputs that are first available after the specified stage completed processing.\"\"\" return self . get_pipeline_outputs_by_stage () . get ( stage , []) def get_pipeline_inputs_for_step ( self , step_id : str ) -> typing . List [ str ]: result = [] for field_name , value_slot in self . inputs . _value_slots . items (): refs = self . _value_refs [ value_slot ] for ref in refs : if not isinstance ( ref , PipelineInputRef ): continue for ci in ref . connected_inputs : if ci . step_id == step_id and ref . value_name not in result : result . append ( ref . value_name ) return result def get_pipeline_outputs_for_step ( self , step_id : str ) -> typing . List [ str ]: result = [] for field_name , value_slot in self . outputs . _value_slots . items (): refs = self . _value_refs [ value_slot ] for ref in refs : if not isinstance ( ref , PipelineOutputRef ): continue if ( ref . connected_output . step_id == step_id and ref . value_name not in result ): result . append ( ref . value_name ) return result def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all inputs for a step id (incl. inputs that are not pipeline inputs but connected to other modules output).\"\"\" return self . _step_inputs [ step_id ] def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all outputs for a step id (incl. outputs that are not pipeline outputs).\"\"\" return self . _step_outputs [ step_id ] def add_listener ( self , listener : PipelineListener ) -> None : \"\"\"Add a listener taht gets notified on any internal pipeline input/output events.\"\"\" self . _listeners . append ( listener ) @property def status ( self ) -> StepStatus : \"\"\"Return the current status of this pipeline.\"\"\" return self . _state def _update_status ( self ): \"\"\"Make sure internal state variable is up to date.\"\"\" if self . inputs is None : new_state = StepStatus . STALE elif not self . inputs . items_are_valid (): new_state = StepStatus . STALE elif not self . outputs . items_are_valid (): new_state = StepStatus . INPUTS_READY else : new_state = StepStatus . RESULTS_READY self . _state = new_state def _init_values ( self ): \"\"\"Initialize this object. This should only be called once. Basically, this goes through all the inputs and outputs of all steps, and 'allocates' a PipelineValueInfo object for each of them. In case where output/input or pipeline-input/input points are connected, only one value item is allocated, since those refer to the same value. \"\"\" pipeline_inputs : typing . Dict [ str , ValueSlot ] = {} pipeline_outputs : typing . Dict [ str , ValueSlot ] = {} all_step_inputs : typing . Dict [ str , typing . Dict [ str , ValueSlot ]] = {} all_step_outputs : typing . Dict [ str , typing . Dict [ str , ValueSlot ]] = {} value_refs : typing . Dict [ ValueSlot , typing . List [ ValueRef ]] = {} # create the value objects that are associated with step outputs # all pipeline outputs are created here too, since the only place # those can be associated are step outputs for step_id , step_details in self . _structure . steps_details . items (): step_outputs : typing . Mapping [ str , StepOutputRef ] = step_details [ \"outputs\" ] for output_name , output_point in step_outputs . items (): init_output_value_item = self . _data_registry . register_data ( value_schema = output_point . value_schema ) output_value_slot = self . _data_registry . register_alias ( value_or_schema = init_output_value_item , callbacks = [ self ] ) value_refs . setdefault ( output_value_slot , []) . append ( output_point ) all_step_outputs . setdefault ( step_id , {})[ output_name ] = output_value_slot # not all step outputs necessarily need to be connected to a pipeline output if output_point . pipeline_output : pipeline_outputs [ output_point . pipeline_output ] = output_value_slot po = self . _structure . pipeline_outputs [ output_point . pipeline_output ] value_refs . setdefault ( output_value_slot , []) . append ( po ) # create the value objects that are associated with step inputs for step_id , step_details in self . _structure . steps_details . items (): step_inputs : typing . Mapping [ str , StepInputRef ] = step_details [ \"inputs\" ] for input_name , input_point in step_inputs . items (): # if this step input gets fed from a pipeline_input (meaning user input in most cases), # we need to create a DataValue for that pipeline input # vm = ValueMetadata( # origin=f\"{self.id}.steps.{step_id}.inputs.{input_point.value_name}\" # ) if input_point . connected_pipeline_input : connected_pipeline_input_name = input_point . connected_pipeline_input pipeline_input_field : PipelineInputRef = ( self . _structure . pipeline_inputs [ connected_pipeline_input_name ] ) pipeline_input_slot : ValueSlot = pipeline_inputs . get ( connected_pipeline_input_name , None ) if pipeline_input_slot is None : # if the pipeline input wasn't created by another step input before, # we need to take care of it here if pipeline_input_field . is_constant : init_value = self . structure . constants [ pipeline_input_field . value_name ] else : init_value = self . structure . defaults . get ( pipeline_input_field . value_name , SpecialValue . NOT_SET ) init_pipeline_input_value = self . _data_registry . register_data ( value_data = init_value , value_schema = pipeline_input_field . value_schema , ) # TODO: check whether it's a constant? pipeline_input_slot = self . _data_registry . register_alias ( value_or_schema = init_pipeline_input_value , callbacks = [ self ] ) value_refs . setdefault ( pipeline_input_slot , []) . append ( pipeline_input_field ) pipeline_inputs [ connected_pipeline_input_name ] = pipeline_input_slot all_step_inputs . setdefault ( step_id , {})[ input_name ] = pipeline_input_slot value_refs . setdefault ( pipeline_input_slot , []) . append ( input_point ) elif input_point . connected_outputs : for co in input_point . connected_outputs : if len ( input_point . connected_outputs ) == 1 and not co . sub_value : # this means the input is the same value as the connected output output_value : ValueSlot = all_step_outputs [ co . step_id ][ co . value_name ] all_step_inputs . setdefault ( input_point . step_id , {})[ input_point . value_name ] = output_value value_refs . setdefault ( output_value , []) . append ( input_point ) else : print ( input_point . connected_outputs ) raise NotImplementedError () # sub_value = co.sub_value # linked_values = {} # for co in input_point.connected_outputs: # output_value = all_step_outputs[co.step_id][co.value_name] # sub_value = co.sub_value # if len(input_point.connected_outputs) > 1 and not sub_value: # raise NotImplementedError() # sub_value = {\"config\": co.step_id} # if sub_value is not None: # raise NotImplementedError # # linked_values[output_value.id] = sub_value # # step_input = self._data_registry.register_linked_value( # parent_id=self.id, # linked_values=linked_values, # value_schema=input_point.value_schema, # value_refs=input_point, # ) # self._data_registry.register_callback( # self.values_updated, step_input # ) # all_step_inputs.setdefault(input_point.step_id, {})[ # input_point.value_name # ] = step_input else : raise Exception ( f \"Invalid value point type for this location: { input_point } \" ) if not pipeline_inputs : raise Exception ( f \"Can't init pipeline ' { self . title } ': no pipeline inputs\" ) self . _pipeline_inputs = SlottedValueSet ( items = pipeline_inputs , read_only = False , title = f \"Inputs for pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) if not pipeline_outputs : raise Exception ( f \"Can't init pipeline ' { self . title } ': no pipeline outputs\" ) self . _pipeline_outputs = SlottedValueSet ( items = pipeline_outputs , read_only = True , title = f \"Outputs for pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) self . _step_inputs = {} for step_id , inputs in all_step_inputs . items (): self . _step_inputs [ step_id ] = SlottedValueSet ( items = inputs , read_only = True , title = f \"Inputs for step ' { step_id } ' of pipeline ' { self . title } \" , kiara = self . _kiara , registry = self . _data_registry , ) self . _step_outputs = {} for step_id , outputs in all_step_outputs . items (): self . _step_outputs [ step_id ] = SlottedValueSet ( read_only = False , items = outputs , title = f \"Outputs for step ' { step_id } ' of pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) self . _value_refs = value_refs self . _steps_by_stage = None self . _inputs_by_stage = None def values_updated ( self , * items : ValueSlot ) -> None : updated_inputs : typing . Dict [ str , typing . List [ str ]] = {} updated_outputs : typing . Dict [ str , typing . List [ str ]] = {} updated_pipeline_inputs : typing . List [ str ] = [] updated_pipeline_outputs : typing . List [ str ] = [] # print(\"===================================================\") # for item in items: # print(item) # print(\"===================================================\") self . _update_status () if self . _value_refs is None : # means init is not finished yet return for item in items : # TODO: multiple value fields, also check pipeline id references = self . _value_refs . get ( item , None ) assert references for p in references : if isinstance ( p , StepInputRef ): updated_inputs . setdefault ( p . step_id , []) . append ( p . value_name ) elif isinstance ( p , StepOutputRef ): updated_outputs . setdefault ( p . step_id , []) . append ( p . value_name ) elif isinstance ( p , PipelineInputRef ): updated_pipeline_inputs . append ( p . value_name ) elif isinstance ( p , PipelineOutputRef ): updated_pipeline_outputs . append ( p . value_name ) else : raise TypeError ( f \"Can't update, invalid type: { type ( p ) } \" ) # print('========================================') # print('---') # print(\"Upaded pipeline input\") # print(updated_pipeline_inputs) # print('---') # print(\"Upaded step inputs\") # print(updated_inputs) # print('---') # print(\"Upaded step outputs\") # print(updated_outputs) # print('---') # print(\"Upaded pipeline outputs\") # print(updated_pipeline_outputs) if updated_pipeline_inputs : event_pi = PipelineInputEvent ( pipeline_id = self . id , updated_pipeline_inputs = updated_pipeline_inputs , ) self . _controller . pipeline_inputs_changed ( event_pi ) self . _notify_pipeline_listeners ( event_pi ) if updated_outputs : event_so = StepOutputEvent ( pipeline_id = self . id , updated_step_outputs = updated_outputs , ) self . _controller . step_outputs_changed ( event_so ) self . _notify_pipeline_listeners ( event_so ) if updated_inputs : event_si = StepInputEvent ( pipeline_id = self . id , updated_step_inputs = updated_inputs , ) self . _controller . step_inputs_changed ( event_si ) self . _notify_pipeline_listeners ( event_si ) if updated_pipeline_outputs : event_po = PipelineOutputEvent ( pipeline_id = self . id , updated_pipeline_outputs = updated_pipeline_outputs , ) self . _controller . pipeline_outputs_changed ( event_po ) self . _notify_pipeline_listeners ( event_po ) def _notify_pipeline_listeners ( self , event : StepEvent ): for listener in self . _listeners : if event . type == \"step_input\" : # type: ignore listener . step_inputs_changed ( event ) # type: ignore elif event . type == \"step_output\" : # type: ignore listener . step_outputs_changed ( event ) # type: ignore elif event . type == \"pipeline_input\" : # type: ignore listener . pipeline_inputs_changed ( event ) # type: ignore elif event . type == \"pipeline_output\" : # type: ignore listener . pipeline_outputs_changed ( event ) # type: ignore else : raise Exception ( f \"Unsupported type: { event . type } \" ) # type: ignore def get_current_state ( self ) -> \"PipelineState\" : step_inputs = {} step_states = {} for k , v in self . _step_inputs . items (): step_inputs [ k ] = PipelineValuesInfo . from_value_set ( v ) if v . items_are_valid (): step_states [ k ] = StepStatus . INPUTS_READY else : step_states [ k ] = StepStatus . STALE step_outputs = {} for k , v in self . _step_outputs . items (): step_outputs [ k ] = PipelineValuesInfo . from_value_set ( v ) if v . items_are_valid (): step_states [ k ] = StepStatus . RESULTS_READY from kiara.info.pipelines import PipelineState state = PipelineState ( structure = self . structure . to_details (), pipeline_inputs = self . _pipeline_inputs . to_details (), pipeline_outputs = self . _pipeline_outputs . to_details (), step_states = step_states , step_inputs = step_inputs , step_outputs = step_outputs , status = self . status , ) return state def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield self . get_current_state () inputs : SlottedValueSet property readonly \u00b6 All (pipeline) input values of this pipeline. outputs : SlottedValueSet property readonly \u00b6 All (pipeline) output values of this pipeline. status : StepStatus property readonly \u00b6 Return the current status of this pipeline. step_ids : Iterable [ str ] property readonly \u00b6 Return all ids of the steps of this pipeline. add_listener ( self , listener ) \u00b6 Add a listener taht gets notified on any internal pipeline input/output events. Source code in kiara/pipeline/pipeline.py def add_listener ( self , listener : PipelineListener ) -> None : \"\"\"Add a listener taht gets notified on any internal pipeline input/output events.\"\"\" self . _listeners . append ( listener ) get_pipeline_inputs_by_stage ( self ) \u00b6 Return a list of pipeline input names, ordered by stage they are first required. Source code in kiara/pipeline/pipeline.py def get_pipeline_inputs_by_stage ( self ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by stage they are first required.\"\"\" if self . _inputs_by_stage is not None : return self . _inputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . inputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepInputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _inputs_by_stage = result return self . _inputs_by_stage get_pipeline_inputs_for_stage ( self , stage ) \u00b6 Return a list of pipeline inputs that are required for a stage to be processed. The result of this method does not include inputs that were required earlier already. Source code in kiara/pipeline/pipeline.py def get_pipeline_inputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline inputs that are required for a stage to be processed. The result of this method does not include inputs that were required earlier already. \"\"\" return self . get_pipeline_inputs_by_stage () . get ( stage , []) get_pipeline_outputs_by_stage ( self ) \u00b6 Return a list of pipeline input names, ordered by the stage that needs to be executed before they are available. Source code in kiara/pipeline/pipeline.py def get_pipeline_outputs_by_stage ( self , ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by the stage that needs to be executed before they are available.\"\"\" if self . _outputs_by_stage is not None : return self . _outputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . outputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepOutputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _outputs_by_stage = result return self . _outputs_by_stage get_pipeline_outputs_for_stage ( self , stage ) \u00b6 Return a list of pipeline outputs that are first available after the specified stage completed processing. Source code in kiara/pipeline/pipeline.py def get_pipeline_outputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline outputs that are first available after the specified stage completed processing.\"\"\" return self . get_pipeline_outputs_by_stage () . get ( stage , []) get_step ( self , step_id ) \u00b6 Return the object representing a step in this workflow, identified by the step id. Source code in kiara/pipeline/pipeline.py def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the object representing a step in this workflow, identified by the step id.\"\"\" return self . _structure . get_step ( step_id ) get_step_inputs ( self , step_id ) \u00b6 Return all inputs for a step id (incl. inputs that are not pipeline inputs but connected to other modules output). Source code in kiara/pipeline/pipeline.py def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all inputs for a step id (incl. inputs that are not pipeline inputs but connected to other modules output).\"\"\" return self . _step_inputs [ step_id ] get_step_outputs ( self , step_id ) \u00b6 Return all outputs for a step id (incl. outputs that are not pipeline outputs). Source code in kiara/pipeline/pipeline.py def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all outputs for a step id (incl. outputs that are not pipeline outputs).\"\"\" return self . _step_outputs [ step_id ] get_steps_by_stage ( self ) \u00b6 Return a all pipeline steps, ordered by stage they belong to. Source code in kiara/pipeline/pipeline.py def get_steps_by_stage ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , PipelineStep ]]: \"\"\"Return a all pipeline steps, ordered by stage they belong to.\"\"\" if self . _steps_by_stage is not None : return self . _steps_by_stage result : typing . Dict [ int , typing . Dict [ str , PipelineStep ]] = {} for step_id in self . step_ids : step = self . get_step ( step_id ) stage = step . processing_stage assert stage is not None result . setdefault ( stage , {})[ step_id ] = step self . _steps_by_stage = result return self . _steps_by_stage structure \u00b6 PipelineStep ( BaseModel ) pydantic-model \u00b6 A step within a pipeline-structure, includes information about it's connection(s) and other metadata. Source code in kiara/pipeline/structure.py class PipelineStep ( BaseModel ): \"\"\"A step within a pipeline-structure, includes information about it's connection(s) and other metadata.\"\"\" class Config : validate_assignment = True extra = Extra . forbid @classmethod def create_steps ( cls , * steps : \"PipelineStepConfig\" , kiara : \"Kiara\" ) -> typing . List [ \"PipelineStep\" ]: result : typing . List [ PipelineStep ] = [] if kiara is None : from kiara.module import Kiara kiara = Kiara . instance () for step in steps : _s = PipelineStep ( step_id = step . step_id , module_type = step . module_type , module_config = copy . deepcopy ( step . module_config ), input_links = copy . deepcopy ( step . input_links ), _kiara = kiara , # type: ignore ) result . append ( _s ) return result _module : typing . Optional [ \"KiaraModule\" ] = PrivateAttr ( default = None ) @validator ( \"step_id\" ) def _validate_step_id ( cls , v ): assert isinstance ( v , str ) if \".\" in v : raise ValueError ( \"Step ids can't contain '.' characters.\" ) return v step_id : str module_type : str = Field ( description = \"The module type.\" ) module_config : typing . Mapping [ str , typing . Any ] = Field ( description = \"The module config.\" , default_factory = dict ) required : bool = Field ( description = \"Whether this step is required within the workflow. \\n\\n In some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required.\" , default = True , ) processing_stage : typing . Optional [ int ] = Field ( default = None , description = \"The stage number this step is executed within the pipeline.\" , ) input_links : typing . Mapping [ str , typing . List [ StepValueAddress ]] = Field ( description = \"The links that connect to inputs of the module.\" , default_factory = list , ) _kiara : typing . Optional [ \"Kiara\" ] = PrivateAttr ( default = None ) _id : str = PrivateAttr () def __init__ ( self , ** data ): # type: ignore self . _id = str ( uuid . uuid4 ()) kiara = data . pop ( \"_kiara\" , None ) if kiara is None : from kiara import Kiara kiara = Kiara . instance () super () . __init__ ( ** data ) self . _kiara : \"Kiara\" = kiara @property def kiara ( self ): return self . _kiara @property def module ( self ) -> \"KiaraModule\" : if self . _module is None : try : if ( self . module_type in self . kiara . operation_mgmt . profiles . keys () and not self . module_config ): op = self . kiara . operation_mgmt . profiles [ self . module_type ] self . _module = op . module else : self . _module = self . kiara . create_module ( id = self . step_id , module_type = self . module_type , module_config = self . module_config , ) except Exception as e : raise Exception ( f \"Can't assemble pipeline step ' { self . step_id } ': { e } \" ) return self . _module def __eq__ ( self , other ): if not isinstance ( other , PipelineStep ): return False return self . _id == other . _id # # TODO: also check whether _kiara obj is equal? # eq = (self.step_id, self.parent_id, self.module, self.processing_stage,) == ( # other.step_id, # other.parent_id, # other.module, # other.processing_stage, # ) # # if not eq: # return False # # hs = DeepHash(self.input_links) # ho = DeepHash(other.input_links) # # return hs[self.input_links] == ho[other.input_links] def __hash__ ( self ): return hash ( self . _id ) # # TODO: also include _kiara obj? # # TODO: figure out whether that can be made to work without deephash # hs = DeepHash(self.input_links) # return hash( # ( # self.step_id, # self.parent_id, # self.module, # self.processing_stage, # hs[self.input_links], # ) # ) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (step_id= { self . step_id } module_type= { self . module_type } processing_stage= { self . processing_stage } )\" def __str__ ( self ): return f \"step: { self . step_id } (module: { self . module_type } )\" input_links : Mapping [ str , List [ kiara . pipeline . StepValueAddress ]] pydantic-field \u00b6 The links that connect to inputs of the module. module_config : Mapping [ str , Any ] pydantic-field \u00b6 The module config. module_type : str pydantic-field required \u00b6 The module type. processing_stage : int pydantic-field \u00b6 The stage number this step is executed within the pipeline. required : bool pydantic-field \u00b6 Whether this step is required within the workflow. In some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/pipeline/structure.py def __eq__ ( self , other ): if not isinstance ( other , PipelineStep ): return False return self . _id == other . _id # # TODO: also check whether _kiara obj is equal? # eq = (self.step_id, self.parent_id, self.module, self.processing_stage,) == ( # other.step_id, # other.parent_id, # other.module, # other.processing_stage, # ) # # if not eq: # return False # # hs = DeepHash(self.input_links) # ho = DeepHash(other.input_links) # # return hs[self.input_links] == ho[other.input_links] __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/pipeline/structure.py def __hash__ ( self ): return hash ( self . _id ) # # TODO: also include _kiara obj? # # TODO: figure out whether that can be made to work without deephash # hs = DeepHash(self.input_links) # return hash( # ( # self.step_id, # self.parent_id, # self.module, # self.processing_stage, # hs[self.input_links], # ) # ) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/pipeline/structure.py def __repr__ ( self ): return f \" { self . __class__ . __name__ } (step_id= { self . step_id } module_type= { self . module_type } processing_stage= { self . processing_stage } )\" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/pipeline/structure.py def __str__ ( self ): return f \"step: { self . step_id } (module: { self . module_type } )\" PipelineStructure \u00b6 An object that holds one or several steps, and describes the connections between them. Source code in kiara/pipeline/structure.py class PipelineStructure ( object ): \"\"\"An object that holds one or several steps, and describes the connections between them.\"\"\" def __init__ ( self , config : \"PipelineConfig\" , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _structure_config : \"PipelineConfig\" = config steps = self . _structure_config . steps input_aliases = self . _structure_config . input_aliases output_aliases = self . _structure_config . output_aliases if not steps : raise Exception ( \"No steps provided.\" ) if kiara is None : kiara = Kiara . instance () self . _kiara : Kiara = kiara self . _steps : typing . List [ PipelineStep ] = PipelineStep . create_steps ( * steps , kiara = self . _kiara ) # self._pipeline_id: str = parent_id if input_aliases is None : input_aliases = {} if isinstance ( input_aliases , str ): if input_aliases not in ALLOWED_INPUT_ALIAS_MARKERS : raise Exception ( f \"Can't create pipeline, invalid value ' { input_aliases } ' for 'input_aliases'. Either specify a dict, or use one of: { ', ' . join ( ALLOWED_INPUT_ALIAS_MARKERS ) } \" ) input_aliases = calculate_shortest_field_aliases ( self . _steps , input_aliases , \"inputs\" ) if isinstance ( output_aliases , str ): if output_aliases not in ALLOWED_INPUT_ALIAS_MARKERS : raise Exception ( f \"Can't create pipeline, invalid value ' { output_aliases } ' for 'output_aliases'. Either specify a dict, or use one of: { ', ' . join ( ALLOWED_INPUT_ALIAS_MARKERS ) } \" ) output_aliases = calculate_shortest_field_aliases ( self . _steps , output_aliases , \"outputs\" ) self . _input_aliases : typing . Dict [ str , str ] = dict ( input_aliases ) # type: ignore if output_aliases is None : output_aliases = {} self . _output_aliases : typing . Dict [ str , str ] = dict ( output_aliases ) # type: ignore # this is hardcoded for now self . _add_all_workflow_outputs : bool = False self . _constants : typing . Dict [ str , typing . Any ] = None # type: ignore self . _defaults : typing . Dict [ str , typing . Any ] = None # type: ignore self . _execution_graph : nx . DiGraph = None # type: ignore self . _data_flow_graph : nx . DiGraph = None # type: ignore self . _data_flow_graph_simple : nx . DiGraph = None # type: ignore self . _processing_stages : typing . List [ typing . List [ str ]] = None # type: ignore self . _steps_details : typing . Dict [ str , typing . Any ] = None # type: ignore \"\"\"Holds details about the (current) processing steps contained in this workflow.\"\"\" # @property # def pipeline_id(self) -> str: # return self._pipeline_id @property def structure_config ( self ) -> \"PipelineConfig\" : return self . _structure_config @property def steps ( self ) -> typing . Iterable [ PipelineStep ]: return self . _steps @property def modules ( self ) -> typing . Iterable [ \"KiaraModule\" ]: return ( s . module for s in self . steps ) @property def steps_details ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _steps_details is None : self . _process_steps () return self . _steps_details @property def step_ids ( self ) -> typing . Iterable [ str ]: if self . _steps_details is None : self . _process_steps () return self . _steps_details . keys () @property def constants ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _constants is None : self . _process_steps () return self . _constants @property def defaults ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _defaults is None : self . _process_steps () return self . _defaults def get_step ( self , step_id : str ) -> PipelineStep : d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"step\" ] def get_step_inputs ( self , step_id : str ) -> typing . Mapping [ str , StepInputRef ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"inputs\" ] def get_step_outputs ( self , step_id : str ) -> typing . Mapping [ str , StepOutputRef ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"outputs\" ] def get_step_details ( self , step_id : str ) -> typing . Mapping [ str , typing . Any ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d @property def execution_graph ( self ) -> nx . DiGraph : if self . _execution_graph is None : self . _process_steps () return self . _execution_graph @property def data_flow_graph ( self ) -> nx . DiGraph : if self . _data_flow_graph is None : self . _process_steps () return self . _data_flow_graph @property def data_flow_graph_simple ( self ) -> nx . DiGraph : if self . _data_flow_graph_simple is None : self . _process_steps () return self . _data_flow_graph_simple @property def processing_stages ( self ) -> typing . List [ typing . List [ str ]]: if self . _steps_details is None : self . _process_steps () return self . _processing_stages @lru_cache () def _get_node_of_type ( self , node_type : str ): if self . _steps_details is None : self . _process_steps () return [ node for node , attr in self . _data_flow_graph . nodes ( data = True ) if attr [ \"type\" ] == node_type ] @property def steps_inputs ( self ) -> typing . Dict [ str , StepInputRef ]: return { node . alias : node for node in self . _get_node_of_type ( node_type = StepInputRef . __name__ ) } @property def steps_outputs ( self ) -> typing . Dict [ str , StepOutputRef ]: return { node . alias : node for node in self . _get_node_of_type ( node_type = StepOutputRef . __name__ ) } @property def pipeline_inputs ( self ) -> typing . Dict [ str , PipelineInputRef ]: return { node . value_name : node for node in self . _get_node_of_type ( node_type = PipelineInputRef . __name__ ) } @property def pipeline_outputs ( self ) -> typing . Dict [ str , PipelineOutputRef ]: return { node . value_name : node for node in self . _get_node_of_type ( node_type = PipelineOutputRef . __name__ ) } @property def pipeline_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return { input_name : w_in . value_schema for input_name , w_in in self . pipeline_inputs . items () } @property def pipeline_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return { output_name : w_out . value_schema for output_name , w_out in self . pipeline_outputs . items () } def _process_steps ( self ): \"\"\"The core method of this class, it connects all the processing modules, their inputs and outputs.\"\"\" steps_details : typing . Dict [ str , typing . Any ] = {} execution_graph = nx . DiGraph () execution_graph . add_node ( \"__root__\" ) data_flow_graph = nx . DiGraph () data_flow_graph_simple = nx . DiGraph () processing_stages = [] constants = {} structure_defaults = {} # temp variable, to hold all outputs outputs : typing . Dict [ str , StepOutputRef ] = {} # process all pipeline and step outputs first _temp_steps_map : typing . Dict [ str , PipelineStep ] = {} pipeline_outputs : typing . Dict [ str , PipelineOutputRef ] = {} for step in self . _steps : _temp_steps_map [ step . step_id ] = step if step . step_id in steps_details . keys (): raise Exception ( f \"Can't process steps: duplicate step_id ' { step . step_id } '\" ) steps_details [ step . step_id ] = { \"step\" : step , \"outputs\" : {}, \"inputs\" : {}, } data_flow_graph . add_node ( step , type = \"step\" ) # go through all the module outputs, create points for them and connect them to pipeline outputs for output_name , schema in step . module . output_schemas . items (): step_output = StepOutputRef ( value_name = output_name , value_schema = schema , step_id = step . step_id , ) steps_details [ step . step_id ][ \"outputs\" ][ output_name ] = step_output step_alias = generate_step_alias ( step . step_id , output_name ) outputs [ step_alias ] = step_output step_output_name = generate_pipeline_endpoint_name ( step_id = step . step_id , value_name = output_name ) if self . _output_aliases : if step_output_name in self . _output_aliases . keys (): step_output_name = self . _output_aliases [ step_output_name ] else : if not self . _add_all_workflow_outputs : # this output is not interesting for the workflow step_output_name = None if step_output_name : step_output_address = StepValueAddress ( step_id = step . step_id , value_name = output_name ) pipeline_output = PipelineOutputRef ( value_name = step_output_name , connected_output = step_output_address , value_schema = schema , ) pipeline_outputs [ step_output_name ] = pipeline_output step_output . pipeline_output = pipeline_output . value_name data_flow_graph . add_node ( pipeline_output , type = PipelineOutputRef . __name__ ) data_flow_graph . add_edge ( step_output , pipeline_output ) data_flow_graph_simple . add_node ( pipeline_output , type = PipelineOutputRef . __name__ ) data_flow_graph_simple . add_edge ( step , pipeline_output ) data_flow_graph . add_node ( step_output , type = StepOutputRef . __name__ ) data_flow_graph . add_edge ( step , step_output ) # now process inputs, and connect them to the appropriate output/pipeline-input points existing_pipeline_input_points : typing . Dict [ str , PipelineInputRef ] = {} for step in self . _steps : other_step_dependency : typing . Set = set () # go through all the inputs of a module, create input points and connect them to either # other module outputs, or pipeline inputs (which need to be created) module_constants : typing . Mapping [ str , typing . Any ] = step . module . get_config_value ( \"constants\" ) for input_name , schema in step . module . input_schemas . items (): matching_input_links : typing . List [ StepValueAddress ] = [] is_constant = input_name in module_constants . keys () for value_name , input_links in step . input_links . items (): if value_name == input_name : for input_link in input_links : if input_link in matching_input_links : raise Exception ( f \"Duplicate input link: { input_link } \" ) matching_input_links . append ( input_link ) if matching_input_links : # this means we connect to other steps output connected_output_points : typing . List [ StepOutputRef ] = [] connected_outputs : typing . List [ StepValueAddress ] = [] for input_link in matching_input_links : output_id = generate_step_alias ( input_link . step_id , input_link . value_name ) if output_id not in outputs . keys (): raise Exception ( f \"Can't connect input ' { input_name } ' for step ' { step . step_id } ': no output ' { output_id } ' available. Available output names: { ', ' . join ( outputs . keys ()) } \" ) connected_output_points . append ( outputs [ output_id ]) connected_outputs . append ( input_link ) other_step_dependency . add ( input_link . step_id ) step_input_point = StepInputRef ( step_id = step . step_id , value_name = input_name , value_schema = schema , is_constant = is_constant , connected_pipeline_input = None , connected_outputs = connected_outputs , ) for op in connected_output_points : op . connected_inputs . append ( step_input_point . address ) data_flow_graph . add_edge ( op , step_input_point ) data_flow_graph_simple . add_edge ( _temp_steps_map [ op . step_id ], step_input_point ) # TODO: name edge data_flow_graph_simple . add_edge ( step_input_point , step ) # TODO: name edge else : # this means we connect to pipeline input pipeline_input_name = generate_pipeline_endpoint_name ( step_id = step . step_id , value_name = input_name ) # check whether this input has an alias associated with it if self . _input_aliases : if pipeline_input_name in self . _input_aliases . keys (): # this means we use the pipeline alias pipeline_input_name = self . _input_aliases [ pipeline_input_name ] if pipeline_input_name in existing_pipeline_input_points . keys (): # we already created a pipeline input with this name # TODO: check whether schema fits connected_pipeline_input = existing_pipeline_input_points [ pipeline_input_name ] assert connected_pipeline_input . is_constant == is_constant else : # we need to create the pipeline input connected_pipeline_input = PipelineInputRef ( value_name = pipeline_input_name , value_schema = schema , is_constant = is_constant , ) existing_pipeline_input_points [ pipeline_input_name ] = connected_pipeline_input data_flow_graph . add_node ( connected_pipeline_input , type = PipelineInputRef . __name__ ) data_flow_graph_simple . add_node ( connected_pipeline_input , type = PipelineInputRef . __name__ ) if is_constant : constants [ pipeline_input_name ] = step . module . get_config_value ( \"constants\" )[ input_name ] default_val = step . module . get_config_value ( \"defaults\" ) . get ( input_name , None ) if is_constant and default_val is not None : raise Exception ( f \"Module config invalid for step ' { step . step_id } ': both default value and constant provided for input ' { input_name } '.\" ) elif default_val is not None : structure_defaults [ pipeline_input_name ] = default_val step_input_point = StepInputRef ( step_id = step . step_id , value_name = input_name , value_schema = schema , connected_pipeline_input = connected_pipeline_input . value_name , connected_outputs = None , ) connected_pipeline_input . connected_inputs . append ( step_input_point . address ) data_flow_graph . add_edge ( connected_pipeline_input , step_input_point ) data_flow_graph_simple . add_edge ( connected_pipeline_input , step ) data_flow_graph . add_node ( step_input_point , type = StepInputRef . __name__ ) steps_details [ step . step_id ][ \"inputs\" ][ input_name ] = step_input_point data_flow_graph . add_edge ( step_input_point , step ) if other_step_dependency : for module_id in other_step_dependency : execution_graph . add_edge ( module_id , step . step_id ) else : execution_graph . add_edge ( \"__root__\" , step . step_id ) # calculate execution order path_lengths : typing . Dict [ str , int ] = {} for step in self . _steps : step_id = step . step_id paths = list ( nx . all_simple_paths ( execution_graph , \"__root__\" , step_id )) max_steps = max ( paths , key = lambda x : len ( x )) path_lengths [ step_id ] = len ( max_steps ) - 1 max_length = max ( path_lengths . values ()) for i in range ( 1 , max_length + 1 ): stage : typing . List [ str ] = [ m for m , length in path_lengths . items () if length == i ] processing_stages . append ( stage ) for _step_id in stage : steps_details [ _step_id ][ \"processing_stage\" ] = i steps_details [ _step_id ][ \"step\" ] . processing_stage = i self . _constants = constants self . _defaults = structure_defaults self . _steps_details = steps_details self . _execution_graph = execution_graph self . _data_flow_graph = data_flow_graph self . _data_flow_graph_simple = data_flow_graph_simple self . _processing_stages = processing_stages self . _get_node_of_type . cache_clear () # calculating which steps are always required to execute to compute one of the required pipeline outputs. # this is done because in some cases it's possible that some steps can be skipped to execute if they # don't have a valid input set, because the inputs downstream they are connecting to are 'non-required' # optional_steps = [] last_stage = self . _processing_stages [ - 1 ] step_nodes : typing . List [ PipelineStep ] = [ node for node in self . _data_flow_graph_simple . nodes if isinstance ( node , PipelineStep ) ] all_required_inputs = [] for step_id in last_stage : step = self . get_step ( step_id ) step_nodes . remove ( step ) for k , s_inp in self . get_step_inputs ( step_id ) . items (): if not s_inp . value_schema . is_required (): continue all_required_inputs . append ( s_inp ) for pipeline_input in self . pipeline_inputs . values (): for last_step_input in all_required_inputs : try : path = nx . shortest_path ( self . _data_flow_graph_simple , pipeline_input , last_step_input ) for p in path : if p in step_nodes : step_nodes . remove ( p ) except ( NetworkXNoPath , NodeNotFound ): pass # print(\"NO PATH\") # print(f\"{pipeline_input} -> {last_step_input}\") for s in step_nodes : s . required = False for input_name , inp in self . pipeline_inputs . items (): steps = set () for ci in inp . connected_inputs : steps . add ( ci . step_id ) optional = True for step_id in steps : step = self . get_step ( step_id ) if step . required : optional = False break if optional : inp . value_schema . optional = True def extend ( self , other : typing . Union [ \"Pipeline\" , \"PipelineStructure\" , \"PipelineConfig\" , typing . Mapping [ str , typing . Any ], ], input_links : typing . Optional [ typing . Mapping [ str , typing . Iterable [ StepValueAddress ]] ] = None , ) -> \"PipelineStructure\" : return extend_pipeline ( self , other ) def to_details ( self ) -> \"PipelineStructureDesc\" : from kiara.info.pipelines import PipelineStructureDesc return PipelineStructureDesc . create_pipeline_structure_desc ( pipeline = self ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : d = self . to_details () yield d values \u00b6 PipelineInputRef ( ValueRef ) pydantic-model \u00b6 An input to a pipeline. Source code in kiara/pipeline/values.py class PipelineInputRef ( ValueRef ): \"\"\"An input to a pipeline.\"\"\" connected_inputs : typing . List [ StepValueAddress ] = Field ( description = \"The step inputs that are connected to this pipeline input\" , default_factory = list , ) is_constant : bool = Field ( \"Whether this input is a constant and can't be changed by the user.\" ) @property def alias ( self ) -> str : return generate_step_alias ( PIPELINE_PARENT_MARKER , self . value_name ) connected_inputs : List [ kiara . pipeline . StepValueAddress ] pydantic-field \u00b6 The step inputs that are connected to this pipeline input PipelineOutputRef ( ValueRef ) pydantic-model \u00b6 An output to a pipeline. Source code in kiara/pipeline/values.py class PipelineOutputRef ( ValueRef ): \"\"\"An output to a pipeline.\"\"\" connected_output : StepValueAddress = Field ( description = \"Connected step outputs.\" ) @property def alias ( self ) -> str : return generate_step_alias ( PIPELINE_PARENT_MARKER , self . value_name ) connected_output : StepValueAddress pydantic-field required \u00b6 Connected step outputs. StepInputRef ( ValueRef ) pydantic-model \u00b6 An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both. Source code in kiara/pipeline/values.py class StepInputRef ( ValueRef ): \"\"\"An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both. \"\"\" step_id : str = Field ( description = \"The step id.\" ) connected_outputs : typing . Optional [ typing . List [ StepValueAddress ]] = Field ( default = None , description = \"A potential connected list of one or several module outputs.\" , ) connected_pipeline_input : typing . Optional [ str ] = Field ( default = None , description = \"A potential pipeline input.\" ) is_constant : bool = Field ( \"Whether this input is a constant and can't be changed by the user.\" ) @root_validator ( pre = True ) def ensure_single_connected_item ( cls , values ): if values . get ( \"connected_outputs\" , None ) and values . get ( \"connected_pipeline_input\" ): raise ValueError ( \"Multiple connected items, only one allowed.\" ) return values @property def alias ( self ) -> str : return generate_step_alias ( self . step_id , self . value_name ) @property def address ( self ) -> StepValueAddress : return StepValueAddress ( step_id = self . step_id , value_name = self . value_name ) def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . step_id } . { self . value_name } ( { self . value_schema . type } )\" connected_outputs : List [ kiara . pipeline . StepValueAddress ] pydantic-field \u00b6 A potential connected list of one or several module outputs. connected_pipeline_input : str pydantic-field \u00b6 A potential pipeline input. step_id : str pydantic-field required \u00b6 The step id. StepOutputRef ( ValueRef ) pydantic-model \u00b6 An output to a step. Source code in kiara/pipeline/values.py class StepOutputRef ( ValueRef ): \"\"\"An output to a step.\"\"\" class Config : allow_mutation = True step_id : str = Field ( description = \"The step id.\" ) pipeline_output : typing . Optional [ str ] = Field ( description = \"The connected pipeline output.\" ) connected_inputs : typing . List [ StepValueAddress ] = Field ( description = \"The step inputs that are connected to this step output\" , default_factory = list , ) @property def alias ( self ) -> str : return generate_step_alias ( self . step_id , self . value_name ) @property def address ( self ) -> StepValueAddress : return StepValueAddress ( step_id = self . step_id , value_name = self . value_name ) def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . step_id } . { self . value_name } ( { self . value_schema . type } )\" connected_inputs : List [ kiara . pipeline . StepValueAddress ] pydantic-field \u00b6 The step inputs that are connected to this step output pipeline_output : str pydantic-field \u00b6 The connected pipeline output. step_id : str pydantic-field required \u00b6 The step id. ValueRef ( BaseModel ) pydantic-model \u00b6 An object that holds information about the location of a value within a pipeline (or other structure). Basically, a ValueRef helps the containing object where in its structure the value belongs (for example so it can update dependent other values). A ValueRef object (obviously) does not contain the value itself. There are four different ValueRef type that are relevant for pipelines: kiara.pipeline.values.StepInputRef : an input to a step kiara.pipeline.values.StepOutputRef : an output of a step kiara.pipeline.values.PipelineInputRef : an input to a pipeline kiara.pipeline.values.PipelineOutputRef : an output for a pipeline Several ValueRef objects can target the same value, for example a step output and a connected step input would reference the same Value (in most cases).. Source code in kiara/pipeline/values.py class ValueRef ( BaseModel ): \"\"\"An object that holds information about the location of a value within a pipeline (or other structure). Basically, a `ValueRef` helps the containing object where in its structure the value belongs (for example so it can update dependent other values). A `ValueRef` object (obviously) does not contain the value itself. There are four different ValueRef type that are relevant for pipelines: - [kiara.pipeline.values.StepInputRef][]: an input to a step - [kiara.pipeline.values.StepOutputRef][]: an output of a step - [kiara.pipeline.values.PipelineInputRef][]: an input to a pipeline - [kiara.pipeline.values.PipelineOutputRef][]: an output for a pipeline Several `ValueRef` objects can target the same value, for example a step output and a connected step input would reference the same `Value` (in most cases).. \"\"\" class Config : allow_mutation = True extra = Extra . forbid _id : uuid . UUID = PrivateAttr ( default_factory = uuid . uuid4 ) value_name : str value_schema : ValueSchema # pipeline_id: str def __eq__ ( self , other ): if not isinstance ( other , self . __class__ ): return False return self . _id == other . _id def __hash__ ( self ): return hash ( self . _id ) def __repr__ ( self ): step_id = \"\" if hasattr ( self , \"step_id\" ): step_id = f \" step_id=' { self . step_id } '\" return f \" { self . __class__ . __name__ } (value_name=' { self . value_name } ' { step_id } )\" def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . value_name } ( { self . value_schema . type } )\" __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/pipeline/values.py def __eq__ ( self , other ): if not isinstance ( other , self . __class__ ): return False return self . _id == other . _id __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/pipeline/values.py def __hash__ ( self ): return hash ( self . _id ) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/pipeline/values.py def __repr__ ( self ): step_id = \"\" if hasattr ( self , \"step_id\" ): step_id = f \" step_id=' { self . step_id } '\" return f \" { self . __class__ . __name__ } (value_name=' { self . value_name } ' { step_id } )\" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/pipeline/values.py def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . value_name } ( { self . value_schema . type } )\" processing special \u00b6 Job ( ProcessingInfo ) pydantic-model \u00b6 Source code in kiara/processing/__init__.py class Job ( ProcessingInfo ): @classmethod def create_event_msg ( cls , job : \"Job\" ): topic = job . status . value [ 2 : - 2 ] payload = f \" { topic } { job . json () } \" return payload class Config : use_enum_values = True _exception : typing . Optional [ Exception ] = PrivateAttr ( default = None ) pipeline_id : str = Field ( description = \"The id of the pipeline this jobs runs for.\" ) pipeline_name : str = Field ( description = \"The name/type of the pipeline.\" ) step_id : str = Field ( description = \"The id of the step within the pipeline.\" ) inputs : PipelineValuesInfo = Field ( description = \"The input values.\" ) outputs : PipelineValuesInfo = Field ( description = \"The output values.\" ) status : JobStatus = Field ( description = \"The current status of the job.\" , default = JobStatus . CREATED , ) error : typing . Optional [ str ] = Field ( description = \"Potential error message.\" ) @property def exception ( self ) -> typing . Optional [ Exception ]: return self . _exception @property def runtime ( self ) -> typing . Optional [ float ]: if self . started is None or self . finished is None : return None runtime = self . finished - self . started return runtime . total_seconds () @validator ( \"status\" ) def _validate_status ( cls , v ): if isinstance ( v , int ): if v < 0 or v > 100 : raise ValueError ( \"Status must be a status string, or an integer between 0 and 100.\" ) return v error : str pydantic-field \u00b6 Potential error message. inputs : PipelineValuesInfo pydantic-field required \u00b6 The input values. outputs : PipelineValuesInfo pydantic-field required \u00b6 The output values. pipeline_id : str pydantic-field required \u00b6 The id of the pipeline this jobs runs for. pipeline_name : str pydantic-field required \u00b6 The name/type of the pipeline. status : JobStatus pydantic-field \u00b6 The current status of the job. step_id : str pydantic-field required \u00b6 The id of the step within the pipeline. JobLog ( BaseModel ) pydantic-model \u00b6 Source code in kiara/processing/__init__.py class JobLog ( BaseModel ): log : typing . Dict [ int , LogMessage ] = Field ( description = \"The logs for this job.\" , default_factory = dict ) percent_finished : int = Field ( description = \"Describes how much of the job is finished. A negative number means the module does not support progress tracking.\" , default =- 1 , ) def add_log ( self , msg : str , log_level : int = logging . DEBUG ): _msg = LogMessage ( msg = msg , log_level = log_level ) self . log [ len ( self . log )] = _msg log : Dict [ int , kiara . processing . LogMessage ] pydantic-field \u00b6 The logs for this job. percent_finished : int pydantic-field \u00b6 Describes how much of the job is finished. A negative number means the module does not support progress tracking. JobStatus ( Enum ) \u00b6 An enumeration. Source code in kiara/processing/__init__.py class JobStatus ( Enum ): CREATED = \"__job_created__\" STARTED = \"__job_started__\" SUCCESS = \"__job_success__\" FAILED = \"__job_failed__\" LogMessage ( BaseModel ) pydantic-model \u00b6 Source code in kiara/processing/__init__.py class LogMessage ( BaseModel ): timestamp : datetime = Field ( description = \"The time the message was logged.\" , default_factory = datetime . now ) log_level : int = Field ( description = \"The log level.\" ) msg : str = Field ( description = \"The log message\" ) log_level : int pydantic-field required \u00b6 The log level. msg : str pydantic-field required \u00b6 The log message timestamp : datetime pydantic-field \u00b6 The time the message was logged. ProcessingInfo ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/processing/__init__.py class ProcessingInfo ( MetadataModel ): id : str = Field ( description = \"The id of the job.\" ) module_type : str = Field ( description = \"The module type name.\" ) module_config : typing . Dict [ str , typing . Any ] = Field ( description = \"The module configuration.\" ) module_doc : DocumentationMetadataModel = Field ( description = \"Documentation for the module that runs the job.\" ) job_log : JobLog = Field ( description = \"Details about the job progress.\" , default_factory = JobLog ) submitted : datetime = Field ( description = \"When the job was submitted.\" , default_factory = datetime . now ) started : typing . Optional [ datetime ] = Field ( description = \"When the job was started.\" , default = None ) finished : typing . Optional [ datetime ] = Field ( description = \"When the job was finished.\" , default = None ) finished : datetime pydantic-field \u00b6 When the job was finished. id : str pydantic-field required \u00b6 The id of the job. job_log : JobLog pydantic-field \u00b6 Details about the job progress. module_config : Dict [ str , Any ] pydantic-field required \u00b6 The module configuration. module_doc : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the module that runs the job. module_type : str pydantic-field required \u00b6 The module type name. started : datetime pydantic-field \u00b6 When the job was started. submitted : datetime pydantic-field \u00b6 When the job was submitted. parallel \u00b6 ThreadPoolProcessorConfig ( ProcessorConfig ) pydantic-model \u00b6 Source code in kiara/processing/parallel.py class ThreadPoolProcessorConfig ( ProcessorConfig ): max_workers : typing . Optional [ int ] = Field ( description = \"The max mount of workers for the thread pool.\" , default = None ) max_workers : int pydantic-field \u00b6 The max mount of workers for the thread pool. processor \u00b6 ModuleProcessor ( ABC ) \u00b6 Source code in kiara/processing/processor.py class ModuleProcessor ( abc . ABC ): @classmethod def from_config ( cls , config : typing . Union [ None , typing . Mapping [ str , typing . Any ], ProcessorConfig ], kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleProcessor\" : from kiara.processing.parallel import ThreadPoolProcessorConfig from kiara.processing.synchronous import SynchronousProcessorConfig if not config : config = SynchronousProcessorConfig ( module_processor_type = \"synchronous\" ) if isinstance ( config , typing . Mapping ): processor_type = config . get ( \"module_processor_type\" , None ) if not processor_type : raise ValueError ( \"No 'module_processor_type' provided: {config} \" ) if processor_type == \"synchronous\" : config = SynchronousProcessorConfig ( ** config ) elif processor_type == \"multi-threaded\" : config = ThreadPoolProcessorConfig () else : raise ValueError ( f \"Invalid processor type: { processor_type } \" ) if isinstance ( config , SynchronousProcessorConfig ): from kiara.processing.synchronous import SynchronousProcessor proc : ModuleProcessor = SynchronousProcessor ( kiara = kiara , ** config . dict ( exclude = { \"module_processor_type\" }) ) elif isinstance ( config , ThreadPoolProcessorConfig ): from kiara.processing.parallel import ThreadPoolProcessor proc = ThreadPoolProcessor ( kiara = kiara , ** config . dict ( exclude = { \"module_processor_type\" }) ) else : raise TypeError ( f \"Invalid processor config class: { type ( config ) } \" ) return proc def __init__ ( self , zmq_context : typing . Optional [ Context ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : Kiara = kiara if zmq_context is None : zmq_context = Context . instance () self . _zmq_context : Context = zmq_context self . _socket : Socket = self . _zmq_context . socket ( zmq . PUB ) self . _socket . connect ( \"inproc://kiara_in\" ) self . _active_jobs : typing . Dict [ str , Job ] = {} self . _finished_jobs : typing . Dict [ str , Job ] = {} # TODO: clean up those? self . _inputs : typing . Dict [ str , StepInputs ] = {} self . _outputs : typing . Dict [ str , StepOutputs ] = {} def get_job_details ( self , job_id : str ) -> typing . Optional [ Job ]: if job_id in self . _active_jobs . keys (): return self . _active_jobs [ job_id ] elif job_id in self . _finished_jobs . keys (): return self . _finished_jobs [ job_id ] else : return None def start ( self , pipeline_id : str , pipeline_name : str , step_id : str , module : \"KiaraModule\" , inputs : ValueSet , outputs : ValueSet , ) -> str : job_id = str ( uuid . uuid4 ()) # TODO: make snapshot of current state of data? full_inputs = module . create_full_inputs ( ** inputs ) wrapped_inputs = StepInputs ( inputs = full_inputs , kiara = self . _kiara ) wrapped_outputs = StepOutputs ( outputs = outputs , kiara = self . _kiara ) self . _inputs [ job_id ] = wrapped_inputs self . _outputs [ job_id ] = wrapped_outputs job = Job ( id = job_id , pipeline_id = pipeline_id , pipeline_name = pipeline_name , step_id = step_id , module_type = module . _module_type_id , # type: ignore module_config = module . config . dict (), module_doc = module . get_type_metadata () . documentation , inputs = PipelineValuesInfo . from_value_set ( inputs ), outputs = PipelineValuesInfo . from_value_set ( outputs ), ) job . job_log . add_log ( \"job created\" ) self . _active_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) try : self . process ( job_id = job_id , module = module , inputs = wrapped_inputs , outputs = wrapped_outputs , job_log = job . job_log , ) return job_id except Exception as e : job . error = str ( e ) if is_debug (): try : import traceback traceback . print_exc () except Exception : pass if isinstance ( e , KiaraProcessingException ): e . _module = module e . _inputs = inputs job . _exception = e raise e else : kpe = KiaraProcessingException ( e , module = module , inputs = inputs ) job . _exception = kpe raise kpe def job_status_updated ( self , job_id : str , status : typing . Union [ JobStatus , str , Exception ] ): job = self . _active_jobs . get ( job_id , None ) if job is None : raise Exception ( f \"Can't retrieve active job with id ' { job_id } ', no such job registered.\" ) if status == JobStatus . SUCCESS : job . job_log . add_log ( \"job finished successfully\" ) job . status = JobStatus . SUCCESS job = self . _active_jobs . pop ( job_id ) job . finished = datetime . now () self . _finished_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) elif status == JobStatus . FAILED or isinstance ( status , ( str , Exception )): job . job_log . add_log ( \"job failed\" ) job . status = JobStatus . FAILED job . finished = datetime . now () if isinstance ( status , str ): job . error = status elif isinstance ( status , Exception ): job . error = str ( status ) job . _exception = status job = self . _active_jobs . pop ( job_id ) self . _finished_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) elif status == JobStatus . STARTED : job . job_log . add_log ( \"job started\" ) job . status = JobStatus . STARTED job . started = datetime . now () self . _socket . send_string ( Job . create_event_msg ( job )) else : raise ValueError ( f \"Invalid value for status: { status } \" ) def sync_outputs ( self , * job_ids : str ): for j_id in job_ids : job_details = self . get_job_details ( j_id ) if not job_details : raise Exception ( f \"Can't sync outputs, no job with id: { j_id } \" ) job_inputs = self . _inputs [ j_id ] proc_info = ProcessingInfo ( ** job_details . dict ()) input_infos : typing . Dict [ str , ValueInfo ] = { k : v . get_info () for k , v in job_inputs . items () } # for field_name, value in d.items(): value_lineage = ValueLineage . create ( module_type = job_details . module_type , module_config = job_details . module_config , module_doc = job_details . module_doc , # output_name=field_name, inputs = input_infos , ) d = self . _outputs [ j_id ] d . sync ( proccessing_info = proc_info , lineage = value_lineage ) def wait_for ( self , * job_ids : str , sync_outputs : bool = True ): \"\"\"Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state.\"\"\" self . _wait_for ( * job_ids ) for job_id in job_ids : job = self . get_job_details ( job_id ) if job is None : raise Exception ( f \"Can't find job with id: { job_id } \" ) if job . status == JobStatus . SUCCESS : job . job_log . percent_finished = 100 if sync_outputs : self . sync_outputs ( * job_ids ) @abc . abstractmethod def process ( self , job_id : str , module : \"KiaraModule\" , inputs : ValueSet , outputs : ValueSet , job_log : JobLog , ) -> str : pass @abc . abstractmethod def _wait_for ( self , * job_ids : str ): pass wait_for ( self , * job_ids , * , sync_outputs = True ) \u00b6 Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state. Source code in kiara/processing/processor.py def wait_for ( self , * job_ids : str , sync_outputs : bool = True ): \"\"\"Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state.\"\"\" self . _wait_for ( * job_ids ) for job_id in job_ids : job = self . get_job_details ( job_id ) if job is None : raise Exception ( f \"Can't find job with id: { job_id } \" ) if job . status == JobStatus . SUCCESS : job . job_log . percent_finished = 100 if sync_outputs : self . sync_outputs ( * job_ids ) utils special \u00b6 check_valid_field_names ( * field_names ) \u00b6 Check whether the provided field names are all valid. Returns: Type Description List[str] an iterable of strings with invalid field names Source code in kiara/utils/__init__.py def check_valid_field_names ( * field_names ) -> typing . List [ str ]: \"\"\"Check whether the provided field names are all valid. Returns: an iterable of strings with invalid field names \"\"\" return [ x for x in field_names if x in INVALID_VALUE_NAMES or x . startswith ( \"_\" )] find_free_id ( stem , current_ids , sep = '_' ) \u00b6 Find a free var (or other name) based on a stem string, based on a list of provided existing names. Parameters: Name Type Description Default stem str the base string to use required current_ids Iterable[str] currently existing names required method str the method to create new names (allowed: 'count' -- for now) required method_args dict prototing_config for the creation method required Returns: Type Description str a free name Source code in kiara/utils/__init__.py def find_free_id ( stem : str , current_ids : typing . Iterable [ str ], sep = \"_\" , ) -> str : \"\"\"Find a free var (or other name) based on a stem string, based on a list of provided existing names. Args: stem (str): the base string to use current_ids (Iterable[str]): currently existing names method (str): the method to create new names (allowed: 'count' -- for now) method_args (dict): prototing_config for the creation method Returns: str: a free name \"\"\" start_count = 1 if stem not in current_ids : return stem i = start_count # new_name = None while True : new_name = f \" { stem }{ sep }{ i } \" if new_name in current_ids : i = i + 1 continue break return new_name get_auto_workflow_alias ( module_type , use_incremental_ids = False ) \u00b6 Return an id for a workflow obj of a provided module class. If 'use_incremental_ids' is set to True, a unique id is returned. Parameters: Name Type Description Default module_type str the name of the module type required use_incremental_ids bool whether to return a unique (incremental) id False Returns: Type Description str a module id Source code in kiara/utils/__init__.py def get_auto_workflow_alias ( module_type : str , use_incremental_ids : bool = False ) -> str : \"\"\"Return an id for a workflow obj of a provided module class. If 'use_incremental_ids' is set to True, a unique id is returned. Args: module_type (str): the name of the module type use_incremental_ids (bool): whether to return a unique (incremental) id Returns: str: a module id \"\"\" if not use_incremental_ids : return module_type nr = _AUTO_MODULE_ID . setdefault ( module_type , 0 ) _AUTO_MODULE_ID [ module_type ] = nr + 1 return f \" { module_type } _ { nr } \" class_loading \u00b6 find_all_kiara_modules () \u00b6 Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_kiara_modules () -> typing . Dict [ str , typing . Type [ \"KiaraModule\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" from kiara.module import KiaraModule modules = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.modules\" , base_class = KiaraModule , # type: ignore set_id_attribute = \"_module_type_name\" , remove_namespace_tokens = [ \"core.\" ], ) result = {} # need to test this, since I couldn't add an abstract method to the KiaraModule class itself (mypy complained because it is potentially overloaded) for k , cls in modules . items (): if not hasattr ( cls , \"process\" ): msg = f \"Ignoring module class ' { cls } ': no 'process' method.\" if is_debug (): log . warning ( msg ) else : log . debug ( msg ) continue # TODO: check signature of process method if k . startswith ( \"_\" ): tokens = k . split ( \".\" ) if len ( tokens ) == 1 : k = k [ 1 :] else : k = \".\" . join ( tokens [ 1 :]) result [ k ] = cls return result find_all_metadata_models () \u00b6 Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_metadata_models () -> typing . Dict [ str , typing . Type [ \"MetadataModel\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" return load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.metadata_models\" , base_class = MetadataModel , set_id_attribute = \"_metadata_key\" , remove_namespace_tokens = [ \"core.\" ], ) find_all_value_types () \u00b6 Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_value_types () -> typing . Dict [ str , typing . Type [ \"ValueType\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" all_value_types = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.value_types\" , base_class = ValueType , # type: ignore set_id_attribute = \"_value_type_name\" , remove_namespace_tokens = True , ) invalid = [ x for x in all_value_types . keys () if \".\" in x ] if invalid : raise Exception ( f \"Invalid value type name(s), type names can't contain '.': { ', ' . join ( invalid ) } \" ) return all_value_types find_subclasses_under ( base_class , module , prefix = '' , remove_namespace_tokens = None , module_name_func = None ) \u00b6 Find all (non-abstract) subclasses of a base class that live under a module (recursively). Parameters: Name Type Description Default base_class Type[~SUBCLASS_TYPE] the parent class required module Union[str, module] the module to search required prefix Optional[str] a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used '' remove_namespace_tokens Optional[Iterable[str]] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None None Returns: Type Description Mapping[str, Type[~SUBCLASS_TYPE]] a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value Source code in kiara/utils/class_loading.py def find_subclasses_under ( base_class : typing . Type [ SUBCLASS_TYPE ], module : typing . Union [ str , ModuleType ], prefix : typing . Optional [ str ] = \"\" , remove_namespace_tokens : typing . Optional [ typing . Iterable [ str ]] = None , module_name_func : typing . Callable = None , ) -> typing . Mapping [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all (non-abstract) subclasses of a base class that live under a module (recursively). Arguments: base_class: the parent class module: the module to search prefix: a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None Returns: a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value \"\"\" if hasattr ( sys , \"frozen\" ): raise NotImplementedError ( \"Pyinstaller bundling not supported yet.\" ) if isinstance ( module , str ): module = importlib . import_module ( module ) _import_modules_recursively ( module ) subclasses : typing . Iterable [ typing . Type [ SUBCLASS_TYPE ]] = _get_all_subclasses ( base_class ) result = {} for sc in subclasses : if not sc . __module__ . startswith ( module . __name__ ): continue if inspect . isabstract ( sc ): if is_debug (): # import traceback # traceback.print_stack() log . warning ( f \"Ignoring abstract subclass: { sc } \" ) else : log . debug ( f \"Ignoring abstract subclass: { sc } \" ) continue if module_name_func is None : module_name_func = _get_subclass_name name = module_name_func ( sc ) path = sc . __module__ [ len ( module . __name__ ) + 1 :] # noqa if path : full_name = f \" { path } . { name } \" else : full_name = name if prefix is None : prefix = module . __name__ + \".\" if remove_namespace_tokens : for rnt in remove_namespace_tokens : if prefix . startswith ( rnt ): prefix = prefix [ 0 : - len ( rnt )] # noqa if prefix : full_name = f \" { prefix } . { full_name } \" result [ full_name ] = sc return result load_all_subclasses_for_entry_point ( entry_point_name , base_class , set_id_attribute = None , remove_namespace_tokens = None ) \u00b6 Find all subclasses of a base class via package entry points. Parameters: Name Type Description Default entry_point_name str the entry point name to query entries for required base_class Type[~SUBCLASS_TYPE] the base class to look for required set_id_attribute Optional[str] whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set None remove_namespace_tokens Union[Iterable[str], bool] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed None TODO Source code in kiara/utils/class_loading.py def load_all_subclasses_for_entry_point ( entry_point_name : str , base_class : typing . Type [ SUBCLASS_TYPE ], set_id_attribute : typing . Union [ None , str ] = None , remove_namespace_tokens : typing . Union [ typing . Iterable [ str ], bool , None ] = None , ) -> typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all subclasses of a base class via package entry points. Arguments: entry_point_name: the entry point name to query entries for base_class: the base class to look for set_id_attribute: whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed TODO \"\"\" log2 = logging . getLogger ( \"stevedore\" ) out_hdlr = logging . StreamHandler ( sys . stdout ) out_hdlr . setFormatter ( logging . Formatter ( f \" { entry_point_name } plugin search error -> %(message)s\" ) ) out_hdlr . setLevel ( logging . INFO ) log2 . addHandler ( out_hdlr ) log2 . setLevel ( logging . INFO ) log . debug ( f \"Finding { entry_point_name } items from search paths...\" ) mgr = ExtensionManager ( namespace = entry_point_name , invoke_on_load = False , propagate_map_exceptions = True , ) result_entrypoints : typing . Dict [ str , typing . Type ] = {} result_dynamic : typing . Dict [ str , typing . Type ] = {} for plugin in mgr : name = plugin . name if isinstance ( plugin . plugin , type ) and issubclass ( plugin . plugin , base_class ): ep = plugin . entry_point module_cls = ep . load () if set_id_attribute : if hasattr ( module_cls , set_id_attribute ): if not getattr ( module_cls , set_id_attribute ) == name : log . warning ( f \"Item id mismatch for type { entry_point_name } : { getattr ( module_cls , set_id_attribute ) } != { name } , entry point key takes precedence: { name } )\" ) setattr ( module_cls , set_id_attribute , name ) else : setattr ( module_cls , set_id_attribute , name ) result_entrypoints [ name ] = module_cls elif ( isinstance ( plugin . plugin , tuple ) and len ( plugin . plugin ) >= 1 and callable ( plugin . plugin [ 0 ]) ) or callable ( plugin . plugin ): modules = _callable_wrapper ( plugin . plugin ) for k , v in modules . items (): _name = f \" { name } . { k } \" if _name in result_dynamic . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { _name } \" ) result_dynamic [ _name ] = v else : raise Exception ( f \"Can't load subclasses for entry point { entry_point_name } and base class { base_class } : invalid plugin type { type ( plugin . plugin ) } \" ) for k , v in result_dynamic . items (): if k in result_entrypoints . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { k } \" ) result_entrypoints [ k ] = v result : typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]] = {} for k , v in result_entrypoints . items (): if remove_namespace_tokens : if remove_namespace_tokens is True : k = k . split ( \".\" )[ - 1 ] elif isinstance ( remove_namespace_tokens , typing . Iterable ): for rnt in remove_namespace_tokens : if k . startswith ( rnt ): k = k [ len ( rnt ) :] # noqa if k in result . keys (): msg = \"\" if set_id_attribute : msg = f \" Check whether ' { v . __name__ } ' is missing the ' { set_id_attribute } ' class attribute (in case this is a sub-class), or it's ' { k } ' value is also set in another class?\" raise Exception ( f \"Duplicate item name for base class { base_class } : { k } . { msg } \" ) result [ k ] = v return result concurrency \u00b6 ThreadSaveCounter \u00b6 A thread-safe counter, can be used in kiara modules to update completion percentage. Source code in kiara/utils/concurrency.py class ThreadSaveCounter ( object ): \"\"\"A thread-safe counter, can be used in kiara modules to update completion percentage.\"\"\" def __init__ ( self ): self . _current = 0 self . _lock = threading . Lock () @property def current ( self ): return self . _current def current_percent ( self , total : int ) -> int : return int (( self . current / total ) * 100 ) def increment ( self ): with self . _lock : self . _current += 1 return self . _current def decrement ( self ): with self . _lock : self . _current -= 1 return self . _current modules \u00b6 find_file_for_module ( module_name , kiara = None ) \u00b6 Find the python file a module belongs to. Source code in kiara/utils/modules.py def find_file_for_module ( module_name : str , kiara : typing . Optional [ \"Kiara\" ] = None ) -> str : \"\"\"Find the python file a module belongs to.\"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () m_cls = kiara . get_module_class ( module_type = module_name ) python_module = m_cls . get_type_metadata () . python_class . get_module () # TODO: some sanity checks module_file = python_module . __file__ assert module_file is not None if module_file . endswith ( \"__init__.py\" ): extra_bit = ( python_module . __name__ . replace ( \".\" , os . path . sep ) + os . path . sep + \"__init__.py\" ) else : extra_bit = python_module . __name__ . replace ( \".\" , os . path . sep ) + \".py\" python_file_path = module_file [ 0 : - len ( extra_bit )] # noqa return python_file_path output \u00b6 OutputDetails ( BaseModel ) pydantic-model \u00b6 Source code in kiara/utils/output.py class OutputDetails ( BaseModel ): @classmethod def from_data ( cls , data : typing . Any ): if isinstance ( data , str ): if \"=\" in data : data = [ data ] else : data = [ f \"format= { data } \" ] if isinstance ( data , typing . Iterable ): data = list ( data ) if len ( data ) == 1 and isinstance ( data [ 0 ], str ) and \"=\" not in data [ 0 ]: data = [ f \"format= { data [ 0 ] } \" ] output_details_dict = dict_from_cli_args ( * data ) else : raise TypeError ( f \"Can't parse output detail config: invalid input type ' { type ( data ) } '.\" ) output_details = OutputDetails ( ** output_details_dict ) return output_details format : str = Field ( description = \"The output format.\" ) target : str = Field ( description = \"The output target.\" ) config : typing . Dict [ str , typing . Any ] = Field ( description = \"Output configuration.\" , default_factory = dict ) @root_validator ( pre = True ) def _set_defaults ( cls , values ): target : str = values . pop ( \"target\" , \"terminal\" ) format : str = values . pop ( \"format\" , None ) if format is None : if target == \"terminal\" : format = \"terminal\" else : if target == \"file\" : format = \"json\" else : ext = target . split ( \".\" )[ - 1 ] if ext in [ \"yaml\" , \"json\" ]: format = ext else : format = \"json\" result = { \"format\" : format , \"target\" : target , \"config\" : dict ( values )} return result config : Dict [ str , Any ] pydantic-field \u00b6 Output configuration. format : str pydantic-field required \u00b6 The output format. target : str pydantic-field required \u00b6 The output target. workflow special \u00b6 kiara_workflow \u00b6 KiaraWorkflow \u00b6 A thin wrapper class around a PipelineModule , mostly handling initialization from simplified configuration data. Source code in kiara/workflow/kiara_workflow.py class KiaraWorkflow ( object ): \"\"\"A thin wrapper class around a [PipelineModule][kiara.pipeline.PipelineModule], mostly handling initialization from simplified configuration data.\"\"\" def __init__ ( self , workflow_id : str , config : ModuleConfig , kiara : \"Kiara\" , controller : typing . Optional [ PipelineController ] = None , ): self . _controller : typing . Optional [ PipelineController ] = controller self . _workflow_id : str = workflow_id self . _workflow_config : ModuleConfig = config self . _kiara : Kiara = kiara root_module_args : typing . Dict [ str , typing . Any ] = { \"id\" : self . _workflow_id } if self . _workflow_config . module_type == \"pipeline\" : root_module_args [ \"module_type\" ] = \"pipeline\" root_module_args [ \"module_config\" ] = self . _workflow_config . module_config elif self . _kiara . is_pipeline_module ( self . _workflow_config . module_type ): root_module_args [ \"module_type\" ] = self . _workflow_config . module_type root_module_args [ \"module_config\" ] = self . _workflow_config . module_config else : # means it's a python module, and we wrap it into a single-module pipeline root_module_args [ \"module_type\" ] = \"pipeline\" steps_conf = { \"steps\" : [ { \"module_type\" : self . _workflow_config . module_type , \"step_id\" : slugify ( self . _workflow_config . module_type , separator = \"_\" ), \"module_config\" : self . _workflow_config . module_config , } ], \"input_aliases\" : \"auto\" , \"output_aliases\" : \"auto\" , } root_module_args [ \"module_config\" ] = steps_conf self . _root_module : PipelineModule = self . _kiara . create_module ( ** root_module_args ) # type: ignore assert isinstance ( self . _root_module , PipelineModule ) self . _pipeline : typing . Optional [ Pipeline ] = None @property def structure ( self ) -> PipelineStructure : return self . _root_module . structure @property def pipeline ( self ) -> Pipeline : if self . _pipeline is None : self . _pipeline = Pipeline ( self . structure , controller = self . _controller ) return self . _pipeline @property def controller ( self ) -> PipelineController : return self . pipeline . controller @property def status ( self ) -> StepStatus : return self . pipeline . status @property def inputs ( self ) -> ValueSet : return self . pipeline . inputs @property def outputs ( self ) -> ValueSet : return self . pipeline . outputs def get_current_state ( self ) -> PipelineState : return self . pipeline . get_current_state () @property def current_state ( self ) -> PipelineState : return self . get_current_state () # @inputs.setter # def inputs(self, inputs: typing.Mapping[str, typing.Any]): # self.pipeline.set_pipeline_inputs(**inputs) @property def input_names ( self ) -> typing . List [ str ]: return list ( self . inputs . get_all_field_names ()) @property def output_names ( self ) -> typing . List [ str ]: return list ( self . outputs . get_all_field_names ()) @property def workflow_id ( self ) -> str : return self . _workflow_id @property def steps ( self ) -> StepsInfo : return self . pipeline . structure . to_details () . steps_info def __repr__ ( self ): return f \" { self . __class__ . __name__ } (workflow_id= { self . workflow_id } , root_module= { self . _root_module } )\" def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield f \"[b]Workflow: { self . workflow_id } [/b]\" doc = self . _root_module . get_type_metadata () . documentation . description if doc and doc != DEFAULT_NO_DESC_VALUE : yield f \" \\n { doc } \\n \" table = Table ( box = box . SIMPLE , show_header = False ) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) doc_link = self . _root_module . get_type_metadata () . context . references . get ( \"documentation\" , None ) if doc_link : # TODO: use direct link url = doc_link . url module_str = f \"[link= { url } ] { self . _root_module . _module_type_id } [/link]\" # type: ignore else : module_str = self . _root_module . _module_type_id # type: ignore table . add_row ( \"root module\" , module_str ) table . add_row ( \"current status\" , self . status . name ) inputs_table = self . inputs . _create_rich_table ( show_headers = True ) table . add_row ( \"inputs\" , inputs_table ) outputs_table = self . outputs . _create_rich_table ( show_headers = True ) table . add_row ( \"outputs\" , outputs_table ) yield table","title":"kiara"},{"location":"reference/kiara/__init__/#kiara.__author__","text":"The author of this package.","title":"__author__"},{"location":"reference/kiara/__init__/#kiara.__email__","text":"Email address of the author.","title":"__email__"},{"location":"reference/kiara/__init__/#kiara.get_version","text":"Return the current version of Kiara . Source code in kiara/__init__.py def get_version () -> str : \"\"\"Return the current version of *Kiara*.\"\"\" from pkg_resources import DistributionNotFound , get_distribution try : # Change here if project is renamed and does not equal the package name dist_name = __name__ __version__ = get_distribution ( dist_name ) . version except DistributionNotFound : try : version_file = os . path . join ( os . path . dirname ( __file__ ), \"version.txt\" ) if os . path . exists ( version_file ): with open ( version_file , encoding = \"utf-8\" ) as vf : __version__ = vf . read () else : __version__ = \"unknown\" except ( Exception ): pass if __version__ is None : __version__ = \"unknown\" return __version__","title":"get_version()"},{"location":"reference/kiara/__init__/#kiara.config","text":"","title":"config"},{"location":"reference/kiara/__init__/#kiara.config.KiaraConfig","text":"Source code in kiara/config.py class KiaraConfig ( BaseSettings ): class Config : extra = Extra . forbid env_file_encoding = \"utf-8\" env_prefix = \"kiara_\" @classmethod def customise_sources ( cls , init_settings , env_settings , file_secret_settings , ): return ( init_settings , env_settings , yaml_config_settings_source , file_secret_settings , ) module_managers : typing . Optional [ typing . List [ typing . Union [ PythonModuleManagerConfig , PipelineModuleManagerConfig ] ] ] = Field ( description = \"The module managers to use in this kiara instance.\" , default = None ) default_processor : typing . Optional [ typing . Union [ SynchronousProcessorConfig , ThreadPoolProcessorConfig ] ] = Field ( description = \"The configuration for the default processor to use.\" , default_factory = SynchronousProcessorConfig , ) data_store : str = Field ( description = \"The path to the local kiara data store.\" , default = KIARA_DATA_STORE_DIR , ) extra_pipeline_folders : typing . List [ str ] = Field ( description = \"Paths to local folders that contain kiara pipelines.\" , default_factory = list , ) ignore_errors : bool = Field ( description = \"If set, kiara will try to ignore most errors (that can be ignored).\" , default = False , ) @validator ( \"module_managers\" , pre = True ) def _validate_managers ( cls , v ): if v is None : return [] if isinstance ( v , typing . Mapping ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( item , ModuleManager ): result . append ( item ) else : assert isinstance ( item , typing . Mapping ) mm_type = item . get ( \"module_manager_type\" , None ) if not mm_type : raise ValueError ( f \"No module manager type provided in config: { item } \" ) if mm_type == \"python\" : item_config = PythonModuleManagerConfig ( ** item ) elif mm_type == \"pipeline\" : item_config = PipelineModuleManagerConfig ( ** item ) else : raise ValueError ( f \"Invalid module manager type: { mm_type } \" ) result . append ( item_config ) return result @validator ( \"default_processor\" , pre = True ) def _validate_default_processor ( cls , v ): if not v : return SynchronousProcessorConfig () if isinstance ( v , ( SynchronousProcessorConfig , ThreadPoolProcessorConfig )): return v if v == \"synchronous\" : return SynchronousProcessorConfig () if v == \"multi-threaded\" : return ThreadPoolProcessorConfig () if not isinstance ( v , typing . Mapping ): raise ValueError ( f \"Invalid type ' { type ( v ) } ' for default_processor config: { v } \" ) processor_type = v . get ( \"module_processor_type\" , None ) if not processor_type : raise ValueError ( \"No 'module_processor_type' provided: {config} \" ) if processor_type == \"synchronous\" : config = SynchronousProcessorConfig ( ** v ) elif processor_type == \"multi-threaded\" : config = ThreadPoolProcessorConfig () return config","title":"KiaraConfig"},{"location":"reference/kiara/__init__/#kiara.config.yaml_config_settings_source","text":"A simple settings source that loads variables from a JSON file at the project's root. Here we happen to choose to use the env_file_encoding from Config when reading config.json Source code in kiara/config.py def yaml_config_settings_source ( settings : BaseSettings ) -> typing . Dict [ str , typing . Any ]: \"\"\" A simple settings source that loads variables from a JSON file at the project's root. Here we happen to choose to use the `env_file_encoding` from Config when reading `config.json` \"\"\" config_file = os . path . join ( kiara_app_dirs . user_config_dir , \"config.yaml\" ) if os . path . exists ( config_file ): data = get_data_from_file ( config_file ) return data else : return {}","title":"yaml_config_settings_source()"},{"location":"reference/kiara/__init__/#kiara.data","text":"Data and value related classes for Kiara .","title":"data"},{"location":"reference/kiara/__init__/#kiara.data.onboarding","text":"","title":"onboarding"},{"location":"reference/kiara/__init__/#kiara.data.registry","text":"","title":"registry"},{"location":"reference/kiara/__init__/#kiara.data.types","text":"This is the base module that contains everything data type-related in kiara . I'm still not 100% sure how to best implement the kiara type system, there are several ways it could be done, for example based on Python type-hints, using JSON-schema, Avro (which is my 2nd favourite option), as well as by implementing a custom type-class hierarchy. Which is what I have choosen to try first. For now, it looks like it'll work out, but there is a chance requirements I haven't forseen will crop up that could make this become ugly. Anyway, the way it works (for now) is that kiara comes with a set of often used types (the standard set of: scalars, list, dict, table & array, etc.) which each come with 2 functions that can serialize and deserialize values of that type in a persistant fashion -- which could be storing as a file on disk, or as a cell/row in a database. Those functions will most likley be kiara modules themselves, with even more restricted input/output type options. In addition, packages that contain modules can implement their own, custom types, if suitable ones are not available in core- kiara . Those can either be 'serialized/deserialized' into kiara -native types (which in turn will serialize them using their own serializing functions), or will have to implement custom serializing functionality (which will probably be discouraged, since this might not be trivial and there are quite a few things to consider).","title":"types"},{"location":"reference/kiara/__init__/#kiara.data.values","text":"A module that contains value-related classes for Kiara . A value in Kiara-speak is a pointer to actual data (aka 'bytes'). It contains metadata about that data (like whether it's valid/set, what type/schema it has, when it was last modified, ...), but it does not contain the data itself. The reason for that is that such data can be fairly large, and in a lot of cases it is not necessary for the code involved to have access to it, access to the metadata is enough. Each Value has a unique id, which can be used to retrieve the data (whole, or parts of it) from a DataRegistry . In addition, that id can be used to subscribe to change events for a value (published whenever the data that is associated with a value was changed).","title":"values"},{"location":"reference/kiara/__init__/#kiara.defaults","text":"","title":"defaults"},{"location":"reference/kiara/__init__/#kiara.defaults.DEFAULT_EXCLUDE_DIRS","text":"List of directory names to exclude by default when walking a folder recursively.","title":"DEFAULT_EXCLUDE_DIRS"},{"location":"reference/kiara/__init__/#kiara.defaults.DEFAULT_EXCLUDE_FILES","text":"List of file names to exclude by default when reading folders.","title":"DEFAULT_EXCLUDE_FILES"},{"location":"reference/kiara/__init__/#kiara.defaults.DEFAULT_PIPELINE_PARENT_ID","text":"Default parent id for pipeline objects that are not associated with a workflow.","title":"DEFAULT_PIPELINE_PARENT_ID"},{"location":"reference/kiara/__init__/#kiara.defaults.INVALID_VALUE_NAMES","text":"List of reserved names, inputs/outputs can't use those.","title":"INVALID_VALUE_NAMES"},{"location":"reference/kiara/__init__/#kiara.defaults.KIARA_MODULE_BASE_FOLDER","text":"Marker to indicate the base folder for the kiara module.","title":"KIARA_MODULE_BASE_FOLDER"},{"location":"reference/kiara/__init__/#kiara.defaults.KIARA_RESOURCES_FOLDER","text":"Default resources folder for this package.","title":"KIARA_RESOURCES_FOLDER"},{"location":"reference/kiara/__init__/#kiara.defaults.MODULE_TYPE_KEY","text":"The key to specify the type of a module.","title":"MODULE_TYPE_KEY"},{"location":"reference/kiara/__init__/#kiara.defaults.MODULE_TYPE_NAME_KEY","text":"The string for the module type name in a module configuration dict.","title":"MODULE_TYPE_NAME_KEY"},{"location":"reference/kiara/__init__/#kiara.defaults.NO_HASH_MARKER","text":"Marker string to indicate no hash was calculated.","title":"NO_HASH_MARKER"},{"location":"reference/kiara/__init__/#kiara.defaults.NO_VALUE_ID_MARKER","text":"Marker string to indicate no value id exists.","title":"NO_VALUE_ID_MARKER"},{"location":"reference/kiara/__init__/#kiara.defaults.PIPELINE_PARENT_MARKER","text":"Marker string in the pipeline structure that indicates a parent pipeline element.","title":"PIPELINE_PARENT_MARKER"},{"location":"reference/kiara/__init__/#kiara.defaults.STEP_ID_KEY","text":"The key to specify the step id.","title":"STEP_ID_KEY"},{"location":"reference/kiara/__init__/#kiara.defaults.VALID_PIPELINE_FILE_EXTENSIONS","text":"File extensions a kiara pipeline/workflow file can have.","title":"VALID_PIPELINE_FILE_EXTENSIONS"},{"location":"reference/kiara/__init__/#kiara.defaults.SpecialValue","text":"An enumeration. Source code in kiara/defaults.py class SpecialValue ( Enum ): NOT_SET = \"__not_set__\" NO_VALUE = \"__no_value__\" IGNORE = \"__ignore__\"","title":"SpecialValue"},{"location":"reference/kiara/__init__/#kiara.doc","text":"Main module for code that helps with documentation auto-generation in supported projects.","title":"doc"},{"location":"reference/kiara/__init__/#kiara.doc.FrklDocumentationPlugin","text":"mkdocs plugin to render API documentation for a project. To add to a project, add this to the 'plugins' section of a mkdocs config file: - frkl-docgen : main_module : \"module_name\" This will add an API reference navigation item to your page navigation, with auto-generated entries for every Python module in your package. Source code in kiara/doc/__init__.py class FrklDocumentationPlugin ( BasePlugin ): \"\"\"[mkdocs](https://www.mkdocs.org/) plugin to render API documentation for a project. To add to a project, add this to the 'plugins' section of a mkdocs config file: ```yaml - frkl-docgen: main_module: \"module_name\" ``` This will add an ``API reference`` navigation item to your page navigation, with auto-generated entries for every Python module in your package. \"\"\" config_scheme = (( \"main_module\" , mkdocs . config . config_options . Type ( str )),) def __init__ ( self ): self . _doc_paths = None self . _dir = tempfile . TemporaryDirectory ( prefix = \"frkl_doc_gen_\" ) self . _doc_files = None super () . __init__ () def on_files ( self , files : Files , config : Config ) -> Files : self . _doc_paths = gen_pages_for_module ( self . config [ \"main_module\" ]) self . _doc_files = {} for k in sorted ( self . _doc_paths , key = lambda x : os . path . splitext ( x )[ 0 ]): content = self . _doc_paths [ k ][ \"content\" ] _file = File ( k , src_dir = self . _dir . name , dest_dir = config [ \"site_dir\" ], use_directory_urls = config [ \"use_directory_urls\" ], ) os . makedirs ( os . path . dirname ( _file . abs_src_path ), exist_ok = True ) with open ( _file . abs_src_path , \"w\" ) as f : f . write ( content ) self . _doc_files [ k ] = _file files . append ( _file ) return files def on_page_content ( self , html , page : Page , config : Config , files : Files ): repo_url = config . get ( \"repo_url\" , None ) python_src = config . get ( \"edit_uri\" , None ) if page . file . src_path in self . _doc_paths . keys (): src_path = self . _doc_paths . get ( page . file . src_path )[ \"python_src\" ][ \"rel_path\" ] rel_base = urllib . parse . urljoin ( repo_url , f \" { python_src } /../src/ { src_path } \" ) page . edit_url = rel_base return html def on_nav ( self , nav : Navigation , config : Config , files : Files ): for item in nav . items : if item . title and \"Api reference\" in item . title : return nav pages = [] for _file in self . _doc_files . values (): pages . append ( _file . page ) section = Section ( title = \"API reference\" , children = pages ) nav . items . append ( section ) nav . pages . extend ( pages ) _add_previous_and_next_links ( nav . pages ) _add_parent_links ( nav . items ) return nav def on_post_build ( self , config : Config ): self . _dir . cleanup ()","title":"FrklDocumentationPlugin"},{"location":"reference/kiara/__init__/#kiara.doc.gen_info_pages","text":"","title":"gen_info_pages"},{"location":"reference/kiara/__init__/#kiara.doc.generate_api_doc","text":"","title":"generate_api_doc"},{"location":"reference/kiara/__init__/#kiara.doc.mkdocs_macros_cli","text":"","title":"mkdocs_macros_cli"},{"location":"reference/kiara/__init__/#kiara.doc.mkdocs_macros_kiara","text":"","title":"mkdocs_macros_kiara"},{"location":"reference/kiara/__init__/#kiara.doc.mkdocstrings","text":"","title":"mkdocstrings"},{"location":"reference/kiara/__init__/#kiara.environment","text":"","title":"environment"},{"location":"reference/kiara/__init__/#kiara.environment.RuntimeEnvironmentConfig","text":"Source code in kiara/environment/__init__.py class RuntimeEnvironmentConfig ( BaseModel ): class Config : allow_mutation = False include_all_info : bool = Field ( default = False , description = \"Whether to include all properties, even if it might include potentially private/sensitive information. This is mainly used for debug purposes.\" , )","title":"RuntimeEnvironmentConfig"},{"location":"reference/kiara/__init__/#kiara.environment.operating_system","text":"","title":"operating_system"},{"location":"reference/kiara/__init__/#kiara.environment.python","text":"","title":"python"},{"location":"reference/kiara/__init__/#kiara.events","text":"","title":"events"},{"location":"reference/kiara/__init__/#kiara.events.PipelineInputEvent","text":"Event that gets fired when one or several inputs for the pipeline itself have changed. Source code in kiara/events.py class PipelineInputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several inputs for the pipeline itself have changed.\"\"\" type : Literal [ \"pipeline_input\" ] = \"pipeline_input\" updated_pipeline_inputs : typing . List [ str ] = Field ( description = \"list of pipeline input names that where changed\" )","title":"PipelineInputEvent"},{"location":"reference/kiara/__init__/#kiara.events.PipelineOutputEvent","text":"Event that gets fired when one or several outputs for the pipeline itself have changed. Source code in kiara/events.py class PipelineOutputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several outputs for the pipeline itself have changed.\"\"\" type : Literal [ \"pipeline_output\" ] = \"pipeline_output\" updated_pipeline_outputs : typing . List [ str ] = Field ( description = \"list of pipeline output names that where changed\" )","title":"PipelineOutputEvent"},{"location":"reference/kiara/__init__/#kiara.events.StepEvent","text":"Source code in kiara/events.py class StepEvent ( BaseModel ): class Config : allow_mutation = False pipeline_id : str def __repr__ ( self ): d = self . dict () d . pop ( \"pipeline_id\" ) return f \" { self . __class__ . __name__ } (pipeline_id= { self . pipeline_id } data= { d } \" def __str__ ( self ): return self . __repr__ ()","title":"StepEvent"},{"location":"reference/kiara/__init__/#kiara.events.StepInputEvent","text":"Event that gets fired when one or several inputs for steps within a pipeline have changed. Source code in kiara/events.py class StepInputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several inputs for steps within a pipeline have changed.\"\"\" type : Literal [ \"step_input\" ] = \"step_input\" updated_step_inputs : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"steps (keys) with updated inputs which need re-processing (value is list of updated input names)\" ) @property def newly_stale_steps ( self ) -> typing . List [ str ]: \"\"\"Convenience method to display the steps that have been rendered 'stale' by this event.\"\"\" return list ( self . updated_step_inputs . keys ())","title":"StepInputEvent"},{"location":"reference/kiara/__init__/#kiara.events.StepOutputEvent","text":"Event that gets fired when one or several outputs for steps within a pipeline have changed. Source code in kiara/events.py class StepOutputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several outputs for steps within a pipeline have changed.\"\"\" type : Literal [ \"step_output\" ] = \"step_output\" updated_step_outputs : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"steps (keys) that finished processing of one, several or all outputs (values are list of 'finished' output fields)\" )","title":"StepOutputEvent"},{"location":"reference/kiara/__init__/#kiara.examples","text":"","title":"examples"},{"location":"reference/kiara/__init__/#kiara.examples.example_controller","text":"","title":"example_controller"},{"location":"reference/kiara/__init__/#kiara.info","text":"","title":"info"},{"location":"reference/kiara/__init__/#kiara.info.kiara","text":"","title":"kiara"},{"location":"reference/kiara/__init__/#kiara.info.modules","text":"","title":"modules"},{"location":"reference/kiara/__init__/#kiara.info.operations","text":"","title":"operations"},{"location":"reference/kiara/__init__/#kiara.info.pipelines","text":"","title":"pipelines"},{"location":"reference/kiara/__init__/#kiara.info.types","text":"","title":"types"},{"location":"reference/kiara/__init__/#kiara.interfaces","text":"Implementation of interfaces for Kiara .","title":"interfaces"},{"location":"reference/kiara/__init__/#kiara.interfaces.get_console","text":"Get a global Console instance. Returns: Type Description Console A console instance. Source code in kiara/interfaces/__init__.py def get_console () -> Console : \"\"\"Get a global Console instance. Returns: Console: A console instance. \"\"\" global _console if _console is None or True : console_width = os . environ . get ( \"CONSOLE_WIDTH\" , None ) width = None if console_width : try : width = int ( console_width ) except Exception : pass _console = Console ( width = width ) return _console","title":"get_console()"},{"location":"reference/kiara/__init__/#kiara.interfaces.cli","text":"A command-line interface for Kiara .","title":"cli"},{"location":"reference/kiara/__init__/#kiara.interfaces.python_api","text":"A Python API for creating workflow sessions and dynamic pipelines in kiara .","title":"python_api"},{"location":"reference/kiara/__init__/#kiara.kiara","text":"Main module.","title":"kiara"},{"location":"reference/kiara/__init__/#kiara.kiara.Kiara","text":"The core context of a kiara session. The Kiara object holds all information related to the current environment the user does works in. This includes: available modules, operations & pipelines available value types available metadata schemas available data items available controller and processor types misc. configuration options It's possible to use kiara without ever manually touching the 'Kiara' class, by default all relevant classes and functions will use a default instance of this class (available via the Kiara.instance() method. The Kiara class is highly dependent on the Python environment it lives in, because it auto-discovers available sub-classes of its building blocks (modules, value types, etc.). So, you can't assume that, for example, a pipeline you create will work the same way (or at all) in a different environment. kiara will always be able to tell you all the details of this environment, though, and it will attach those details to things like data, so there is always a record of how something was created, and in which environment. Source code in kiara/kiara.py class Kiara ( object ): \"\"\"The core context of a kiara session. The `Kiara` object holds all information related to the current environment the user does works in. This includes: - available modules, operations & pipelines - available value types - available metadata schemas - available data items - available controller and processor types - misc. configuration options It's possible to use *kiara* without ever manually touching the 'Kiara' class, by default all relevant classes and functions will use a default instance of this class (available via the `Kiara.instance()` method. The Kiara class is highly dependent on the Python environment it lives in, because it auto-discovers available sub-classes of its building blocks (modules, value types, etc.). So, you can't assume that, for example, a pipeline you create will work the same way (or at all) in a different environment. *kiara* will always be able to tell you all the details of this environment, though, and it will attach those details to things like data, so there is always a record of how something was created, and in which environment. \"\"\" _instance = None @classmethod def instance ( cls ): \"\"\"The default *kiara* context. In most cases, it's recommended you create and manage your own, though.\"\"\" if cls . _instance is None : cls . _instance = Kiara () return cls . _instance def __init__ ( self , config : typing . Optional [ KiaraConfig ] = None ): if not config : config = KiaraConfig () self . _id : str = str ( uuid . uuid4 ()) self . _config : KiaraConfig = config # self._zmq_context: Context = Context.instance() self . _operation_mgmt = OperationMgmt ( kiara = self ) self . _metadata_mgmt = MetadataMgmt ( kiara = self ) self . _data_store = LocalDataStore ( kiara = self , base_path = config . data_store ) # self.start_zmq_device() # self.start_log_thread() self . _default_processor : ModuleProcessor = ModuleProcessor . from_config ( config . default_processor , kiara = self ) self . _type_mgmt_obj : TypeMgmt = TypeMgmt ( self ) self . _data_registry : InMemoryDataRegistry = InMemoryDataRegistry ( self ) self . _module_mgr : MergedModuleManager = MergedModuleManager ( config . module_managers , extra_pipeline_folders = self . _config . extra_pipeline_folders , ignore_errors = self . _config . ignore_errors , ) self . _template_mgmt : TemplateRenderingMgmt = TemplateRenderingMgmt . create ( kiara = self ) @property def config ( self ) -> KiaraConfig : \"\"\"The configuration of this *kiara* environment.\"\"\" return self . _config @property def default_processor ( self ) -> \"ModuleProcessor\" : \"\"\"The default module processor that will be used in this environment, unless otherwise specified.\"\"\" return self . _default_processor def start_zmq_device ( self ): pd = ThreadDevice ( zmq . QUEUE , zmq . SUB , zmq . PUB ) pd . bind_in ( \"inproc://kiara_in\" ) pd . bind_out ( \"inproc://kiara_out\" ) pd . setsockopt_in ( zmq . SUBSCRIBE , b \"\" ) pd . setsockopt_in ( zmq . IDENTITY , b \"SUB\" ) pd . setsockopt_out ( zmq . IDENTITY , b \"PUB\" ) pd . start () def start_log_thread ( self ): def log_messages (): socket = self . _zmq_context . socket ( zmq . SUB ) socket . setsockopt_string ( zmq . SUBSCRIBE , \"\" ) socket . connect ( \"inproc://kiara_out\" ) debug = is_debug () while True : message = socket . recv () topic , details = message . decode () . split ( \" \" , maxsplit = 1 ) try : job = Job . parse_raw ( details ) if debug : print ( f \" { topic } : { job . pipeline_name } . { job . step_id } \" ) else : log . debug ( f \" { topic } : { job . pipeline_name } . { job . step_id } \" ) except Exception as e : if debug : import traceback traceback . print_exception () else : log . debug ( e ) t = Thread ( target = log_messages , daemon = True ) t . start () def explain ( self , item : typing . Any ): explain ( item ) @property def type_mgmt ( self ) -> TypeMgmt : return self . _type_mgmt_obj @property def data_store ( self ) -> LocalDataStore : return self . _data_store @property def module_mgmt ( self ) -> MergedModuleManager : return self . _module_mgr @property def metadata_mgmt ( self ) -> MetadataMgmt : return self . _metadata_mgmt @property def value_types ( self ) -> typing . Mapping [ str , typing . Type [ ValueType ]]: return self . type_mgmt . value_types @property def value_type_names ( self ) -> typing . List [ str ]: return self . type_mgmt . value_type_names def determine_type ( self , data : typing . Any ) -> typing . Optional [ ValueType ]: raise NotImplementedError () # return self.type_mgmt.determine_type(data) def get_value_type_cls ( self , type_name : str ) -> typing . Type [ ValueType ]: return self . type_mgmt . get_value_type_cls ( type_name = type_name ) def get_value ( self , value_id : str ) -> Value : if not isinstance ( value_id , str ): raise TypeError ( f \"Invalid type ' { type ( value_id ) } ' for value id, must be a string.\" ) if value_id . startswith ( \"value:\" ): value_id = value_id [ 6 :] if not value_id : raise Exception ( \"No value id provided.\" ) try : value = self . data_registry . get_value_obj ( value_id ) if value is None : raise Exception ( f \"No value with id: { value_id } \" ) return value except Exception : pass try : value = self . data_store . get_value_obj ( value_id ) if value is None : raise Exception ( f \"No value with id: { value_id } \" ) return value except Exception : pass raise Exception ( f \"No value registered for id: { value_id } \" ) def add_module_manager ( self , module_manager : ModuleManager ): self . _module_mgr . add_module_manager ( module_manager ) self . _type_mgmt_obj . invalidate_types () @property def data_registry ( self ) -> DataRegistry : return self . _data_registry @property def operation_mgmt ( self ) -> OperationMgmt : return self . _operation_mgmt @property def template_mgmt ( self ) -> TemplateRenderingMgmt : return self . _template_mgmt def get_module_class ( self , module_type : str ) -> typing . Type [ \"KiaraModule\" ]: return self . _module_mgr . get_module_class ( module_type = module_type ) # def get_module_info(self, module_type: str) -> \"ModuleInfo\": # # if module_type not in self.available_module_types: # raise ValueError(f\"Module type '{module_type}' not available.\") # # if module_type in self.available_pipeline_module_types: # from kiara.pipeline.module import PipelineModuleInfo # # info = PipelineModuleInfo(module_type=module_type, _kiara=self) # type: ignore # return info # else: # from kiara.module import ModuleInfo # # info = ModuleInfo.from_module_cls(module_cls=module_type) # return info @property def available_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available modules\"\"\" return self . _module_mgr . available_module_types @property def available_non_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return self . _module_mgr . available_non_pipeline_module_types @property def available_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return self . _module_mgr . available_pipeline_module_types @property def available_operation_ids ( self ) -> typing . List [ str ]: return self . _operation_mgmt . operation_ids def is_pipeline_module ( self , module_type : str ): return self . _module_mgr . is_pipeline_module ( module_type = module_type ) def register_pipeline_description ( self , data : typing . Union [ Path , str , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , raise_exception : bool = False , ) -> typing . Optional [ str ]: return self . _module_mgr . register_pipeline_description ( data = data , module_type_name = module_type_name , namespace = namespace , raise_exception = raise_exception , ) def create_module ( self , module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Arguments: module_type: the module type- or operation-id module_config: the module instance configuration (must be empty in case of the module_type being an operation id id: the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline parent_id: a reference to the pipeline that contains this module (if applicable) Returns: The instantiated module object. \"\"\" if module_type == \"pipeline\" : from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = module_config , kiara = self ) return module elif ( module_type not in self . available_module_types and module_type in self . operation_mgmt . operation_ids ): op = self . operation_mgmt . get_operation ( module_type ) assert op is not None module_type = op . module_type op_config = op . module_config if not module_config : _module_config : typing . Optional [ typing . Mapping [ str , typing . Any ] ] = op_config else : _module_config = dict ( op_config ) _module_config . update ( module_config ) elif ( module_type not in self . available_module_types and isinstance ( module_type , str ) and os . path . isfile ( os . path . realpath ( os . path . expanduser ( module_type ))) ): if module_config : raise Exception ( \"Creating pipeline module from file with extra module_config not supported (yet).\" ) path = os . path . expanduser ( module_type ) pipeline_config_data = get_data_from_file ( path ) pipeline_config = PipelineConfig ( ** pipeline_config_data ) from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = pipeline_config , kiara = self ) return module else : _module_config = module_config return self . _module_mgr . create_module ( kiara = self , id = id , module_type = module_type , module_config = _module_config , parent_id = parent_id , ) def get_module_doc ( self , module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ): m = self . create_module ( module_type = module_type , module_config = module_config ) return m . module_instance_doc def get_operation ( self , operation_id : str ) -> Operation : op = self . operation_mgmt . profiles . get ( operation_id , None ) if op is None : raise Exception ( f \"No operation with id ' { operation_id } ' available.\" ) return op def run ( self , module_type : typing . Union [ str , Operation ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , inputs : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , output_name : typing . Optional [ str ] = None , resolve_result : bool = False , ) -> typing . Union [ ValueSet , Value , typing . Any ]: if isinstance ( module_type , str ): if module_type in self . available_module_types : module = self . create_module ( module_type = module_type , module_config = module_config ) elif module_type in self . operation_mgmt . profiles . keys (): if module_config : raise NotImplementedError () op = self . operation_mgmt . profiles [ module_type ] module = op . module elif os . path . isfile ( os . path . realpath ( os . path . expanduser ( module_type ))): path = os . path . expanduser ( module_type ) pipeline_config_data = get_data_from_file ( path ) # pipeline_config = PipelineConfig(**pipeline_config_data) module = self . create_module ( \"pipeline\" , module_config = pipeline_config_data ) else : raise Exception ( f \"Can't run operation: invalid module type ' { module_type } '\" ) elif isinstance ( module_type , Operation ): if module_config : raise NotImplementedError () module = module_type . module else : raise Exception ( f \"Invalid class for module_type: { type ( module_type ) } \" ) return self . run_module ( module = module , inputs = inputs , output_name = output_name , resolve_result = resolve_result , ) def run_module ( self , module : \"KiaraModule\" , inputs : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , output_name : typing . Optional [ str ] = None , resolve_result : bool = False , ): if inputs is None : inputs = {} result = module . run ( ** inputs ) if output_name is not None : v = result . get_value_obj ( output_name ) if resolve_result : return v . get_value_data () else : return v else : if resolve_result : return result . get_all_value_data () else : return result def create_pipeline ( self , config : typing . Union [ PipelineConfig , typing . Mapping [ str , typing . Any ], str ], controller : typing . Optional [ PipelineController ] = None , ) -> Pipeline : if isinstance ( config , typing . Mapping ): pipeline_config : PipelineConfig = PipelineConfig ( ** config ) elif isinstance ( config , str ): if config == \"pipeline\" : raise Exception ( \"Can't create pipeline from 'pipeline' module type without further configuration.\" ) # TODO: if already a pipeline, don't wrap # if config in self.available_pipeline_module_types: # pass if config in self . available_module_types : config_data = { \"steps\" : [ { \"module_type\" : config , \"step_id\" : create_valid_identifier ( config ), } ] } pipeline_config = PipelineConfig ( ** config_data ) elif os . path . isfile ( os . path . realpath ( os . path . expanduser ( config ))): path = os . path . expanduser ( config ) pipeline_config_data = get_data_from_file ( path ) pipeline_config = PipelineConfig ( ** pipeline_config_data ) else : raise Exception ( f \"Can't create pipeline config from string ' { config } '. Value must be path to a file, or one of: { ', ' . join ( self . available_module_types ) } \" ) elif isinstance ( config , PipelineConfig ): pipeline_config = config else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for pipeline configuration.\" ) pipeline = pipeline_config . create_pipeline ( controller = controller , kiara = self ) return pipeline def create_workflow_from_operation_config ( self , config : \"ModuleConfig\" , workflow_id : typing . Optional [ str ] = None , controller : typing . Optional [ PipelineController ] = None , ): if not workflow_id : workflow_id = get_auto_workflow_alias ( config . module_type , use_incremental_ids = True ) workflow = KiaraWorkflow ( workflow_id = workflow_id , config = config , controller = controller , kiara = self , ) return workflow def create_workflow ( self , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], workflow_id : typing . Optional [ str ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , controller : typing . Optional [ PipelineController ] = None , ) -> KiaraWorkflow : _config = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = self ) return self . create_workflow_from_operation_config ( config = _config , workflow_id = workflow_id , controller = controller ) def pretty_print ( self , value : Value , target_type : str = \"renderables\" , print_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ) -> typing . Any : pretty_print_ops : PrettyPrintOperationType = self . operation_mgmt . get_operations ( \"pretty_print\" ) # type: ignore return pretty_print_ops . pretty_print ( value = value , target_type = target_type , print_config = print_config )","title":"Kiara"},{"location":"reference/kiara/__init__/#kiara.kiara.explain","text":"Pretty print information about an item on the terminal. Source code in kiara/kiara.py def explain ( item : typing . Any ): \"\"\"Pretty print information about an item on the terminal.\"\"\" if isinstance ( item , type ): from kiara.module import KiaraModule if issubclass ( item , KiaraModule ): item = KiaraModuleTypeMetadata . from_module_class ( module_cls = item ) elif isinstance ( item , Value ): item . get_metadata () console = get_console () console . print ( item )","title":"explain()"},{"location":"reference/kiara/__init__/#kiara.metadata","text":"","title":"metadata"},{"location":"reference/kiara/__init__/#kiara.metadata.MetadataModel","text":"Base class for classes that represent value metadata in kiara. Source code in kiara/metadata/__init__.py class MetadataModel ( KiaraInfoModel ): \"\"\"Base class for classes that represent value metadata in kiara.\"\"\" @classmethod def get_type_metadata ( cls ) -> \"MetadataModelMetadata\" : \"\"\"Return all metadata associated with this module type.\"\"\" from kiara.metadata.core_models import MetadataModelMetadata return MetadataModelMetadata . from_model_class ( model_cls = cls ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) for k in self . __fields__ . keys (): if k == \"type_name\" and config . get ( \"omit_type_name\" , False ): continue attr = getattr ( self , k ) v = extract_renderable ( attr ) table . add_row ( k , v ) if \"operations\" in config . keys (): ids = list ( config [ \"operations\" ] . keys ()) table . add_row ( \"operations\" , \" \\n \" . join ( ids )) return table","title":"MetadataModel"},{"location":"reference/kiara/__init__/#kiara.metadata.ValueTypeAndDescription","text":"Source code in kiara/metadata/__init__.py class ValueTypeAndDescription ( BaseModel ): description : str = Field ( description = \"The description for the value.\" ) type : str = Field ( description = \"The value type.\" ) value_default : typing . Any = Field ( description = \"Default for the value.\" , default = None ) required : bool = Field ( description = \"Whether this value is required\" )","title":"ValueTypeAndDescription"},{"location":"reference/kiara/__init__/#kiara.metadata.core_models","text":"","title":"core_models"},{"location":"reference/kiara/__init__/#kiara.metadata.data","text":"","title":"data"},{"location":"reference/kiara/__init__/#kiara.metadata.module_models","text":"","title":"module_models"},{"location":"reference/kiara/__init__/#kiara.metadata.operation_models","text":"","title":"operation_models"},{"location":"reference/kiara/__init__/#kiara.metadata.type_models","text":"","title":"type_models"},{"location":"reference/kiara/__init__/#kiara.module","text":"","title":"module"},{"location":"reference/kiara/__init__/#kiara.module.KiaraModule","text":"The base class that every custom module in Kiara needs to inherit from. The core of every KiaraModule is a process method, which should be a 'pure', idempotent function that creates one or several output values from the given input(s), and its purpose is to transfor a set of inputs into a set of outputs. Every module can be configured. The module configuration schema can differ, but every one such configuration needs to subclass the ModuleTypeConfigSchema class and set as the value to the _config_cls attribute of the module class. This is useful, because it allows for some modules to serve a much larger variety of use-cases than non-configurable modules would be, which would mean more code duplication because of very simlilar, but slightly different module types. Each module class (type) has a unique -- within a kiara context -- module type id which can be accessed via the _module_type_id class attribute. Examples: A simple example would be an 'addition' module, with a and b configured as inputs, and z as the output field name. An implementing class would look something like this: TODO Parameters: Name Type Description Default id str the id for this module (needs to be unique within a pipeline) None parent_id Optional[str] the id of the parent, in case this module is part of a pipeline None module_config Any the configuation for this module None metadata Mapping[str, Any] metadata for this module (not implemented yet) required Source code in kiara/module.py class KiaraModule ( typing . Generic [ KIARA_CONFIG ], abc . ABC ): \"\"\"The base class that every custom module in *Kiara* needs to inherit from. The core of every ``KiaraModule`` is a ``process`` method, which should be a 'pure', idempotent function that creates one or several output values from the given input(s), and its purpose is to transfor a set of inputs into a set of outputs. Every module can be configured. The module configuration schema can differ, but every one such configuration needs to subclass the [ModuleTypeConfigSchema][kiara.module_config.ModuleTypeConfigSchema] class and set as the value to the ``_config_cls`` attribute of the module class. This is useful, because it allows for some modules to serve a much larger variety of use-cases than non-configurable modules would be, which would mean more code duplication because of very simlilar, but slightly different module types. Each module class (type) has a unique -- within a *kiara* context -- module type id which can be accessed via the ``_module_type_id`` class attribute. Examples: A simple example would be an 'addition' module, with ``a`` and ``b`` configured as inputs, and ``z`` as the output field name. An implementing class would look something like this: TODO Arguments: id (str): the id for this module (needs to be unique within a pipeline) parent_id (typing.Optional[str]): the id of the parent, in case this module is part of a pipeline module_config (typing.Any): the configuation for this module metadata (typing.Mapping[str, typing.Any]): metadata for this module (not implemented yet) \"\"\" # TODO: not quite sure about this generic type here, mypy doesn't seem to like it _config_cls : typing . Type [ KIARA_CONFIG ] = ModuleTypeConfigSchema # type: ignore @classmethod def create_instance ( cls , module_type : typing . Optional [ str ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"KiaraModule\" : \"\"\"Create an instance of a *kiara* module. This class method is overloaded in a way that you can either provide the `module_type` argument, in which case the relevant sub-class will be queried from the *kiara* context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Arguments: module_type: must be None if called on the ``KiaraModule`` base class, otherwise the module or operation id module_config: the configuration of the module instance kiara: the *kiara* context \"\"\" if cls == KiaraModule : if not module_type : raise Exception ( \"This method must be either called on a subclass of KiaraModule, not KiaraModule itself, or it needs the 'module_type' argument specified.\" ) else : if module_type : raise Exception ( \"This method must be either called without the 'module_type' argument specified, or on a subclass of the KiaraModule class, but not both.\" ) if cls == KiaraModule : assert module_type is not None module_conf = ModuleConfig . create_module_config ( config = module_type , module_config = module_config , kiara = kiara ) else : module_conf = ModuleConfig . create_module_config ( config = cls , module_config = module_config , kiara = kiara ) return module_conf . create_module ( kiara = kiara ) @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: \"\"\"Retrieve a collection of profiles (pre-set module configs) for this *kiara* module type. This is used to automatically create generally useful operations (incl. their ids). \"\"\" @classmethod def get_type_metadata ( cls ) -> KiaraModuleTypeMetadata : \"\"\"Return all metadata associated with this module type.\"\"\" return KiaraModuleTypeMetadata . from_module_class ( cls ) # @classmethod # def profiles( # cls, # ) -> typing.Optional[typing.Mapping[str, typing.Mapping[str, typing.Any]]]: # \"\"\"Retrieve a collection of profiles (pre-set module configs) for this *kiara* module type. # # This is used to automatically create generally useful operations (incl. their ids). # \"\"\" # return None @classmethod def is_pipeline ( cls ) -> bool : \"\"\"Check whether this module type is a pipeline, or not.\"\"\" return False def __init__ ( self , id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , module_config : typing . Union [ None , KIARA_CONFIG , typing . Mapping [ str , typing . Any ] ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if id is None : id = str ( uuid . uuid4 ()) self . _id : str = id self . _parent_id = parent_id if kiara is None : from kiara import Kiara kiara = Kiara . instance () self . _kiara = kiara if isinstance ( module_config , ModuleTypeConfigSchema ): self . _config : KIARA_CONFIG = module_config # type: ignore elif module_config is None : self . _config = self . __class__ . _config_cls () elif isinstance ( module_config , typing . Mapping ): try : self . _config = self . __class__ . _config_cls ( ** module_config ) except ValidationError as ve : raise KiaraModuleConfigException ( f \"Error creating module ' { id } '. { ve } \" , self . __class__ , module_config , ve , ) else : raise TypeError ( f \"Invalid type for module config: { type ( module_config ) } \" ) self . _module_hash : typing . Optional [ int ] = None self . _info : typing . Optional [ KiaraModuleInstanceMetadata ] = None self . _input_schemas : typing . Mapping [ str , ValueSchema ] = None # type: ignore self . _constants : typing . Mapping [ str , ValueSchema ] = None # type: ignore self . _merged_input_schemas : typing . Mapping [ str , ValueSchema ] = None # type: ignore self . _output_schemas : typing . Mapping [ str , ValueSchema ] = None # type: ignore @property def id ( self ) -> str : \"\"\"The id of this module. This is only unique within a pipeline. \"\"\" return self . _id @property def parent_id ( self ) -> typing . Optional [ str ]: \"\"\"The id of the parent of this module (if part of a pipeline).\"\"\" return self . _parent_id @property def full_id ( self ) -> str : \"\"\"The full id for this module.\"\"\" if self . parent_id : return f \" { self . parent_id } . { self . id } \" else : return self . id @property def config ( self ) -> KIARA_CONFIG : \"\"\"Retrieve the configuration object for this module. Returns: the module-class-specific config object \"\"\" return self . _config def input_required ( self , input_name : str ): if input_name not in self . _input_schemas . keys (): raise Exception ( f \"No input ' { input_name } ' for module ' { self . id } '.\" ) if not self . _input_schemas [ input_name ] . is_required (): return False if input_name in self . constants . keys (): return False else : return True def get_config_value ( self , key : str ) -> typing . Any : \"\"\"Retrieve the value for a specific configuration option. Arguments: key: the config key Returns: the value for the provided key \"\"\" try : return self . config . get ( key ) except Exception : raise Exception ( f \"Error accessing config value ' { key } ' in module { self . __class__ . _module_type_id } .\" # type: ignore ) @abstractmethod def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\" @abstractmethod def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\" @property def input_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: \"\"\"The input schema for this module.\"\"\" if self . _input_schemas is None : self . _create_input_schemas () return self . _input_schemas # type: ignore @property def full_input_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: if self . _merged_input_schemas is not None : return self . _merged_input_schemas self . _merged_input_schemas = dict ( self . input_schemas ) self . _merged_input_schemas . update ( self . constants ) return self . _merged_input_schemas @property def constants ( self ) -> typing . Mapping [ str , ValueSchema ]: if self . _constants is None : self . _create_input_schemas () return self . _constants # type: ignore def _create_input_schemas ( self ) -> None : try : _input_schemas_data = self . create_input_schema () if not _input_schemas_data : raise Exception ( f \"Invalid module implementation for ' { self . __class__ . __name__ } ': empty input schema\" ) try : _input_schemas = create_schemas ( schema_config = _input_schemas_data , kiara = self . _kiara ) except Exception as e : raise Exception ( f \"Can't create input schemas for module { self . full_id } : { e } \" ) defaults = self . config . defaults constants = self . config . constants for k , v in defaults . items (): if k not in _input_schemas . keys (): raise Exception ( f \"Can't create inputs for module ' { self . _module_type_id } ', invalid default field name ' { k } '. Available field names: ' { ', ' . join ( _input_schemas . keys ()) } '\" # type: ignore ) for k , v in constants . items (): if k not in _input_schemas . keys (): raise Exception ( f \"Can't create inputs for module ' { self . _module_type_id } ', invalid constant field name ' { k } '. Available field names: ' { ', ' . join ( _input_schemas . keys ()) } '\" # type: ignore ) self . _input_schemas , self . _constants = overlay_constants_and_defaults ( _input_schemas , defaults = defaults , constants = constants ) except Exception as e : raise Exception ( f \"Can't create input schemas for module of type ' { self . __class__ . _module_type_id } ': { e } \" ) # type: ignore @property def output_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: \"\"\"The output schema for this module.\"\"\" if self . _output_schemas is not None : return self . _output_schemas try : _output_schema = self . create_output_schema () if not _output_schema : raise Exception ( f \"Invalid module implementation for ' { self . __class__ . __name__ } ': empty output schema\" ) try : self . _output_schemas = create_schemas ( schema_config = _output_schema , kiara = self . _kiara ) except Exception as e : raise Exception ( f \"Can't create output schemas for module { self . full_id } : { e } \" ) return self . _output_schemas except Exception as e : raise Exception ( f \"Can't create output schemas for module of type ' { self . __class__ . _module_type_id } ': { e } \" ) # type: ignore @property def input_names ( self ) -> typing . Iterable [ str ]: \"\"\"A list of input field names for this module.\"\"\" return self . input_schemas . keys () @property def output_names ( self ) -> typing . Iterable [ str ]: \"\"\"A list of output field names for this module.\"\"\" return self . output_schemas . keys () def process_step ( self , inputs : ValueSet , outputs : ValueSet , job_log : JobLog ) -> None : \"\"\"Kick off processing for a specific set of input/outputs. This method calls the implemented [process][kiara.module.KiaraModule.process] method of the inheriting class, as well as wrapping input/output-data related functionality. Arguments: inputs: the input value set outputs: the output value set \"\"\" signature = inspect . signature ( self . process ) # type: ignore if \"job_log\" not in signature . parameters . keys (): try : self . process ( inputs = inputs , outputs = outputs ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e else : try : self . process ( inputs = inputs , outputs = outputs , job_log = job_log ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e # @typing.overload # def process(self, inputs: ValueSet, outputs: ValueSet) -> None: # \"\"\"Abstract method to implement by child classes, should be a pure, idempotent function that uses the values from ``inputs``, and stores results in the provided ``outputs`` object. # # Arguments: # inputs: the input value set # outputs: the output value set # \"\"\" # pass # # @typing.overload # def process(self, inputs: ValueSet, outputs: ValueSet, job_log: typing.Optional[JobLog]=None) -> None: # pass # # def process(self, inputs, outputs, job_log=None) -> None: # pass def create_full_inputs ( self , ** inputs : typing . Any ) -> typing . Mapping [ str , Value ]: # TODO: find a generic way to do this kind of stuff def clean_value ( v : typing . Any ) -> typing . Any : if hasattr ( v , \"as_py\" ): return v . as_py () # type: ignore else : return v resolved_inputs : typing . Dict [ str , Value ] = {} for k , v in self . constants . items (): if k in inputs . keys (): raise Exception ( f \"Invalid input: value provided for constant ' { k } '\" ) inputs [ k ] = v for k , value in inputs . items (): value = clean_value ( value ) if not isinstance ( value , Value ): if ( k not in self . input_schemas . keys () and k not in self . constants . keys () ): raise Exception ( f \"Invalid input name ' { k } for module { self . _module_type_id } . Not part of the schema, allowed input names: { ', ' . join ( self . input_names ) } \" # type: ignore ) if k in self . input_schemas . keys (): schema = self . input_schemas [ k ] value = self . _kiara . data_registry . register_data ( value_data = value , value_schema = schema ) # value = Value( # value_data=value, # type: ignore # value_schema=schema, # is_constant=False, # registry=self._kiara.data_registry, # type: ignore # ) else : schema = self . constants [ k ] value = self . _kiara . data_registry . register_data ( value_data = SpecialValue . NOT_SET , value_schema = schema , ) # value = Value( # value_schema=schema, # is_constant=False, # kiara=self._kiara, # type: ignore # registry=self._kiara.data_registry, # type: ignore # ) resolved_inputs [ k ] = value return resolved_inputs def run ( self , _attach_lineage : bool = True , ** inputs : typing . Any ) -> ValueSet : \"\"\"Execute the module with the provided inputs directly. Arguments: inputs: a map of the input values (as described by the input schema Returns: a map of the output values (as described by the output schema) \"\"\" resolved_inputs = self . create_full_inputs ( ** inputs ) # TODO: introduce a 'temp' value set implementation and use that here input_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . full_input_schemas , read_only = True , initial_values = resolved_inputs , title = f \"module_inputs_ { self . id } \" , ) if not input_value_set . items_are_valid (): invalid_details = input_value_set . check_invalid () raise Exception ( f \"Can't process module ' { self . _module_type_name } ', input field(s) not valid: { ', ' . join ( invalid_details . keys ()) } \" # type: ignore ) output_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = False , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore default_value = SpecialValue . NOT_SET , ) self . process ( inputs = input_value_set , outputs = output_value_set ) # type: ignore result_outputs : typing . MutableMapping [ str , Value ] = {} if _attach_lineage : input_infos = { k : v . get_info () for k , v in resolved_inputs . items ()} for field_name , output in output_value_set . items (): value_lineage = ValueLineage . from_module_and_inputs ( module = self , output_name = field_name , inputs = input_infos ) # value_lineage = None output_val = self . _kiara . data_registry . register_data ( value_data = output , lineage = value_lineage ) result_outputs [ field_name ] = output_val else : result_outputs = output_value_set result_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = True , initial_values = result_outputs , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore ) return result_set # result = output_value_set.get_all_value_objects() # return output_value_set # return ValueSetImpl(items=result, read_only=True) @property def module_instance_doc ( self ) -> str : \"\"\"Return documentation for this instance of the module. If not overwritten, will return this class' method ``doc()``. \"\"\" # TODO: auto create instance doc? return self . get_type_metadata () . documentation . full_doc @property def module_instance_hash ( self ) -> int : \"\"\"Return this modules 'module_hash'. If two module instances ``module_instance_hash`` values are the same, it is guaranteed that their ``process`` methods will return the same output, given the same inputs (except if that processing step uses randomness). It can also be assumed that the two instances have the same input and output fields, with the same schemas. !!! note This implementation is preliminary, since it's not yet 100% clear to me how much that will be needed, and in which situations. Also, module versioning needs to be implemented before this can work reliably. Also, for now it is assumed that a module configuration is not changed once set, this also might change in the future Returns: this modules 'module_instance_hash' \"\"\" # TODO: if self . _module_hash is None : _d = { \"module_cls\" : f \" { self . __class__ . __module__ } . { self . __class__ . __name__ } \" , \"version\" : \"0.0.0\" , # TODO: implement module versioning, package name might also need to be included here \"config_hash\" : self . config . config_hash , } hashes = deepdiff . DeepHash ( _d ) self . _module_hash = hashes [ _d ] return self . _module_hash @property def info ( self ) -> KiaraModuleInstanceMetadata : \"\"\"Return an info wrapper class for this module.\"\"\" if self . _info is None : self . _info = KiaraModuleInstanceMetadata . from_module_obj ( self ) return self . _info def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return ( self . full_id , self . config ) == ( self . full_id , other . config ) def __hash__ ( self ): return hash (( self . __class__ , self . full_id , self . config )) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (id= { self . id } input_names= { list ( self . input_names ) } output_names= { list ( self . output_names ) } )\" def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : if not hasattr ( self . __class__ , \"_module_type_id\" ): raise Exception ( \"Invalid model class, no '_module_type_id' attribute added. This is a bug\" ) r_gro : typing . List [ typing . Any ] = [] md = self . info table = md . create_renderable () r_gro . append ( table ) yield Panel ( RenderGroup ( * r_gro ), box = box . ROUNDED , title_align = \"left\" , title = f \"Module: [b] { self . id } [/b]\" , )","title":"KiaraModule"},{"location":"reference/kiara/__init__/#kiara.module.StepInputs","text":"Wrapper class to hold a set of inputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Parameters: Name Type Description Default inputs ValueSet the input values of a pipeline step required Source code in kiara/module.py class StepInputs ( ValueSet ): \"\"\"Wrapper class to hold a set of inputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Arguments: inputs (ValueSet): the input values of a pipeline step \"\"\" def __init__ ( self , inputs : typing . Mapping [ str , Value ], title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _inputs : typing . Mapping [ str , Value ] = inputs super () . __init__ ( read_only = True , title = title , kiara = kiara ) def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _inputs . keys () def _get_value_obj ( self , field_name : str , ensure_metadata : typing . Union [ bool , typing . Iterable [ str ], str ] = False , ) -> Value : \"\"\"Retrieve the value object for the specified field.\"\"\" value = self . _inputs [ field_name ] if ensure_metadata : if isinstance ( ensure_metadata , bool ): value . get_metadata () elif isinstance ( ensure_metadata , str ): value . get_metadata ( ensure_metadata ) elif isinstance ( ensure_metadata , typing . Iterable ): value . get_metadata ( * ensure_metadata ) else : raise ValueError ( f \"Invalid type ' { type ( ensure_metadata ) } ' for 'ensure_metadata' argument.\" ) return value def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: raise Exception ( \"Inputs are read-only.\" )","title":"StepInputs"},{"location":"reference/kiara/__init__/#kiara.module.StepOutputs","text":"Wrapper class to hold a set of outputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Internally, this class stores two sets of its values: the 'actual', up-to-date values, and the referenced (original) ones that were used when creating an object of this class. It's not a good idea to keep both synced all the time, because that could potentially involve unnecessary data transfer and I/O. Also, in some cases a developer might want to avoid events that could be triggered by a changed value. Both value sets can be synced manually using the 'sync()' method. Parameters: Name Type Description Default outputs ValueSet the output values of a pipeline step required Source code in kiara/module.py class StepOutputs ( ValueSet ): \"\"\"Wrapper class to hold a set of outputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Internally, this class stores two sets of its values: the 'actual', up-to-date values, and the referenced (original) ones that were used when creating an object of this class. It's not a good idea to keep both synced all the time, because that could potentially involve unnecessary data transfer and I/O. Also, in some cases a developer might want to avoid events that could be triggered by a changed value. Both value sets can be synced manually using the 'sync()' method. Arguments: outputs (ValueSet): the output values of a pipeline step \"\"\" def __init__ ( self , outputs : ValueSet , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _outputs_staging : typing . Dict [ str , typing . Any ] = {} self . _outputs : ValueSet = outputs super () . __init__ ( read_only = False , title = title , kiara = kiara ) def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _outputs . get_all_field_names () def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: wrong = [] for key in values . keys (): if key not in self . _outputs . keys (): # type: ignore wrong . append ( key ) if wrong : av = \", \" . join ( self . _outputs . keys ()) # type: ignore raise Exception ( f \"Can't set output value(s), invalid key name(s): { ', ' . join ( wrong ) } . Available: { av } \" ) if metadata : raise NotImplementedError () if lineage : raise NotImplementedError () result = {} for output_name , value in values . items (): # value_obj = self._outputs.get_value_obj(output_name) if ( output_name not in self . _outputs_staging . keys () # type: ignore or value != self . _outputs_staging [ output_name ] # type: ignore ): self . _outputs_staging [ output_name ] = value # type: ignore result [ output_name ] = True else : result [ output_name ] = False return result def _get_value_obj ( self , output_name ): \"\"\"Retrieve the value object for the specified field.\"\"\" # self.sync() return self . _outputs . get_value_obj ( output_name ) def sync ( self , lineage : typing . Optional [ ValueLineage ] = None , ** metadata : MetadataModel ): \"\"\"Sync this value sets 'shadow' values with the ones a user would retrieve.\"\"\" self . _outputs . set_values ( lineage = lineage , metadata = metadata , ** self . _outputs_staging ) # type: ignore self . _outputs_staging . clear () # type: ignore","title":"StepOutputs"},{"location":"reference/kiara/__init__/#kiara.module_config","text":"Module-related configuration models for the Kiara package.","title":"module_config"},{"location":"reference/kiara/__init__/#kiara.module_config.ModuleConfig","text":"A class to hold the type and configuration for a module instance. Source code in kiara/module_config.py class ModuleConfig ( KiaraInfoModel ): \"\"\"A class to hold the type and configuration for a module instance.\"\"\" @classmethod def create_module_config ( cls , config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str , typing . Type [ \"KiaraModule\" ], \"PipelineConfig\" , ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleConfig\" : conf = parse_and_create_module_config ( config = config , module_config = module_config , kiara = kiara ) return conf @classmethod def create_renderable_from_module_instance_configs ( cls , configs : typing . Mapping [ str , \"ModuleConfig\" ], ** render_config : typing . Any , ): \"\"\"Convenience method to create a renderable for this module configuration, to be printed to terminal.\"\"\" table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" , style = \"i\" , no_wrap = True ) table . add_column ( \"Description\" ) if not render_config . get ( \"omit_config\" , False ): table . add_column ( \"Configuration\" ) for id , config in configs . items (): if config . doc : doc = config . doc . description else : doc = DEFAULT_NO_DESC_VALUE row : typing . List [ RenderableType ] = [ id , doc ] if not render_config . get ( \"omit_config\" , False ): conf = config . json ( exclude = { \"doc\" }, indent = 2 ) row . append ( Syntax ( conf , \"json\" , background_color = \"default\" )) table . add_row ( * row ) return table class Config : extra = Extra . forbid validate_all = True _module : typing . Optional [ \"KiaraModule\" ] = PrivateAttr ( default = None ) module_type : str = Field ( description = \"The module type.\" ) module_config : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"The configuration for the module.\" ) doc : DocumentationMetadataModel = Field ( description = \"Documentation for this operation.\" , default = None ) @validator ( \"doc\" , pre = True ) def create_doc ( cls , value ): return DocumentationMetadataModel . create ( value ) def create_module ( self , kiara : typing . Optional [ \"Kiara\" ] = None , module_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance from this configuration.\"\"\" if module_id and not isinstance ( module_id , str ): raise TypeError ( f \"Invalid type, module_id must be a string, not: { type ( module_id ) } \" ) if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if self . _module is None : self . _module = kiara . create_module ( id = module_id , module_type = self . module_type , module_config = self . module_config , ) return self . _module def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a renderable for this module configuration.\"\"\" conf = Syntax ( self . json ( exclude_none = True , indent = 2 ), \"json\" , background_color = \"default\" , ) return conf","title":"ModuleConfig"},{"location":"reference/kiara/__init__/#kiara.module_config.ModuleTypeConfigSchema","text":"Base class that describes the configuration a KiaraModule class accepts. This is stored in the _config_cls class attribute in each KiaraModule class. There are two config options every KiaraModule supports: constants , and defaults Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default values that override the schema defaults, and those can be overwritten by users. If both a constant and a default value is set for an input field, an error is thrown. Source code in kiara/module_config.py class ModuleTypeConfigSchema ( BaseModel ): \"\"\"Base class that describes the configuration a [``KiaraModule``][kiara.module.KiaraModule] class accepts. This is stored in the ``_config_cls`` class attribute in each ``KiaraModule`` class. There are two config options every ``KiaraModule`` supports: - ``constants``, and - ``defaults`` Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default values that override the schema defaults, and those can be overwritten by users. If both a constant and a default value is set for an input field, an error is thrown. \"\"\" @classmethod def requires_config ( cls , config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : if config : if config . get ( field_name , None ) is None : return True else : return True return False _config_hash : str = PrivateAttr ( default = None ) constants : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"Value constants for this module.\" ) defaults : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"Value defaults for this module.\" ) class Config : extra = Extra . forbid validate_assignment = True def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) @property def config_hash ( self ): if self . _config_hash is None : _d = self . dict () hashes = deepdiff . DeepHash ( _d ) self . _config_hash = hashes [ _d ] return self . _config_hash def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () def __hash__ ( self ): return hash ( self . config_hash ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : my_table = Table ( box = box . MINIMAL , show_header = False ) my_table . add_column ( \"Field name\" , style = \"i\" ) my_table . add_column ( \"Value\" ) for field in self . __fields__ : my_table . add_row ( field , getattr ( self , field )) yield my_table","title":"ModuleTypeConfigSchema"},{"location":"reference/kiara/__init__/#kiara.module_config.parse_and_create_module_config","text":"Utility method to create a ModuleConfig object from different input types. Parameters: Name Type Description Default config Union[ModuleConfig, Mapping, str, Type[KiaraModule], PipelineConfig] the 'main' configuation required module_config Optional[Mapping[str, Any]] if the 'main' configuration value is an (unconfigured) module type, this argument can contain the configuration for the module instance None kiara Optional[Kiara] the kiara context None Source code in kiara/module_config.py def parse_and_create_module_config ( config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str , typing . Type [ \"KiaraModule\" ], \"PipelineConfig\" , ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleConfig\" : \"\"\"Utility method to create a `ModuleConfig` object from different input types. Arguments: config: the 'main' configuation module_config: if the 'main' configuration value is an (unconfigured) module type, this argument can contain the configuration for the module instance kiara: the *kiara* context \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if isinstance ( config , type ): from kiara.module import KiaraModule if issubclass ( config , KiaraModule ): # this basically makes sure the class is augmented with the '_module_type_id` attribute if not hasattr ( config , \"_module_type_id\" ): match = False for mod_name in kiara . available_module_types : mod_cls = kiara . get_module_class ( mod_name ) if mod_cls == config : match = True break if not match : raise Exception ( f \"Class ' { config } ' not a valid kiara module.\" ) config = config . _module_type_id # type: ignore else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for module configuration.\" ) if isinstance ( config , typing . Mapping ): if module_config : raise NotImplementedError () module_instance_config : ModuleConfig = ModuleConfig ( ** config ) elif isinstance ( config , str ): if config == \"pipeline\" : if not module_config : raise Exception ( \"Can't create module from 'pipeline' module type without further configuration.\" ) elif isinstance ( module_config , typing . Mapping ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = module_config ) else : from kiara.pipeline.config import PipelineConfig if isinstance ( config , PipelineConfig ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = config . dict () ) else : raise Exception ( f \"Can't create module config, invalid type for 'module_config' parameter: { type ( module_config ) } \" ) elif config in kiara . available_module_types : if module_config : module_instance_config = ModuleConfig ( module_type = config , module_config = module_config ) else : module_instance_config = ModuleConfig ( module_type = config ) elif config in kiara . operation_mgmt . profiles . keys (): module_instance_config = kiara . operation_mgmt . profiles [ config ] elif os . path . isfile ( os . path . expanduser ( config )): path = os . path . expanduser ( config ) module_config_data = get_data_from_file ( path ) if module_config : raise NotImplementedError () if not isinstance ( module_config_data , typing . Mapping ): raise Exception ( f \"Invalid module/pipeline config, must be a mapping type: { module_config_data } \" ) if \"steps\" in module_config_data . keys (): module_type = \"pipeline\" module_instance_config = ModuleConfig ( module_type = module_type , module_config = module_config_data ) elif \"module_type\" in module_config_data . keys (): module_instance_config = ModuleConfig ( ** module_config_data ) else : raise Exception ( f \"Can't create module config from string ' { config } '. Value must be path to a file, or one of: { ', ' . join ( kiara . available_module_types ) } \" ) elif isinstance ( config , ModuleConfig ): module_instance_config = config if module_config : raise NotImplementedError () else : from kiara.pipeline.config import PipelineConfig if isinstance ( config , PipelineConfig ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = config . dict () ) else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for module configuration.\" ) return module_instance_config","title":"parse_and_create_module_config()"},{"location":"reference/kiara/__init__/#kiara.module_mgmt","text":"Base module for code that handles the import and management of KiaraModule sub-classes.","title":"module_mgmt"},{"location":"reference/kiara/__init__/#kiara.module_mgmt.merged","text":"","title":"merged"},{"location":"reference/kiara/__init__/#kiara.module_mgmt.pipelines","text":"","title":"pipelines"},{"location":"reference/kiara/__init__/#kiara.module_mgmt.python_classes","text":"","title":"python_classes"},{"location":"reference/kiara/__init__/#kiara.modules","text":"Base module under which the 'official' KiaraModule implementations live.","title":"modules"},{"location":"reference/kiara/__init__/#kiara.modules.metadata","text":"","title":"metadata"},{"location":"reference/kiara/__init__/#kiara.modules.pipelines","text":"Base module that holds PipelineModule classes that are auto-generated from pipeline descriptions in the pipelines folder.","title":"pipelines"},{"location":"reference/kiara/__init__/#kiara.modules.type_conversion","text":"","title":"type_conversion"},{"location":"reference/kiara/__init__/#kiara.operations","text":"","title":"operations"},{"location":"reference/kiara/__init__/#kiara.operations.Operation","text":"Source code in kiara/operations/__init__.py class Operation ( ModuleConfig ): @classmethod def create_operation ( cls , kiara : \"Kiara\" , operation_id : str , config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , ) -> \"Operation\" : _config = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) _config_dict = _config . dict () _config_dict [ \"id\" ] = operation_id op_config = cls ( ** _config_dict ) op_config . _kiara = kiara return op_config _kiara : typing . Optional [ \"Kiara\" ] = PrivateAttr ( default = None ) _module : typing . Optional [ \"KiaraModule\" ] id : str = Field ( description = \"The operation id.\" ) type_category : typing . Optional [ str ] = Field ( description = \"The operation category this belongs to.\" , default = None ) @property def kiara ( self ) -> \"Kiara\" : if self . _kiara is None : raise Exception ( \"Kiara context not set for operation.\" ) return self . _kiara @property def module ( self ) -> \"KiaraModule\" : if self . _module is None : self . _module = self . create_module ( kiara = self . kiara ) return self . _module @property def input_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . module . input_schemas @property def output_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . module . output_schemas @property def module_cls ( self ) -> typing . Type [ \"KiaraModule\" ]: return self . kiara . get_module_class ( self . module_type ) def run ( self , _attach_lineage : bool = True , ** inputs : typing . Any ) -> ValueSet : return self . module . run ( _attach_lineage = _attach_lineage , ** inputs ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code \"\"\" include_full_doc = config . get ( \"include_full_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . doc : if include_full_doc : table . add_row ( \"Documentation\" , self . doc . full_doc ) else : table . add_row ( \"Description\" , self . doc . description ) module_type_md = self . module . get_type_metadata () table . add_row ( \"Module type\" , self . module_type ) conf = Syntax ( json . dumps ( self . module_config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Module config\" , conf ) constants = self . module_config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . module . input_schemas , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . module . output_schemas , ) table . add_row ( \"Outputs\" , outputs_table ) m_md_o = module_type_md . origin . create_renderable () m_md_c = module_type_md . context . create_renderable () m_md = RenderGroup ( m_md_o , m_md_c ) table . add_row ( \"Module metadata\" , m_md ) if config . get ( \"include_src\" , False ): table . add_row ( \"Source code\" , module_type_md . process_src ) return table","title":"Operation"},{"location":"reference/kiara/__init__/#kiara.operations.calculate_hash","text":"","title":"calculate_hash"},{"location":"reference/kiara/__init__/#kiara.operations.create_value","text":"","title":"create_value"},{"location":"reference/kiara/__init__/#kiara.operations.data_export","text":"","title":"data_export"},{"location":"reference/kiara/__init__/#kiara.operations.data_import","text":"","title":"data_import"},{"location":"reference/kiara/__init__/#kiara.operations.extract_metadata","text":"","title":"extract_metadata"},{"location":"reference/kiara/__init__/#kiara.operations.merge_values","text":"","title":"merge_values"},{"location":"reference/kiara/__init__/#kiara.operations.pretty_print","text":"","title":"pretty_print"},{"location":"reference/kiara/__init__/#kiara.operations.sample","text":"","title":"sample"},{"location":"reference/kiara/__init__/#kiara.operations.serialize","text":"","title":"serialize"},{"location":"reference/kiara/__init__/#kiara.operations.store_value","text":"","title":"store_value"},{"location":"reference/kiara/__init__/#kiara.pipeline","text":"","title":"pipeline"},{"location":"reference/kiara/__init__/#kiara.pipeline.PipelineValueInfo","text":"Convenience wrapper to make the PipelineState json/dict export prettier. Source code in kiara/pipeline/__init__.py class PipelineValueInfo ( BaseModel ): \"\"\"Convenience wrapper to make the [PipelineState][kiara.pipeline.pipeline.PipelineState] json/dict export prettier.\"\"\" @classmethod def from_value_obj ( cls , value : Value , ensure_metadata : bool = False ): if ensure_metadata : value . get_metadata () return PipelineValueInfo ( id = value . id , value_schema = value . value_schema , is_valid = value . item_is_valid (), is_set = value . is_set , status = value . item_status (), # value_metadata=value.value_metadata, # last_update=value.last_update, # value_hash=value.value_hash, # is_streaming=value.is_streaming, metadata = value . metadata , ) class Config : extra = Extra . forbid allow_mutation = False id : str = Field ( description = \"A unique id for this value.\" ) is_valid : bool = Field ( description = \"Whether the value is set and valid.\" , default = False ) status : str = Field ( description = \"The value status string\" ) is_set : bool = Field ( description = \"Whether the value is set.\" ) value_schema : ValueSchema = Field ( description = \"The schema of this value.\" ) # is_constant: bool = Field( # description=\"Whether this value is a constant.\", default=False # ) # value_metadata: ValueMetadata = Field( # description=\"The metadata of the value itself (not the actual data).\" # ) # last_update: datetime = Field( # default=None, description=\"The time the last update to this value happened.\" # ) # value_hash: typing.Union[ValueHashMarker, int] = Field( # description=\"The hash of the current value.\" # ) # is_streaming: bool = Field( # default=False, # description=\"Whether the value is currently streamed into this object.\", # ) metadata : typing . Dict [ str , typing . Any ] = Field ( description = \"Metadata relating to the actual data (size, no. of rows, etc. -- depending on data type).\" , default_factory = dict , )","title":"PipelineValueInfo"},{"location":"reference/kiara/__init__/#kiara.pipeline.PipelineValuesInfo","text":"Convenience wrapper to make the PipelineState json/dict export prettier. This is basically just a simplified version of the ValueSet class that is using pydantic, in order to make it easy to export to json. Source code in kiara/pipeline/__init__.py class PipelineValuesInfo ( BaseModel ): \"\"\"Convenience wrapper to make the [PipelineState][kiara.pipeline.pipeline.PipelineState] json/dict export prettier. This is basically just a simplified version of the [ValueSet][kiara.data.values.ValueSet] class that is using pydantic, in order to make it easy to export to json. \"\"\" @classmethod def from_value_set ( cls , value_set : ValueSet , ensure_metadata : bool = False ): values : typing . Dict [ str , PipelineValueInfo ] = {} for k in value_set . get_all_field_names (): v = value_set . get_value_obj ( k , ensure_metadata = ensure_metadata ) values [ k ] = PipelineValueInfo . from_value_obj ( v , ensure_metadata = ensure_metadata ) return PipelineValuesInfo ( values = values ) values : typing . Dict [ str , PipelineValueInfo ] = Field ( description = \"Field names are keys, and the data as values.\" ) class Config : use_enum_values = True","title":"PipelineValuesInfo"},{"location":"reference/kiara/__init__/#kiara.pipeline.StepStatus","text":"Enum to describe the state of a workflow. Source code in kiara/pipeline/__init__.py class StepStatus ( Enum ): \"\"\"Enum to describe the state of a workflow.\"\"\" STALE = \"stale\" INPUTS_READY = \"inputs_ready\" RESULTS_INCOMING = \"processing\" RESULTS_READY = \"results_ready\"","title":"StepStatus"},{"location":"reference/kiara/__init__/#kiara.pipeline.StepValueAddress","text":"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure. Source code in kiara/pipeline/__init__.py class StepValueAddress ( BaseModel ): \"\"\"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.\"\"\" class Config : extra = Extra . forbid step_id : str = Field ( description = \"The id of a step within a pipeline.\" ) value_name : str = Field ( description = \"The name of the value (output name or pipeline input name).\" ) sub_value : typing . Optional [ typing . Dict [ str , typing . Any ]] = Field ( default = None , description = \"A reference to a subitem of a value (e.g. column, list item)\" , ) @property def alias ( self ): \"\"\"An alias string for this address (in the form ``[step_id].[value_name]``).\"\"\" return generate_step_alias ( self . step_id , self . value_name ) def __eq__ ( self , other ): if not isinstance ( other , StepValueAddress ): return False return ( self . step_id , self . value_name , self . sub_value ) == ( other . step_id , other . value_name , other . sub_value , ) def __hash__ ( self ): return hash (( self . step_id , self . value_name , self . sub_value )) def __repr__ ( self ): if self . sub_value : sub_value = f \" sub_value= { self . sub_value } \" else : sub_value = \"\" return f \"StepValueAddres(step_id= { self . step_id } , value_name= { self . value_name }{ sub_value } )\" def __str__ ( self ): return self . __repr__ ()","title":"StepValueAddress"},{"location":"reference/kiara/__init__/#kiara.pipeline.config","text":"","title":"config"},{"location":"reference/kiara/__init__/#kiara.pipeline.controller","text":"","title":"controller"},{"location":"reference/kiara/__init__/#kiara.pipeline.listeners","text":"","title":"listeners"},{"location":"reference/kiara/__init__/#kiara.pipeline.module","text":"","title":"module"},{"location":"reference/kiara/__init__/#kiara.pipeline.pipeline","text":"","title":"pipeline"},{"location":"reference/kiara/__init__/#kiara.pipeline.structure","text":"","title":"structure"},{"location":"reference/kiara/__init__/#kiara.pipeline.values","text":"","title":"values"},{"location":"reference/kiara/__init__/#kiara.processing","text":"","title":"processing"},{"location":"reference/kiara/__init__/#kiara.processing.Job","text":"Source code in kiara/processing/__init__.py class Job ( ProcessingInfo ): @classmethod def create_event_msg ( cls , job : \"Job\" ): topic = job . status . value [ 2 : - 2 ] payload = f \" { topic } { job . json () } \" return payload class Config : use_enum_values = True _exception : typing . Optional [ Exception ] = PrivateAttr ( default = None ) pipeline_id : str = Field ( description = \"The id of the pipeline this jobs runs for.\" ) pipeline_name : str = Field ( description = \"The name/type of the pipeline.\" ) step_id : str = Field ( description = \"The id of the step within the pipeline.\" ) inputs : PipelineValuesInfo = Field ( description = \"The input values.\" ) outputs : PipelineValuesInfo = Field ( description = \"The output values.\" ) status : JobStatus = Field ( description = \"The current status of the job.\" , default = JobStatus . CREATED , ) error : typing . Optional [ str ] = Field ( description = \"Potential error message.\" ) @property def exception ( self ) -> typing . Optional [ Exception ]: return self . _exception @property def runtime ( self ) -> typing . Optional [ float ]: if self . started is None or self . finished is None : return None runtime = self . finished - self . started return runtime . total_seconds () @validator ( \"status\" ) def _validate_status ( cls , v ): if isinstance ( v , int ): if v < 0 or v > 100 : raise ValueError ( \"Status must be a status string, or an integer between 0 and 100.\" ) return v","title":"Job"},{"location":"reference/kiara/__init__/#kiara.processing.JobLog","text":"Source code in kiara/processing/__init__.py class JobLog ( BaseModel ): log : typing . Dict [ int , LogMessage ] = Field ( description = \"The logs for this job.\" , default_factory = dict ) percent_finished : int = Field ( description = \"Describes how much of the job is finished. A negative number means the module does not support progress tracking.\" , default =- 1 , ) def add_log ( self , msg : str , log_level : int = logging . DEBUG ): _msg = LogMessage ( msg = msg , log_level = log_level ) self . log [ len ( self . log )] = _msg","title":"JobLog"},{"location":"reference/kiara/__init__/#kiara.processing.JobStatus","text":"An enumeration. Source code in kiara/processing/__init__.py class JobStatus ( Enum ): CREATED = \"__job_created__\" STARTED = \"__job_started__\" SUCCESS = \"__job_success__\" FAILED = \"__job_failed__\"","title":"JobStatus"},{"location":"reference/kiara/__init__/#kiara.processing.LogMessage","text":"Source code in kiara/processing/__init__.py class LogMessage ( BaseModel ): timestamp : datetime = Field ( description = \"The time the message was logged.\" , default_factory = datetime . now ) log_level : int = Field ( description = \"The log level.\" ) msg : str = Field ( description = \"The log message\" )","title":"LogMessage"},{"location":"reference/kiara/__init__/#kiara.processing.ProcessingInfo","text":"Source code in kiara/processing/__init__.py class ProcessingInfo ( MetadataModel ): id : str = Field ( description = \"The id of the job.\" ) module_type : str = Field ( description = \"The module type name.\" ) module_config : typing . Dict [ str , typing . Any ] = Field ( description = \"The module configuration.\" ) module_doc : DocumentationMetadataModel = Field ( description = \"Documentation for the module that runs the job.\" ) job_log : JobLog = Field ( description = \"Details about the job progress.\" , default_factory = JobLog ) submitted : datetime = Field ( description = \"When the job was submitted.\" , default_factory = datetime . now ) started : typing . Optional [ datetime ] = Field ( description = \"When the job was started.\" , default = None ) finished : typing . Optional [ datetime ] = Field ( description = \"When the job was finished.\" , default = None )","title":"ProcessingInfo"},{"location":"reference/kiara/__init__/#kiara.processing.parallel","text":"","title":"parallel"},{"location":"reference/kiara/__init__/#kiara.processing.processor","text":"","title":"processor"},{"location":"reference/kiara/__init__/#kiara.utils","text":"","title":"utils"},{"location":"reference/kiara/__init__/#kiara.utils.check_valid_field_names","text":"Check whether the provided field names are all valid. Returns: Type Description List[str] an iterable of strings with invalid field names Source code in kiara/utils/__init__.py def check_valid_field_names ( * field_names ) -> typing . List [ str ]: \"\"\"Check whether the provided field names are all valid. Returns: an iterable of strings with invalid field names \"\"\" return [ x for x in field_names if x in INVALID_VALUE_NAMES or x . startswith ( \"_\" )]","title":"check_valid_field_names()"},{"location":"reference/kiara/__init__/#kiara.utils.find_free_id","text":"Find a free var (or other name) based on a stem string, based on a list of provided existing names. Parameters: Name Type Description Default stem str the base string to use required current_ids Iterable[str] currently existing names required method str the method to create new names (allowed: 'count' -- for now) required method_args dict prototing_config for the creation method required Returns: Type Description str a free name Source code in kiara/utils/__init__.py def find_free_id ( stem : str , current_ids : typing . Iterable [ str ], sep = \"_\" , ) -> str : \"\"\"Find a free var (or other name) based on a stem string, based on a list of provided existing names. Args: stem (str): the base string to use current_ids (Iterable[str]): currently existing names method (str): the method to create new names (allowed: 'count' -- for now) method_args (dict): prototing_config for the creation method Returns: str: a free name \"\"\" start_count = 1 if stem not in current_ids : return stem i = start_count # new_name = None while True : new_name = f \" { stem }{ sep }{ i } \" if new_name in current_ids : i = i + 1 continue break return new_name","title":"find_free_id()"},{"location":"reference/kiara/__init__/#kiara.utils.get_auto_workflow_alias","text":"Return an id for a workflow obj of a provided module class. If 'use_incremental_ids' is set to True, a unique id is returned. Parameters: Name Type Description Default module_type str the name of the module type required use_incremental_ids bool whether to return a unique (incremental) id False Returns: Type Description str a module id Source code in kiara/utils/__init__.py def get_auto_workflow_alias ( module_type : str , use_incremental_ids : bool = False ) -> str : \"\"\"Return an id for a workflow obj of a provided module class. If 'use_incremental_ids' is set to True, a unique id is returned. Args: module_type (str): the name of the module type use_incremental_ids (bool): whether to return a unique (incremental) id Returns: str: a module id \"\"\" if not use_incremental_ids : return module_type nr = _AUTO_MODULE_ID . setdefault ( module_type , 0 ) _AUTO_MODULE_ID [ module_type ] = nr + 1 return f \" { module_type } _ { nr } \"","title":"get_auto_workflow_alias()"},{"location":"reference/kiara/__init__/#kiara.utils.class_loading","text":"","title":"class_loading"},{"location":"reference/kiara/__init__/#kiara.utils.concurrency","text":"","title":"concurrency"},{"location":"reference/kiara/__init__/#kiara.utils.modules","text":"","title":"modules"},{"location":"reference/kiara/__init__/#kiara.utils.output","text":"","title":"output"},{"location":"reference/kiara/__init__/#kiara.workflow","text":"","title":"workflow"},{"location":"reference/kiara/__init__/#kiara.workflow.kiara_workflow","text":"","title":"kiara_workflow"},{"location":"reference/kiara/config/","text":"KiaraConfig ( BaseSettings ) pydantic-model \u00b6 Source code in kiara/config.py class KiaraConfig ( BaseSettings ): class Config : extra = Extra . forbid env_file_encoding = \"utf-8\" env_prefix = \"kiara_\" @classmethod def customise_sources ( cls , init_settings , env_settings , file_secret_settings , ): return ( init_settings , env_settings , yaml_config_settings_source , file_secret_settings , ) module_managers : typing . Optional [ typing . List [ typing . Union [ PythonModuleManagerConfig , PipelineModuleManagerConfig ] ] ] = Field ( description = \"The module managers to use in this kiara instance.\" , default = None ) default_processor : typing . Optional [ typing . Union [ SynchronousProcessorConfig , ThreadPoolProcessorConfig ] ] = Field ( description = \"The configuration for the default processor to use.\" , default_factory = SynchronousProcessorConfig , ) data_store : str = Field ( description = \"The path to the local kiara data store.\" , default = KIARA_DATA_STORE_DIR , ) extra_pipeline_folders : typing . List [ str ] = Field ( description = \"Paths to local folders that contain kiara pipelines.\" , default_factory = list , ) ignore_errors : bool = Field ( description = \"If set, kiara will try to ignore most errors (that can be ignored).\" , default = False , ) @validator ( \"module_managers\" , pre = True ) def _validate_managers ( cls , v ): if v is None : return [] if isinstance ( v , typing . Mapping ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( item , ModuleManager ): result . append ( item ) else : assert isinstance ( item , typing . Mapping ) mm_type = item . get ( \"module_manager_type\" , None ) if not mm_type : raise ValueError ( f \"No module manager type provided in config: { item } \" ) if mm_type == \"python\" : item_config = PythonModuleManagerConfig ( ** item ) elif mm_type == \"pipeline\" : item_config = PipelineModuleManagerConfig ( ** item ) else : raise ValueError ( f \"Invalid module manager type: { mm_type } \" ) result . append ( item_config ) return result @validator ( \"default_processor\" , pre = True ) def _validate_default_processor ( cls , v ): if not v : return SynchronousProcessorConfig () if isinstance ( v , ( SynchronousProcessorConfig , ThreadPoolProcessorConfig )): return v if v == \"synchronous\" : return SynchronousProcessorConfig () if v == \"multi-threaded\" : return ThreadPoolProcessorConfig () if not isinstance ( v , typing . Mapping ): raise ValueError ( f \"Invalid type ' { type ( v ) } ' for default_processor config: { v } \" ) processor_type = v . get ( \"module_processor_type\" , None ) if not processor_type : raise ValueError ( \"No 'module_processor_type' provided: {config} \" ) if processor_type == \"synchronous\" : config = SynchronousProcessorConfig ( ** v ) elif processor_type == \"multi-threaded\" : config = ThreadPoolProcessorConfig () return config data_store : str pydantic-field \u00b6 The path to the local kiara data store. default_processor : Union [ kiara . processing . synchronous . SynchronousProcessorConfig , kiara . processing . parallel . ThreadPoolProcessorConfig ] pydantic-field \u00b6 The configuration for the default processor to use. extra_pipeline_folders : List [ str ] pydantic-field \u00b6 Paths to local folders that contain kiara pipelines. ignore_errors : bool pydantic-field \u00b6 If set, kiara will try to ignore most errors (that can be ignored). module_managers : List [ Union [ kiara . module_mgmt . python_classes . PythonModuleManagerConfig , kiara . module_mgmt . pipelines . PipelineModuleManagerConfig ]] pydantic-field \u00b6 The module managers to use in this kiara instance. yaml_config_settings_source ( settings ) \u00b6 A simple settings source that loads variables from a JSON file at the project's root. Here we happen to choose to use the env_file_encoding from Config when reading config.json Source code in kiara/config.py def yaml_config_settings_source ( settings : BaseSettings ) -> typing . Dict [ str , typing . Any ]: \"\"\" A simple settings source that loads variables from a JSON file at the project's root. Here we happen to choose to use the `env_file_encoding` from Config when reading `config.json` \"\"\" config_file = os . path . join ( kiara_app_dirs . user_config_dir , \"config.yaml\" ) if os . path . exists ( config_file ): data = get_data_from_file ( config_file ) return data else : return {}","title":"config"},{"location":"reference/kiara/config/#kiara.config.KiaraConfig","text":"Source code in kiara/config.py class KiaraConfig ( BaseSettings ): class Config : extra = Extra . forbid env_file_encoding = \"utf-8\" env_prefix = \"kiara_\" @classmethod def customise_sources ( cls , init_settings , env_settings , file_secret_settings , ): return ( init_settings , env_settings , yaml_config_settings_source , file_secret_settings , ) module_managers : typing . Optional [ typing . List [ typing . Union [ PythonModuleManagerConfig , PipelineModuleManagerConfig ] ] ] = Field ( description = \"The module managers to use in this kiara instance.\" , default = None ) default_processor : typing . Optional [ typing . Union [ SynchronousProcessorConfig , ThreadPoolProcessorConfig ] ] = Field ( description = \"The configuration for the default processor to use.\" , default_factory = SynchronousProcessorConfig , ) data_store : str = Field ( description = \"The path to the local kiara data store.\" , default = KIARA_DATA_STORE_DIR , ) extra_pipeline_folders : typing . List [ str ] = Field ( description = \"Paths to local folders that contain kiara pipelines.\" , default_factory = list , ) ignore_errors : bool = Field ( description = \"If set, kiara will try to ignore most errors (that can be ignored).\" , default = False , ) @validator ( \"module_managers\" , pre = True ) def _validate_managers ( cls , v ): if v is None : return [] if isinstance ( v , typing . Mapping ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( item , ModuleManager ): result . append ( item ) else : assert isinstance ( item , typing . Mapping ) mm_type = item . get ( \"module_manager_type\" , None ) if not mm_type : raise ValueError ( f \"No module manager type provided in config: { item } \" ) if mm_type == \"python\" : item_config = PythonModuleManagerConfig ( ** item ) elif mm_type == \"pipeline\" : item_config = PipelineModuleManagerConfig ( ** item ) else : raise ValueError ( f \"Invalid module manager type: { mm_type } \" ) result . append ( item_config ) return result @validator ( \"default_processor\" , pre = True ) def _validate_default_processor ( cls , v ): if not v : return SynchronousProcessorConfig () if isinstance ( v , ( SynchronousProcessorConfig , ThreadPoolProcessorConfig )): return v if v == \"synchronous\" : return SynchronousProcessorConfig () if v == \"multi-threaded\" : return ThreadPoolProcessorConfig () if not isinstance ( v , typing . Mapping ): raise ValueError ( f \"Invalid type ' { type ( v ) } ' for default_processor config: { v } \" ) processor_type = v . get ( \"module_processor_type\" , None ) if not processor_type : raise ValueError ( \"No 'module_processor_type' provided: {config} \" ) if processor_type == \"synchronous\" : config = SynchronousProcessorConfig ( ** v ) elif processor_type == \"multi-threaded\" : config = ThreadPoolProcessorConfig () return config","title":"KiaraConfig"},{"location":"reference/kiara/config/#kiara.config.KiaraConfig.data_store","text":"The path to the local kiara data store.","title":"data_store"},{"location":"reference/kiara/config/#kiara.config.KiaraConfig.default_processor","text":"The configuration for the default processor to use.","title":"default_processor"},{"location":"reference/kiara/config/#kiara.config.KiaraConfig.extra_pipeline_folders","text":"Paths to local folders that contain kiara pipelines.","title":"extra_pipeline_folders"},{"location":"reference/kiara/config/#kiara.config.KiaraConfig.ignore_errors","text":"If set, kiara will try to ignore most errors (that can be ignored).","title":"ignore_errors"},{"location":"reference/kiara/config/#kiara.config.KiaraConfig.module_managers","text":"The module managers to use in this kiara instance.","title":"module_managers"},{"location":"reference/kiara/config/#kiara.config.yaml_config_settings_source","text":"A simple settings source that loads variables from a JSON file at the project's root. Here we happen to choose to use the env_file_encoding from Config when reading config.json Source code in kiara/config.py def yaml_config_settings_source ( settings : BaseSettings ) -> typing . Dict [ str , typing . Any ]: \"\"\" A simple settings source that loads variables from a JSON file at the project's root. Here we happen to choose to use the `env_file_encoding` from Config when reading `config.json` \"\"\" config_file = os . path . join ( kiara_app_dirs . user_config_dir , \"config.yaml\" ) if os . path . exists ( config_file ): data = get_data_from_file ( config_file ) return data else : return {}","title":"yaml_config_settings_source()"},{"location":"reference/kiara/defaults/","text":"DEFAULT_EXCLUDE_DIRS \u00b6 List of directory names to exclude by default when walking a folder recursively. DEFAULT_EXCLUDE_FILES \u00b6 List of file names to exclude by default when reading folders. DEFAULT_PIPELINE_PARENT_ID \u00b6 Default parent id for pipeline objects that are not associated with a workflow. INVALID_VALUE_NAMES \u00b6 List of reserved names, inputs/outputs can't use those. KIARA_MODULE_BASE_FOLDER \u00b6 Marker to indicate the base folder for the kiara module. KIARA_RESOURCES_FOLDER \u00b6 Default resources folder for this package. MODULE_TYPE_KEY \u00b6 The key to specify the type of a module. MODULE_TYPE_NAME_KEY \u00b6 The string for the module type name in a module configuration dict. NO_HASH_MARKER \u00b6 Marker string to indicate no hash was calculated. NO_VALUE_ID_MARKER \u00b6 Marker string to indicate no value id exists. PIPELINE_PARENT_MARKER \u00b6 Marker string in the pipeline structure that indicates a parent pipeline element. STEP_ID_KEY \u00b6 The key to specify the step id. VALID_PIPELINE_FILE_EXTENSIONS \u00b6 File extensions a kiara pipeline/workflow file can have. SpecialValue ( Enum ) \u00b6 An enumeration. Source code in kiara/defaults.py class SpecialValue ( Enum ): NOT_SET = \"__not_set__\" NO_VALUE = \"__no_value__\" IGNORE = \"__ignore__\"","title":"defaults"},{"location":"reference/kiara/defaults/#kiara.defaults.DEFAULT_EXCLUDE_DIRS","text":"List of directory names to exclude by default when walking a folder recursively.","title":"DEFAULT_EXCLUDE_DIRS"},{"location":"reference/kiara/defaults/#kiara.defaults.DEFAULT_EXCLUDE_FILES","text":"List of file names to exclude by default when reading folders.","title":"DEFAULT_EXCLUDE_FILES"},{"location":"reference/kiara/defaults/#kiara.defaults.DEFAULT_PIPELINE_PARENT_ID","text":"Default parent id for pipeline objects that are not associated with a workflow.","title":"DEFAULT_PIPELINE_PARENT_ID"},{"location":"reference/kiara/defaults/#kiara.defaults.INVALID_VALUE_NAMES","text":"List of reserved names, inputs/outputs can't use those.","title":"INVALID_VALUE_NAMES"},{"location":"reference/kiara/defaults/#kiara.defaults.KIARA_MODULE_BASE_FOLDER","text":"Marker to indicate the base folder for the kiara module.","title":"KIARA_MODULE_BASE_FOLDER"},{"location":"reference/kiara/defaults/#kiara.defaults.KIARA_RESOURCES_FOLDER","text":"Default resources folder for this package.","title":"KIARA_RESOURCES_FOLDER"},{"location":"reference/kiara/defaults/#kiara.defaults.MODULE_TYPE_KEY","text":"The key to specify the type of a module.","title":"MODULE_TYPE_KEY"},{"location":"reference/kiara/defaults/#kiara.defaults.MODULE_TYPE_NAME_KEY","text":"The string for the module type name in a module configuration dict.","title":"MODULE_TYPE_NAME_KEY"},{"location":"reference/kiara/defaults/#kiara.defaults.NO_HASH_MARKER","text":"Marker string to indicate no hash was calculated.","title":"NO_HASH_MARKER"},{"location":"reference/kiara/defaults/#kiara.defaults.NO_VALUE_ID_MARKER","text":"Marker string to indicate no value id exists.","title":"NO_VALUE_ID_MARKER"},{"location":"reference/kiara/defaults/#kiara.defaults.PIPELINE_PARENT_MARKER","text":"Marker string in the pipeline structure that indicates a parent pipeline element.","title":"PIPELINE_PARENT_MARKER"},{"location":"reference/kiara/defaults/#kiara.defaults.STEP_ID_KEY","text":"The key to specify the step id.","title":"STEP_ID_KEY"},{"location":"reference/kiara/defaults/#kiara.defaults.VALID_PIPELINE_FILE_EXTENSIONS","text":"File extensions a kiara pipeline/workflow file can have.","title":"VALID_PIPELINE_FILE_EXTENSIONS"},{"location":"reference/kiara/defaults/#kiara.defaults.SpecialValue","text":"An enumeration. Source code in kiara/defaults.py class SpecialValue ( Enum ): NOT_SET = \"__not_set__\" NO_VALUE = \"__no_value__\" IGNORE = \"__ignore__\"","title":"SpecialValue"},{"location":"reference/kiara/events/","text":"PipelineInputEvent ( StepEvent ) pydantic-model \u00b6 Event that gets fired when one or several inputs for the pipeline itself have changed. Source code in kiara/events.py class PipelineInputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several inputs for the pipeline itself have changed.\"\"\" type : Literal [ \"pipeline_input\" ] = \"pipeline_input\" updated_pipeline_inputs : typing . List [ str ] = Field ( description = \"list of pipeline input names that where changed\" ) updated_pipeline_inputs : List [ str ] pydantic-field required \u00b6 list of pipeline input names that where changed PipelineOutputEvent ( StepEvent ) pydantic-model \u00b6 Event that gets fired when one or several outputs for the pipeline itself have changed. Source code in kiara/events.py class PipelineOutputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several outputs for the pipeline itself have changed.\"\"\" type : Literal [ \"pipeline_output\" ] = \"pipeline_output\" updated_pipeline_outputs : typing . List [ str ] = Field ( description = \"list of pipeline output names that where changed\" ) updated_pipeline_outputs : List [ str ] pydantic-field required \u00b6 list of pipeline output names that where changed StepEvent ( BaseModel ) pydantic-model \u00b6 Source code in kiara/events.py class StepEvent ( BaseModel ): class Config : allow_mutation = False pipeline_id : str def __repr__ ( self ): d = self . dict () d . pop ( \"pipeline_id\" ) return f \" { self . __class__ . __name__ } (pipeline_id= { self . pipeline_id } data= { d } \" def __str__ ( self ): return self . __repr__ () __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/events.py def __repr__ ( self ): d = self . dict () d . pop ( \"pipeline_id\" ) return f \" { self . __class__ . __name__ } (pipeline_id= { self . pipeline_id } data= { d } \" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/events.py def __str__ ( self ): return self . __repr__ () StepInputEvent ( StepEvent ) pydantic-model \u00b6 Event that gets fired when one or several inputs for steps within a pipeline have changed. Source code in kiara/events.py class StepInputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several inputs for steps within a pipeline have changed.\"\"\" type : Literal [ \"step_input\" ] = \"step_input\" updated_step_inputs : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"steps (keys) with updated inputs which need re-processing (value is list of updated input names)\" ) @property def newly_stale_steps ( self ) -> typing . List [ str ]: \"\"\"Convenience method to display the steps that have been rendered 'stale' by this event.\"\"\" return list ( self . updated_step_inputs . keys ()) newly_stale_steps : List [ str ] property readonly \u00b6 Convenience method to display the steps that have been rendered 'stale' by this event. updated_step_inputs : Dict [ str , List [ str ]] pydantic-field required \u00b6 steps (keys) with updated inputs which need re-processing (value is list of updated input names) StepOutputEvent ( StepEvent ) pydantic-model \u00b6 Event that gets fired when one or several outputs for steps within a pipeline have changed. Source code in kiara/events.py class StepOutputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several outputs for steps within a pipeline have changed.\"\"\" type : Literal [ \"step_output\" ] = \"step_output\" updated_step_outputs : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"steps (keys) that finished processing of one, several or all outputs (values are list of 'finished' output fields)\" ) updated_step_outputs : Dict [ str , List [ str ]] pydantic-field required \u00b6 steps (keys) that finished processing of one, several or all outputs (values are list of 'finished' output fields)","title":"events"},{"location":"reference/kiara/events/#kiara.events.PipelineInputEvent","text":"Event that gets fired when one or several inputs for the pipeline itself have changed. Source code in kiara/events.py class PipelineInputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several inputs for the pipeline itself have changed.\"\"\" type : Literal [ \"pipeline_input\" ] = \"pipeline_input\" updated_pipeline_inputs : typing . List [ str ] = Field ( description = \"list of pipeline input names that where changed\" )","title":"PipelineInputEvent"},{"location":"reference/kiara/events/#kiara.events.PipelineInputEvent.updated_pipeline_inputs","text":"list of pipeline input names that where changed","title":"updated_pipeline_inputs"},{"location":"reference/kiara/events/#kiara.events.PipelineOutputEvent","text":"Event that gets fired when one or several outputs for the pipeline itself have changed. Source code in kiara/events.py class PipelineOutputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several outputs for the pipeline itself have changed.\"\"\" type : Literal [ \"pipeline_output\" ] = \"pipeline_output\" updated_pipeline_outputs : typing . List [ str ] = Field ( description = \"list of pipeline output names that where changed\" )","title":"PipelineOutputEvent"},{"location":"reference/kiara/events/#kiara.events.PipelineOutputEvent.updated_pipeline_outputs","text":"list of pipeline output names that where changed","title":"updated_pipeline_outputs"},{"location":"reference/kiara/events/#kiara.events.StepEvent","text":"Source code in kiara/events.py class StepEvent ( BaseModel ): class Config : allow_mutation = False pipeline_id : str def __repr__ ( self ): d = self . dict () d . pop ( \"pipeline_id\" ) return f \" { self . __class__ . __name__ } (pipeline_id= { self . pipeline_id } data= { d } \" def __str__ ( self ): return self . __repr__ ()","title":"StepEvent"},{"location":"reference/kiara/events/#kiara.events.StepEvent.__repr__","text":"Return repr(self). Source code in kiara/events.py def __repr__ ( self ): d = self . dict () d . pop ( \"pipeline_id\" ) return f \" { self . __class__ . __name__ } (pipeline_id= { self . pipeline_id } data= { d } \"","title":"__repr__()"},{"location":"reference/kiara/events/#kiara.events.StepEvent.__str__","text":"Return str(self). Source code in kiara/events.py def __str__ ( self ): return self . __repr__ ()","title":"__str__()"},{"location":"reference/kiara/events/#kiara.events.StepInputEvent","text":"Event that gets fired when one or several inputs for steps within a pipeline have changed. Source code in kiara/events.py class StepInputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several inputs for steps within a pipeline have changed.\"\"\" type : Literal [ \"step_input\" ] = \"step_input\" updated_step_inputs : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"steps (keys) with updated inputs which need re-processing (value is list of updated input names)\" ) @property def newly_stale_steps ( self ) -> typing . List [ str ]: \"\"\"Convenience method to display the steps that have been rendered 'stale' by this event.\"\"\" return list ( self . updated_step_inputs . keys ())","title":"StepInputEvent"},{"location":"reference/kiara/events/#kiara.events.StepInputEvent.newly_stale_steps","text":"Convenience method to display the steps that have been rendered 'stale' by this event.","title":"newly_stale_steps"},{"location":"reference/kiara/events/#kiara.events.StepInputEvent.updated_step_inputs","text":"steps (keys) with updated inputs which need re-processing (value is list of updated input names)","title":"updated_step_inputs"},{"location":"reference/kiara/events/#kiara.events.StepOutputEvent","text":"Event that gets fired when one or several outputs for steps within a pipeline have changed. Source code in kiara/events.py class StepOutputEvent ( StepEvent ): \"\"\"Event that gets fired when one or several outputs for steps within a pipeline have changed.\"\"\" type : Literal [ \"step_output\" ] = \"step_output\" updated_step_outputs : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"steps (keys) that finished processing of one, several or all outputs (values are list of 'finished' output fields)\" )","title":"StepOutputEvent"},{"location":"reference/kiara/events/#kiara.events.StepOutputEvent.updated_step_outputs","text":"steps (keys) that finished processing of one, several or all outputs (values are list of 'finished' output fields)","title":"updated_step_outputs"},{"location":"reference/kiara/exceptions/","text":"","title":"exceptions"},{"location":"reference/kiara/kiara/","text":"Main module. Kiara \u00b6 The core context of a kiara session. The Kiara object holds all information related to the current environment the user does works in. This includes: available modules, operations & pipelines available value types available metadata schemas available data items available controller and processor types misc. configuration options It's possible to use kiara without ever manually touching the 'Kiara' class, by default all relevant classes and functions will use a default instance of this class (available via the Kiara.instance() method. The Kiara class is highly dependent on the Python environment it lives in, because it auto-discovers available sub-classes of its building blocks (modules, value types, etc.). So, you can't assume that, for example, a pipeline you create will work the same way (or at all) in a different environment. kiara will always be able to tell you all the details of this environment, though, and it will attach those details to things like data, so there is always a record of how something was created, and in which environment. Source code in kiara/kiara.py class Kiara ( object ): \"\"\"The core context of a kiara session. The `Kiara` object holds all information related to the current environment the user does works in. This includes: - available modules, operations & pipelines - available value types - available metadata schemas - available data items - available controller and processor types - misc. configuration options It's possible to use *kiara* without ever manually touching the 'Kiara' class, by default all relevant classes and functions will use a default instance of this class (available via the `Kiara.instance()` method. The Kiara class is highly dependent on the Python environment it lives in, because it auto-discovers available sub-classes of its building blocks (modules, value types, etc.). So, you can't assume that, for example, a pipeline you create will work the same way (or at all) in a different environment. *kiara* will always be able to tell you all the details of this environment, though, and it will attach those details to things like data, so there is always a record of how something was created, and in which environment. \"\"\" _instance = None @classmethod def instance ( cls ): \"\"\"The default *kiara* context. In most cases, it's recommended you create and manage your own, though.\"\"\" if cls . _instance is None : cls . _instance = Kiara () return cls . _instance def __init__ ( self , config : typing . Optional [ KiaraConfig ] = None ): if not config : config = KiaraConfig () self . _id : str = str ( uuid . uuid4 ()) self . _config : KiaraConfig = config # self._zmq_context: Context = Context.instance() self . _operation_mgmt = OperationMgmt ( kiara = self ) self . _metadata_mgmt = MetadataMgmt ( kiara = self ) self . _data_store = LocalDataStore ( kiara = self , base_path = config . data_store ) # self.start_zmq_device() # self.start_log_thread() self . _default_processor : ModuleProcessor = ModuleProcessor . from_config ( config . default_processor , kiara = self ) self . _type_mgmt_obj : TypeMgmt = TypeMgmt ( self ) self . _data_registry : InMemoryDataRegistry = InMemoryDataRegistry ( self ) self . _module_mgr : MergedModuleManager = MergedModuleManager ( config . module_managers , extra_pipeline_folders = self . _config . extra_pipeline_folders , ignore_errors = self . _config . ignore_errors , ) self . _template_mgmt : TemplateRenderingMgmt = TemplateRenderingMgmt . create ( kiara = self ) @property def config ( self ) -> KiaraConfig : \"\"\"The configuration of this *kiara* environment.\"\"\" return self . _config @property def default_processor ( self ) -> \"ModuleProcessor\" : \"\"\"The default module processor that will be used in this environment, unless otherwise specified.\"\"\" return self . _default_processor def start_zmq_device ( self ): pd = ThreadDevice ( zmq . QUEUE , zmq . SUB , zmq . PUB ) pd . bind_in ( \"inproc://kiara_in\" ) pd . bind_out ( \"inproc://kiara_out\" ) pd . setsockopt_in ( zmq . SUBSCRIBE , b \"\" ) pd . setsockopt_in ( zmq . IDENTITY , b \"SUB\" ) pd . setsockopt_out ( zmq . IDENTITY , b \"PUB\" ) pd . start () def start_log_thread ( self ): def log_messages (): socket = self . _zmq_context . socket ( zmq . SUB ) socket . setsockopt_string ( zmq . SUBSCRIBE , \"\" ) socket . connect ( \"inproc://kiara_out\" ) debug = is_debug () while True : message = socket . recv () topic , details = message . decode () . split ( \" \" , maxsplit = 1 ) try : job = Job . parse_raw ( details ) if debug : print ( f \" { topic } : { job . pipeline_name } . { job . step_id } \" ) else : log . debug ( f \" { topic } : { job . pipeline_name } . { job . step_id } \" ) except Exception as e : if debug : import traceback traceback . print_exception () else : log . debug ( e ) t = Thread ( target = log_messages , daemon = True ) t . start () def explain ( self , item : typing . Any ): explain ( item ) @property def type_mgmt ( self ) -> TypeMgmt : return self . _type_mgmt_obj @property def data_store ( self ) -> LocalDataStore : return self . _data_store @property def module_mgmt ( self ) -> MergedModuleManager : return self . _module_mgr @property def metadata_mgmt ( self ) -> MetadataMgmt : return self . _metadata_mgmt @property def value_types ( self ) -> typing . Mapping [ str , typing . Type [ ValueType ]]: return self . type_mgmt . value_types @property def value_type_names ( self ) -> typing . List [ str ]: return self . type_mgmt . value_type_names def determine_type ( self , data : typing . Any ) -> typing . Optional [ ValueType ]: raise NotImplementedError () # return self.type_mgmt.determine_type(data) def get_value_type_cls ( self , type_name : str ) -> typing . Type [ ValueType ]: return self . type_mgmt . get_value_type_cls ( type_name = type_name ) def get_value ( self , value_id : str ) -> Value : if not isinstance ( value_id , str ): raise TypeError ( f \"Invalid type ' { type ( value_id ) } ' for value id, must be a string.\" ) if value_id . startswith ( \"value:\" ): value_id = value_id [ 6 :] if not value_id : raise Exception ( \"No value id provided.\" ) try : value = self . data_registry . get_value_obj ( value_id ) if value is None : raise Exception ( f \"No value with id: { value_id } \" ) return value except Exception : pass try : value = self . data_store . get_value_obj ( value_id ) if value is None : raise Exception ( f \"No value with id: { value_id } \" ) return value except Exception : pass raise Exception ( f \"No value registered for id: { value_id } \" ) def add_module_manager ( self , module_manager : ModuleManager ): self . _module_mgr . add_module_manager ( module_manager ) self . _type_mgmt_obj . invalidate_types () @property def data_registry ( self ) -> DataRegistry : return self . _data_registry @property def operation_mgmt ( self ) -> OperationMgmt : return self . _operation_mgmt @property def template_mgmt ( self ) -> TemplateRenderingMgmt : return self . _template_mgmt def get_module_class ( self , module_type : str ) -> typing . Type [ \"KiaraModule\" ]: return self . _module_mgr . get_module_class ( module_type = module_type ) # def get_module_info(self, module_type: str) -> \"ModuleInfo\": # # if module_type not in self.available_module_types: # raise ValueError(f\"Module type '{module_type}' not available.\") # # if module_type in self.available_pipeline_module_types: # from kiara.pipeline.module import PipelineModuleInfo # # info = PipelineModuleInfo(module_type=module_type, _kiara=self) # type: ignore # return info # else: # from kiara.module import ModuleInfo # # info = ModuleInfo.from_module_cls(module_cls=module_type) # return info @property def available_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available modules\"\"\" return self . _module_mgr . available_module_types @property def available_non_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return self . _module_mgr . available_non_pipeline_module_types @property def available_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return self . _module_mgr . available_pipeline_module_types @property def available_operation_ids ( self ) -> typing . List [ str ]: return self . _operation_mgmt . operation_ids def is_pipeline_module ( self , module_type : str ): return self . _module_mgr . is_pipeline_module ( module_type = module_type ) def register_pipeline_description ( self , data : typing . Union [ Path , str , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , raise_exception : bool = False , ) -> typing . Optional [ str ]: return self . _module_mgr . register_pipeline_description ( data = data , module_type_name = module_type_name , namespace = namespace , raise_exception = raise_exception , ) def create_module ( self , module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Arguments: module_type: the module type- or operation-id module_config: the module instance configuration (must be empty in case of the module_type being an operation id id: the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline parent_id: a reference to the pipeline that contains this module (if applicable) Returns: The instantiated module object. \"\"\" if module_type == \"pipeline\" : from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = module_config , kiara = self ) return module elif ( module_type not in self . available_module_types and module_type in self . operation_mgmt . operation_ids ): op = self . operation_mgmt . get_operation ( module_type ) assert op is not None module_type = op . module_type op_config = op . module_config if not module_config : _module_config : typing . Optional [ typing . Mapping [ str , typing . Any ] ] = op_config else : _module_config = dict ( op_config ) _module_config . update ( module_config ) elif ( module_type not in self . available_module_types and isinstance ( module_type , str ) and os . path . isfile ( os . path . realpath ( os . path . expanduser ( module_type ))) ): if module_config : raise Exception ( \"Creating pipeline module from file with extra module_config not supported (yet).\" ) path = os . path . expanduser ( module_type ) pipeline_config_data = get_data_from_file ( path ) pipeline_config = PipelineConfig ( ** pipeline_config_data ) from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = pipeline_config , kiara = self ) return module else : _module_config = module_config return self . _module_mgr . create_module ( kiara = self , id = id , module_type = module_type , module_config = _module_config , parent_id = parent_id , ) def get_module_doc ( self , module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ): m = self . create_module ( module_type = module_type , module_config = module_config ) return m . module_instance_doc def get_operation ( self , operation_id : str ) -> Operation : op = self . operation_mgmt . profiles . get ( operation_id , None ) if op is None : raise Exception ( f \"No operation with id ' { operation_id } ' available.\" ) return op def run ( self , module_type : typing . Union [ str , Operation ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , inputs : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , output_name : typing . Optional [ str ] = None , resolve_result : bool = False , ) -> typing . Union [ ValueSet , Value , typing . Any ]: if isinstance ( module_type , str ): if module_type in self . available_module_types : module = self . create_module ( module_type = module_type , module_config = module_config ) elif module_type in self . operation_mgmt . profiles . keys (): if module_config : raise NotImplementedError () op = self . operation_mgmt . profiles [ module_type ] module = op . module elif os . path . isfile ( os . path . realpath ( os . path . expanduser ( module_type ))): path = os . path . expanduser ( module_type ) pipeline_config_data = get_data_from_file ( path ) # pipeline_config = PipelineConfig(**pipeline_config_data) module = self . create_module ( \"pipeline\" , module_config = pipeline_config_data ) else : raise Exception ( f \"Can't run operation: invalid module type ' { module_type } '\" ) elif isinstance ( module_type , Operation ): if module_config : raise NotImplementedError () module = module_type . module else : raise Exception ( f \"Invalid class for module_type: { type ( module_type ) } \" ) return self . run_module ( module = module , inputs = inputs , output_name = output_name , resolve_result = resolve_result , ) def run_module ( self , module : \"KiaraModule\" , inputs : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , output_name : typing . Optional [ str ] = None , resolve_result : bool = False , ): if inputs is None : inputs = {} result = module . run ( ** inputs ) if output_name is not None : v = result . get_value_obj ( output_name ) if resolve_result : return v . get_value_data () else : return v else : if resolve_result : return result . get_all_value_data () else : return result def create_pipeline ( self , config : typing . Union [ PipelineConfig , typing . Mapping [ str , typing . Any ], str ], controller : typing . Optional [ PipelineController ] = None , ) -> Pipeline : if isinstance ( config , typing . Mapping ): pipeline_config : PipelineConfig = PipelineConfig ( ** config ) elif isinstance ( config , str ): if config == \"pipeline\" : raise Exception ( \"Can't create pipeline from 'pipeline' module type without further configuration.\" ) # TODO: if already a pipeline, don't wrap # if config in self.available_pipeline_module_types: # pass if config in self . available_module_types : config_data = { \"steps\" : [ { \"module_type\" : config , \"step_id\" : create_valid_identifier ( config ), } ] } pipeline_config = PipelineConfig ( ** config_data ) elif os . path . isfile ( os . path . realpath ( os . path . expanduser ( config ))): path = os . path . expanduser ( config ) pipeline_config_data = get_data_from_file ( path ) pipeline_config = PipelineConfig ( ** pipeline_config_data ) else : raise Exception ( f \"Can't create pipeline config from string ' { config } '. Value must be path to a file, or one of: { ', ' . join ( self . available_module_types ) } \" ) elif isinstance ( config , PipelineConfig ): pipeline_config = config else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for pipeline configuration.\" ) pipeline = pipeline_config . create_pipeline ( controller = controller , kiara = self ) return pipeline def create_workflow_from_operation_config ( self , config : \"ModuleConfig\" , workflow_id : typing . Optional [ str ] = None , controller : typing . Optional [ PipelineController ] = None , ): if not workflow_id : workflow_id = get_auto_workflow_alias ( config . module_type , use_incremental_ids = True ) workflow = KiaraWorkflow ( workflow_id = workflow_id , config = config , controller = controller , kiara = self , ) return workflow def create_workflow ( self , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], workflow_id : typing . Optional [ str ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , controller : typing . Optional [ PipelineController ] = None , ) -> KiaraWorkflow : _config = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = self ) return self . create_workflow_from_operation_config ( config = _config , workflow_id = workflow_id , controller = controller ) def pretty_print ( self , value : Value , target_type : str = \"renderables\" , print_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ) -> typing . Any : pretty_print_ops : PrettyPrintOperationType = self . operation_mgmt . get_operations ( \"pretty_print\" ) # type: ignore return pretty_print_ops . pretty_print ( value = value , target_type = target_type , print_config = print_config ) available_module_types : List [ str ] property readonly \u00b6 Return the names of all available modules available_non_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. available_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. config : KiaraConfig property readonly \u00b6 The configuration of this kiara environment. default_processor : ModuleProcessor property readonly \u00b6 The default module processor that will be used in this environment, unless otherwise specified. create_module ( self , module_type , module_config = None , id = None , parent_id = None ) \u00b6 Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Parameters: Name Type Description Default module_type str the module type- or operation-id required module_config Optional[Mapping[str, Any]] the module instance configuration (must be empty in case of the module_type being an operation id None id Optional[str] the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline None parent_id Optional[str] a reference to the pipeline that contains this module (if applicable) None Returns: Type Description KiaraModule The instantiated module object. Source code in kiara/kiara.py def create_module ( self , module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Arguments: module_type: the module type- or operation-id module_config: the module instance configuration (must be empty in case of the module_type being an operation id id: the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline parent_id: a reference to the pipeline that contains this module (if applicable) Returns: The instantiated module object. \"\"\" if module_type == \"pipeline\" : from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = module_config , kiara = self ) return module elif ( module_type not in self . available_module_types and module_type in self . operation_mgmt . operation_ids ): op = self . operation_mgmt . get_operation ( module_type ) assert op is not None module_type = op . module_type op_config = op . module_config if not module_config : _module_config : typing . Optional [ typing . Mapping [ str , typing . Any ] ] = op_config else : _module_config = dict ( op_config ) _module_config . update ( module_config ) elif ( module_type not in self . available_module_types and isinstance ( module_type , str ) and os . path . isfile ( os . path . realpath ( os . path . expanduser ( module_type ))) ): if module_config : raise Exception ( \"Creating pipeline module from file with extra module_config not supported (yet).\" ) path = os . path . expanduser ( module_type ) pipeline_config_data = get_data_from_file ( path ) pipeline_config = PipelineConfig ( ** pipeline_config_data ) from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = pipeline_config , kiara = self ) return module else : _module_config = module_config return self . _module_mgr . create_module ( kiara = self , id = id , module_type = module_type , module_config = _module_config , parent_id = parent_id , ) instance () classmethod \u00b6 The default kiara context. In most cases, it's recommended you create and manage your own, though. Source code in kiara/kiara.py @classmethod def instance ( cls ): \"\"\"The default *kiara* context. In most cases, it's recommended you create and manage your own, though.\"\"\" if cls . _instance is None : cls . _instance = Kiara () return cls . _instance explain ( item ) \u00b6 Pretty print information about an item on the terminal. Source code in kiara/kiara.py def explain ( item : typing . Any ): \"\"\"Pretty print information about an item on the terminal.\"\"\" if isinstance ( item , type ): from kiara.module import KiaraModule if issubclass ( item , KiaraModule ): item = KiaraModuleTypeMetadata . from_module_class ( module_cls = item ) elif isinstance ( item , Value ): item . get_metadata () console = get_console () console . print ( item )","title":"kiara"},{"location":"reference/kiara/kiara/#kiara.kiara.Kiara","text":"The core context of a kiara session. The Kiara object holds all information related to the current environment the user does works in. This includes: available modules, operations & pipelines available value types available metadata schemas available data items available controller and processor types misc. configuration options It's possible to use kiara without ever manually touching the 'Kiara' class, by default all relevant classes and functions will use a default instance of this class (available via the Kiara.instance() method. The Kiara class is highly dependent on the Python environment it lives in, because it auto-discovers available sub-classes of its building blocks (modules, value types, etc.). So, you can't assume that, for example, a pipeline you create will work the same way (or at all) in a different environment. kiara will always be able to tell you all the details of this environment, though, and it will attach those details to things like data, so there is always a record of how something was created, and in which environment. Source code in kiara/kiara.py class Kiara ( object ): \"\"\"The core context of a kiara session. The `Kiara` object holds all information related to the current environment the user does works in. This includes: - available modules, operations & pipelines - available value types - available metadata schemas - available data items - available controller and processor types - misc. configuration options It's possible to use *kiara* without ever manually touching the 'Kiara' class, by default all relevant classes and functions will use a default instance of this class (available via the `Kiara.instance()` method. The Kiara class is highly dependent on the Python environment it lives in, because it auto-discovers available sub-classes of its building blocks (modules, value types, etc.). So, you can't assume that, for example, a pipeline you create will work the same way (or at all) in a different environment. *kiara* will always be able to tell you all the details of this environment, though, and it will attach those details to things like data, so there is always a record of how something was created, and in which environment. \"\"\" _instance = None @classmethod def instance ( cls ): \"\"\"The default *kiara* context. In most cases, it's recommended you create and manage your own, though.\"\"\" if cls . _instance is None : cls . _instance = Kiara () return cls . _instance def __init__ ( self , config : typing . Optional [ KiaraConfig ] = None ): if not config : config = KiaraConfig () self . _id : str = str ( uuid . uuid4 ()) self . _config : KiaraConfig = config # self._zmq_context: Context = Context.instance() self . _operation_mgmt = OperationMgmt ( kiara = self ) self . _metadata_mgmt = MetadataMgmt ( kiara = self ) self . _data_store = LocalDataStore ( kiara = self , base_path = config . data_store ) # self.start_zmq_device() # self.start_log_thread() self . _default_processor : ModuleProcessor = ModuleProcessor . from_config ( config . default_processor , kiara = self ) self . _type_mgmt_obj : TypeMgmt = TypeMgmt ( self ) self . _data_registry : InMemoryDataRegistry = InMemoryDataRegistry ( self ) self . _module_mgr : MergedModuleManager = MergedModuleManager ( config . module_managers , extra_pipeline_folders = self . _config . extra_pipeline_folders , ignore_errors = self . _config . ignore_errors , ) self . _template_mgmt : TemplateRenderingMgmt = TemplateRenderingMgmt . create ( kiara = self ) @property def config ( self ) -> KiaraConfig : \"\"\"The configuration of this *kiara* environment.\"\"\" return self . _config @property def default_processor ( self ) -> \"ModuleProcessor\" : \"\"\"The default module processor that will be used in this environment, unless otherwise specified.\"\"\" return self . _default_processor def start_zmq_device ( self ): pd = ThreadDevice ( zmq . QUEUE , zmq . SUB , zmq . PUB ) pd . bind_in ( \"inproc://kiara_in\" ) pd . bind_out ( \"inproc://kiara_out\" ) pd . setsockopt_in ( zmq . SUBSCRIBE , b \"\" ) pd . setsockopt_in ( zmq . IDENTITY , b \"SUB\" ) pd . setsockopt_out ( zmq . IDENTITY , b \"PUB\" ) pd . start () def start_log_thread ( self ): def log_messages (): socket = self . _zmq_context . socket ( zmq . SUB ) socket . setsockopt_string ( zmq . SUBSCRIBE , \"\" ) socket . connect ( \"inproc://kiara_out\" ) debug = is_debug () while True : message = socket . recv () topic , details = message . decode () . split ( \" \" , maxsplit = 1 ) try : job = Job . parse_raw ( details ) if debug : print ( f \" { topic } : { job . pipeline_name } . { job . step_id } \" ) else : log . debug ( f \" { topic } : { job . pipeline_name } . { job . step_id } \" ) except Exception as e : if debug : import traceback traceback . print_exception () else : log . debug ( e ) t = Thread ( target = log_messages , daemon = True ) t . start () def explain ( self , item : typing . Any ): explain ( item ) @property def type_mgmt ( self ) -> TypeMgmt : return self . _type_mgmt_obj @property def data_store ( self ) -> LocalDataStore : return self . _data_store @property def module_mgmt ( self ) -> MergedModuleManager : return self . _module_mgr @property def metadata_mgmt ( self ) -> MetadataMgmt : return self . _metadata_mgmt @property def value_types ( self ) -> typing . Mapping [ str , typing . Type [ ValueType ]]: return self . type_mgmt . value_types @property def value_type_names ( self ) -> typing . List [ str ]: return self . type_mgmt . value_type_names def determine_type ( self , data : typing . Any ) -> typing . Optional [ ValueType ]: raise NotImplementedError () # return self.type_mgmt.determine_type(data) def get_value_type_cls ( self , type_name : str ) -> typing . Type [ ValueType ]: return self . type_mgmt . get_value_type_cls ( type_name = type_name ) def get_value ( self , value_id : str ) -> Value : if not isinstance ( value_id , str ): raise TypeError ( f \"Invalid type ' { type ( value_id ) } ' for value id, must be a string.\" ) if value_id . startswith ( \"value:\" ): value_id = value_id [ 6 :] if not value_id : raise Exception ( \"No value id provided.\" ) try : value = self . data_registry . get_value_obj ( value_id ) if value is None : raise Exception ( f \"No value with id: { value_id } \" ) return value except Exception : pass try : value = self . data_store . get_value_obj ( value_id ) if value is None : raise Exception ( f \"No value with id: { value_id } \" ) return value except Exception : pass raise Exception ( f \"No value registered for id: { value_id } \" ) def add_module_manager ( self , module_manager : ModuleManager ): self . _module_mgr . add_module_manager ( module_manager ) self . _type_mgmt_obj . invalidate_types () @property def data_registry ( self ) -> DataRegistry : return self . _data_registry @property def operation_mgmt ( self ) -> OperationMgmt : return self . _operation_mgmt @property def template_mgmt ( self ) -> TemplateRenderingMgmt : return self . _template_mgmt def get_module_class ( self , module_type : str ) -> typing . Type [ \"KiaraModule\" ]: return self . _module_mgr . get_module_class ( module_type = module_type ) # def get_module_info(self, module_type: str) -> \"ModuleInfo\": # # if module_type not in self.available_module_types: # raise ValueError(f\"Module type '{module_type}' not available.\") # # if module_type in self.available_pipeline_module_types: # from kiara.pipeline.module import PipelineModuleInfo # # info = PipelineModuleInfo(module_type=module_type, _kiara=self) # type: ignore # return info # else: # from kiara.module import ModuleInfo # # info = ModuleInfo.from_module_cls(module_cls=module_type) # return info @property def available_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available modules\"\"\" return self . _module_mgr . available_module_types @property def available_non_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return self . _module_mgr . available_non_pipeline_module_types @property def available_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return self . _module_mgr . available_pipeline_module_types @property def available_operation_ids ( self ) -> typing . List [ str ]: return self . _operation_mgmt . operation_ids def is_pipeline_module ( self , module_type : str ): return self . _module_mgr . is_pipeline_module ( module_type = module_type ) def register_pipeline_description ( self , data : typing . Union [ Path , str , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , raise_exception : bool = False , ) -> typing . Optional [ str ]: return self . _module_mgr . register_pipeline_description ( data = data , module_type_name = module_type_name , namespace = namespace , raise_exception = raise_exception , ) def create_module ( self , module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Arguments: module_type: the module type- or operation-id module_config: the module instance configuration (must be empty in case of the module_type being an operation id id: the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline parent_id: a reference to the pipeline that contains this module (if applicable) Returns: The instantiated module object. \"\"\" if module_type == \"pipeline\" : from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = module_config , kiara = self ) return module elif ( module_type not in self . available_module_types and module_type in self . operation_mgmt . operation_ids ): op = self . operation_mgmt . get_operation ( module_type ) assert op is not None module_type = op . module_type op_config = op . module_config if not module_config : _module_config : typing . Optional [ typing . Mapping [ str , typing . Any ] ] = op_config else : _module_config = dict ( op_config ) _module_config . update ( module_config ) elif ( module_type not in self . available_module_types and isinstance ( module_type , str ) and os . path . isfile ( os . path . realpath ( os . path . expanduser ( module_type ))) ): if module_config : raise Exception ( \"Creating pipeline module from file with extra module_config not supported (yet).\" ) path = os . path . expanduser ( module_type ) pipeline_config_data = get_data_from_file ( path ) pipeline_config = PipelineConfig ( ** pipeline_config_data ) from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = pipeline_config , kiara = self ) return module else : _module_config = module_config return self . _module_mgr . create_module ( kiara = self , id = id , module_type = module_type , module_config = _module_config , parent_id = parent_id , ) def get_module_doc ( self , module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ): m = self . create_module ( module_type = module_type , module_config = module_config ) return m . module_instance_doc def get_operation ( self , operation_id : str ) -> Operation : op = self . operation_mgmt . profiles . get ( operation_id , None ) if op is None : raise Exception ( f \"No operation with id ' { operation_id } ' available.\" ) return op def run ( self , module_type : typing . Union [ str , Operation ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , inputs : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , output_name : typing . Optional [ str ] = None , resolve_result : bool = False , ) -> typing . Union [ ValueSet , Value , typing . Any ]: if isinstance ( module_type , str ): if module_type in self . available_module_types : module = self . create_module ( module_type = module_type , module_config = module_config ) elif module_type in self . operation_mgmt . profiles . keys (): if module_config : raise NotImplementedError () op = self . operation_mgmt . profiles [ module_type ] module = op . module elif os . path . isfile ( os . path . realpath ( os . path . expanduser ( module_type ))): path = os . path . expanduser ( module_type ) pipeline_config_data = get_data_from_file ( path ) # pipeline_config = PipelineConfig(**pipeline_config_data) module = self . create_module ( \"pipeline\" , module_config = pipeline_config_data ) else : raise Exception ( f \"Can't run operation: invalid module type ' { module_type } '\" ) elif isinstance ( module_type , Operation ): if module_config : raise NotImplementedError () module = module_type . module else : raise Exception ( f \"Invalid class for module_type: { type ( module_type ) } \" ) return self . run_module ( module = module , inputs = inputs , output_name = output_name , resolve_result = resolve_result , ) def run_module ( self , module : \"KiaraModule\" , inputs : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , output_name : typing . Optional [ str ] = None , resolve_result : bool = False , ): if inputs is None : inputs = {} result = module . run ( ** inputs ) if output_name is not None : v = result . get_value_obj ( output_name ) if resolve_result : return v . get_value_data () else : return v else : if resolve_result : return result . get_all_value_data () else : return result def create_pipeline ( self , config : typing . Union [ PipelineConfig , typing . Mapping [ str , typing . Any ], str ], controller : typing . Optional [ PipelineController ] = None , ) -> Pipeline : if isinstance ( config , typing . Mapping ): pipeline_config : PipelineConfig = PipelineConfig ( ** config ) elif isinstance ( config , str ): if config == \"pipeline\" : raise Exception ( \"Can't create pipeline from 'pipeline' module type without further configuration.\" ) # TODO: if already a pipeline, don't wrap # if config in self.available_pipeline_module_types: # pass if config in self . available_module_types : config_data = { \"steps\" : [ { \"module_type\" : config , \"step_id\" : create_valid_identifier ( config ), } ] } pipeline_config = PipelineConfig ( ** config_data ) elif os . path . isfile ( os . path . realpath ( os . path . expanduser ( config ))): path = os . path . expanduser ( config ) pipeline_config_data = get_data_from_file ( path ) pipeline_config = PipelineConfig ( ** pipeline_config_data ) else : raise Exception ( f \"Can't create pipeline config from string ' { config } '. Value must be path to a file, or one of: { ', ' . join ( self . available_module_types ) } \" ) elif isinstance ( config , PipelineConfig ): pipeline_config = config else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for pipeline configuration.\" ) pipeline = pipeline_config . create_pipeline ( controller = controller , kiara = self ) return pipeline def create_workflow_from_operation_config ( self , config : \"ModuleConfig\" , workflow_id : typing . Optional [ str ] = None , controller : typing . Optional [ PipelineController ] = None , ): if not workflow_id : workflow_id = get_auto_workflow_alias ( config . module_type , use_incremental_ids = True ) workflow = KiaraWorkflow ( workflow_id = workflow_id , config = config , controller = controller , kiara = self , ) return workflow def create_workflow ( self , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], workflow_id : typing . Optional [ str ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , controller : typing . Optional [ PipelineController ] = None , ) -> KiaraWorkflow : _config = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = self ) return self . create_workflow_from_operation_config ( config = _config , workflow_id = workflow_id , controller = controller ) def pretty_print ( self , value : Value , target_type : str = \"renderables\" , print_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ) -> typing . Any : pretty_print_ops : PrettyPrintOperationType = self . operation_mgmt . get_operations ( \"pretty_print\" ) # type: ignore return pretty_print_ops . pretty_print ( value = value , target_type = target_type , print_config = print_config )","title":"Kiara"},{"location":"reference/kiara/kiara/#kiara.kiara.Kiara.available_module_types","text":"Return the names of all available modules","title":"available_module_types"},{"location":"reference/kiara/kiara/#kiara.kiara.Kiara.available_non_pipeline_module_types","text":"Return the names of all available pipeline-type modules.","title":"available_non_pipeline_module_types"},{"location":"reference/kiara/kiara/#kiara.kiara.Kiara.available_pipeline_module_types","text":"Return the names of all available pipeline-type modules.","title":"available_pipeline_module_types"},{"location":"reference/kiara/kiara/#kiara.kiara.Kiara.config","text":"The configuration of this kiara environment.","title":"config"},{"location":"reference/kiara/kiara/#kiara.kiara.Kiara.default_processor","text":"The default module processor that will be used in this environment, unless otherwise specified.","title":"default_processor"},{"location":"reference/kiara/kiara/#kiara.kiara.Kiara.create_module","text":"Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Parameters: Name Type Description Default module_type str the module type- or operation-id required module_config Optional[Mapping[str, Any]] the module instance configuration (must be empty in case of the module_type being an operation id None id Optional[str] the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline None parent_id Optional[str] a reference to the pipeline that contains this module (if applicable) None Returns: Type Description KiaraModule The instantiated module object. Source code in kiara/kiara.py def create_module ( self , module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance. The 'module_type' argument can be a module type id, or an operation id. In case of the latter, the 'real' module_type and module_config will be resolved via the operation management. Arguments: module_type: the module type- or operation-id module_config: the module instance configuration (must be empty in case of the module_type being an operation id id: the id of this module, only relevant if the module is part of a pipeline, in which case it must be unique within the pipeline parent_id: a reference to the pipeline that contains this module (if applicable) Returns: The instantiated module object. \"\"\" if module_type == \"pipeline\" : from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = module_config , kiara = self ) return module elif ( module_type not in self . available_module_types and module_type in self . operation_mgmt . operation_ids ): op = self . operation_mgmt . get_operation ( module_type ) assert op is not None module_type = op . module_type op_config = op . module_config if not module_config : _module_config : typing . Optional [ typing . Mapping [ str , typing . Any ] ] = op_config else : _module_config = dict ( op_config ) _module_config . update ( module_config ) elif ( module_type not in self . available_module_types and isinstance ( module_type , str ) and os . path . isfile ( os . path . realpath ( os . path . expanduser ( module_type ))) ): if module_config : raise Exception ( \"Creating pipeline module from file with extra module_config not supported (yet).\" ) path = os . path . expanduser ( module_type ) pipeline_config_data = get_data_from_file ( path ) pipeline_config = PipelineConfig ( ** pipeline_config_data ) from kiara import PipelineModule module = PipelineModule ( id = id , parent_id = parent_id , module_config = pipeline_config , kiara = self ) return module else : _module_config = module_config return self . _module_mgr . create_module ( kiara = self , id = id , module_type = module_type , module_config = _module_config , parent_id = parent_id , )","title":"create_module()"},{"location":"reference/kiara/kiara/#kiara.kiara.Kiara.instance","text":"The default kiara context. In most cases, it's recommended you create and manage your own, though. Source code in kiara/kiara.py @classmethod def instance ( cls ): \"\"\"The default *kiara* context. In most cases, it's recommended you create and manage your own, though.\"\"\" if cls . _instance is None : cls . _instance = Kiara () return cls . _instance","title":"instance()"},{"location":"reference/kiara/kiara/#kiara.kiara.explain","text":"Pretty print information about an item on the terminal. Source code in kiara/kiara.py def explain ( item : typing . Any ): \"\"\"Pretty print information about an item on the terminal.\"\"\" if isinstance ( item , type ): from kiara.module import KiaraModule if issubclass ( item , KiaraModule ): item = KiaraModuleTypeMetadata . from_module_class ( module_cls = item ) elif isinstance ( item , Value ): item . get_metadata () console = get_console () console . print ( item )","title":"explain()"},{"location":"reference/kiara/module/","text":"KiaraModule ( Generic , ABC ) \u00b6 The base class that every custom module in Kiara needs to inherit from. The core of every KiaraModule is a process method, which should be a 'pure', idempotent function that creates one or several output values from the given input(s), and its purpose is to transfor a set of inputs into a set of outputs. Every module can be configured. The module configuration schema can differ, but every one such configuration needs to subclass the ModuleTypeConfigSchema class and set as the value to the _config_cls attribute of the module class. This is useful, because it allows for some modules to serve a much larger variety of use-cases than non-configurable modules would be, which would mean more code duplication because of very simlilar, but slightly different module types. Each module class (type) has a unique -- within a kiara context -- module type id which can be accessed via the _module_type_id class attribute. Examples: A simple example would be an 'addition' module, with a and b configured as inputs, and z as the output field name. An implementing class would look something like this: TODO Parameters: Name Type Description Default id str the id for this module (needs to be unique within a pipeline) None parent_id Optional[str] the id of the parent, in case this module is part of a pipeline None module_config Any the configuation for this module None metadata Mapping[str, Any] metadata for this module (not implemented yet) required Source code in kiara/module.py class KiaraModule ( typing . Generic [ KIARA_CONFIG ], abc . ABC ): \"\"\"The base class that every custom module in *Kiara* needs to inherit from. The core of every ``KiaraModule`` is a ``process`` method, which should be a 'pure', idempotent function that creates one or several output values from the given input(s), and its purpose is to transfor a set of inputs into a set of outputs. Every module can be configured. The module configuration schema can differ, but every one such configuration needs to subclass the [ModuleTypeConfigSchema][kiara.module_config.ModuleTypeConfigSchema] class and set as the value to the ``_config_cls`` attribute of the module class. This is useful, because it allows for some modules to serve a much larger variety of use-cases than non-configurable modules would be, which would mean more code duplication because of very simlilar, but slightly different module types. Each module class (type) has a unique -- within a *kiara* context -- module type id which can be accessed via the ``_module_type_id`` class attribute. Examples: A simple example would be an 'addition' module, with ``a`` and ``b`` configured as inputs, and ``z`` as the output field name. An implementing class would look something like this: TODO Arguments: id (str): the id for this module (needs to be unique within a pipeline) parent_id (typing.Optional[str]): the id of the parent, in case this module is part of a pipeline module_config (typing.Any): the configuation for this module metadata (typing.Mapping[str, typing.Any]): metadata for this module (not implemented yet) \"\"\" # TODO: not quite sure about this generic type here, mypy doesn't seem to like it _config_cls : typing . Type [ KIARA_CONFIG ] = ModuleTypeConfigSchema # type: ignore @classmethod def create_instance ( cls , module_type : typing . Optional [ str ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"KiaraModule\" : \"\"\"Create an instance of a *kiara* module. This class method is overloaded in a way that you can either provide the `module_type` argument, in which case the relevant sub-class will be queried from the *kiara* context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Arguments: module_type: must be None if called on the ``KiaraModule`` base class, otherwise the module or operation id module_config: the configuration of the module instance kiara: the *kiara* context \"\"\" if cls == KiaraModule : if not module_type : raise Exception ( \"This method must be either called on a subclass of KiaraModule, not KiaraModule itself, or it needs the 'module_type' argument specified.\" ) else : if module_type : raise Exception ( \"This method must be either called without the 'module_type' argument specified, or on a subclass of the KiaraModule class, but not both.\" ) if cls == KiaraModule : assert module_type is not None module_conf = ModuleConfig . create_module_config ( config = module_type , module_config = module_config , kiara = kiara ) else : module_conf = ModuleConfig . create_module_config ( config = cls , module_config = module_config , kiara = kiara ) return module_conf . create_module ( kiara = kiara ) @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: \"\"\"Retrieve a collection of profiles (pre-set module configs) for this *kiara* module type. This is used to automatically create generally useful operations (incl. their ids). \"\"\" @classmethod def get_type_metadata ( cls ) -> KiaraModuleTypeMetadata : \"\"\"Return all metadata associated with this module type.\"\"\" return KiaraModuleTypeMetadata . from_module_class ( cls ) # @classmethod # def profiles( # cls, # ) -> typing.Optional[typing.Mapping[str, typing.Mapping[str, typing.Any]]]: # \"\"\"Retrieve a collection of profiles (pre-set module configs) for this *kiara* module type. # # This is used to automatically create generally useful operations (incl. their ids). # \"\"\" # return None @classmethod def is_pipeline ( cls ) -> bool : \"\"\"Check whether this module type is a pipeline, or not.\"\"\" return False def __init__ ( self , id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , module_config : typing . Union [ None , KIARA_CONFIG , typing . Mapping [ str , typing . Any ] ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if id is None : id = str ( uuid . uuid4 ()) self . _id : str = id self . _parent_id = parent_id if kiara is None : from kiara import Kiara kiara = Kiara . instance () self . _kiara = kiara if isinstance ( module_config , ModuleTypeConfigSchema ): self . _config : KIARA_CONFIG = module_config # type: ignore elif module_config is None : self . _config = self . __class__ . _config_cls () elif isinstance ( module_config , typing . Mapping ): try : self . _config = self . __class__ . _config_cls ( ** module_config ) except ValidationError as ve : raise KiaraModuleConfigException ( f \"Error creating module ' { id } '. { ve } \" , self . __class__ , module_config , ve , ) else : raise TypeError ( f \"Invalid type for module config: { type ( module_config ) } \" ) self . _module_hash : typing . Optional [ int ] = None self . _info : typing . Optional [ KiaraModuleInstanceMetadata ] = None self . _input_schemas : typing . Mapping [ str , ValueSchema ] = None # type: ignore self . _constants : typing . Mapping [ str , ValueSchema ] = None # type: ignore self . _merged_input_schemas : typing . Mapping [ str , ValueSchema ] = None # type: ignore self . _output_schemas : typing . Mapping [ str , ValueSchema ] = None # type: ignore @property def id ( self ) -> str : \"\"\"The id of this module. This is only unique within a pipeline. \"\"\" return self . _id @property def parent_id ( self ) -> typing . Optional [ str ]: \"\"\"The id of the parent of this module (if part of a pipeline).\"\"\" return self . _parent_id @property def full_id ( self ) -> str : \"\"\"The full id for this module.\"\"\" if self . parent_id : return f \" { self . parent_id } . { self . id } \" else : return self . id @property def config ( self ) -> KIARA_CONFIG : \"\"\"Retrieve the configuration object for this module. Returns: the module-class-specific config object \"\"\" return self . _config def input_required ( self , input_name : str ): if input_name not in self . _input_schemas . keys (): raise Exception ( f \"No input ' { input_name } ' for module ' { self . id } '.\" ) if not self . _input_schemas [ input_name ] . is_required (): return False if input_name in self . constants . keys (): return False else : return True def get_config_value ( self , key : str ) -> typing . Any : \"\"\"Retrieve the value for a specific configuration option. Arguments: key: the config key Returns: the value for the provided key \"\"\" try : return self . config . get ( key ) except Exception : raise Exception ( f \"Error accessing config value ' { key } ' in module { self . __class__ . _module_type_id } .\" # type: ignore ) @abstractmethod def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\" @abstractmethod def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\" @property def input_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: \"\"\"The input schema for this module.\"\"\" if self . _input_schemas is None : self . _create_input_schemas () return self . _input_schemas # type: ignore @property def full_input_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: if self . _merged_input_schemas is not None : return self . _merged_input_schemas self . _merged_input_schemas = dict ( self . input_schemas ) self . _merged_input_schemas . update ( self . constants ) return self . _merged_input_schemas @property def constants ( self ) -> typing . Mapping [ str , ValueSchema ]: if self . _constants is None : self . _create_input_schemas () return self . _constants # type: ignore def _create_input_schemas ( self ) -> None : try : _input_schemas_data = self . create_input_schema () if not _input_schemas_data : raise Exception ( f \"Invalid module implementation for ' { self . __class__ . __name__ } ': empty input schema\" ) try : _input_schemas = create_schemas ( schema_config = _input_schemas_data , kiara = self . _kiara ) except Exception as e : raise Exception ( f \"Can't create input schemas for module { self . full_id } : { e } \" ) defaults = self . config . defaults constants = self . config . constants for k , v in defaults . items (): if k not in _input_schemas . keys (): raise Exception ( f \"Can't create inputs for module ' { self . _module_type_id } ', invalid default field name ' { k } '. Available field names: ' { ', ' . join ( _input_schemas . keys ()) } '\" # type: ignore ) for k , v in constants . items (): if k not in _input_schemas . keys (): raise Exception ( f \"Can't create inputs for module ' { self . _module_type_id } ', invalid constant field name ' { k } '. Available field names: ' { ', ' . join ( _input_schemas . keys ()) } '\" # type: ignore ) self . _input_schemas , self . _constants = overlay_constants_and_defaults ( _input_schemas , defaults = defaults , constants = constants ) except Exception as e : raise Exception ( f \"Can't create input schemas for module of type ' { self . __class__ . _module_type_id } ': { e } \" ) # type: ignore @property def output_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: \"\"\"The output schema for this module.\"\"\" if self . _output_schemas is not None : return self . _output_schemas try : _output_schema = self . create_output_schema () if not _output_schema : raise Exception ( f \"Invalid module implementation for ' { self . __class__ . __name__ } ': empty output schema\" ) try : self . _output_schemas = create_schemas ( schema_config = _output_schema , kiara = self . _kiara ) except Exception as e : raise Exception ( f \"Can't create output schemas for module { self . full_id } : { e } \" ) return self . _output_schemas except Exception as e : raise Exception ( f \"Can't create output schemas for module of type ' { self . __class__ . _module_type_id } ': { e } \" ) # type: ignore @property def input_names ( self ) -> typing . Iterable [ str ]: \"\"\"A list of input field names for this module.\"\"\" return self . input_schemas . keys () @property def output_names ( self ) -> typing . Iterable [ str ]: \"\"\"A list of output field names for this module.\"\"\" return self . output_schemas . keys () def process_step ( self , inputs : ValueSet , outputs : ValueSet , job_log : JobLog ) -> None : \"\"\"Kick off processing for a specific set of input/outputs. This method calls the implemented [process][kiara.module.KiaraModule.process] method of the inheriting class, as well as wrapping input/output-data related functionality. Arguments: inputs: the input value set outputs: the output value set \"\"\" signature = inspect . signature ( self . process ) # type: ignore if \"job_log\" not in signature . parameters . keys (): try : self . process ( inputs = inputs , outputs = outputs ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e else : try : self . process ( inputs = inputs , outputs = outputs , job_log = job_log ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e # @typing.overload # def process(self, inputs: ValueSet, outputs: ValueSet) -> None: # \"\"\"Abstract method to implement by child classes, should be a pure, idempotent function that uses the values from ``inputs``, and stores results in the provided ``outputs`` object. # # Arguments: # inputs: the input value set # outputs: the output value set # \"\"\" # pass # # @typing.overload # def process(self, inputs: ValueSet, outputs: ValueSet, job_log: typing.Optional[JobLog]=None) -> None: # pass # # def process(self, inputs, outputs, job_log=None) -> None: # pass def create_full_inputs ( self , ** inputs : typing . Any ) -> typing . Mapping [ str , Value ]: # TODO: find a generic way to do this kind of stuff def clean_value ( v : typing . Any ) -> typing . Any : if hasattr ( v , \"as_py\" ): return v . as_py () # type: ignore else : return v resolved_inputs : typing . Dict [ str , Value ] = {} for k , v in self . constants . items (): if k in inputs . keys (): raise Exception ( f \"Invalid input: value provided for constant ' { k } '\" ) inputs [ k ] = v for k , value in inputs . items (): value = clean_value ( value ) if not isinstance ( value , Value ): if ( k not in self . input_schemas . keys () and k not in self . constants . keys () ): raise Exception ( f \"Invalid input name ' { k } for module { self . _module_type_id } . Not part of the schema, allowed input names: { ', ' . join ( self . input_names ) } \" # type: ignore ) if k in self . input_schemas . keys (): schema = self . input_schemas [ k ] value = self . _kiara . data_registry . register_data ( value_data = value , value_schema = schema ) # value = Value( # value_data=value, # type: ignore # value_schema=schema, # is_constant=False, # registry=self._kiara.data_registry, # type: ignore # ) else : schema = self . constants [ k ] value = self . _kiara . data_registry . register_data ( value_data = SpecialValue . NOT_SET , value_schema = schema , ) # value = Value( # value_schema=schema, # is_constant=False, # kiara=self._kiara, # type: ignore # registry=self._kiara.data_registry, # type: ignore # ) resolved_inputs [ k ] = value return resolved_inputs def run ( self , _attach_lineage : bool = True , ** inputs : typing . Any ) -> ValueSet : \"\"\"Execute the module with the provided inputs directly. Arguments: inputs: a map of the input values (as described by the input schema Returns: a map of the output values (as described by the output schema) \"\"\" resolved_inputs = self . create_full_inputs ( ** inputs ) # TODO: introduce a 'temp' value set implementation and use that here input_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . full_input_schemas , read_only = True , initial_values = resolved_inputs , title = f \"module_inputs_ { self . id } \" , ) if not input_value_set . items_are_valid (): invalid_details = input_value_set . check_invalid () raise Exception ( f \"Can't process module ' { self . _module_type_name } ', input field(s) not valid: { ', ' . join ( invalid_details . keys ()) } \" # type: ignore ) output_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = False , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore default_value = SpecialValue . NOT_SET , ) self . process ( inputs = input_value_set , outputs = output_value_set ) # type: ignore result_outputs : typing . MutableMapping [ str , Value ] = {} if _attach_lineage : input_infos = { k : v . get_info () for k , v in resolved_inputs . items ()} for field_name , output in output_value_set . items (): value_lineage = ValueLineage . from_module_and_inputs ( module = self , output_name = field_name , inputs = input_infos ) # value_lineage = None output_val = self . _kiara . data_registry . register_data ( value_data = output , lineage = value_lineage ) result_outputs [ field_name ] = output_val else : result_outputs = output_value_set result_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = True , initial_values = result_outputs , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore ) return result_set # result = output_value_set.get_all_value_objects() # return output_value_set # return ValueSetImpl(items=result, read_only=True) @property def module_instance_doc ( self ) -> str : \"\"\"Return documentation for this instance of the module. If not overwritten, will return this class' method ``doc()``. \"\"\" # TODO: auto create instance doc? return self . get_type_metadata () . documentation . full_doc @property def module_instance_hash ( self ) -> int : \"\"\"Return this modules 'module_hash'. If two module instances ``module_instance_hash`` values are the same, it is guaranteed that their ``process`` methods will return the same output, given the same inputs (except if that processing step uses randomness). It can also be assumed that the two instances have the same input and output fields, with the same schemas. !!! note This implementation is preliminary, since it's not yet 100% clear to me how much that will be needed, and in which situations. Also, module versioning needs to be implemented before this can work reliably. Also, for now it is assumed that a module configuration is not changed once set, this also might change in the future Returns: this modules 'module_instance_hash' \"\"\" # TODO: if self . _module_hash is None : _d = { \"module_cls\" : f \" { self . __class__ . __module__ } . { self . __class__ . __name__ } \" , \"version\" : \"0.0.0\" , # TODO: implement module versioning, package name might also need to be included here \"config_hash\" : self . config . config_hash , } hashes = deepdiff . DeepHash ( _d ) self . _module_hash = hashes [ _d ] return self . _module_hash @property def info ( self ) -> KiaraModuleInstanceMetadata : \"\"\"Return an info wrapper class for this module.\"\"\" if self . _info is None : self . _info = KiaraModuleInstanceMetadata . from_module_obj ( self ) return self . _info def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return ( self . full_id , self . config ) == ( self . full_id , other . config ) def __hash__ ( self ): return hash (( self . __class__ , self . full_id , self . config )) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (id= { self . id } input_names= { list ( self . input_names ) } output_names= { list ( self . output_names ) } )\" def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : if not hasattr ( self . __class__ , \"_module_type_id\" ): raise Exception ( \"Invalid model class, no '_module_type_id' attribute added. This is a bug\" ) r_gro : typing . List [ typing . Any ] = [] md = self . info table = md . create_renderable () r_gro . append ( table ) yield Panel ( RenderGroup ( * r_gro ), box = box . ROUNDED , title_align = \"left\" , title = f \"Module: [b] { self . id } [/b]\" , ) config : ~ KIARA_CONFIG property readonly \u00b6 Retrieve the configuration object for this module. Returns: Type Description ~KIARA_CONFIG the module-class-specific config object full_id : str property readonly \u00b6 The full id for this module. id : str property readonly \u00b6 The id of this module. This is only unique within a pipeline. info : KiaraModuleInstanceMetadata property readonly \u00b6 Return an info wrapper class for this module. input_names : Iterable [ str ] property readonly \u00b6 A list of input field names for this module. input_schemas : Mapping [ str , kiara . data . values . ValueSchema ] property readonly \u00b6 The input schema for this module. module_instance_doc : str property readonly \u00b6 Return documentation for this instance of the module. If not overwritten, will return this class' method doc() . module_instance_hash : int property readonly \u00b6 Return this modules 'module_hash'. If two module instances module_instance_hash values are the same, it is guaranteed that their process methods will return the same output, given the same inputs (except if that processing step uses randomness). It can also be assumed that the two instances have the same input and output fields, with the same schemas. Note This implementation is preliminary, since it's not yet 100% clear to me how much that will be needed, and in which situations. Also, module versioning needs to be implemented before this can work reliably. Also, for now it is assumed that a module configuration is not changed once set, this also might change in the future Returns: Type Description int this modules 'module_instance_hash' output_names : Iterable [ str ] property readonly \u00b6 A list of output field names for this module. output_schemas : Mapping [ str , kiara . data . values . ValueSchema ] property readonly \u00b6 The output schema for this module. parent_id : Optional [ str ] property readonly \u00b6 The id of the parent of this module (if part of a pipeline). create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/module.py @abstractmethod def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\" create_instance ( module_type = None , module_config = None , kiara = None ) classmethod \u00b6 Create an instance of a kiara module. This class method is overloaded in a way that you can either provide the module_type argument, in which case the relevant sub-class will be queried from the kiara context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Parameters: Name Type Description Default module_type Optional[str] must be None if called on the KiaraModule base class, otherwise the module or operation id None module_config Optional[Mapping[str, Any]] the configuration of the module instance None kiara Optional[Kiara] the kiara context None Source code in kiara/module.py @classmethod def create_instance ( cls , module_type : typing . Optional [ str ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"KiaraModule\" : \"\"\"Create an instance of a *kiara* module. This class method is overloaded in a way that you can either provide the `module_type` argument, in which case the relevant sub-class will be queried from the *kiara* context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Arguments: module_type: must be None if called on the ``KiaraModule`` base class, otherwise the module or operation id module_config: the configuration of the module instance kiara: the *kiara* context \"\"\" if cls == KiaraModule : if not module_type : raise Exception ( \"This method must be either called on a subclass of KiaraModule, not KiaraModule itself, or it needs the 'module_type' argument specified.\" ) else : if module_type : raise Exception ( \"This method must be either called without the 'module_type' argument specified, or on a subclass of the KiaraModule class, but not both.\" ) if cls == KiaraModule : assert module_type is not None module_conf = ModuleConfig . create_module_config ( config = module_type , module_config = module_config , kiara = kiara ) else : module_conf = ModuleConfig . create_module_config ( config = cls , module_config = module_config , kiara = kiara ) return module_conf . create_module ( kiara = kiara ) create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/module.py @abstractmethod def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\" get_config_value ( self , key ) \u00b6 Retrieve the value for a specific configuration option. Parameters: Name Type Description Default key str the config key required Returns: Type Description Any the value for the provided key Source code in kiara/module.py def get_config_value ( self , key : str ) -> typing . Any : \"\"\"Retrieve the value for a specific configuration option. Arguments: key: the config key Returns: the value for the provided key \"\"\" try : return self . config . get ( key ) except Exception : raise Exception ( f \"Error accessing config value ' { key } ' in module { self . __class__ . _module_type_id } .\" # type: ignore ) get_type_metadata () classmethod \u00b6 Return all metadata associated with this module type. Source code in kiara/module.py @classmethod def get_type_metadata ( cls ) -> KiaraModuleTypeMetadata : \"\"\"Return all metadata associated with this module type.\"\"\" return KiaraModuleTypeMetadata . from_module_class ( cls ) is_pipeline () classmethod \u00b6 Check whether this module type is a pipeline, or not. Source code in kiara/module.py @classmethod def is_pipeline ( cls ) -> bool : \"\"\"Check whether this module type is a pipeline, or not.\"\"\" return False process_step ( self , inputs , outputs , job_log ) \u00b6 Kick off processing for a specific set of input/outputs. This method calls the implemented process method of the inheriting class, as well as wrapping input/output-data related functionality. Parameters: Name Type Description Default inputs ValueSet the input value set required outputs ValueSet the output value set required Source code in kiara/module.py def process_step ( self , inputs : ValueSet , outputs : ValueSet , job_log : JobLog ) -> None : \"\"\"Kick off processing for a specific set of input/outputs. This method calls the implemented [process][kiara.module.KiaraModule.process] method of the inheriting class, as well as wrapping input/output-data related functionality. Arguments: inputs: the input value set outputs: the output value set \"\"\" signature = inspect . signature ( self . process ) # type: ignore if \"job_log\" not in signature . parameters . keys (): try : self . process ( inputs = inputs , outputs = outputs ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e else : try : self . process ( inputs = inputs , outputs = outputs , job_log = job_log ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/module.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: \"\"\"Retrieve a collection of profiles (pre-set module configs) for this *kiara* module type. This is used to automatically create generally useful operations (incl. their ids). \"\"\" run ( self , _attach_lineage = True , ** inputs ) \u00b6 Execute the module with the provided inputs directly. Parameters: Name Type Description Default inputs Any a map of the input values (as described by the input schema {} Returns: Type Description ValueSet a map of the output values (as described by the output schema) Source code in kiara/module.py def run ( self , _attach_lineage : bool = True , ** inputs : typing . Any ) -> ValueSet : \"\"\"Execute the module with the provided inputs directly. Arguments: inputs: a map of the input values (as described by the input schema Returns: a map of the output values (as described by the output schema) \"\"\" resolved_inputs = self . create_full_inputs ( ** inputs ) # TODO: introduce a 'temp' value set implementation and use that here input_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . full_input_schemas , read_only = True , initial_values = resolved_inputs , title = f \"module_inputs_ { self . id } \" , ) if not input_value_set . items_are_valid (): invalid_details = input_value_set . check_invalid () raise Exception ( f \"Can't process module ' { self . _module_type_name } ', input field(s) not valid: { ', ' . join ( invalid_details . keys ()) } \" # type: ignore ) output_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = False , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore default_value = SpecialValue . NOT_SET , ) self . process ( inputs = input_value_set , outputs = output_value_set ) # type: ignore result_outputs : typing . MutableMapping [ str , Value ] = {} if _attach_lineage : input_infos = { k : v . get_info () for k , v in resolved_inputs . items ()} for field_name , output in output_value_set . items (): value_lineage = ValueLineage . from_module_and_inputs ( module = self , output_name = field_name , inputs = input_infos ) # value_lineage = None output_val = self . _kiara . data_registry . register_data ( value_data = output , lineage = value_lineage ) result_outputs [ field_name ] = output_val else : result_outputs = output_value_set result_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = True , initial_values = result_outputs , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore ) return result_set # result = output_value_set.get_all_value_objects() # return output_value_set # return ValueSetImpl(items=result, read_only=True) StepInputs ( ValueSet ) \u00b6 Wrapper class to hold a set of inputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Parameters: Name Type Description Default inputs ValueSet the input values of a pipeline step required Source code in kiara/module.py class StepInputs ( ValueSet ): \"\"\"Wrapper class to hold a set of inputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Arguments: inputs (ValueSet): the input values of a pipeline step \"\"\" def __init__ ( self , inputs : typing . Mapping [ str , Value ], title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _inputs : typing . Mapping [ str , Value ] = inputs super () . __init__ ( read_only = True , title = title , kiara = kiara ) def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _inputs . keys () def _get_value_obj ( self , field_name : str , ensure_metadata : typing . Union [ bool , typing . Iterable [ str ], str ] = False , ) -> Value : \"\"\"Retrieve the value object for the specified field.\"\"\" value = self . _inputs [ field_name ] if ensure_metadata : if isinstance ( ensure_metadata , bool ): value . get_metadata () elif isinstance ( ensure_metadata , str ): value . get_metadata ( ensure_metadata ) elif isinstance ( ensure_metadata , typing . Iterable ): value . get_metadata ( * ensure_metadata ) else : raise ValueError ( f \"Invalid type ' { type ( ensure_metadata ) } ' for 'ensure_metadata' argument.\" ) return value def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: raise Exception ( \"Inputs are read-only.\" ) get_all_field_names ( self ) \u00b6 All field names included in this ValueSet. Source code in kiara/module.py def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _inputs . keys () StepOutputs ( ValueSet ) \u00b6 Wrapper class to hold a set of outputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Internally, this class stores two sets of its values: the 'actual', up-to-date values, and the referenced (original) ones that were used when creating an object of this class. It's not a good idea to keep both synced all the time, because that could potentially involve unnecessary data transfer and I/O. Also, in some cases a developer might want to avoid events that could be triggered by a changed value. Both value sets can be synced manually using the 'sync()' method. Parameters: Name Type Description Default outputs ValueSet the output values of a pipeline step required Source code in kiara/module.py class StepOutputs ( ValueSet ): \"\"\"Wrapper class to hold a set of outputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Internally, this class stores two sets of its values: the 'actual', up-to-date values, and the referenced (original) ones that were used when creating an object of this class. It's not a good idea to keep both synced all the time, because that could potentially involve unnecessary data transfer and I/O. Also, in some cases a developer might want to avoid events that could be triggered by a changed value. Both value sets can be synced manually using the 'sync()' method. Arguments: outputs (ValueSet): the output values of a pipeline step \"\"\" def __init__ ( self , outputs : ValueSet , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _outputs_staging : typing . Dict [ str , typing . Any ] = {} self . _outputs : ValueSet = outputs super () . __init__ ( read_only = False , title = title , kiara = kiara ) def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _outputs . get_all_field_names () def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: wrong = [] for key in values . keys (): if key not in self . _outputs . keys (): # type: ignore wrong . append ( key ) if wrong : av = \", \" . join ( self . _outputs . keys ()) # type: ignore raise Exception ( f \"Can't set output value(s), invalid key name(s): { ', ' . join ( wrong ) } . Available: { av } \" ) if metadata : raise NotImplementedError () if lineage : raise NotImplementedError () result = {} for output_name , value in values . items (): # value_obj = self._outputs.get_value_obj(output_name) if ( output_name not in self . _outputs_staging . keys () # type: ignore or value != self . _outputs_staging [ output_name ] # type: ignore ): self . _outputs_staging [ output_name ] = value # type: ignore result [ output_name ] = True else : result [ output_name ] = False return result def _get_value_obj ( self , output_name ): \"\"\"Retrieve the value object for the specified field.\"\"\" # self.sync() return self . _outputs . get_value_obj ( output_name ) def sync ( self , lineage : typing . Optional [ ValueLineage ] = None , ** metadata : MetadataModel ): \"\"\"Sync this value sets 'shadow' values with the ones a user would retrieve.\"\"\" self . _outputs . set_values ( lineage = lineage , metadata = metadata , ** self . _outputs_staging ) # type: ignore self . _outputs_staging . clear () # type: ignore get_all_field_names ( self ) \u00b6 All field names included in this ValueSet. Source code in kiara/module.py def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _outputs . get_all_field_names () sync ( self , lineage = None , ** metadata ) \u00b6 Sync this value sets 'shadow' values with the ones a user would retrieve. Source code in kiara/module.py def sync ( self , lineage : typing . Optional [ ValueLineage ] = None , ** metadata : MetadataModel ): \"\"\"Sync this value sets 'shadow' values with the ones a user would retrieve.\"\"\" self . _outputs . set_values ( lineage = lineage , metadata = metadata , ** self . _outputs_staging ) # type: ignore self . _outputs_staging . clear () # type: ignore","title":"module"},{"location":"reference/kiara/module/#kiara.module.KiaraModule","text":"The base class that every custom module in Kiara needs to inherit from. The core of every KiaraModule is a process method, which should be a 'pure', idempotent function that creates one or several output values from the given input(s), and its purpose is to transfor a set of inputs into a set of outputs. Every module can be configured. The module configuration schema can differ, but every one such configuration needs to subclass the ModuleTypeConfigSchema class and set as the value to the _config_cls attribute of the module class. This is useful, because it allows for some modules to serve a much larger variety of use-cases than non-configurable modules would be, which would mean more code duplication because of very simlilar, but slightly different module types. Each module class (type) has a unique -- within a kiara context -- module type id which can be accessed via the _module_type_id class attribute. Examples: A simple example would be an 'addition' module, with a and b configured as inputs, and z as the output field name. An implementing class would look something like this: TODO Parameters: Name Type Description Default id str the id for this module (needs to be unique within a pipeline) None parent_id Optional[str] the id of the parent, in case this module is part of a pipeline None module_config Any the configuation for this module None metadata Mapping[str, Any] metadata for this module (not implemented yet) required Source code in kiara/module.py class KiaraModule ( typing . Generic [ KIARA_CONFIG ], abc . ABC ): \"\"\"The base class that every custom module in *Kiara* needs to inherit from. The core of every ``KiaraModule`` is a ``process`` method, which should be a 'pure', idempotent function that creates one or several output values from the given input(s), and its purpose is to transfor a set of inputs into a set of outputs. Every module can be configured. The module configuration schema can differ, but every one such configuration needs to subclass the [ModuleTypeConfigSchema][kiara.module_config.ModuleTypeConfigSchema] class and set as the value to the ``_config_cls`` attribute of the module class. This is useful, because it allows for some modules to serve a much larger variety of use-cases than non-configurable modules would be, which would mean more code duplication because of very simlilar, but slightly different module types. Each module class (type) has a unique -- within a *kiara* context -- module type id which can be accessed via the ``_module_type_id`` class attribute. Examples: A simple example would be an 'addition' module, with ``a`` and ``b`` configured as inputs, and ``z`` as the output field name. An implementing class would look something like this: TODO Arguments: id (str): the id for this module (needs to be unique within a pipeline) parent_id (typing.Optional[str]): the id of the parent, in case this module is part of a pipeline module_config (typing.Any): the configuation for this module metadata (typing.Mapping[str, typing.Any]): metadata for this module (not implemented yet) \"\"\" # TODO: not quite sure about this generic type here, mypy doesn't seem to like it _config_cls : typing . Type [ KIARA_CONFIG ] = ModuleTypeConfigSchema # type: ignore @classmethod def create_instance ( cls , module_type : typing . Optional [ str ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"KiaraModule\" : \"\"\"Create an instance of a *kiara* module. This class method is overloaded in a way that you can either provide the `module_type` argument, in which case the relevant sub-class will be queried from the *kiara* context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Arguments: module_type: must be None if called on the ``KiaraModule`` base class, otherwise the module or operation id module_config: the configuration of the module instance kiara: the *kiara* context \"\"\" if cls == KiaraModule : if not module_type : raise Exception ( \"This method must be either called on a subclass of KiaraModule, not KiaraModule itself, or it needs the 'module_type' argument specified.\" ) else : if module_type : raise Exception ( \"This method must be either called without the 'module_type' argument specified, or on a subclass of the KiaraModule class, but not both.\" ) if cls == KiaraModule : assert module_type is not None module_conf = ModuleConfig . create_module_config ( config = module_type , module_config = module_config , kiara = kiara ) else : module_conf = ModuleConfig . create_module_config ( config = cls , module_config = module_config , kiara = kiara ) return module_conf . create_module ( kiara = kiara ) @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: \"\"\"Retrieve a collection of profiles (pre-set module configs) for this *kiara* module type. This is used to automatically create generally useful operations (incl. their ids). \"\"\" @classmethod def get_type_metadata ( cls ) -> KiaraModuleTypeMetadata : \"\"\"Return all metadata associated with this module type.\"\"\" return KiaraModuleTypeMetadata . from_module_class ( cls ) # @classmethod # def profiles( # cls, # ) -> typing.Optional[typing.Mapping[str, typing.Mapping[str, typing.Any]]]: # \"\"\"Retrieve a collection of profiles (pre-set module configs) for this *kiara* module type. # # This is used to automatically create generally useful operations (incl. their ids). # \"\"\" # return None @classmethod def is_pipeline ( cls ) -> bool : \"\"\"Check whether this module type is a pipeline, or not.\"\"\" return False def __init__ ( self , id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , module_config : typing . Union [ None , KIARA_CONFIG , typing . Mapping [ str , typing . Any ] ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if id is None : id = str ( uuid . uuid4 ()) self . _id : str = id self . _parent_id = parent_id if kiara is None : from kiara import Kiara kiara = Kiara . instance () self . _kiara = kiara if isinstance ( module_config , ModuleTypeConfigSchema ): self . _config : KIARA_CONFIG = module_config # type: ignore elif module_config is None : self . _config = self . __class__ . _config_cls () elif isinstance ( module_config , typing . Mapping ): try : self . _config = self . __class__ . _config_cls ( ** module_config ) except ValidationError as ve : raise KiaraModuleConfigException ( f \"Error creating module ' { id } '. { ve } \" , self . __class__ , module_config , ve , ) else : raise TypeError ( f \"Invalid type for module config: { type ( module_config ) } \" ) self . _module_hash : typing . Optional [ int ] = None self . _info : typing . Optional [ KiaraModuleInstanceMetadata ] = None self . _input_schemas : typing . Mapping [ str , ValueSchema ] = None # type: ignore self . _constants : typing . Mapping [ str , ValueSchema ] = None # type: ignore self . _merged_input_schemas : typing . Mapping [ str , ValueSchema ] = None # type: ignore self . _output_schemas : typing . Mapping [ str , ValueSchema ] = None # type: ignore @property def id ( self ) -> str : \"\"\"The id of this module. This is only unique within a pipeline. \"\"\" return self . _id @property def parent_id ( self ) -> typing . Optional [ str ]: \"\"\"The id of the parent of this module (if part of a pipeline).\"\"\" return self . _parent_id @property def full_id ( self ) -> str : \"\"\"The full id for this module.\"\"\" if self . parent_id : return f \" { self . parent_id } . { self . id } \" else : return self . id @property def config ( self ) -> KIARA_CONFIG : \"\"\"Retrieve the configuration object for this module. Returns: the module-class-specific config object \"\"\" return self . _config def input_required ( self , input_name : str ): if input_name not in self . _input_schemas . keys (): raise Exception ( f \"No input ' { input_name } ' for module ' { self . id } '.\" ) if not self . _input_schemas [ input_name ] . is_required (): return False if input_name in self . constants . keys (): return False else : return True def get_config_value ( self , key : str ) -> typing . Any : \"\"\"Retrieve the value for a specific configuration option. Arguments: key: the config key Returns: the value for the provided key \"\"\" try : return self . config . get ( key ) except Exception : raise Exception ( f \"Error accessing config value ' { key } ' in module { self . __class__ . _module_type_id } .\" # type: ignore ) @abstractmethod def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\" @abstractmethod def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\" @property def input_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: \"\"\"The input schema for this module.\"\"\" if self . _input_schemas is None : self . _create_input_schemas () return self . _input_schemas # type: ignore @property def full_input_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: if self . _merged_input_schemas is not None : return self . _merged_input_schemas self . _merged_input_schemas = dict ( self . input_schemas ) self . _merged_input_schemas . update ( self . constants ) return self . _merged_input_schemas @property def constants ( self ) -> typing . Mapping [ str , ValueSchema ]: if self . _constants is None : self . _create_input_schemas () return self . _constants # type: ignore def _create_input_schemas ( self ) -> None : try : _input_schemas_data = self . create_input_schema () if not _input_schemas_data : raise Exception ( f \"Invalid module implementation for ' { self . __class__ . __name__ } ': empty input schema\" ) try : _input_schemas = create_schemas ( schema_config = _input_schemas_data , kiara = self . _kiara ) except Exception as e : raise Exception ( f \"Can't create input schemas for module { self . full_id } : { e } \" ) defaults = self . config . defaults constants = self . config . constants for k , v in defaults . items (): if k not in _input_schemas . keys (): raise Exception ( f \"Can't create inputs for module ' { self . _module_type_id } ', invalid default field name ' { k } '. Available field names: ' { ', ' . join ( _input_schemas . keys ()) } '\" # type: ignore ) for k , v in constants . items (): if k not in _input_schemas . keys (): raise Exception ( f \"Can't create inputs for module ' { self . _module_type_id } ', invalid constant field name ' { k } '. Available field names: ' { ', ' . join ( _input_schemas . keys ()) } '\" # type: ignore ) self . _input_schemas , self . _constants = overlay_constants_and_defaults ( _input_schemas , defaults = defaults , constants = constants ) except Exception as e : raise Exception ( f \"Can't create input schemas for module of type ' { self . __class__ . _module_type_id } ': { e } \" ) # type: ignore @property def output_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: \"\"\"The output schema for this module.\"\"\" if self . _output_schemas is not None : return self . _output_schemas try : _output_schema = self . create_output_schema () if not _output_schema : raise Exception ( f \"Invalid module implementation for ' { self . __class__ . __name__ } ': empty output schema\" ) try : self . _output_schemas = create_schemas ( schema_config = _output_schema , kiara = self . _kiara ) except Exception as e : raise Exception ( f \"Can't create output schemas for module { self . full_id } : { e } \" ) return self . _output_schemas except Exception as e : raise Exception ( f \"Can't create output schemas for module of type ' { self . __class__ . _module_type_id } ': { e } \" ) # type: ignore @property def input_names ( self ) -> typing . Iterable [ str ]: \"\"\"A list of input field names for this module.\"\"\" return self . input_schemas . keys () @property def output_names ( self ) -> typing . Iterable [ str ]: \"\"\"A list of output field names for this module.\"\"\" return self . output_schemas . keys () def process_step ( self , inputs : ValueSet , outputs : ValueSet , job_log : JobLog ) -> None : \"\"\"Kick off processing for a specific set of input/outputs. This method calls the implemented [process][kiara.module.KiaraModule.process] method of the inheriting class, as well as wrapping input/output-data related functionality. Arguments: inputs: the input value set outputs: the output value set \"\"\" signature = inspect . signature ( self . process ) # type: ignore if \"job_log\" not in signature . parameters . keys (): try : self . process ( inputs = inputs , outputs = outputs ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e else : try : self . process ( inputs = inputs , outputs = outputs , job_log = job_log ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e # @typing.overload # def process(self, inputs: ValueSet, outputs: ValueSet) -> None: # \"\"\"Abstract method to implement by child classes, should be a pure, idempotent function that uses the values from ``inputs``, and stores results in the provided ``outputs`` object. # # Arguments: # inputs: the input value set # outputs: the output value set # \"\"\" # pass # # @typing.overload # def process(self, inputs: ValueSet, outputs: ValueSet, job_log: typing.Optional[JobLog]=None) -> None: # pass # # def process(self, inputs, outputs, job_log=None) -> None: # pass def create_full_inputs ( self , ** inputs : typing . Any ) -> typing . Mapping [ str , Value ]: # TODO: find a generic way to do this kind of stuff def clean_value ( v : typing . Any ) -> typing . Any : if hasattr ( v , \"as_py\" ): return v . as_py () # type: ignore else : return v resolved_inputs : typing . Dict [ str , Value ] = {} for k , v in self . constants . items (): if k in inputs . keys (): raise Exception ( f \"Invalid input: value provided for constant ' { k } '\" ) inputs [ k ] = v for k , value in inputs . items (): value = clean_value ( value ) if not isinstance ( value , Value ): if ( k not in self . input_schemas . keys () and k not in self . constants . keys () ): raise Exception ( f \"Invalid input name ' { k } for module { self . _module_type_id } . Not part of the schema, allowed input names: { ', ' . join ( self . input_names ) } \" # type: ignore ) if k in self . input_schemas . keys (): schema = self . input_schemas [ k ] value = self . _kiara . data_registry . register_data ( value_data = value , value_schema = schema ) # value = Value( # value_data=value, # type: ignore # value_schema=schema, # is_constant=False, # registry=self._kiara.data_registry, # type: ignore # ) else : schema = self . constants [ k ] value = self . _kiara . data_registry . register_data ( value_data = SpecialValue . NOT_SET , value_schema = schema , ) # value = Value( # value_schema=schema, # is_constant=False, # kiara=self._kiara, # type: ignore # registry=self._kiara.data_registry, # type: ignore # ) resolved_inputs [ k ] = value return resolved_inputs def run ( self , _attach_lineage : bool = True , ** inputs : typing . Any ) -> ValueSet : \"\"\"Execute the module with the provided inputs directly. Arguments: inputs: a map of the input values (as described by the input schema Returns: a map of the output values (as described by the output schema) \"\"\" resolved_inputs = self . create_full_inputs ( ** inputs ) # TODO: introduce a 'temp' value set implementation and use that here input_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . full_input_schemas , read_only = True , initial_values = resolved_inputs , title = f \"module_inputs_ { self . id } \" , ) if not input_value_set . items_are_valid (): invalid_details = input_value_set . check_invalid () raise Exception ( f \"Can't process module ' { self . _module_type_name } ', input field(s) not valid: { ', ' . join ( invalid_details . keys ()) } \" # type: ignore ) output_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = False , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore default_value = SpecialValue . NOT_SET , ) self . process ( inputs = input_value_set , outputs = output_value_set ) # type: ignore result_outputs : typing . MutableMapping [ str , Value ] = {} if _attach_lineage : input_infos = { k : v . get_info () for k , v in resolved_inputs . items ()} for field_name , output in output_value_set . items (): value_lineage = ValueLineage . from_module_and_inputs ( module = self , output_name = field_name , inputs = input_infos ) # value_lineage = None output_val = self . _kiara . data_registry . register_data ( value_data = output , lineage = value_lineage ) result_outputs [ field_name ] = output_val else : result_outputs = output_value_set result_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = True , initial_values = result_outputs , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore ) return result_set # result = output_value_set.get_all_value_objects() # return output_value_set # return ValueSetImpl(items=result, read_only=True) @property def module_instance_doc ( self ) -> str : \"\"\"Return documentation for this instance of the module. If not overwritten, will return this class' method ``doc()``. \"\"\" # TODO: auto create instance doc? return self . get_type_metadata () . documentation . full_doc @property def module_instance_hash ( self ) -> int : \"\"\"Return this modules 'module_hash'. If two module instances ``module_instance_hash`` values are the same, it is guaranteed that their ``process`` methods will return the same output, given the same inputs (except if that processing step uses randomness). It can also be assumed that the two instances have the same input and output fields, with the same schemas. !!! note This implementation is preliminary, since it's not yet 100% clear to me how much that will be needed, and in which situations. Also, module versioning needs to be implemented before this can work reliably. Also, for now it is assumed that a module configuration is not changed once set, this also might change in the future Returns: this modules 'module_instance_hash' \"\"\" # TODO: if self . _module_hash is None : _d = { \"module_cls\" : f \" { self . __class__ . __module__ } . { self . __class__ . __name__ } \" , \"version\" : \"0.0.0\" , # TODO: implement module versioning, package name might also need to be included here \"config_hash\" : self . config . config_hash , } hashes = deepdiff . DeepHash ( _d ) self . _module_hash = hashes [ _d ] return self . _module_hash @property def info ( self ) -> KiaraModuleInstanceMetadata : \"\"\"Return an info wrapper class for this module.\"\"\" if self . _info is None : self . _info = KiaraModuleInstanceMetadata . from_module_obj ( self ) return self . _info def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return ( self . full_id , self . config ) == ( self . full_id , other . config ) def __hash__ ( self ): return hash (( self . __class__ , self . full_id , self . config )) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (id= { self . id } input_names= { list ( self . input_names ) } output_names= { list ( self . output_names ) } )\" def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : if not hasattr ( self . __class__ , \"_module_type_id\" ): raise Exception ( \"Invalid model class, no '_module_type_id' attribute added. This is a bug\" ) r_gro : typing . List [ typing . Any ] = [] md = self . info table = md . create_renderable () r_gro . append ( table ) yield Panel ( RenderGroup ( * r_gro ), box = box . ROUNDED , title_align = \"left\" , title = f \"Module: [b] { self . id } [/b]\" , )","title":"KiaraModule"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.config","text":"Retrieve the configuration object for this module. Returns: Type Description ~KIARA_CONFIG the module-class-specific config object","title":"config"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.full_id","text":"The full id for this module.","title":"full_id"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.id","text":"The id of this module. This is only unique within a pipeline.","title":"id"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.info","text":"Return an info wrapper class for this module.","title":"info"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.input_names","text":"A list of input field names for this module.","title":"input_names"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.input_schemas","text":"The input schema for this module.","title":"input_schemas"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.module_instance_doc","text":"Return documentation for this instance of the module. If not overwritten, will return this class' method doc() .","title":"module_instance_doc"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.module_instance_hash","text":"Return this modules 'module_hash'. If two module instances module_instance_hash values are the same, it is guaranteed that their process methods will return the same output, given the same inputs (except if that processing step uses randomness). It can also be assumed that the two instances have the same input and output fields, with the same schemas. Note This implementation is preliminary, since it's not yet 100% clear to me how much that will be needed, and in which situations. Also, module versioning needs to be implemented before this can work reliably. Also, for now it is assumed that a module configuration is not changed once set, this also might change in the future Returns: Type Description int this modules 'module_instance_hash'","title":"module_instance_hash"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.output_names","text":"A list of output field names for this module.","title":"output_names"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.output_schemas","text":"The output schema for this module.","title":"output_schemas"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.parent_id","text":"The id of the parent of this module (if part of a pipeline).","title":"parent_id"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/module.py @abstractmethod def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\"","title":"create_input_schema()"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.create_instance","text":"Create an instance of a kiara module. This class method is overloaded in a way that you can either provide the module_type argument, in which case the relevant sub-class will be queried from the kiara context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Parameters: Name Type Description Default module_type Optional[str] must be None if called on the KiaraModule base class, otherwise the module or operation id None module_config Optional[Mapping[str, Any]] the configuration of the module instance None kiara Optional[Kiara] the kiara context None Source code in kiara/module.py @classmethod def create_instance ( cls , module_type : typing . Optional [ str ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"KiaraModule\" : \"\"\"Create an instance of a *kiara* module. This class method is overloaded in a way that you can either provide the `module_type` argument, in which case the relevant sub-class will be queried from the *kiara* context, or you can call this method directly on any of the inehreting sub-classes. You can't do both, though. Arguments: module_type: must be None if called on the ``KiaraModule`` base class, otherwise the module or operation id module_config: the configuration of the module instance kiara: the *kiara* context \"\"\" if cls == KiaraModule : if not module_type : raise Exception ( \"This method must be either called on a subclass of KiaraModule, not KiaraModule itself, or it needs the 'module_type' argument specified.\" ) else : if module_type : raise Exception ( \"This method must be either called without the 'module_type' argument specified, or on a subclass of the KiaraModule class, but not both.\" ) if cls == KiaraModule : assert module_type is not None module_conf = ModuleConfig . create_module_config ( config = module_type , module_config = module_config , kiara = kiara ) else : module_conf = ModuleConfig . create_module_config ( config = cls , module_config = module_config , kiara = kiara ) return module_conf . create_module ( kiara = kiara )","title":"create_instance()"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/module.py @abstractmethod def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: \"\"\"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): ``` { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } ``` \"\"\"","title":"create_output_schema()"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.get_config_value","text":"Retrieve the value for a specific configuration option. Parameters: Name Type Description Default key str the config key required Returns: Type Description Any the value for the provided key Source code in kiara/module.py def get_config_value ( self , key : str ) -> typing . Any : \"\"\"Retrieve the value for a specific configuration option. Arguments: key: the config key Returns: the value for the provided key \"\"\" try : return self . config . get ( key ) except Exception : raise Exception ( f \"Error accessing config value ' { key } ' in module { self . __class__ . _module_type_id } .\" # type: ignore )","title":"get_config_value()"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.get_type_metadata","text":"Return all metadata associated with this module type. Source code in kiara/module.py @classmethod def get_type_metadata ( cls ) -> KiaraModuleTypeMetadata : \"\"\"Return all metadata associated with this module type.\"\"\" return KiaraModuleTypeMetadata . from_module_class ( cls )","title":"get_type_metadata()"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.is_pipeline","text":"Check whether this module type is a pipeline, or not. Source code in kiara/module.py @classmethod def is_pipeline ( cls ) -> bool : \"\"\"Check whether this module type is a pipeline, or not.\"\"\" return False","title":"is_pipeline()"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.process_step","text":"Kick off processing for a specific set of input/outputs. This method calls the implemented process method of the inheriting class, as well as wrapping input/output-data related functionality. Parameters: Name Type Description Default inputs ValueSet the input value set required outputs ValueSet the output value set required Source code in kiara/module.py def process_step ( self , inputs : ValueSet , outputs : ValueSet , job_log : JobLog ) -> None : \"\"\"Kick off processing for a specific set of input/outputs. This method calls the implemented [process][kiara.module.KiaraModule.process] method of the inheriting class, as well as wrapping input/output-data related functionality. Arguments: inputs: the input value set outputs: the output value set \"\"\" signature = inspect . signature ( self . process ) # type: ignore if \"job_log\" not in signature . parameters . keys (): try : self . process ( inputs = inputs , outputs = outputs ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e else : try : self . process ( inputs = inputs , outputs = outputs , job_log = job_log ) # type: ignore except Exception as e : if is_debug (): try : import traceback traceback . print_exc () except Exception : pass raise e","title":"process_step()"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/module.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: \"\"\"Retrieve a collection of profiles (pre-set module configs) for this *kiara* module type. This is used to automatically create generally useful operations (incl. their ids). \"\"\"","title":"retrieve_module_profiles()"},{"location":"reference/kiara/module/#kiara.module.KiaraModule.run","text":"Execute the module with the provided inputs directly. Parameters: Name Type Description Default inputs Any a map of the input values (as described by the input schema {} Returns: Type Description ValueSet a map of the output values (as described by the output schema) Source code in kiara/module.py def run ( self , _attach_lineage : bool = True , ** inputs : typing . Any ) -> ValueSet : \"\"\"Execute the module with the provided inputs directly. Arguments: inputs: a map of the input values (as described by the input schema Returns: a map of the output values (as described by the output schema) \"\"\" resolved_inputs = self . create_full_inputs ( ** inputs ) # TODO: introduce a 'temp' value set implementation and use that here input_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . full_input_schemas , read_only = True , initial_values = resolved_inputs , title = f \"module_inputs_ { self . id } \" , ) if not input_value_set . items_are_valid (): invalid_details = input_value_set . check_invalid () raise Exception ( f \"Can't process module ' { self . _module_type_name } ', input field(s) not valid: { ', ' . join ( invalid_details . keys ()) } \" # type: ignore ) output_value_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = False , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore default_value = SpecialValue . NOT_SET , ) self . process ( inputs = input_value_set , outputs = output_value_set ) # type: ignore result_outputs : typing . MutableMapping [ str , Value ] = {} if _attach_lineage : input_infos = { k : v . get_info () for k , v in resolved_inputs . items ()} for field_name , output in output_value_set . items (): value_lineage = ValueLineage . from_module_and_inputs ( module = self , output_name = field_name , inputs = input_infos ) # value_lineage = None output_val = self . _kiara . data_registry . register_data ( value_data = output , lineage = value_lineage ) result_outputs [ field_name ] = output_val else : result_outputs = output_value_set result_set = SlottedValueSet . from_schemas ( kiara = self . _kiara , schemas = self . output_schemas , read_only = True , initial_values = result_outputs , title = f \" { self . _module_type_name } _module_outputs_ { self . id } \" , # type: ignore ) return result_set # result = output_value_set.get_all_value_objects() # return output_value_set # return ValueSetImpl(items=result, read_only=True)","title":"run()"},{"location":"reference/kiara/module/#kiara.module.StepInputs","text":"Wrapper class to hold a set of inputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Parameters: Name Type Description Default inputs ValueSet the input values of a pipeline step required Source code in kiara/module.py class StepInputs ( ValueSet ): \"\"\"Wrapper class to hold a set of inputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Arguments: inputs (ValueSet): the input values of a pipeline step \"\"\" def __init__ ( self , inputs : typing . Mapping [ str , Value ], title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _inputs : typing . Mapping [ str , Value ] = inputs super () . __init__ ( read_only = True , title = title , kiara = kiara ) def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _inputs . keys () def _get_value_obj ( self , field_name : str , ensure_metadata : typing . Union [ bool , typing . Iterable [ str ], str ] = False , ) -> Value : \"\"\"Retrieve the value object for the specified field.\"\"\" value = self . _inputs [ field_name ] if ensure_metadata : if isinstance ( ensure_metadata , bool ): value . get_metadata () elif isinstance ( ensure_metadata , str ): value . get_metadata ( ensure_metadata ) elif isinstance ( ensure_metadata , typing . Iterable ): value . get_metadata ( * ensure_metadata ) else : raise ValueError ( f \"Invalid type ' { type ( ensure_metadata ) } ' for 'ensure_metadata' argument.\" ) return value def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: raise Exception ( \"Inputs are read-only.\" )","title":"StepInputs"},{"location":"reference/kiara/module/#kiara.module.StepInputs.get_all_field_names","text":"All field names included in this ValueSet. Source code in kiara/module.py def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _inputs . keys ()","title":"get_all_field_names()"},{"location":"reference/kiara/module/#kiara.module.StepOutputs","text":"Wrapper class to hold a set of outputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Internally, this class stores two sets of its values: the 'actual', up-to-date values, and the referenced (original) ones that were used when creating an object of this class. It's not a good idea to keep both synced all the time, because that could potentially involve unnecessary data transfer and I/O. Also, in some cases a developer might want to avoid events that could be triggered by a changed value. Both value sets can be synced manually using the 'sync()' method. Parameters: Name Type Description Default outputs ValueSet the output values of a pipeline step required Source code in kiara/module.py class StepOutputs ( ValueSet ): \"\"\"Wrapper class to hold a set of outputs for a pipeline processing step. This is necessary because we can't assume the processing will be done on the same machine (or in the same process) as the pipeline controller. By disconnecting the value from the processing code, we can react appropriately to those circumstances. Internally, this class stores two sets of its values: the 'actual', up-to-date values, and the referenced (original) ones that were used when creating an object of this class. It's not a good idea to keep both synced all the time, because that could potentially involve unnecessary data transfer and I/O. Also, in some cases a developer might want to avoid events that could be triggered by a changed value. Both value sets can be synced manually using the 'sync()' method. Arguments: outputs (ValueSet): the output values of a pipeline step \"\"\" def __init__ ( self , outputs : ValueSet , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _outputs_staging : typing . Dict [ str , typing . Any ] = {} self . _outputs : ValueSet = outputs super () . __init__ ( read_only = False , title = title , kiara = kiara ) def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _outputs . get_all_field_names () def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: wrong = [] for key in values . keys (): if key not in self . _outputs . keys (): # type: ignore wrong . append ( key ) if wrong : av = \", \" . join ( self . _outputs . keys ()) # type: ignore raise Exception ( f \"Can't set output value(s), invalid key name(s): { ', ' . join ( wrong ) } . Available: { av } \" ) if metadata : raise NotImplementedError () if lineage : raise NotImplementedError () result = {} for output_name , value in values . items (): # value_obj = self._outputs.get_value_obj(output_name) if ( output_name not in self . _outputs_staging . keys () # type: ignore or value != self . _outputs_staging [ output_name ] # type: ignore ): self . _outputs_staging [ output_name ] = value # type: ignore result [ output_name ] = True else : result [ output_name ] = False return result def _get_value_obj ( self , output_name ): \"\"\"Retrieve the value object for the specified field.\"\"\" # self.sync() return self . _outputs . get_value_obj ( output_name ) def sync ( self , lineage : typing . Optional [ ValueLineage ] = None , ** metadata : MetadataModel ): \"\"\"Sync this value sets 'shadow' values with the ones a user would retrieve.\"\"\" self . _outputs . set_values ( lineage = lineage , metadata = metadata , ** self . _outputs_staging ) # type: ignore self . _outputs_staging . clear () # type: ignore","title":"StepOutputs"},{"location":"reference/kiara/module/#kiara.module.StepOutputs.get_all_field_names","text":"All field names included in this ValueSet. Source code in kiara/module.py def get_all_field_names ( self ) -> typing . Iterable [ str ]: \"\"\"All field names included in this ValueSet.\"\"\" return self . _outputs . get_all_field_names ()","title":"get_all_field_names()"},{"location":"reference/kiara/module/#kiara.module.StepOutputs.sync","text":"Sync this value sets 'shadow' values with the ones a user would retrieve. Source code in kiara/module.py def sync ( self , lineage : typing . Optional [ ValueLineage ] = None , ** metadata : MetadataModel ): \"\"\"Sync this value sets 'shadow' values with the ones a user would retrieve.\"\"\" self . _outputs . set_values ( lineage = lineage , metadata = metadata , ** self . _outputs_staging ) # type: ignore self . _outputs_staging . clear () # type: ignore","title":"sync()"},{"location":"reference/kiara/module_config/","text":"Module-related configuration models for the Kiara package. ModuleConfig ( KiaraInfoModel ) pydantic-model \u00b6 A class to hold the type and configuration for a module instance. Source code in kiara/module_config.py class ModuleConfig ( KiaraInfoModel ): \"\"\"A class to hold the type and configuration for a module instance.\"\"\" @classmethod def create_module_config ( cls , config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str , typing . Type [ \"KiaraModule\" ], \"PipelineConfig\" , ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleConfig\" : conf = parse_and_create_module_config ( config = config , module_config = module_config , kiara = kiara ) return conf @classmethod def create_renderable_from_module_instance_configs ( cls , configs : typing . Mapping [ str , \"ModuleConfig\" ], ** render_config : typing . Any , ): \"\"\"Convenience method to create a renderable for this module configuration, to be printed to terminal.\"\"\" table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" , style = \"i\" , no_wrap = True ) table . add_column ( \"Description\" ) if not render_config . get ( \"omit_config\" , False ): table . add_column ( \"Configuration\" ) for id , config in configs . items (): if config . doc : doc = config . doc . description else : doc = DEFAULT_NO_DESC_VALUE row : typing . List [ RenderableType ] = [ id , doc ] if not render_config . get ( \"omit_config\" , False ): conf = config . json ( exclude = { \"doc\" }, indent = 2 ) row . append ( Syntax ( conf , \"json\" , background_color = \"default\" )) table . add_row ( * row ) return table class Config : extra = Extra . forbid validate_all = True _module : typing . Optional [ \"KiaraModule\" ] = PrivateAttr ( default = None ) module_type : str = Field ( description = \"The module type.\" ) module_config : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"The configuration for the module.\" ) doc : DocumentationMetadataModel = Field ( description = \"Documentation for this operation.\" , default = None ) @validator ( \"doc\" , pre = True ) def create_doc ( cls , value ): return DocumentationMetadataModel . create ( value ) def create_module ( self , kiara : typing . Optional [ \"Kiara\" ] = None , module_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance from this configuration.\"\"\" if module_id and not isinstance ( module_id , str ): raise TypeError ( f \"Invalid type, module_id must be a string, not: { type ( module_id ) } \" ) if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if self . _module is None : self . _module = kiara . create_module ( id = module_id , module_type = self . module_type , module_config = self . module_config , ) return self . _module def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a renderable for this module configuration.\"\"\" conf = Syntax ( self . json ( exclude_none = True , indent = 2 ), \"json\" , background_color = \"default\" , ) return conf doc : DocumentationMetadataModel pydantic-field \u00b6 Documentation for this operation. module_config : Dict [ str , Any ] pydantic-field \u00b6 The configuration for the module. module_type : str pydantic-field required \u00b6 The module type. create_module ( self , kiara = None , module_id = None ) \u00b6 Create a module instance from this configuration. Source code in kiara/module_config.py def create_module ( self , kiara : typing . Optional [ \"Kiara\" ] = None , module_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance from this configuration.\"\"\" if module_id and not isinstance ( module_id , str ): raise TypeError ( f \"Invalid type, module_id must be a string, not: { type ( module_id ) } \" ) if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if self . _module is None : self . _module = kiara . create_module ( id = module_id , module_type = self . module_type , module_config = self . module_config , ) return self . _module create_renderable ( self , ** config ) \u00b6 Create a renderable for this module configuration. Source code in kiara/module_config.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a renderable for this module configuration.\"\"\" conf = Syntax ( self . json ( exclude_none = True , indent = 2 ), \"json\" , background_color = \"default\" , ) return conf create_renderable_from_module_instance_configs ( configs , ** render_config ) classmethod \u00b6 Convenience method to create a renderable for this module configuration, to be printed to terminal. Source code in kiara/module_config.py @classmethod def create_renderable_from_module_instance_configs ( cls , configs : typing . Mapping [ str , \"ModuleConfig\" ], ** render_config : typing . Any , ): \"\"\"Convenience method to create a renderable for this module configuration, to be printed to terminal.\"\"\" table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" , style = \"i\" , no_wrap = True ) table . add_column ( \"Description\" ) if not render_config . get ( \"omit_config\" , False ): table . add_column ( \"Configuration\" ) for id , config in configs . items (): if config . doc : doc = config . doc . description else : doc = DEFAULT_NO_DESC_VALUE row : typing . List [ RenderableType ] = [ id , doc ] if not render_config . get ( \"omit_config\" , False ): conf = config . json ( exclude = { \"doc\" }, indent = 2 ) row . append ( Syntax ( conf , \"json\" , background_color = \"default\" )) table . add_row ( * row ) return table ModuleTypeConfigSchema ( BaseModel ) pydantic-model \u00b6 Base class that describes the configuration a KiaraModule class accepts. This is stored in the _config_cls class attribute in each KiaraModule class. There are two config options every KiaraModule supports: constants , and defaults Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default values that override the schema defaults, and those can be overwritten by users. If both a constant and a default value is set for an input field, an error is thrown. Source code in kiara/module_config.py class ModuleTypeConfigSchema ( BaseModel ): \"\"\"Base class that describes the configuration a [``KiaraModule``][kiara.module.KiaraModule] class accepts. This is stored in the ``_config_cls`` class attribute in each ``KiaraModule`` class. There are two config options every ``KiaraModule`` supports: - ``constants``, and - ``defaults`` Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default values that override the schema defaults, and those can be overwritten by users. If both a constant and a default value is set for an input field, an error is thrown. \"\"\" @classmethod def requires_config ( cls , config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : if config : if config . get ( field_name , None ) is None : return True else : return True return False _config_hash : str = PrivateAttr ( default = None ) constants : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"Value constants for this module.\" ) defaults : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"Value defaults for this module.\" ) class Config : extra = Extra . forbid validate_assignment = True def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) @property def config_hash ( self ): if self . _config_hash is None : _d = self . dict () hashes = deepdiff . DeepHash ( _d ) self . _config_hash = hashes [ _d ] return self . _config_hash def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () def __hash__ ( self ): return hash ( self . config_hash ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : my_table = Table ( box = box . MINIMAL , show_header = False ) my_table . add_column ( \"Field name\" , style = \"i\" ) my_table . add_column ( \"Value\" ) for field in self . __fields__ : my_table . add_row ( field , getattr ( self , field )) yield my_table constants : Dict [ str , Any ] pydantic-field \u00b6 Value constants for this module. defaults : Dict [ str , Any ] pydantic-field \u00b6 Value defaults for this module. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/module_config.py def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/module_config.py def __hash__ ( self ): return hash ( self . config_hash ) get ( self , key ) \u00b6 Get the value for the specified configuation key. Source code in kiara/module_config.py def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) requires_config ( config = None ) classmethod \u00b6 Return whether this class can be used as-is, or requires configuration before an instance can be created. Source code in kiara/module_config.py @classmethod def requires_config ( cls , config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : if config : if config . get ( field_name , None ) is None : return True else : return True return False parse_and_create_module_config ( config , module_config = None , kiara = None ) \u00b6 Utility method to create a ModuleConfig object from different input types. Parameters: Name Type Description Default config Union[ModuleConfig, Mapping, str, Type[KiaraModule], PipelineConfig] the 'main' configuation required module_config Optional[Mapping[str, Any]] if the 'main' configuration value is an (unconfigured) module type, this argument can contain the configuration for the module instance None kiara Optional[Kiara] the kiara context None Source code in kiara/module_config.py def parse_and_create_module_config ( config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str , typing . Type [ \"KiaraModule\" ], \"PipelineConfig\" , ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleConfig\" : \"\"\"Utility method to create a `ModuleConfig` object from different input types. Arguments: config: the 'main' configuation module_config: if the 'main' configuration value is an (unconfigured) module type, this argument can contain the configuration for the module instance kiara: the *kiara* context \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if isinstance ( config , type ): from kiara.module import KiaraModule if issubclass ( config , KiaraModule ): # this basically makes sure the class is augmented with the '_module_type_id` attribute if not hasattr ( config , \"_module_type_id\" ): match = False for mod_name in kiara . available_module_types : mod_cls = kiara . get_module_class ( mod_name ) if mod_cls == config : match = True break if not match : raise Exception ( f \"Class ' { config } ' not a valid kiara module.\" ) config = config . _module_type_id # type: ignore else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for module configuration.\" ) if isinstance ( config , typing . Mapping ): if module_config : raise NotImplementedError () module_instance_config : ModuleConfig = ModuleConfig ( ** config ) elif isinstance ( config , str ): if config == \"pipeline\" : if not module_config : raise Exception ( \"Can't create module from 'pipeline' module type without further configuration.\" ) elif isinstance ( module_config , typing . Mapping ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = module_config ) else : from kiara.pipeline.config import PipelineConfig if isinstance ( config , PipelineConfig ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = config . dict () ) else : raise Exception ( f \"Can't create module config, invalid type for 'module_config' parameter: { type ( module_config ) } \" ) elif config in kiara . available_module_types : if module_config : module_instance_config = ModuleConfig ( module_type = config , module_config = module_config ) else : module_instance_config = ModuleConfig ( module_type = config ) elif config in kiara . operation_mgmt . profiles . keys (): module_instance_config = kiara . operation_mgmt . profiles [ config ] elif os . path . isfile ( os . path . expanduser ( config )): path = os . path . expanduser ( config ) module_config_data = get_data_from_file ( path ) if module_config : raise NotImplementedError () if not isinstance ( module_config_data , typing . Mapping ): raise Exception ( f \"Invalid module/pipeline config, must be a mapping type: { module_config_data } \" ) if \"steps\" in module_config_data . keys (): module_type = \"pipeline\" module_instance_config = ModuleConfig ( module_type = module_type , module_config = module_config_data ) elif \"module_type\" in module_config_data . keys (): module_instance_config = ModuleConfig ( ** module_config_data ) else : raise Exception ( f \"Can't create module config from string ' { config } '. Value must be path to a file, or one of: { ', ' . join ( kiara . available_module_types ) } \" ) elif isinstance ( config , ModuleConfig ): module_instance_config = config if module_config : raise NotImplementedError () else : from kiara.pipeline.config import PipelineConfig if isinstance ( config , PipelineConfig ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = config . dict () ) else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for module configuration.\" ) return module_instance_config","title":"module_config"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleConfig","text":"A class to hold the type and configuration for a module instance. Source code in kiara/module_config.py class ModuleConfig ( KiaraInfoModel ): \"\"\"A class to hold the type and configuration for a module instance.\"\"\" @classmethod def create_module_config ( cls , config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str , typing . Type [ \"KiaraModule\" ], \"PipelineConfig\" , ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleConfig\" : conf = parse_and_create_module_config ( config = config , module_config = module_config , kiara = kiara ) return conf @classmethod def create_renderable_from_module_instance_configs ( cls , configs : typing . Mapping [ str , \"ModuleConfig\" ], ** render_config : typing . Any , ): \"\"\"Convenience method to create a renderable for this module configuration, to be printed to terminal.\"\"\" table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" , style = \"i\" , no_wrap = True ) table . add_column ( \"Description\" ) if not render_config . get ( \"omit_config\" , False ): table . add_column ( \"Configuration\" ) for id , config in configs . items (): if config . doc : doc = config . doc . description else : doc = DEFAULT_NO_DESC_VALUE row : typing . List [ RenderableType ] = [ id , doc ] if not render_config . get ( \"omit_config\" , False ): conf = config . json ( exclude = { \"doc\" }, indent = 2 ) row . append ( Syntax ( conf , \"json\" , background_color = \"default\" )) table . add_row ( * row ) return table class Config : extra = Extra . forbid validate_all = True _module : typing . Optional [ \"KiaraModule\" ] = PrivateAttr ( default = None ) module_type : str = Field ( description = \"The module type.\" ) module_config : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"The configuration for the module.\" ) doc : DocumentationMetadataModel = Field ( description = \"Documentation for this operation.\" , default = None ) @validator ( \"doc\" , pre = True ) def create_doc ( cls , value ): return DocumentationMetadataModel . create ( value ) def create_module ( self , kiara : typing . Optional [ \"Kiara\" ] = None , module_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance from this configuration.\"\"\" if module_id and not isinstance ( module_id , str ): raise TypeError ( f \"Invalid type, module_id must be a string, not: { type ( module_id ) } \" ) if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if self . _module is None : self . _module = kiara . create_module ( id = module_id , module_type = self . module_type , module_config = self . module_config , ) return self . _module def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a renderable for this module configuration.\"\"\" conf = Syntax ( self . json ( exclude_none = True , indent = 2 ), \"json\" , background_color = \"default\" , ) return conf","title":"ModuleConfig"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleConfig.doc","text":"Documentation for this operation.","title":"doc"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleConfig.module_config","text":"The configuration for the module.","title":"module_config"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleConfig.module_type","text":"The module type.","title":"module_type"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleConfig.create_module","text":"Create a module instance from this configuration. Source code in kiara/module_config.py def create_module ( self , kiara : typing . Optional [ \"Kiara\" ] = None , module_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : \"\"\"Create a module instance from this configuration.\"\"\" if module_id and not isinstance ( module_id , str ): raise TypeError ( f \"Invalid type, module_id must be a string, not: { type ( module_id ) } \" ) if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if self . _module is None : self . _module = kiara . create_module ( id = module_id , module_type = self . module_type , module_config = self . module_config , ) return self . _module","title":"create_module()"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleConfig.create_renderable","text":"Create a renderable for this module configuration. Source code in kiara/module_config.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a renderable for this module configuration.\"\"\" conf = Syntax ( self . json ( exclude_none = True , indent = 2 ), \"json\" , background_color = \"default\" , ) return conf","title":"create_renderable()"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleConfig.create_renderable_from_module_instance_configs","text":"Convenience method to create a renderable for this module configuration, to be printed to terminal. Source code in kiara/module_config.py @classmethod def create_renderable_from_module_instance_configs ( cls , configs : typing . Mapping [ str , \"ModuleConfig\" ], ** render_config : typing . Any , ): \"\"\"Convenience method to create a renderable for this module configuration, to be printed to terminal.\"\"\" table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" , style = \"i\" , no_wrap = True ) table . add_column ( \"Description\" ) if not render_config . get ( \"omit_config\" , False ): table . add_column ( \"Configuration\" ) for id , config in configs . items (): if config . doc : doc = config . doc . description else : doc = DEFAULT_NO_DESC_VALUE row : typing . List [ RenderableType ] = [ id , doc ] if not render_config . get ( \"omit_config\" , False ): conf = config . json ( exclude = { \"doc\" }, indent = 2 ) row . append ( Syntax ( conf , \"json\" , background_color = \"default\" )) table . add_row ( * row ) return table","title":"create_renderable_from_module_instance_configs()"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleTypeConfigSchema","text":"Base class that describes the configuration a KiaraModule class accepts. This is stored in the _config_cls class attribute in each KiaraModule class. There are two config options every KiaraModule supports: constants , and defaults Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default values that override the schema defaults, and those can be overwritten by users. If both a constant and a default value is set for an input field, an error is thrown. Source code in kiara/module_config.py class ModuleTypeConfigSchema ( BaseModel ): \"\"\"Base class that describes the configuration a [``KiaraModule``][kiara.module.KiaraModule] class accepts. This is stored in the ``_config_cls`` class attribute in each ``KiaraModule`` class. There are two config options every ``KiaraModule`` supports: - ``constants``, and - ``defaults`` Constants are pre-set inputs, and users can't change them and an error is thrown if they try. Defaults are default values that override the schema defaults, and those can be overwritten by users. If both a constant and a default value is set for an input field, an error is thrown. \"\"\" @classmethod def requires_config ( cls , config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : if config : if config . get ( field_name , None ) is None : return True else : return True return False _config_hash : str = PrivateAttr ( default = None ) constants : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"Value constants for this module.\" ) defaults : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"Value defaults for this module.\" ) class Config : extra = Extra . forbid validate_assignment = True def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) @property def config_hash ( self ): if self . _config_hash is None : _d = self . dict () hashes = deepdiff . DeepHash ( _d ) self . _config_hash = hashes [ _d ] return self . _config_hash def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () def __hash__ ( self ): return hash ( self . config_hash ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : my_table = Table ( box = box . MINIMAL , show_header = False ) my_table . add_column ( \"Field name\" , style = \"i\" ) my_table . add_column ( \"Value\" ) for field in self . __fields__ : my_table . add_row ( field , getattr ( self , field )) yield my_table","title":"ModuleTypeConfigSchema"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleTypeConfigSchema.constants","text":"Value constants for this module.","title":"constants"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleTypeConfigSchema.defaults","text":"Value defaults for this module.","title":"defaults"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleTypeConfigSchema.__eq__","text":"Return self==value. Source code in kiara/module_config.py def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict ()","title":"__eq__()"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleTypeConfigSchema.__hash__","text":"Return hash(self). Source code in kiara/module_config.py def __hash__ ( self ): return hash ( self . config_hash )","title":"__hash__()"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleTypeConfigSchema.get","text":"Get the value for the specified configuation key. Source code in kiara/module_config.py def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key )","title":"get()"},{"location":"reference/kiara/module_config/#kiara.module_config.ModuleTypeConfigSchema.requires_config","text":"Return whether this class can be used as-is, or requires configuration before an instance can be created. Source code in kiara/module_config.py @classmethod def requires_config ( cls , config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : if config : if config . get ( field_name , None ) is None : return True else : return True return False","title":"requires_config()"},{"location":"reference/kiara/module_config/#kiara.module_config.parse_and_create_module_config","text":"Utility method to create a ModuleConfig object from different input types. Parameters: Name Type Description Default config Union[ModuleConfig, Mapping, str, Type[KiaraModule], PipelineConfig] the 'main' configuation required module_config Optional[Mapping[str, Any]] if the 'main' configuration value is an (unconfigured) module type, this argument can contain the configuration for the module instance None kiara Optional[Kiara] the kiara context None Source code in kiara/module_config.py def parse_and_create_module_config ( config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str , typing . Type [ \"KiaraModule\" ], \"PipelineConfig\" , ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleConfig\" : \"\"\"Utility method to create a `ModuleConfig` object from different input types. Arguments: config: the 'main' configuation module_config: if the 'main' configuration value is an (unconfigured) module type, this argument can contain the configuration for the module instance kiara: the *kiara* context \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if isinstance ( config , type ): from kiara.module import KiaraModule if issubclass ( config , KiaraModule ): # this basically makes sure the class is augmented with the '_module_type_id` attribute if not hasattr ( config , \"_module_type_id\" ): match = False for mod_name in kiara . available_module_types : mod_cls = kiara . get_module_class ( mod_name ) if mod_cls == config : match = True break if not match : raise Exception ( f \"Class ' { config } ' not a valid kiara module.\" ) config = config . _module_type_id # type: ignore else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for module configuration.\" ) if isinstance ( config , typing . Mapping ): if module_config : raise NotImplementedError () module_instance_config : ModuleConfig = ModuleConfig ( ** config ) elif isinstance ( config , str ): if config == \"pipeline\" : if not module_config : raise Exception ( \"Can't create module from 'pipeline' module type without further configuration.\" ) elif isinstance ( module_config , typing . Mapping ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = module_config ) else : from kiara.pipeline.config import PipelineConfig if isinstance ( config , PipelineConfig ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = config . dict () ) else : raise Exception ( f \"Can't create module config, invalid type for 'module_config' parameter: { type ( module_config ) } \" ) elif config in kiara . available_module_types : if module_config : module_instance_config = ModuleConfig ( module_type = config , module_config = module_config ) else : module_instance_config = ModuleConfig ( module_type = config ) elif config in kiara . operation_mgmt . profiles . keys (): module_instance_config = kiara . operation_mgmt . profiles [ config ] elif os . path . isfile ( os . path . expanduser ( config )): path = os . path . expanduser ( config ) module_config_data = get_data_from_file ( path ) if module_config : raise NotImplementedError () if not isinstance ( module_config_data , typing . Mapping ): raise Exception ( f \"Invalid module/pipeline config, must be a mapping type: { module_config_data } \" ) if \"steps\" in module_config_data . keys (): module_type = \"pipeline\" module_instance_config = ModuleConfig ( module_type = module_type , module_config = module_config_data ) elif \"module_type\" in module_config_data . keys (): module_instance_config = ModuleConfig ( ** module_config_data ) else : raise Exception ( f \"Can't create module config from string ' { config } '. Value must be path to a file, or one of: { ', ' . join ( kiara . available_module_types ) } \" ) elif isinstance ( config , ModuleConfig ): module_instance_config = config if module_config : raise NotImplementedError () else : from kiara.pipeline.config import PipelineConfig if isinstance ( config , PipelineConfig ): module_instance_config = ModuleConfig ( module_type = \"pipeline\" , module_config = config . dict () ) else : raise TypeError ( f \"Invalid type ' { type ( config ) } ' for module configuration.\" ) return module_instance_config","title":"parse_and_create_module_config()"},{"location":"reference/kiara/data/__init__/","text":"Data and value related classes for Kiara . onboarding special \u00b6 ValueStoreConfig ( BaseModel ) pydantic-model \u00b6 A configuration that describes which step outputs of a pipeline to save, and how. Source code in kiara/data/onboarding/__init__.py class ValueStoreConfig ( BaseModel ): \"\"\"A configuration that describes which step outputs of a pipeline to save, and how.\"\"\" @classmethod def save_fields ( self , base_id : str , value_set : typing . Mapping [ str , Value ], matchers : typing . Optional [ typing . List [ FieldMatcher ]] = None , ) -> typing . Dict [ Value , typing . List [ str ]]: if matchers is None : matchers = FieldMatcher . create_default_matchers () for matcher in matchers : if matcher . matcher_type not in [ \"glob\" , \"regex\" ]: raise NotImplementedError ( \"Only 'glob' and 'regex' field matcher type implemented yet.\" ) if matcher . matcher_type == \"glob\" : matcher . matcher_type = \"regex\" matcher . match_expr = fnmatch . translate ( matcher . match_expr ) env = Environment () to_save : typing . Dict [ str , typing . Set [ str ]] = {} for field_name in value_set . keys (): for matcher in matchers : if not re . match ( matcher . match_expr , field_name ): continue template = env . from_string ( matcher . alias_template ) rendered = template . render ( field_name = field_name , base_id = base_id ) to_save . setdefault ( field_name , set ()) . add ( rendered ) if not matcher . check_next_on_match : break result : typing . Dict [ Value , typing . List [ str ]] = {} for field_name , aliases in to_save . items (): value = value_set [ field_name ] _v = value . save ( aliases = aliases ) result . setdefault ( _v , []) . extend ( aliases ) return result inputs : typing . List [ FieldMatcher ] = Field ( description = \"Whether and how to save inputs.\" , default_factory = list ) outputs : typing . List [ FieldMatcher ] = Field ( description = \"Whether and how to save inputs.\" , default_factory = FieldMatcher . create_default_matchers , ) steps : typing . List [ \"ValueStoreConfig\" ] = Field ( description = \"Whether and how to save step inputs and outputs.\" , default_factory = list , ) inputs : List [ kiara . data . onboarding . FieldMatcher ] pydantic-field \u00b6 Whether and how to save inputs. outputs : List [ kiara . data . onboarding . FieldMatcher ] pydantic-field \u00b6 Whether and how to save inputs. steps : List [ ValueStoreConfig ] pydantic-field \u00b6 Whether and how to save step inputs and outputs. batch \u00b6 BatchOnboard ( BaseModel ) pydantic-model \u00b6 Source code in kiara/data/onboarding/batch.py class BatchOnboard ( BaseModel ): @classmethod def create ( cls , module_type : str , inputs : typing . Mapping [ str , typing . Any ], store_config : typing . Optional [ ValueStoreConfig ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ Kiara ] = None , ): if kiara is None : kiara = Kiara . instance () operation = Operation . create_operation ( kiara = kiara , operation_id = \"_batch_onboarding_\" , config = module_type , module_config = module_config , ) if store_config is None : store_config = ValueStoreConfig () elif isinstance ( store_config , typing . Mapping ): store_config = ValueStoreConfig ( ** store_config ) return BatchOnboard ( operation = operation , inputs = inputs , store_config = store_config ) inputs : typing . Mapping [ str , typing . Any ] = Field ( description = \"The inputs.\" ) operation : Operation = Field ( description = \"The operation to use with the inputs.\" ) store_config : ValueStoreConfig = Field ( description = \"The configuration for storing operation/pipeline values.\" ) def run ( self , base_id : str ) -> typing . Mapping [ Value , typing . Iterable [ str ]]: run_result = self . operation . module . run ( ** self . inputs ) result : typing . Dict [ Value , typing . List [ str ]] = {} if self . store_config . inputs : raise NotImplementedError ( \"Storing inputs not supported yet.\" ) if self . store_config . steps : raise NotImplementedError ( \"Storing step inputs/outputs not supported yet.\" ) if self . store_config . outputs : r = ValueStoreConfig . save_fields ( base_id = base_id , value_set = run_result , matchers = self . store_config . outputs , ) for value , aliases in r . items (): result . setdefault ( value , []) . extend ( aliases ) return result inputs : Mapping [ str , Any ] pydantic-field required \u00b6 The inputs. operation : Operation pydantic-field required \u00b6 The operation to use with the inputs. store_config : ValueStoreConfig pydantic-field required \u00b6 The configuration for storing operation/pipeline values. registry special \u00b6 BaseDataRegistry ( ABC ) \u00b6 Base class to extend if you want to write a kiara data registry. This outlines the main methods that must be available for an entity that holds some values and their data, as well as their associated aliases. In most cases users will interact with subclasses of the more fully featured DataRegistry class, but there are cases where it will make sense to sub-class this base class directly (like for example read-only 'archive' data registries). Source code in kiara/data/registry/__init__.py class BaseDataRegistry ( abc . ABC ): \"\"\"Base class to extend if you want to write a *kiara* data registry. This outlines the main methods that must be available for an entity that holds some values and their data, as well as their associated aliases. In most cases users will interact with subclasses of the more fully featured [DataRegistry][kiara.data.registry.DataRegistry] class, but there are cases where it will make sense to sub-class this base class directly (like for example read-only 'archive' data registries). \"\"\" def __init__ ( self , kiara : \"Kiara\" ): self . _id : str = str ( uuid . uuid4 ()) self . _kiara : Kiara = kiara # self._hashes: typing.Dict[str, typing.Dict[str, str]] = {} self . _register_tokens : typing . Set = set () @property def id ( self ) -> str : return self . _id @property def value_ids ( self ) -> typing . List [ str ]: return list ( self . _get_available_value_ids ()) @property def alias_names ( self ) -> typing . List [ str ]: return sorted ( self . _get_available_aliases ()) # ====================================================================== # main abstract methods @abc . abstractmethod def _get_available_value_ids ( self ) -> typing . Iterable [ str ]: \"\"\"Return all of the registries available value ids.\"\"\" @abc . abstractmethod def _get_value_obj_for_id ( self , value_id : str ) -> Value : pass @abc . abstractmethod def _get_value_data_for_id ( self , value_item : str ) -> typing . Any : pass @abc . abstractmethod def _get_available_aliases ( self ) -> typing . Iterable [ str ]: pass @abc . abstractmethod def _get_value_slot_for_alias ( self , alias_name : str ) -> ValueSlot : pass @abc . abstractmethod def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: pass def _find_value_for_hashes ( self , * hashes : ValueHash ) -> typing . Optional [ Value ]: \"\"\"Return a value that matches one of the provided hashes. This method does not need to be overwritten, but it is recommended that it is done for performance reasons. Arguments: hashes: a collection of hashes, Returns: A (registered) value that matches one of the hashes \"\"\" return None # ----------------------------------------------------------------------- # main value retrieval methods def _create_alias_obj ( self , alias : str ) -> ValueAlias : if alias . startswith ( \"value:\" ): alias = alias [ 6 :] value_alias = ValueAlias . from_string ( value_alias = alias ) if ( value_alias . alias not in self . _get_available_value_ids () and value_alias . alias not in self . _get_available_aliases () ): raise Exception ( f \"Neither id nor alias ' { alias } ' registered with this registry.\" ) if ( value_alias . alias in self . _get_available_aliases () and value_alias . tag is None and value_alias . version is None ): value_alias . version = self . get_latest_version_for_alias ( value_alias . alias ) return value_alias def get_value_obj ( self , value_item : typing . Union [ str , Value , ValueAlias , ValueSlot ], raise_exception : bool = False , ) -> typing . Optional [ Value ]: if value_item == NO_ID_YET_MARKER : raise Exception ( \"Can't get value object: value not fully registered yet.\" ) if isinstance ( value_item , Value ): value_item = value_item . id elif isinstance ( value_item , ValueSlot ): value_item = value_item . get_latest_value () . id if isinstance ( value_item , str ): value_item = self . _create_alias_obj ( value_item ) elif not isinstance ( value_item , ValueAlias ): raise TypeError ( f \"Invalid type ' { type ( value_item ) } ' for value item parameter.\" ) if value_item . alias in self . _get_available_value_ids (): _value_id = value_item . alias _value : typing . Optional [ Value ] = self . _get_value_obj_for_id ( _value_id ) elif value_item . alias in self . _get_available_aliases (): _value = self . _resolve_alias_to_value ( value_item ) else : _value = None if _value is None and raise_exception : raise Exception ( f \"No value or alias registered in registry for: { value_item } \" ) return _value def get_value_data ( self , value_item : typing . Union [ str , Value , ValueSlot , ValueAlias ] ) -> typing . Any : value_obj = self . get_value_obj ( value_item ) if value_obj is None : raise Exception ( f \"No value registered for: { value_item } \" ) if not value_obj . is_set and value_obj . value_schema . default not in ( SpecialValue . NO_VALUE , SpecialValue . NOT_SET , None , ): return value_obj . value_schema . default elif not value_obj . is_set : raise Exception ( f \"Value not set: { value_obj . id } \" ) data = self . _get_value_data_for_id ( value_obj . id ) if data == SpecialValue . NO_VALUE : return None elif isinstance ( data , Value ): return data . get_value_data () else : return data def get_lineage ( self , value_item : typing . Union [ str , Value , ValueSlot ] ) -> typing . Optional [ ValueLineage ]: value_obj = self . get_value_obj ( value_item = value_item ) if value_obj is None : raise Exception ( f \"No value registered for: { value_item } \" ) return self . _get_value_lineage ( value_obj . id ) # def get_value_info(self, value_item: typing.Union[str, Value, ValueAlias]) -> ValueInfo: # # value_obj = self.get_value_obj(value_item=value_item) # # aliases = self.find_aliases_for_value_id(value_id=value_obj.id, include_all_versions=True, include_tags=True) # # info = ValueInfo(value_id=value_obj.id, value_type=value_obj.value_schema.type, aliases=aliases, metadata=value_obj.get_metadata()) # return info def get_value_slot ( self , alias : typing . Union [ str , ValueSlot ] ) -> typing . Optional [ ValueSlot ]: if isinstance ( alias , ValueSlot ): alias = alias . id if alias not in self . alias_names : return None return self . _get_value_slot_for_alias ( alias_name = alias ) # ----------------------------------------------------------------------- # def _resolve_hash_to_value( # self, hash_str: str, hash_type: typing.Optional[str] = None # ) -> typing.Optional[Value]: # # matches: typing.Set[str] = set() # if hash_type is None: # for hash_type, details in self._hashes.items(): # if hash_str in details.keys(): # matches.add(details[hash_str]) # else: # hashes_for_type = self._hashes.get(hash_type, {}) # if hash_str in hashes_for_type.keys(): # matches.add(hashes_for_type[hash_str]) # # if len(matches) == 0: # return None # elif len(matches) > 1: # raise Exception(f\"Multiple values found for hash '{hash_str}'.\") # # match_id = next(iter(matches)) # value = self.get_value_obj(match_id) # return value def _check_register_token ( self , register_token : uuid . UUID ): return register_token in self . _register_tokens # alias-related methods def _resolve_alias_to_value ( self , alias : ValueAlias ) -> typing . Optional [ Value ]: alias_name = alias . alias alias_version = alias . version alias_tag = alias . tag value_slot = self . get_value_slot ( alias_name ) if not value_slot : raise Exception ( f \"No alias ' { alias_name } ' registered with registry.\" ) if alias_tag : _version = value_slot . tags . get ( alias_tag ) if alias_version : if _version != alias_version : raise Exception ( f \"Value alias object contains both tag and version information, but actual version of tag resolves to different version in the registry: { _version } != { alias_version } \" ) else : alias_version = _version assert alias_version is not None value = value_slot . values . get ( alias_version , None ) return value def _find_aliases_for_value_id ( self , value_id : str ) -> typing . List [ ValueAlias ]: \"\"\"Find all aliases that point to the specified value id. Sub-classes may overwrite this method for performance reasons. \"\"\" result = [] for alias in self . _get_available_aliases (): value_slot = self . get_value_slot ( alias ) assert value_slot aliases = value_slot . find_linked_aliases ( value_id ) result . extend ( aliases ) return result def _get_tags_for_alias ( self , alias : str ) -> typing . Iterable [ str ]: value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { alias } ' registered with registry.\" ) return value_slot . tags . keys () def find_aliases_for_value ( self , value_item : typing . Union [ str , Value ], include_all_versions : bool = False , include_tags : bool = False , ) -> typing . List [ ValueAlias ]: value = self . get_value_obj ( value_item ) if value is None : raise Exception ( f \"Can't find registered value for: { value_item } \" ) aliases = self . _find_aliases_for_value_id ( value_id = value . id ) result = [] latest_cache : typing . Dict [ str , int ] = {} for alias in aliases : if alias . version is not None : if not include_all_versions : if alias . alias in latest_cache . keys (): latest_version = latest_cache [ alias . alias ] else : latest_version = self . get_latest_version_for_alias ( alias . alias ) latest_cache [ alias . alias ] = latest_version if latest_version == alias . version : result . append ( alias ) else : result . append ( alias ) if alias . tag is not None and include_tags : result . append ( alias ) return result def get_latest_version_for_alias ( self , alias : str ) -> int : versions = self . get_versions_for_alias ( alias ) if not versions : return 0 else : return max ( versions ) def get_versions_for_alias ( self , alias : str ) -> typing . Iterable [ int ]: \"\"\"Return all available versions for the specified alias.\"\"\" if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . values . keys ()) def get_tags_for_alias ( self , alias : str ) -> typing . Iterable [ str ]: if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . tags . keys ()) @deprecated ( reason = \"This will be removed soon in favor of a more generic way to query values.\" ) def find_all_values_of_type ( self , value_type : str ) -> typing . Dict [ str , Value ]: \"\"\"Find all values of a certain type.\"\"\" result = {} for value_id in self . value_ids : value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"No value registered for: { value_id } \" ) if value_obj . type_name == value_type : # type: ignore result [ value_id ] = value_obj return result def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , DataRegistry ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id ) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (id= { self . id } \" def __str__ ( self ): return self . __repr__ () find_all_values_of_type ( self , value_type ) \u00b6 Find all values of a certain type. Source code in kiara/data/registry/__init__.py @deprecated ( reason = \"This will be removed soon in favor of a more generic way to query values.\" ) def find_all_values_of_type ( self , value_type : str ) -> typing . Dict [ str , Value ]: \"\"\"Find all values of a certain type.\"\"\" result = {} for value_id in self . value_ids : value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"No value registered for: { value_id } \" ) if value_obj . type_name == value_type : # type: ignore result [ value_id ] = value_obj return result get_versions_for_alias ( self , alias ) \u00b6 Return all available versions for the specified alias. Source code in kiara/data/registry/__init__.py def get_versions_for_alias ( self , alias : str ) -> typing . Iterable [ int ]: \"\"\"Return all available versions for the specified alias.\"\"\" if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . values . keys ()) DataRegistry ( BaseDataRegistry ) \u00b6 Source code in kiara/data/registry/__init__.py class DataRegistry ( BaseDataRegistry ): def __init__ ( self , kiara : \"Kiara\" ): self . _lineages : typing . Dict [ str , typing . Optional [ ValueLineage ]] = {} super () . __init__ ( kiara = kiara ) @abc . abstractmethod def _register_value_and_data ( self , value : Value , data : typing . Any ) -> str : \"\"\"Register data into this registry. Returns the values id. In case the value already has a value set (meaning it's different from string [NO_ID_YET_MARKER][kiara.data.value.NO_ID_YET_MARKER] / '__no_id_yet__' this method must use this id or throw an exception. Otherwise, it is required that the result id is different from ``NO_ID_YET_MARKER`` and a non-empty string. \"\"\" @abc . abstractmethod def _register_remote_value ( self , value : Value ) -> Value : \"\"\"Register an existing value from a different registry into this one. Arguments: value: the original value (with the '_registry' attribute still pointing to the original registry) Returns: either None (in which case the original value object will be copied with the '_registry' and '_kiara' attributes adjusted), or a value object. \"\"\" def register_data ( self , value_data : typing . Any = SpecialValue . NOT_SET , value_schema : typing . Union [ None , ValueSchema , str ] = None , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> Value : # if lineage and not lineage.output_name: # raise Exception(f\"Value lineage for data of type '{value_data.value_schema.type}' does not have 'output_name' field set.\") value_id = str ( uuid . uuid4 ()) if value_id in self . value_ids : raise Exception ( f \"Can't register value: value id ' { value_id } ' already exists.\" ) if value_id in self . alias_names : raise Exception ( f \"Can't register value: value id ' { value_id } ' already exists as alias.\" ) if value_schema is None : if isinstance ( value_data , Value ): value_schema = value_data . value_schema else : raise Exception ( f \"No value schema provided for value: { value_data } \" ) elif isinstance ( value_schema , str ): value_schema = ValueSchema ( type = value_schema ) cls = self . _kiara . get_value_type_cls ( value_schema . type ) _type_obj = cls ( ** value_schema . type_config ) existing_value : typing . Optional [ Value ] = None if isinstance ( value_data , Value ): if value_data . id in self . value_ids and not metadata : if lineage : existing_lineage = self . _lineages . get ( value_data . id , None ) if existing_lineage : log_message ( f \"Overwriting existing value lineage for: { value_data . id } \" ) self . _lineages [ value_data . id ] = lineage # TODO: check it's really the same our_value = self . get_value_obj ( value_data . id ) if our_value is None : raise Exception ( \"Could not retrieve cloned value, this is probably a bug.\" ) return our_value existing_value = self . _find_value_for_hashes ( * value_data . get_hashes ()) if existing_value and not metadata : if lineage and self . _lineages . get ( existing_value . id , None ): log_message ( f \"Overwriting value lineage for: { existing_value . id } \" ) if lineage : self . _lineages [ existing_value . id ] = lineage return existing_value if value_data . id in self . value_ids and metadata : _metadata : typing . Optional [ typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = None if metadata : _metadata = {} for k , v in metadata . items (): _metadata [ k ] = {} _metadata [ k ][ \"metadata_item\" ] = v . dict () _metadata [ k ][ \"metadata_schema\" ] = v . schema_json () # TODO: not resolve value_data ? cloned_new_metadata = self . _create_value_obj ( value_schema = value_data . value_schema , is_set = value_data . is_set , is_none = value_data . is_none , type_obj = value_data . type_obj , metadata = _metadata , value_hashes = None , ) r = self . _register_new_value_obj ( value_obj = cloned_new_metadata , value_data = value_data . get_value_data (), lineage = lineage , # value_hashes=value_hashes, ) return r copied_value = self . _register_remote_value ( value_data ) if value_data . id not in self . value_ids : raise Exception ( f \"Value with id ' { value_data . id } ' wasn't successfully registered. This is most likely a bug.\" ) assert copied_value . _registry == self assert copied_value . _kiara == self . _kiara if value_data . id != copied_value . id : log_message ( f \"Value id for value of type { copied_value . type_name } changed when registering in other registry.\" ) raise Exception ( f \"Imported value object with id ' { value_data . id } ' resulted in copied value with different id. This is a bug.\" ) # for hash_type, value_hash in copied_value.get_hashes().items(): # self._hashes.setdefault(hash_type, {})[value_hash.hash] = value_id self . _lineages [ copied_value . id ] = value_data . get_lineage () return copied_value if value_data in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE , ] and value_schema . default not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE , ]: if metadata : raise NotImplementedError () value_data = copy . deepcopy ( value_schema . default ) if value_data not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ]: # TODO: actually implement that for hash_type in _type_obj . get_supported_hash_types (): hash_str = _type_obj . calculate_value_hash ( value = value_data , hash_type = hash_type ) existing_value = self . _find_value_for_hashes ( ValueHash ( hash_type = hash_type , hash = hash_str ) ) if existing_value : break if existing_value : if metadata : raise NotImplementedError () if lineage and self . _lineages . get ( existing_value . id , None ): log_message ( f \"Overwriting lineage for value ' { existing_value . id } '.\" ) if lineage : self . _lineages [ existing_value . id ] = lineage return existing_value if value_schema . is_constant : if ( value_data not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ] and value_data != value_schema . default ): raise NotImplementedError () value_data = value_schema . default is_set = value_data != SpecialValue . NOT_SET is_none = value_data in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ] _metadata = None if metadata : _metadata = {} for k , v in metadata . items (): _metadata [ k ] = {} _metadata [ k ][ \"metadata_item\" ] = v . dict () _metadata [ k ][ \"metadata_schema\" ] = v . schema_json () # value_hashes: typing.Dict[str] = {} value = self . _create_value_obj ( value_schema = value_schema , is_set = is_set , is_none = is_none , type_obj = _type_obj , metadata = _metadata , value_hashes = None , ) self . _register_new_value_obj ( value_obj = value , value_data = value_data , lineage = lineage , # value_hashes=value_hashes, ) # if not value.is_none: # value.get_metadata() # metadata = self._kiara.metadata_mgmt.get_value_metadata(value=value, also_return_schema=True) # print(metadata) return value def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: return self . _lineages . get ( value_id , None ) def _create_value_obj ( self , value_schema : ValueSchema , is_set : bool , is_none : bool , type_obj : typing . Optional [ ValueType ] = None , value_hashes : typing . Optional [ typing . Iterable [ ValueHash ]] = None , metadata : typing . Optional [ typing . Mapping [ str , typing . Mapping [ str , typing . Any ]] ] = None , ): # if value_schema.is_constant and value_data not in [ # SpecialValue.NO_VALUE, # SpecialValue.NOT_SET, # None, # ]: # raise Exception( # \"Can't create value. Is a constant, but value data was provided.\" # ) if type_obj is None : if value_schema . type not in self . _kiara . value_types : log_message ( f \"Value type ' { value_schema . type } ' not supported in this kiara environment, changing type to 'any'.\" ) type_cls = self . _kiara . get_value_type_cls ( \"any\" ) else : type_cls = self . _kiara . get_value_type_cls ( value_schema . type ) type_obj = type_cls ( ** value_schema . type_config ) register_token = uuid . uuid4 () self . _register_tokens . add ( register_token ) try : value = Value ( registry = self , # type: ignore value_schema = value_schema , type_obj = type_obj , # type: ignore is_set = is_set , is_none = is_none , hashes = value_hashes , metadata = metadata , register_token = register_token , # type: ignore ) return value finally : self . _register_tokens . remove ( register_token ) def _register_new_value_obj ( self , value_obj : Value , value_data : typing . Any , lineage : typing . Optional [ ValueLineage ], # value_hashes: typing.Optional[typing.Mapping[str, ValueHash]] = None, ): # assert value_obj.id == NO_ID_YET_MARKER if value_data not in [ SpecialValue . NO_VALUE , SpecialValue . NOT_SET , SpecialValue . IGNORE , None , ]: # TODO: should we keep the original value? value_data = value_obj . type_obj . import_value ( value_data ) value_id = self . _register_value_and_data ( value = value_obj , data = value_data ) if value_obj . id == NO_ID_YET_MARKER : value_obj . id = value_id if value_obj . id != value_id : raise Exception ( f \"Inconsistent id for value: { value_obj . id } != { value_id } \" ) if value_id not in self . value_ids : raise Exception ( f \"Value id ' { value_id } ' wasn't registered propertly in registry. This is most likely a bug.\" ) if lineage : self . _lineages [ value_id ] = lineage # if value_hashes is None: # value_hashes = {} # for hash_type, value_hash in value_hashes.items(): # self._hashes.setdefault(hash_type, {})[value_hash.hash] = value_id return value_obj def set_value_lineage ( self , value_item : typing . Union [ str , Value ], value_lineage : ValueLineage ): value_obj = self . get_value_obj ( value_item ) if value_obj is None : raise Exception ( f \"No value available for: { value_item } \" ) self . _lineages [ value_obj . id ] = value_lineage def link_alias ( self , value : typing . Union [ str , Value ], alias : str , register_missing_alias : bool = True , ) -> None : value_obj = self . get_value_obj ( value ) if value_obj is None : raise Exception ( f \"No value for: { value } \" ) value_slot = self . get_value_slot ( alias = alias ) if value_slot is None : if not register_missing_alias : raise Exception ( f \"Can't link value to alias ' { alias } ': alias not registered.\" ) else : self . register_alias ( value_or_schema = value_obj , alias_name = alias ) else : value_alias = ValueAlias . from_string ( alias ) if value_alias . tag : value_slot . add_value ( value_obj , tags = [ value_alias . tag ]) else : value_slot . add_value ( value_obj ) def link_aliases ( self , value : typing . Union [ str , Value ], * aliases : str , register_missing_aliases : bool = True , ) -> None : value_obj = self . get_value_obj ( value ) if value_obj is None : raise Exception ( f \"No value for: { value } \" ) if not register_missing_aliases : invalid = [] for alias in aliases : if alias not in self . alias_names : invalid . append ( alias ) if invalid : raise Exception ( f \"Can't link value(s), invalid alias(es): { ', ' . join ( invalid ) } \" ) for alias in aliases : self . link_alias ( value_obj , alias = alias ) def _check_valid_alias ( self , alias_name : str ): match = bool ( re . match ( VALID_ALIAS_PATTERN , alias_name )) return True if match else False @abc . abstractmethod def _register_alias ( self , alias_name : str , value_schema : ValueSchema ) -> ValueSlot : pass def register_aliases ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], aliases : typing . Iterable [ str ], callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> typing . Mapping [ str , ValueSlot ]: invalid = [] for alias in aliases : if not self . _check_valid_alias ( alias_name = alias ): invalid . append ( alias ) if invalid : raise Exception ( f \"Invalid alias(es), only alphanumeric characters, '-', and '_' allowed in alias name: { ', ' . join ( invalid ) } \" ) result = {} for alias in aliases : vs = self . register_alias ( value_or_schema = value_or_schema , alias_name = alias , callbacks = callbacks ) result [ alias ] = vs return result def register_alias ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], # preseed: bool = False, alias_name : typing . Optional [ str ] = None , callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> ValueSlot : \"\"\"Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Arguments: value_or_schema: a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) preseed: whether to add an empty/non-set value to the newly created slot alias_name: the alias name, will be auto-created if not provided callbacks: a list of callbacks to trigger whenever the alias is updated \"\"\" if alias_name is None : alias_name = str ( uuid . uuid4 ()) if not alias_name : raise Exception ( \"Empty alias name not allowed.\" ) match = self . _check_valid_alias ( alias_name ) if not match : raise Exception ( f \"Invalid alias ' { alias_name } ': only alphanumeric characters, '-', and '_' allowed in alias name.\" ) # if the provided value_or_schema argument is a string, it must be a value_id (for now) if isinstance ( value_or_schema , str ): if value_or_schema not in self . value_ids : raise Exception ( f \"Can't register alias ' { alias_name } ', provided string ' { value_or_schema } ' not an existing value id.\" ) _value_or_schema = self . get_value_obj ( value_or_schema ) if _value_or_schema is None : raise Exception ( f \"No value registered for: { value_or_schema } \" ) value_or_schema = _value_or_schema if isinstance ( value_or_schema , Value ): _value_schema = value_or_schema . value_schema _value : typing . Optional [ Value ] = value_or_schema elif isinstance ( value_or_schema , ValueSchema ): _value_schema = value_or_schema _value = None else : raise TypeError ( f \"Invalid value type: { type ( value_or_schema ) } \" ) if alias_name in self . _get_available_aliases (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists.\" ) elif alias_name in self . _get_available_value_ids (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists as value id.\" ) # vs = ValueSlot.from_value(id=_id, value=value_or_schema) vs = self . _register_alias ( alias_name = alias_name , value_schema = _value_schema ) # self._value_slots[vs.id] = vs # if preseed: # _v = Value(value_schema=_value_schema, kiara=self._kiara, registry=self) # vs.add_value(_v) if callbacks : vs . register_callbacks ( * callbacks ) # self.register_callbacks(vs, *callbacks) if _value is not None : vs . add_value ( _value ) return vs # def find_value_slots( # self, value_item: typing.Union[str, Value] # ) -> typing.List[ValueSlot]: # # value_item = self.get_value_obj(value_item) # result = [] # for slot_id, slot in self._value_slots.items(): # if slot.is_latest_value(value_item): # result.append(slot) # return result # def register_callbacks( # self, # alias: typing.Union[str, ValueSlot], # *callbacks: ValueSlotUpdateHandler, # ) -> None: # # _value_slot = self.get_value_slot(alias) # _value_slot.register_callbacks(*callbacks) def update_value_slot ( self , value_slot : typing . Union [ str , Value , ValueSlot ], data : typing . Any , value_lineage : typing . Optional [ ValueLineage ] = None , ) -> bool : # first, resolve a potential string into a value_slot or value if isinstance ( value_slot , str ): if value_slot in self . alias_names : _value_slot : typing . Union [ None , Value , ValueSlot , str ] = self . get_value_slot ( alias = value_slot ) elif value_slot in self . value_ids : _value_slot = self . get_value_obj ( value_slot ) else : _value_slot = value_slot if _value_slot is None : raise Exception ( f \"Can't retrieve target object for id: { value_slot } \" ) value_slot = _value_slot if isinstance ( value_slot , Value ): aliases = self . find_aliases_for_value ( value_slot . id ) # slots = self.find_value_slots(value_slot) if len ( aliases ) == 0 : raise Exception ( f \"No value slot found for value ' { value_slot . id } '.\" ) elif len ( aliases ) > 1 : raise Exception ( f \"Multiple value slots found for value ' { value_slot . id } '. This is not supported (yet).\" ) _value_slot_2 : typing . Optional [ ValueSlot ] = self . get_value_slot ( alias = aliases [ 0 ] . alias ) elif isinstance ( value_slot , ValueSlot ): _value_slot_2 = value_slot else : raise TypeError ( f \"Invalid type for value slot: { type ( value_slot ) } \" ) assert _value_slot_2 is not None if isinstance ( data , Value ): if value_lineage : raise Exception ( \"Can't update value slot with new value lineage data.\" ) _value : Value = data else : _value = self . register_data ( value_data = data , value_schema = value_slot . value_schema , lineage = value_lineage , ) # _value = self.create_value( # value_data=data, # value_schema=_value_slot.value_schema, # value_lineage=value_lineage, # ) return self . _update_value_slot ( value_slot = _value_slot_2 , new_value = _value , trigger_callbacks = True ) def update_value_slots ( self , updated_values : typing . Mapping [ typing . Union [ str , ValueSlot ], typing . Any ], metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> typing . Mapping [ ValueSlot , typing . Union [ bool , Exception ]]: updated : typing . Dict [ str , typing . List [ ValueSlot ]] = {} cb_map : typing . Dict [ str , ValueSlotUpdateHandler ] = {} result : typing . Dict [ ValueSlot , typing . Union [ bool , Exception ]] = {} invalid : typing . Set [ str ] = set () for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id _alias = self . get_value_slot ( alias ) if _alias is None : if isinstance ( alias , ValueSlot ): invalid . add ( alias . id ) else : invalid . add ( alias ) if invalid : raise Exception ( f \"Can't update value slots: invalid alias name(s) ' { ', ' . join ( invalid ) } '.\" ) for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id value_slot = self . get_value_slot ( alias ) if value_slot is None : raise Exception ( f \"Can't retrieve value slot for alias: { alias } \" ) for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id value_slot = self . get_value_slot ( alias ) if value_slot is None : raise Exception ( f \"No value slot for alias: { alias } .\" ) try : if isinstance ( value_item , Value ): _value_item : Value = value_item if _value_item . _registry != self : _value_item = self . register_data ( _value_item , metadata = metadata , lineage = lineage ) elif metadata : _value_item = self . register_data ( _value_item , metadata = metadata , lineage = lineage ) else : _value_item = self . register_data ( value_data = value_item , value_schema = value_slot . value_schema , metadata = metadata , lineage = lineage , ) updated_item = self . _update_value_slot ( value_slot = value_slot , new_value = _value_item , trigger_callbacks = False , ) result [ value_slot ] = updated_item if updated_item : for cb_id , cb in value_slot . _callbacks . items (): cb_map [ cb_id ] = cb updated . setdefault ( cb_id , []) . append ( value_slot ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () result [ value_slot ] = e for cb_id , value_slots in updated . items (): cb = cb_map [ cb_id ] cb . values_updated ( * value_slots ) return result def _update_value_slot ( self , value_slot : ValueSlot , new_value : Value , trigger_callbacks : bool = True ) -> bool : last_version = value_slot . latest_version_nr new_version = value_slot . add_value ( new_value , trigger_callbacks = trigger_callbacks ) updated = last_version != new_version return updated register_alias ( self , value_or_schema , alias_name = None , callbacks = None ) \u00b6 Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Parameters: Name Type Description Default value_or_schema Union[kiara.data.values.Value, kiara.data.values.ValueSchema, str] a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) required preseed whether to add an empty/non-set value to the newly created slot required alias_name Optional[str] the alias name, will be auto-created if not provided None callbacks Optional[Iterable[kiara.data.registry.ValueSlotUpdateHandler]] a list of callbacks to trigger whenever the alias is updated None Source code in kiara/data/registry/__init__.py def register_alias ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], # preseed: bool = False, alias_name : typing . Optional [ str ] = None , callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> ValueSlot : \"\"\"Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Arguments: value_or_schema: a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) preseed: whether to add an empty/non-set value to the newly created slot alias_name: the alias name, will be auto-created if not provided callbacks: a list of callbacks to trigger whenever the alias is updated \"\"\" if alias_name is None : alias_name = str ( uuid . uuid4 ()) if not alias_name : raise Exception ( \"Empty alias name not allowed.\" ) match = self . _check_valid_alias ( alias_name ) if not match : raise Exception ( f \"Invalid alias ' { alias_name } ': only alphanumeric characters, '-', and '_' allowed in alias name.\" ) # if the provided value_or_schema argument is a string, it must be a value_id (for now) if isinstance ( value_or_schema , str ): if value_or_schema not in self . value_ids : raise Exception ( f \"Can't register alias ' { alias_name } ', provided string ' { value_or_schema } ' not an existing value id.\" ) _value_or_schema = self . get_value_obj ( value_or_schema ) if _value_or_schema is None : raise Exception ( f \"No value registered for: { value_or_schema } \" ) value_or_schema = _value_or_schema if isinstance ( value_or_schema , Value ): _value_schema = value_or_schema . value_schema _value : typing . Optional [ Value ] = value_or_schema elif isinstance ( value_or_schema , ValueSchema ): _value_schema = value_or_schema _value = None else : raise TypeError ( f \"Invalid value type: { type ( value_or_schema ) } \" ) if alias_name in self . _get_available_aliases (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists.\" ) elif alias_name in self . _get_available_value_ids (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists as value id.\" ) # vs = ValueSlot.from_value(id=_id, value=value_or_schema) vs = self . _register_alias ( alias_name = alias_name , value_schema = _value_schema ) # self._value_slots[vs.id] = vs # if preseed: # _v = Value(value_schema=_value_schema, kiara=self._kiara, registry=self) # vs.add_value(_v) if callbacks : vs . register_callbacks ( * callbacks ) # self.register_callbacks(vs, *callbacks) if _value is not None : vs . add_value ( _value ) return vs ValueSlotUpdateHandler ( Protocol ) \u00b6 The call signature for callbacks that can be registered as value update handlers. Source code in kiara/data/registry/__init__.py class ValueSlotUpdateHandler ( Protocol ): \"\"\"The call signature for callbacks that can be registered as value update handlers.\"\"\" def values_updated ( self , * items : \"ValueSlot\" ) -> typing . Any : ... store \u00b6 LocalDataStore ( DataRegistry ) \u00b6 An implementation of a kiara data registry that stores data locally, so it can be accessed across sessions. Data is stored under the value for the configured 'base_path' attribute, along with LoadConfig data that describes how the data can be re-loaded into memory. Source code in kiara/data/registry/store.py class LocalDataStore ( DataRegistry ): \"\"\"An implementation of a *kiara* data registry that stores data locally, so it can be accessed across sessions. Data is stored under the value for the configured 'base_path' attribute, along with [LoadConfig][kiara.metadata.data.LoadConfig] data that describes how the data can be re-loaded into memory. \"\"\" def __init__ ( self , kiara : \"Kiara\" , base_path : typing . Union [ str , Path ]): if isinstance ( base_path , str ): base_path = Path ( base_path ) self . _base_path : Path = base_path self . _value_obj_cache : typing . Dict [ str , Value ] = {} self . _value_info_cache : typing . Dict [ str , SavedValueInfo ] = {} self . _data_cache : typing . Dict [ str , Value ] = {} self . _metadata_cache : typing . Dict [ str , typing . Any ] = {} self . _value_slots : typing . Dict [ str , ValueSlot ] = {} super () . __init__ ( kiara = kiara ) @property def base_path ( self ) -> Path : return self . _base_path def _get_saved_value_info ( self , value_id : str ) -> SavedValueInfo : if value_id in self . _value_info_cache . keys (): return self . _value_info_cache [ value_id ] metadata_path = self . get_metadata_path ( value_id = value_id ) if not metadata_path . exists (): raise Exception ( f \"No value with id ' { value_id } ' registered.\" ) info_content = metadata_path . read_text () info_dict = json . loads ( info_content ) value_info = SavedValueInfo ( ** info_dict ) assert value_info . value_id == value_id self . _value_info_cache [ value_id ] = value_info return value_info def _get_value_obj_for_id ( self , value_id : str ) -> Value : if value_id in self . _value_obj_cache . keys (): return self . _value_obj_cache [ value_id ] value_info : SavedValueInfo = self . _get_saved_value_info ( value_id ) value_schema = value_info . value_schema # value_schema = ValueSchema( # type=value_info.value_type, type_config=value_info.value_type_config # ) value_obj = self . _create_value_obj ( value_schema = value_schema , is_set = True , is_none = False , value_hashes = value_info . hashes , metadata = value_info . metadata , ) value_obj . id = value_info . value_id self . _register_new_value_obj ( value_obj = value_obj , value_data = SpecialValue . IGNORE , lineage = value_info . lineage , # value_hashes=value_info.hashes, ) self . _value_obj_cache [ value_obj . id ] = value_obj return value_obj def _get_value_data_for_id ( self , value_id : str ) -> typing . Any : if value_id in self . _data_cache . keys (): return self . _data_cache [ value_id ] . get_value_data () # # value_obj = self.get_value_obj(value_item=value_id) # metadata_path = self.get_metadata_path(value_id=value_id) # if not metadata_path.exists(): # raise Exception(f\"No value with id '{value_id}' registered.\") # # info_content = metadata_path.read_text() # info_dict = json.loads(info_content) # value_info = SavedValueInfo(**info_dict) value_info = self . _get_saved_value_info ( value_id = value_id ) assert value_info . value_id == value_id load_config = value_info . load_config value : Value = self . _kiara . run ( ** load_config . dict ( exclude = { \"value_id\" , \"base_path_input_name\" , \"doc\" })) # type: ignore self . _data_cache [ value_id ] = value return value . get_value_data () def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: return self . _get_saved_value_info ( value_id ) . lineage def _register_value_and_data ( self , value : Value , data : typing . Any ) -> str : if data == SpecialValue . IGNORE : return value . id raise NotImplementedError () # # new_value_id = str(uuid.uuid4()) # # value.id = new_value_id # value_type = value.type_name # # # run the type and repo type specific save module, and retrieve the load_config that is needed to retrieve it again later # save_config = self._prepare_save_config( # value_id=new_value_id, value_type=value_type, value=value # ) # # save_module = save_config.create_module(kiara=self._kiara) # result = save_module.run(**save_config.inputs) # load_config_value: Value = result.get_value_obj(\"load_config\") # # # assemble the value metadata # metadata: typing.Dict[str, typing.Mapping[str, typing.Any]] = dict( # value.get_metadata(also_return_schema=True) # ) def _register_remote_value ( self , value : Value ) -> Value : value_metadata_path = self . get_metadata_path ( value . id ) if value_metadata_path . exists (): raise Exception ( f \"Can't register remote value with id ' { value . id } '. Load config for this id already exists: { value_metadata_path . as_posix () } \" ) value_type = value . type_name # run the type and repo type specific save module, and retrieve the load_config that is needed to retrieve it again later save_config = self . _prepare_save_config ( value_id = value . id , value_type = value_type , value = value ) save_module = save_config . create_module ( kiara = self . _kiara ) result = save_module . run ( ** save_config . inputs ) load_config_value : Value = result . get_value_obj ( \"load_config\" ) field_name = value_type if field_name == \"any\" : field_name = \"value_item\" saved_value : Value = result . get_value_obj ( field_name ) load_config_data = load_config_value . get_value_data () metadata = saved_value . get_metadata ( also_return_schema = True ) value_info = SavedValueInfo ( value_id = value . id , value_schema = saved_value . value_schema , is_valid = saved_value . item_is_valid (), hashes = saved_value . get_hashes (), lineage = value . get_lineage (), save_lineage = saved_value . get_lineage (), metadata = metadata , load_config = load_config_data , ) value_metadata_path . parent . mkdir ( parents = True , exist_ok = True ) value_metadata_path . write_text ( value_info . json ()) copied_value = saved_value . copy () copied_value . _registry = self copied_value . _kiara = self . _kiara if copied_value . id != value . id : log_message ( f \"Saved value id different to original id ' { value . id } .\" ) copied_value . id = value . id self . _value_obj_cache [ copied_value . id ] = copied_value return copied_value def _prepare_save_config ( self , value_id : str , value_type : str , value : \"Value\" ) -> SaveConfig : store_operations : StoreOperationType = self . _kiara . operation_mgmt . get_operations ( \"store_value\" ) # type: ignore op_config = store_operations . get_store_operation_for_type ( value_type ) if not op_config : raise Exception ( f \"Can't save value: no save operation found for value type ' { value_type } '\" ) target_path = self . get_data_path ( value_id = value_id ) metadata_path = self . get_metadata_path ( value_id = value_id ) if os . path . exists ( metadata_path ): raise Exception ( f \"Can't save value, metadata file already exists: { metadata_path } \" ) input_name = value . type_name if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : value . get_value_data (), \"base_path\" : target_path . as_posix (), \"value_id\" : value_id , } # type: ignore save_config = SaveConfig ( module_type = op_config . module_type , module_config = op_config . module_config , inputs = inputs , load_config_output = \"load_config\" , ) return save_config def get_data_path ( self , value_id : str ) -> Path : return self . _base_path / f \"value_ { value_id } \" / \"data\" def get_metadata_path ( self , value_id : str ) -> Path : path = self . _base_path / f \"value_ { value_id } \" / \"value_metadata.json\" return path # def get_load_config_path(self, value_id: str) -> Path: # path = self._base_path / f\"value_{value_id}\" / \"load_config.json\" # return path def _get_value_slot_for_alias ( self , alias_name : str ) -> ValueSlot : if alias_name in self . _value_slots . keys (): return self . _value_slots [ alias_name ] alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" if not alias_file . is_file (): raise Exception ( f \"No alias with name ' { alias_name } ' registered.\" ) file_content = alias_file . read_text () alias_data = json . loads ( file_content ) value_schema = ValueSchema ( ** alias_data [ \"value_schema\" ]) value_slot = ValueSlot ( id = alias_name , value_schema = value_schema , kiara = self . _kiara , registry = self ) value_slot . register_callbacks ( self ) for version in sorted (( int ( x ) for x in alias_data [ \"versions\" ] . keys ())): value_id = alias_data [ \"versions\" ][ str ( version )] value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"Can't find value with id: { value_id } \" ) tags : typing . Optional [ typing . Iterable [ str ]] = None if value_id in alias_data [ \"tags\" ] . values (): tags = [ tag for tag , version in alias_data [ \"tags\" ] . items () if version == version ] new_version = value_slot . add_value ( value_obj , trigger_callbacks = False , tags = tags ) if new_version != int ( version ): raise Exception ( f \"New version different to stored version ( { new_version } != { version } ). This is a bug.\" ) self . _value_slots [ alias_name ] = value_slot return self . _value_slots [ alias_name ] def _get_available_aliases ( self ) -> typing . Iterable [ str ]: alias_path = self . _base_path / \"aliases\" alias_file = alias_path . glob ( \"alias_*.json\" ) result = [] for af in alias_file : if not af . is_file (): log_message ( f \"Ignoring non-file: { af . as_posix () } \" ) continue alias_name = af . name [ 6 : - 5 ] result . append ( alias_name ) return result def _get_available_value_ids ( self ) -> typing . Iterable [ str ]: value_ids = [ x . parent . name [ 6 :] for x in self . _base_path . glob ( \"value_*/value_metadata.json\" ) ] return value_ids def _register_alias ( self , alias_name : str , value_schema : ValueSchema ) -> ValueSlot : alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" if alias_file . exists (): raise Exception ( f \"Alias ' { alias_name } ' already registered.\" ) if alias_name in self . _value_slots . keys (): raise Exception ( f \"Value slot for alias ' { alias_name } ' already exists.\" ) alias_file . parent . mkdir ( parents = True , exist_ok = True ) vs = ValueSlot ( id = alias_name , value_schema = value_schema , kiara = self . _kiara , registry = self ) self . _value_slots [ alias_name ] = vs self . _write_alias_file ( alias_name ) vs . register_callbacks ( self ) return vs def _write_alias_file ( self , alias_name ): alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" value_slot = self . _value_slots [ alias_name ] alias_dict = { \"value_schema\" : value_slot . value_schema . dict (), \"versions\" : {}, \"tags\" : {}, } for version , value in value_slot . values . items (): alias_dict [ \"versions\" ][ version ] = value . id for tag_name , version in value_slot . tags . items (): alias_dict [ \"tags\" ][ tag_name ] = version alias_file . write_text ( json . dumps ( alias_dict )) def values_updated ( self , * items : \"ValueSlot\" ): invalid = [] for item in items : if item . id not in self . _value_slots . keys (): invalid . append ( item . id ) if invalid : raise Exception ( f \"Can't update value(s), invalid aliases: { ', ' . join ( invalid ) } \" ) for item in items : self . _write_alias_file ( alias_name = item . id ) SavedValueInfo ( ValueInfo ) pydantic-model \u00b6 Source code in kiara/data/registry/store.py class SavedValueInfo ( ValueInfo ): load_config : LoadConfig = Field ( description = \"The configuration to load this value from disk (or however it is stored).\" ) save_lineage : ValueLineage = Field ( description = \"Information about how the value was saved.\" ) load_config : LoadConfig pydantic-field required \u00b6 The configuration to load this value from disk (or however it is stored). save_lineage : ValueLineage pydantic-field required \u00b6 Information about how the value was saved. types special \u00b6 This is the base module that contains everything data type-related in kiara . I'm still not 100% sure how to best implement the kiara type system, there are several ways it could be done, for example based on Python type-hints, using JSON-schema, Avro (which is my 2nd favourite option), as well as by implementing a custom type-class hierarchy. Which is what I have choosen to try first. For now, it looks like it'll work out, but there is a chance requirements I haven't forseen will crop up that could make this become ugly. Anyway, the way it works (for now) is that kiara comes with a set of often used types (the standard set of: scalars, list, dict, table & array, etc.) which each come with 2 functions that can serialize and deserialize values of that type in a persistant fashion -- which could be storing as a file on disk, or as a cell/row in a database. Those functions will most likley be kiara modules themselves, with even more restricted input/output type options. In addition, packages that contain modules can implement their own, custom types, if suitable ones are not available in core- kiara . Those can either be 'serialized/deserialized' into kiara -native types (which in turn will serialize them using their own serializing functions), or will have to implement custom serializing functionality (which will probably be discouraged, since this might not be trivial and there are quite a few things to consider). ValueType ( ABC , Generic ) \u00b6 Base class that all kiara types must inherit from. kiara types have 3 main responsibilities: serialize into / deserialize from persistent state data validation metadata extraction Serializing being the arguably most important of those, because without most of the data management features of kiara would be impossible. Validation should not require any explanation. Metadata extraction is important, because that metadata will be available to other components of kiara (or frontends for it), without them having to request the actual data. That will hopefully make kiara very efficient in terms of memory management, as well as data transfer and I/O. Ideally, the actual data (bytes) will only be requested at the last possible moment. For example when a module needs the input data to do processing on it -- and even then it might be that it only requests a part of the data, say a single column of a table. Or when a frontend needs to display/visualize the data. Source code in kiara/data/types/__init__.py class ValueType ( abc . ABC , typing . Generic [ TYPE_PYTHON_CLS , TYPE_CONFIG_CLS ]): \"\"\"Base class that all *kiara* types must inherit from. *kiara* types have 3 main responsibilities: - serialize into / deserialize from persistent state - data validation - metadata extraction Serializing being the arguably most important of those, because without most of the data management features of *kiara* would be impossible. Validation should not require any explanation. Metadata extraction is important, because that metadata will be available to other components of *kiara* (or frontends for it), without them having to request the actual data. That will hopefully make *kiara* very efficient in terms of memory management, as well as data transfer and I/O. Ideally, the actual data (bytes) will only be requested at the last possible moment. For example when a module needs the input data to do processing on it -- and even then it might be that it only requests a part of the data, say a single column of a table. Or when a frontend needs to display/visualize the data. \"\"\" @classmethod def get_type_metadata ( cls ) -> ValueTypeMetadata : return ValueTypeMetadata . from_value_type_class ( cls ) # @classmethod # def doc(cls) -> str: # # return extract_doc_from_cls(cls) # # @classmethod # def desc(cls) -> str: # return extract_doc_from_cls(cls, only_first_line=True) # @classmethod # def conversions( # self, # ) -> typing.Optional[typing.Mapping[str, typing.Mapping[str, typing.Any]]]: # \"\"\"Return a dictionary of configuration for modules that can transform this type. # # The name of the transformation is the key of the result dictionary, the configuration is a module configuration # (dictionary wth 'module_type' and optional 'module_config', 'input_name' and 'output_name' keys). # \"\"\" # # return {\"string\": {\"module_type\": \"string.pretty_print\", \"input_name\": \"item\"}} @classmethod def check_data ( cls , data : typing . Any ) -> typing . Optional [ \"ValueType\" ]: \"\"\"Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). \"\"\" return None @classmethod @abc . abstractmethod def backing_python_type ( cls ) -> typing . Type [ TYPE_PYTHON_CLS ]: pass @classmethod @abc . abstractmethod def type_config_cls ( cls ) -> typing . Type [ TYPE_CONFIG_CLS ]: pass @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return None @classmethod def get_supported_hash_types ( cls ) -> typing . Iterable [ str ]: return [] @classmethod def calculate_value_hash ( cls , value : typing . Any , hash_type : str ) -> str : \"\"\"Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. \"\"\" raise Exception ( f \"Value type ' { cls . _value_type_name } ' does not support hash calculation.\" ) # type: ignore def __init__ ( self , ** type_config : typing . Any ): try : self . _type_config : TYPE_CONFIG_CLS = self . __class__ . type_config_cls ( ** type_config ) # type: ignore # TODO: double-check this is only a mypy issue except ValidationError as ve : raise ValueTypeConfigException ( f \"Error creating object for value_type: { ve } \" , self . __class__ , type_config , ve , ) # self._type_config: typing.Mapping[str, typing.Any] = self # self._transformations: typing.Optional[ # typing.Mapping[str, typing.Mapping[str, typing.Any]] # ] = None @property def type_config ( self ) -> TYPE_CONFIG_CLS : return self . _type_config def import_value ( self , value : typing . Any ) -> typing . Any : assert value is not None try : parsed = self . parse_value ( value ) if parsed is None : parsed = value self . validate ( parsed ) except Exception as e : raise KiaraValueException ( value_type = self . __class__ , value_data = value , exception = e ) return parsed def parse_value ( self , value : typing . Any ) -> typing . Any : \"\"\"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Arguments: v: the value Returns: 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object \"\"\" return None def validate ( cls , value : typing . Any ) -> None : \"\"\"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type).\"\"\" def get_type_hint ( self , context : str = \"python\" ) -> typing . Optional [ typing . Type ]: \"\"\"Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. \"\"\" return None def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : raise NotImplementedError () calculate_value_hash ( value , hash_type ) classmethod \u00b6 Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. Source code in kiara/data/types/__init__.py @classmethod def calculate_value_hash ( cls , value : typing . Any , hash_type : str ) -> str : \"\"\"Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. \"\"\" raise Exception ( f \"Value type ' { cls . _value_type_name } ' does not support hash calculation.\" ) # type: ignore check_data ( data ) classmethod \u00b6 Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). Source code in kiara/data/types/__init__.py @classmethod def check_data ( cls , data : typing . Any ) -> typing . Optional [ \"ValueType\" ]: \"\"\"Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). \"\"\" return None get_type_hint ( self , context = 'python' ) \u00b6 Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. Source code in kiara/data/types/__init__.py def get_type_hint ( self , context : str = \"python\" ) -> typing . Optional [ typing . Type ]: \"\"\"Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. \"\"\" return None parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/__init__.py def parse_value ( self , value : typing . Any ) -> typing . Any : \"\"\"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Arguments: v: the value Returns: 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object \"\"\" return None validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/__init__.py def validate ( cls , value : typing . Any ) -> None : \"\"\"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type).\"\"\" ValueTypeConfigMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/data/types/__init__.py class ValueTypeConfigMetadata ( MetadataModel ): @classmethod def from_config_class ( cls , config_cls : typing . Type [ ValueTypeConfigSchema ], ): flat_models = get_flat_models_from_model ( config_cls ) model_name_map = get_model_name_map ( flat_models ) m_schema , _ , _ = model_process_schema ( config_cls , model_name_map = model_name_map ) fields = m_schema [ \"properties\" ] config_values = {} for field_name , details in fields . items (): type_str = \"-- n/a --\" if \"type\" in details . keys (): type_str = details [ \"type\" ] desc = details . get ( \"description\" , DEFAULT_NO_DESC_VALUE ) default = config_cls . __fields__ [ field_name ] . default if default is None : if callable ( config_cls . __fields__ [ field_name ] . default_factory ): default = config_cls . __fields__ [ field_name ] . default_factory () # type: ignore req = config_cls . __fields__ [ field_name ] . required config_values [ field_name ] = ValueTypeAndDescription ( description = desc , type = type_str , value_default = default , required = req ) python_cls = PythonClassMetadata . from_class ( config_cls ) return ValueTypeConfigMetadata ( python_class = python_cls , config_values = config_values ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this configuration.\" ) config_values : typing . Dict [ str , ValueTypeAndDescription ] = Field ( description = \"The available configuration values.\" ) config_values : Dict [ str , kiara . metadata . ValueTypeAndDescription ] pydantic-field required \u00b6 The available configuration values. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this configuration. ValueTypeConfigSchema ( BaseModel ) pydantic-model \u00b6 Base class that describes the configuration a ValueType class accepts. This is stored in the _config_cls class attribute in each ValueType class. By default, a ValueType is not configurable, unless the _config_cls class attribute points to a sub-class of this class. Source code in kiara/data/types/__init__.py class ValueTypeConfigSchema ( BaseModel ): \"\"\"Base class that describes the configuration a [``ValueType``][kiara.data.types.ValueType] class accepts. This is stored in the ``_config_cls`` class attribute in each ``ValueType`` class. By default, a ``ValueType`` is not configurable, unless the ``_config_cls`` class attribute points to a sub-class of this class. \"\"\" @classmethod def requires_config ( cls ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : return True return False _config_hash : str = PrivateAttr ( default = None ) class Config : extra = Extra . forbid allow_mutation = False def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) @property def config_hash ( self ): if self . _config_hash is None : _d = self . dict () hashes = deepdiff . DeepHash ( _d ) self . _config_hash = hashes [ _d ] return self . _config_hash def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () def __hash__ ( self ): return hash ( self . config_hash ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : my_table = Table ( box = box . MINIMAL , show_header = False ) my_table . add_column ( \"Field name\" , style = \"i\" ) my_table . add_column ( \"Value\" ) for field in self . __fields__ : my_table . add_row ( field , getattr ( self , field )) yield my_table __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/types/__init__.py def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/types/__init__.py def __hash__ ( self ): return hash ( self . config_hash ) get ( self , key ) \u00b6 Get the value for the specified configuation key. Source code in kiara/data/types/__init__.py def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) requires_config () classmethod \u00b6 Return whether this class can be used as-is, or requires configuration before an instance can be created. Source code in kiara/data/types/__init__.py @classmethod def requires_config ( cls ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : return True return False get_type_name ( obj ) \u00b6 Utility function to get a pretty string from the class of an object. Source code in kiara/data/types/__init__.py def get_type_name ( obj : typing . Any ): \"\"\"Utility function to get a pretty string from the class of an object.\"\"\" if obj . __class__ . __module__ == \"builtins\" : return obj . __class__ . __name__ else : return f \" { obj . __class__ . __module__ } . { obj . __class__ . __name__ } \" core \u00b6 AnyType ( ValueType ) \u00b6 Any type / No type information. Source code in kiara/data/types/core.py class AnyType ( ValueType [ object , ValueTypeConfigSchema ]): \"\"\"Any type / No type information.\"\"\" _value_type_name = \"any\" @classmethod def backing_python_type ( cls ) -> typing . Type : return object @classmethod def type_config_cls ( cls ) -> typing . Type [ ValueTypeConfigSchema ]: return ValueTypeConfigSchema def pretty_print_as_renderables ( self , value : \"Value\" , print_config : typing . Mapping [ str , typing . Any ] ) -> typing . Any : data = value . get_value_data () return [ str ( data )] DeserializeConfigData ( KiaraInternalValueType ) \u00b6 A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. Source code in kiara/data/types/core.py class DeserializeConfigData ( KiaraInternalValueType ): \"\"\"A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. \"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , DeserializeConfig ] @classmethod def type_name ( cls ): return \"deserialize_config\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = DeserializeConfig ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , DeserializeConfig ): raise Exception ( f \"Invalid type for deserialize config: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = DeserializeConfig ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , DeserializeConfig ): raise Exception ( f \"Invalid type for deserialize config: { type ( value ) } .\" ) LoadConfigData ( KiaraInternalValueType ) \u00b6 A dictionary representing load config for a kiara value. The load config schema itself can be looked up via the wrapper class ' LoadConfig '. Source code in kiara/data/types/core.py class LoadConfigData ( KiaraInternalValueType ): \"\"\"A dictionary representing load config for a *kiara* value. The load config schema itself can be looked up via the wrapper class '[LoadConfig][kiara.data.persistence.LoadConfig]'. \"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , LoadConfig ] @classmethod def type_name ( cls ): return \"load_config\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = LoadConfig ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , LoadConfig ): raise Exception ( f \"Invalid type for load config: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = LoadConfig ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , LoadConfig ): raise Exception ( f \"Invalid type for load config: { type ( value ) } .\" ) ValueInfoData ( KiaraInternalValueType ) \u00b6 A dictionary representing a kiara ValueInfo object. Source code in kiara/data/types/core.py class ValueInfoData ( KiaraInternalValueType ): \"\"\"A dictionary representing a kiara ValueInfo object.\"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , ValueInfo ] @classmethod def type_name ( cls ): return \"value_info\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueInfo ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueInfo ): raise Exception ( f \"Invalid type for value info: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueInfo ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueInfo ): raise Exception ( f \"Invalid type for value info: { type ( value ) } .\" ) ValueLineageData ( KiaraInternalValueType ) \u00b6 A dictionary representing a kiara ValueLineage. Source code in kiara/data/types/core.py class ValueLineageData ( KiaraInternalValueType ): \"\"\"A dictionary representing a kiara ValueLineage.\"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , ValueLineage ] @classmethod def type_name ( cls ): return \"value_lineage\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueLineage ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueLineage ): raise Exception ( f \"Invalid type for value seed: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueLineage ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueLineage ): raise Exception ( f \"Invalid type for value seed: { type ( value ) } .\" ) type_mgmt \u00b6 TypeMgmt \u00b6 Source code in kiara/data/types/type_mgmt.py class TypeMgmt ( object ): def __init__ ( self , kiara : \"Kiara\" ): self . _kiara : Kiara = kiara self . _value_types : typing . Optional [ bidict [ str , typing . Type [ ValueType ]]] = None self . _value_type_transformations : typing . Dict [ str , typing . Dict [ str , typing . Any ] ] = {} self . _registered_python_classes : typing . Dict [ typing . Type , typing . List [ str ]] = None # type: ignore self . _type_hierarchy : typing . Optional [ nx . DiGraph ] = None def invalidate_types ( self ): self . _value_types = None self . _value_type_transformations . clear () self . _registered_python_classes = None @property def value_types ( self ) -> bidict [ str , typing . Type [ ValueType ]]: if self . _value_types is not None : return self . _value_types self . _value_types = bidict ( find_all_value_types ()) return self . _value_types @property def value_type_hierarchy ( self ) -> \"nx.DiGraph\" : if self . _type_hierarchy is not None : return self . _type_hierarchy def recursive_base_find ( cls : typing . Type , current : typing . Optional [ typing . List [ str ]] = None ): if current is None : current = [] for base in cls . __bases__ : if base in self . value_types . values (): current . append ( self . value_types . inverse [ base ]) recursive_base_find ( base , current = current ) return current bases = {} for name , cls in self . value_types . items (): bases [ name ] = recursive_base_find ( cls ) import networkx as nx hierarchy = nx . DiGraph () for name , _bases in bases . items (): hierarchy . add_node ( name , cls = self . value_types [ name ]) if not _bases : continue # we only need the first parent, all others will be taken care of by the parent of the parent hierarchy . add_edge ( _bases [ 0 ], name ) self . _type_hierarchy = hierarchy return self . _type_hierarchy def get_type_lineage ( self , value_type : str ) -> typing . Iterable [ str ]: \"\"\"Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type.\"\"\" if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx path = nx . shortest_path ( self . value_type_hierarchy , \"any\" , value_type ) return reversed ( path ) def get_sub_types ( self , value_type : str ) -> typing . Set [ str ]: if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx desc = nx . descendants ( self . value_type_hierarchy , value_type ) return desc @property def value_type_names ( self ) -> typing . List [ str ]: return list ( self . value_types . keys ()) @property def registered_python_classes ( self , ) -> typing . Mapping [ typing . Type , typing . Iterable [ str ]]: if self . _registered_python_classes is not None : return self . _registered_python_classes registered_types = {} for name , v_type in self . value_types . items (): rel = v_type . candidate_python_types () if rel : for cls in rel : registered_types . setdefault ( cls , []) . append ( name ) self . _registered_python_classes = registered_types return self . _registered_python_classes def get_type_config_for_data_profile ( self , profile_name : str ) -> typing . Mapping [ str , typing . Any ]: type_name = TYPE_PROFILE_MAP [ profile_name ] return { \"type\" : type_name , \"type_config\" : {}} def determine_type ( self , data : typing . Any ) -> typing . Optional [ ValueType ]: if isinstance ( data , Value ): data = data . get_value_data () result : typing . List [ ValueType ] = [] registered_types = set ( self . registered_python_classes . get ( data . __class__ , [])) for cls in data . __class__ . __bases__ : reg = self . registered_python_classes . get ( cls ) if reg : registered_types . update ( reg ) if registered_types : for rt in registered_types : _cls : typing . Type [ ValueType ] = self . get_value_type_cls ( rt ) match = _cls . check_data ( data ) if match : result . append ( match ) # TODO: re-run all checks on all modules, not just the ones that registered interest in the class if len ( result ) == 0 : return None elif len ( result ) > 1 : result_str = [ x . _value_type_name for x in result ] # type: ignore raise Exception ( f \"Multiple value types found for value: { ', ' . join ( result_str ) } .\" ) else : return result [ 0 ] def get_value_type_cls ( self , type_name : str ) -> typing . Type [ ValueType ]: t = self . value_types . get ( type_name , None ) if t is None : raise Exception ( f \"No value type ' { type_name } ', available types: { ', ' . join ( self . value_types . keys ()) } \" ) return t # def get_value_type_transformations( # self, value_type_name: str # ) -> typing.Mapping[str, typing.Mapping[str, typing.Any]]: # \"\"\"Return available transform pipelines for value types.\"\"\" # # if value_type_name in self._value_type_transformations.keys(): # return self._value_type_transformations[value_type_name] # # type_cls = self.get_value_type_cls(type_name=value_type_name) # _configs = type_cls.conversions() # if _configs is None: # module_configs = {} # else: # module_configs = dict(_configs) # for base in type_cls.__bases__: # if hasattr(base, \"conversions\"): # _b_configs = base.conversions() # type: ignore # if not _b_configs: # continue # for k, v in _b_configs.items(): # if k not in module_configs.keys(): # module_configs[k] = v # # # TODO: check input type compatibility? # result: typing.Dict[str, typing.Dict[str, typing.Any]] = {} # for name, config in module_configs.items(): # config = dict(config) # module_type = config.pop(\"module_type\", None) # if not module_type: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}', no module type specified in config: {config}\" # ) # module_config = config.pop(\"module_config\", {}) # module = self._kiara.create_module( # id=f\"_transform_{value_type_name}_{name}\", # module_type=module_type, # module_config=module_config, # ) # # if \"input_name\" not in config.keys(): # # if len(module.input_schemas) == 1: # config[\"input_name\"] = next(iter(module.input_schemas.keys())) # else: # required_inputs = [ # inp # for inp, schema in module.input_schemas.items() # if schema.is_required() # ] # if len(required_inputs) == 1: # config[\"input_name\"] = required_inputs[0] # else: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}': can't determine input name between those options: '{', '.join(required_inputs)}'\" # ) # # if \"output_name\" not in config.keys(): # # if len(module.input_schemas) == 1: # config[\"output_name\"] = next(iter(module.output_schemas.keys())) # else: # required_outputs = [ # inp # for inp, schema in module.output_schemas.items() # if schema.is_required() # ] # if len(required_outputs) == 1: # config[\"output_name\"] = required_outputs[0] # else: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}': can't determine output name between those options: '{', '.join(required_outputs)}'\" # ) # # result[name] = { # \"module\": module, # \"module_type\": module_type, # \"module_config\": module_config, # \"transformation_config\": config, # } # # self._value_type_transformations[value_type_name] = result # return self._value_type_transformations[value_type_name] def find_value_types_for_package ( self , package_name : str ) -> typing . Dict [ str , typing . Type [ ValueType ]]: result = {} for value_type_name , value_type in self . value_types . items (): value_md = value_type . get_type_metadata () package = value_md . context . labels . get ( \"package\" ) if package == package_name : result [ value_type_name ] = value_type return result # def get_available_transformations_for_type( # self, value_type_name: str # ) -> typing.Iterable[str]: # # return self.get_value_type_transformations(value_type_name=value_type_name) # def transform_value( # self, # transformation_alias: str, # value: Value, # other_inputs: typing.Optional[typing.Mapping[str, typing.Any]] = None, # ) -> Value: # # transformations = self.get_value_type_transformations(value.value_schema.type) # # if transformation_alias not in transformations.keys(): # raise Exception( # f\"Can't transform value of type '{value.value_schema.type}', transformation '{transformation_alias}' not available for this type. Available: {', '.join(transformations.keys())}\" # ) # # config = transformations[transformation_alias] # # transformation_config = config[\"transformation_config\"] # input_name = transformation_config[\"input_name\"] # # module: KiaraModule = config[\"module\"] # # constants = module.get_config_value(\"constants\") # inputs = dict(constants) # # if other_inputs: # # for k, v in other_inputs.items(): # if k in constants.keys(): # raise Exception(f\"Invalid value '{k}' for 'other_inputs', this is a constant that can't be overwrittern.\") # inputs[k] = v # # defaults = transformation_config.get(\"defaults\", None) # if defaults: # for k, v in defaults.items(): # if k in constants.keys(): # raise Exception(f\"Invalid default value '{k}', this is a constant that can't be overwrittern.\") # if k not in inputs.keys(): # inputs[k] = v # # if input_name in inputs.keys(): # raise Exception( # f\"Invalid value for inputs in transform arguments, can't contain the main input key '{input_name}'.\" # ) # # inputs[input_name] = value # # result = module.run(**inputs) # output_name = transformation_config[\"output_name\"] # # result_value = result.get_value_obj(output_name) # return result_value get_type_lineage ( self , value_type ) \u00b6 Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type. Source code in kiara/data/types/type_mgmt.py def get_type_lineage ( self , value_type : str ) -> typing . Iterable [ str ]: \"\"\"Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type.\"\"\" if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx path = nx . shortest_path ( self . value_type_hierarchy , \"any\" , value_type ) return reversed ( path ) values special \u00b6 A module that contains value-related classes for Kiara . A value in Kiara-speak is a pointer to actual data (aka 'bytes'). It contains metadata about that data (like whether it's valid/set, what type/schema it has, when it was last modified, ...), but it does not contain the data itself. The reason for that is that such data can be fairly large, and in a lot of cases it is not necessary for the code involved to have access to it, access to the metadata is enough. Each Value has a unique id, which can be used to retrieve the data (whole, or parts of it) from a DataRegistry . In addition, that id can be used to subscribe to change events for a value (published whenever the data that is associated with a value was changed). Value ( BaseModel , JupyterMixin ) pydantic-model \u00b6 The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry. Source code in kiara/data/values/__init__.py class Value ( BaseModel , JupyterMixin ): \"\"\"The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry. \"\"\" class Config : extra = Extra . forbid use_enum_values = True def __init__ ( self , registry : \"BaseDataRegistry\" , value_schema : ValueSchema , type_obj : ValueType , is_set : bool , is_none : bool , hashes : typing . Optional [ typing . Iterable [ ValueHash ]] = None , metadata : typing . Optional [ typing . Mapping [ str , typing . Dict [ str , typing . Any ]]] = None , register_token : typing . Optional [ uuid . UUID ] = None ): # type: ignore if not register_token : raise Exception ( \"No register token provided.\" ) if not registry . _check_register_token ( register_token ): raise Exception ( f \"Value registration with token ' { register_token } ' not allowed.\" ) if value_schema is None : raise NotImplementedError () assert registry self . _registry = registry self . _kiara = self . _registry . _kiara kwargs : typing . Dict [ str , typing . Any ] = {} kwargs [ \"id\" ] = NO_ID_YET_MARKER kwargs [ \"value_schema\" ] = value_schema # if value_lineage is None: # value_lineage = ValueLineage() # # kwargs[\"value_lineage\"] = value_lineage # kwargs[\"is_streaming\"] = False # not used yet kwargs [ \"creation_date\" ] = datetime . now () kwargs [ \"is_set\" ] = is_set kwargs [ \"is_none\" ] = is_none if hashes : kwargs [ \"hashes\" ] = list ( hashes ) if metadata : kwargs [ \"metadata\" ] = dict ( metadata ) else : kwargs [ \"metadata\" ] = {} super () . __init__ ( ** kwargs ) self . _type_obj = type_obj _kiara : \"Kiara\" = PrivateAttr () _registry : \"BaseDataRegistry\" = PrivateAttr () _type_obj : ValueType = PrivateAttr () _value_info : \"ValueInfo\" = PrivateAttr ( default = None ) id : str = Field ( description = \"A unique id for this value.\" ) value_schema : ValueSchema = Field ( description = \"The schema of this value.\" ) creation_date : typing . Optional [ datetime ] = Field ( description = \"The time this value was created value happened.\" ) # is_streaming: bool = Field( # default=False, # description=\"Whether the value is currently streamed into this object.\", # ) is_set : bool = Field ( description = \"Whether the value was set (in some way: user input, default, constant...).\" , default = False , ) is_none : bool = Field ( description = \"Whether the value is 'None'.\" , default = True ) hashes : typing . List [ ValueHash ] = Field ( description = \"Available hashes relating to the actual value data. This attribute is not populated by default, use the 'get_hashes()' method to request one or several hash items, afterwards those hashes will be reflected in this attribute.\" , default_factory = list , ) metadata : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = Field ( description = \"Available metadata relating to the actual value data (size, no. of rows, etc. -- depending on data type). This attribute is not populated by default, use the 'get_metadata()' method to request one or several metadata items, afterwards those metadata items will be reflected in this attribute.\" , default_factory = dict , ) @property def type_name ( self ) -> str : return self . value_schema . type @property def type_obj ( self ): \"\"\"Return the object that contains all the type information for this value.\"\"\" return self . _type_obj def get_hash ( self , hash_type : str ) -> ValueHash : \"\"\"Calculate the hash of a specified type for this value. Hashes are cached.\"\"\" hashes = self . get_hashes ( hash_type ) return list ( hashes )[ 0 ] def get_hashes ( self , * hash_types : str ) -> typing . Iterable [ ValueHash ]: all_hash_types = self . type_obj . get_supported_hash_types () if not hash_types : try : hash_types = all_hash_types except Exception as e : log_message ( str ( e )) if not hash_types : return [] result = [] missing = list ( hash_types ) for hash_obj in self . hashes : if hash_obj . hash_type in hash_types : result . append ( hash_obj ) missing . remove ( hash_obj . hash_type ) for hash_type in missing : if hash_type not in all_hash_types : raise Exception ( f \"Hash type ' { hash_type } ' not supported for ' { self . type_name } '\" ) hash_str = self . type_obj . calculate_value_hash ( value = self . get_value_data (), hash_type = hash_type ) hash_obj = ValueHash ( hash_type = hash_type , hash = hash_str ) self . hashes . append ( hash_obj ) result . append ( hash_obj ) return result def item_is_valid ( self ) -> bool : \"\"\"Check whether the current value is valid\"\"\" if self . value_schema . optional : return True else : return self . is_set and not self . is_none def item_status ( self ) -> str : \"\"\"Print a human readable short description of this values status.\"\"\" if self . value_schema . optional : if self . is_set : if self . is_none : return \"no value (not required)\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set (not required)\" else : if self . is_set : if self . is_none : return \"no value\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set\" def get_value_data ( self ) -> typing . Any : return self . _registry . get_value_data ( self ) def get_lineage ( self ) -> typing . Optional [ ValueLineage ]: return self . _registry . get_lineage ( self ) def set_value_lineage ( self , value_lineage : ValueLineage ) -> None : if hasattr ( self . _registry , \"set_value_lineage\" ): return self . _registry . set_value_lineage ( self , value_lineage ) # type: ignore else : raise Exception ( \"Can't set value lineage: registry is read only\" ) def get_info ( self ) -> \"ValueInfo\" : if self . _value_info is None : self . _value_info = ValueInfo . from_value ( self ) return self . _value_info def create_info ( self , include_deserialization_config : bool = False ) -> \"ValueInfo\" : return ValueInfo . from_value ( self , include_deserialization_config = include_deserialization_config ) def get_metadata ( self , * metadata_keys : str , also_return_schema : bool = False ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: \"\"\"Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to `True`, the value will be a two-key map with the metadata under the ``metadata_item`` subkey, and the value schema under ``metadata_schema``. If no metadata key is specified, all available metadata for this value type will be returned. Arguments: metadata_keys: the metadata keys to retrieve metadata (empty for all available metadata) also_return_schema: whether to also return the schema for each metadata item \"\"\" if not metadata_keys : _metadata_keys = set ( self . _kiara . metadata_mgmt . get_metadata_keys_for_type ( self . type_name ) ) for key in self . metadata . keys (): _metadata_keys . add ( key ) else : _metadata_keys = set ( metadata_keys ) result = {} missing = set () for metadata_key in sorted ( _metadata_keys ): if metadata_key in self . metadata . keys (): if also_return_schema : result [ metadata_key ] = self . metadata [ metadata_key ] else : result [ metadata_key ] = self . metadata [ metadata_key ][ \"metadata_item\" ] else : missing . add ( metadata_key ) if not missing : return result _md = self . _kiara . metadata_mgmt . get_value_metadata ( self , * missing , also_return_schema = True ) for k , v in _md . items (): self . metadata [ k ] = v if also_return_schema : result [ k ] = v else : result [ k ] = v [ \"metadata_item\" ] return { k : result [ k ] for k in sorted ( result . keys ())} def save ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None , register_missing_aliases : bool = True , ) -> \"Value\" : \"\"\"Save this value, under the specified alias(es).\"\"\" # if self.get_value_lineage(): # for field_name, value in self.get_value_lineage().inputs.items(): # value_obj = self._registry.get_value_obj(value) # try: # value_obj.save() # except Exception as e: # print(e) value = self . _kiara . data_store . register_data ( self ) if aliases : self . _kiara . data_store . link_aliases ( value , * aliases , register_missing_aliases = register_missing_aliases ) return value def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , Value ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id ) def __repr__ ( self ): return f \"Value(id= { self . id } , type= { self . type_name } , is_set= { self . is_set } , is_none= { self . is_none } \" def __str__ ( self ): return self . __repr__ () def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : # table = self._create_value_table() title = \"Value\" yield Panel ( self . get_info (), box = box . ROUNDED , title_align = \"left\" , title = title ) creation_date : datetime pydantic-field \u00b6 The time this value was created value happened. hashes : List [ kiara . data . values . ValueHash ] pydantic-field \u00b6 Available hashes relating to the actual value data. This attribute is not populated by default, use the 'get_hashes()' method to request one or several hash items, afterwards those hashes will be reflected in this attribute. id : str pydantic-field required \u00b6 A unique id for this value. is_none : bool pydantic-field \u00b6 Whether the value is 'None'. is_set : bool pydantic-field \u00b6 Whether the value was set (in some way: user input, default, constant...). metadata : Dict [ str , Dict [ str , Any ]] pydantic-field \u00b6 Available metadata relating to the actual value data (size, no. of rows, etc. -- depending on data type). This attribute is not populated by default, use the 'get_metadata()' method to request one or several metadata items, afterwards those metadata items will be reflected in this attribute. type_obj property readonly \u00b6 Return the object that contains all the type information for this value. value_schema : ValueSchema pydantic-field required \u00b6 The schema of this value. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , Value ): return False return self . id == other . id __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash ( self . id ) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/data/values/__init__.py def __repr__ ( self ): return f \"Value(id= { self . id } , type= { self . type_name } , is_set= { self . is_set } , is_none= { self . is_none } \" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/data/values/__init__.py def __str__ ( self ): return self . __repr__ () get_hash ( self , hash_type ) \u00b6 Calculate the hash of a specified type for this value. Hashes are cached. Source code in kiara/data/values/__init__.py def get_hash ( self , hash_type : str ) -> ValueHash : \"\"\"Calculate the hash of a specified type for this value. Hashes are cached.\"\"\" hashes = self . get_hashes ( hash_type ) return list ( hashes )[ 0 ] get_metadata ( self , * metadata_keys , * , also_return_schema = False ) \u00b6 Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to True , the value will be a two-key map with the metadata under the metadata_item subkey, and the value schema under metadata_schema . If no metadata key is specified, all available metadata for this value type will be returned. Parameters: Name Type Description Default metadata_keys str the metadata keys to retrieve metadata (empty for all available metadata) () also_return_schema bool whether to also return the schema for each metadata item False Source code in kiara/data/values/__init__.py def get_metadata ( self , * metadata_keys : str , also_return_schema : bool = False ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: \"\"\"Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to `True`, the value will be a two-key map with the metadata under the ``metadata_item`` subkey, and the value schema under ``metadata_schema``. If no metadata key is specified, all available metadata for this value type will be returned. Arguments: metadata_keys: the metadata keys to retrieve metadata (empty for all available metadata) also_return_schema: whether to also return the schema for each metadata item \"\"\" if not metadata_keys : _metadata_keys = set ( self . _kiara . metadata_mgmt . get_metadata_keys_for_type ( self . type_name ) ) for key in self . metadata . keys (): _metadata_keys . add ( key ) else : _metadata_keys = set ( metadata_keys ) result = {} missing = set () for metadata_key in sorted ( _metadata_keys ): if metadata_key in self . metadata . keys (): if also_return_schema : result [ metadata_key ] = self . metadata [ metadata_key ] else : result [ metadata_key ] = self . metadata [ metadata_key ][ \"metadata_item\" ] else : missing . add ( metadata_key ) if not missing : return result _md = self . _kiara . metadata_mgmt . get_value_metadata ( self , * missing , also_return_schema = True ) for k , v in _md . items (): self . metadata [ k ] = v if also_return_schema : result [ k ] = v else : result [ k ] = v [ \"metadata_item\" ] return { k : result [ k ] for k in sorted ( result . keys ())} item_is_valid ( self ) \u00b6 Check whether the current value is valid Source code in kiara/data/values/__init__.py def item_is_valid ( self ) -> bool : \"\"\"Check whether the current value is valid\"\"\" if self . value_schema . optional : return True else : return self . is_set and not self . is_none item_status ( self ) \u00b6 Print a human readable short description of this values status. Source code in kiara/data/values/__init__.py def item_status ( self ) -> str : \"\"\"Print a human readable short description of this values status.\"\"\" if self . value_schema . optional : if self . is_set : if self . is_none : return \"no value (not required)\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set (not required)\" else : if self . is_set : if self . is_none : return \"no value\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set\" save ( self , aliases = None , register_missing_aliases = True ) \u00b6 Save this value, under the specified alias(es). Source code in kiara/data/values/__init__.py def save ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None , register_missing_aliases : bool = True , ) -> \"Value\" : \"\"\"Save this value, under the specified alias(es).\"\"\" # if self.get_value_lineage(): # for field_name, value in self.get_value_lineage().inputs.items(): # value_obj = self._registry.get_value_obj(value) # try: # value_obj.save() # except Exception as e: # print(e) value = self . _kiara . data_store . register_data ( self ) if aliases : self . _kiara . data_store . link_aliases ( value , * aliases , register_missing_aliases = register_missing_aliases ) return value ValueAlias ( BaseModel ) pydantic-model \u00b6 Source code in kiara/data/values/__init__.py class ValueAlias ( BaseModel ): @classmethod def from_string ( self , value_alias : str , default_repo_name : typing . Optional [ str ] = None ) -> \"ValueAlias\" : if not isinstance ( value_alias , str ): raise Exception ( \"Invalid id_or_alias: not a string.\" ) if not value_alias : raise Exception ( \"Invalid id_or_alias: can't be empty string.\" ) _repo_name : typing . Optional [ str ] = default_repo_name _version : typing . Optional [ int ] = None _tag : typing . Optional [ str ] = None if \"#\" in value_alias : _repo_name , _value_alias = value_alias . split ( \"#\" , maxsplit = 1 ) else : _value_alias = value_alias if \"@\" in _value_alias : _alias , _postfix = _value_alias . split ( \"@\" , maxsplit = 1 ) try : _version = int ( _postfix ) except ValueError : if not _postfix . isidentifier (): raise Exception ( f \"Invalid format for version/tag element of id_or_alias: { _tag } \" ) _tag = _postfix else : _alias = _value_alias return ValueAlias ( repo_name = _repo_name , alias = _alias , version = _version , tag = _tag ) @classmethod def from_strings ( cls , * value_aliases : typing . Union [ str , \"ValueAlias\" ] ) -> typing . List [ \"ValueAlias\" ]: result = [] for va in value_aliases : if isinstance ( va , str ): result . append ( ValueAlias . from_string ( va )) elif isinstance ( va , ValueAlias ): result . append ( va ) else : raise TypeError ( f \"Invalid type ' { type ( va ) } ' for type alias, expected 'str' or 'ValueAlias'.\" ) return result repo_name : typing . Optional [ str ] = Field ( description = \"The name of the data repo the value lives in.\" , default = None ) alias : str = Field ( \"The alias name.\" ) version : typing . Optional [ int ] = Field ( description = \"The version of this alias.\" , default = None ) tag : typing . Optional [ str ] = Field ( description = \"The tag for the alias.\" , default = None ) @property def full_alias ( self ): if self . tag is not None : return f \" { self . alias } @ { self . tag } \" elif self . version is not None : return f \" { self . alias } @ { self . version } \" else : return self . alias repo_name : str pydantic-field \u00b6 The name of the data repo the value lives in. tag : str pydantic-field \u00b6 The tag for the alias. version : int pydantic-field \u00b6 The version of this alias. ValueHash ( BaseModel ) pydantic-model \u00b6 Source code in kiara/data/values/__init__.py class ValueHash ( BaseModel ): hash : str = Field ( description = \"The value hash.\" ) hash_type : str = Field ( description = \"The value hash method.\" ) hash : str pydantic-field required \u00b6 The value hash. hash_type : str pydantic-field required \u00b6 The value hash method. ValueInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/data/values/__init__.py class ValueInfo ( KiaraInfoModel ): @classmethod def from_value ( cls , value : Value , include_deserialization_config : bool = False ): if value . id not in value . _registry . value_ids : raise Exception ( \"Value not registered (yet).\" ) # aliases = value._registry.find_aliases_for_value(value) hashes = value . get_hashes () metadata = value . get_metadata ( also_return_schema = True ) # metadata = value.metadata value_lineage = value . get_lineage () if include_deserialization_config : # serialize_operation: SerializeValueOperationType = ( # type: ignore # value._kiara.operation_mgmt.get_operation(\"serialize\") # type: ignore # ) raise NotImplementedError () return ValueInfo ( value_id = value . id , value_schema = value . value_schema , hashes = hashes , metadata = metadata , lineage = value_lineage , is_valid = value . item_is_valid (), ) value_id : str = Field ( description = \"The value id.\" ) value_schema : ValueSchema = Field ( description = \"The value schema.\" ) # aliases: typing.List[ValueAlias] = Field( # description=\"All aliases for this value.\", default_factory=list # ) # tags: typing.List[str] = Field( # description=\"All tags for this value.\", default_factory=list # ) # created: str = Field(description=\"The time the data was created.\") is_valid : bool = Field ( description = \"Whether the item is valid (in the context of its schema).\" ) hashes : typing . List [ ValueHash ] = Field ( description = \"All available hashes for this value.\" , default_factory = list ) metadata : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = Field ( description = \"The metadata associated with this value.\" ) lineage : typing . Optional [ ValueLineage ] = Field ( description = \"Information about how the value was created.\" , default = None ) deserialize_config : typing . Optional [ DeserializeConfig ] = Field ( description = \"The module config (incl. inputs) to deserialize the value.\" , default = None , ) def get_metadata_items ( self , * keys : str ) -> typing . Dict [ str , typing . Any ]: if not keys : _keys : typing . Iterable [ str ] = self . metadata . keys () else : _keys = keys result = {} for k in _keys : md = self . metadata . get ( k ) if md is None : raise Exception ( f \"No metadata for key ' { k } ' available.\" ) result [ k ] = md [ \"metadata_item\" ] return result def get_metadata_schemas ( self , * keys : str ) -> typing . Dict [ str , typing . Any ]: if not keys : _keys : typing . Iterable [ str ] = self . metadata . keys () else : _keys = keys result = {} for k in _keys : md = self . metadata . get ( k ) if md is None : raise Exception ( f \"No metadata for key ' { k } ' available.\" ) result [ k ] = md [ \"metadata_schema\" ] return result def create_renderable ( self , ** config : typing . Any ) -> RenderableType : padding = config . get ( \"padding\" , ( 0 , 1 )) skip_metadata = config . get ( \"skip_metadata\" , False ) skip_value_lineage = config . get ( \"skip_lineage\" , True ) include_ids = config . get ( \"include_ids\" , False ) table = Table ( box = box . SIMPLE , show_header = False , padding = padding ) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) table . add_row ( \"id\" , self . value_id ) # type: ignore table . add_row ( \"type\" , self . value_schema . type ) if self . value_schema . type_config : json_data = json . dumps ( self . value_schema . type_config ) tc_content = Syntax ( json_data , \"json\" ) table . add_row ( \"type config\" , tc_content ) table . add_row ( \"desc\" , self . value_schema . doc ) table . add_row ( \"is set\" , \"yes\" if self . is_valid else \"no\" ) # table.add_row(\"is constant\", \"yes\" if self.is_constant else \"no\") # if isinstance(self.value_hash, int): # vh = str(self.value_hash) # else: # vh = self.value_hash.value # table.add_row(\"hash\", vh) if self . hashes : hashes_dict = { hs . hash_type : hs . hash for hs in self . hashes } yaml_string = yaml . dump ( hashes_dict ) hases_str = Syntax ( yaml_string , \"yaml\" , background_color = \"default\" ) table . add_row ( \"\" , \"\" ) table . add_row ( \"hashes\" , hases_str ) if not skip_metadata : if self . metadata : yaml_string = yaml . dump ( data = self . get_metadata_items ()) # json_string = json.dumps(self.get_metadata_items(), indent=2) metadata = Syntax ( yaml_string , \"yaml\" , background_color = \"default\" ) table . add_row ( \"metadata\" , metadata ) else : table . add_row ( \"metadata\" , \"-- no metadata --\" ) if not skip_value_lineage and self . lineage : if self . metadata : table . add_row ( \"\" , \"\" ) # json_string = self.lineage.json(indent=2) # seed_content = Syntax(json_string, \"json\") table . add_row ( \"lineage\" , self . lineage . create_renderable ( include_ids = include_ids ) ) return table deserialize_config : DeserializeConfig pydantic-field \u00b6 The module config (incl. inputs) to deserialize the value. hashes : List [ kiara . data . values . ValueHash ] pydantic-field \u00b6 All available hashes for this value. is_valid : bool pydantic-field required \u00b6 Whether the item is valid (in the context of its schema). lineage : ValueLineage pydantic-field \u00b6 Information about how the value was created. metadata : Dict [ str , Dict [ str , Any ]] pydantic-field required \u00b6 The metadata associated with this value. value_id : str pydantic-field required \u00b6 The value id. value_schema : ValueSchema pydantic-field required \u00b6 The value schema. ValueLineage ( ModuleConfig ) pydantic-model \u00b6 Model containing the lineage of a value. Source code in kiara/data/values/__init__.py class ValueLineage ( ModuleConfig ): \"\"\"Model containing the lineage of a value.\"\"\" @classmethod def from_module_and_inputs ( cls , module : \"KiaraModule\" , output_name : str , inputs : typing . Mapping [ str , typing . Union [ \"Value\" , \"ValueInfo\" ]], ): module_type = module . _module_type_id # type: ignore module_config = module . config . dict () doc = module . get_type_metadata () . documentation _inputs = {} for field_name , value in inputs . items (): if isinstance ( value , Value ): _inputs [ field_name ] = value . get_info () else : _inputs [ field_name ] = value return ValueLineage . construct ( module_type = module_type , module_config = module_config , doc = doc , output_name = output_name , inputs = _inputs , ) @classmethod def create ( cls , module_type : str , module_config : typing . Mapping [ str , typing . Any ], module_doc : DocumentationMetadataModel , inputs : typing . Mapping [ str , typing . Union [ \"Value\" , \"ValueInfo\" ]], output_name : typing . Optional [ str ] = None , ): _inputs = {} for field_name , value in inputs . items (): if isinstance ( value , Value ): _inputs [ field_name ] = value . get_info () else : _inputs [ field_name ] = value return ValueLineage . construct ( module_type = module_type , module_config = dict ( module_config ), doc = module_doc , output_name = output_name , inputs = _inputs , ) output_name : typing . Optional [ str ] = Field ( description = \"The result field name for the value this refers to.\" ) inputs : typing . Dict [ str , \"ValueInfo\" ] = Field ( description = \"The inputs that were used to create the value this refers to.\" ) value_index : typing . Optional [ typing . Dict [ str , \"ValueInfo\" ]] = Field ( description = \"Index of all values that are associated with this value lineage.\" , default = None , ) def to_minimal_dict ( self , include_metadata : bool = False , include_module_doc : bool = False , include_module_config : bool = True , ) -> typing . Dict [ str , typing . Any ]: full_dict = self . dict ( exclude_none = True ) minimal_dict = filter_metadata_schema ( full_dict , include_metadata = include_metadata , include_module_doc = include_module_doc , include_module_config = include_module_config , ) return minimal_dict def create_renderable ( self , ** config : typing . Any ) -> RenderableType : all_ids = sorted ( find_all_ids_in_lineage ( self )) id_color_map = {} for idx , v_id in enumerate ( all_ids ): id_color_map [ v_id ] = COLOR_LIST [ idx % len ( COLOR_LIST )] show_ids = config . get ( \"include_ids\" , False ) tree = fill_lineage_tree ( self , include_ids = show_ids ) return tree def create_graph ( self ) -> nx . DiGraph : return create_lineage_graph ( self ) inputs : Dict [ str , ValueInfo ] pydantic-field required \u00b6 The inputs that were used to create the value this refers to. output_name : str pydantic-field \u00b6 The result field name for the value this refers to. value_index : Dict [ str , ValueInfo ] pydantic-field \u00b6 Index of all values that are associated with this value lineage. create_renderable ( self , ** config ) \u00b6 Create a renderable for this module configuration. Source code in kiara/data/values/__init__.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : all_ids = sorted ( find_all_ids_in_lineage ( self )) id_color_map = {} for idx , v_id in enumerate ( all_ids ): id_color_map [ v_id ] = COLOR_LIST [ idx % len ( COLOR_LIST )] show_ids = config . get ( \"include_ids\" , False ) tree = fill_lineage_tree ( self , include_ids = show_ids ) return tree ValueSchema ( BaseModel ) pydantic-model \u00b6 The schema of a value. The schema contains the ValueType of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the type_config field. Source code in kiara/data/values/__init__.py class ValueSchema ( BaseModel ): \"\"\"The schema of a value. The schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field. \"\"\" class Config : use_enum_values = True # extra = Extra.forbid type : str = Field ( description = \"The type of the value.\" ) type_config : typing . Dict [ str , typing . Any ] = Field ( description = \"Configuration for the type, in case it's complex.\" , default_factory = dict , ) default : typing . Any = Field ( description = \"A default value.\" , default = SpecialValue . NOT_SET ) optional : bool = Field ( description = \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , default = False , ) is_constant : bool = Field ( description = \"Whether the value is a constant.\" , default = False ) # required: typing.Any = Field( # description=\"Whether this value is required to be set.\", default=True # ) doc : str = Field ( default = \"-- n/a --\" , description = \"A description for the value of this input field.\" , ) def is_required ( self ): if self . optional : return False else : if self . default in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ]: return True else : return False def validate_types ( self , kiara : \"Kiara\" ): if self . type not in kiara . value_type_names : raise ValueError ( f \"Invalid value type ' { self . type } ', available types: { kiara . value_type_names } \" ) @property def desc ( self ): \"\"\"The first line of the 'doc' value.\"\"\" return self . doc . split ( \" \\n \" )[ 0 ] def __eq__ ( self , other ): if not isinstance ( other , ValueSchema ): return False return ( self . type , self . default ) == ( other . type , other . default ) def __hash__ ( self ): return hash (( self . type , self . default )) default : Any pydantic-field \u00b6 A default value. desc property readonly \u00b6 The first line of the 'doc' value. doc : str pydantic-field \u00b6 A description for the value of this input field. is_constant : bool pydantic-field \u00b6 Whether the value is a constant. optional : bool pydantic-field \u00b6 Whether this value is required (True), or whether 'None' value is allowed (False). type : str pydantic-field required \u00b6 The type of the value. type_config : Dict [ str , Any ] pydantic-field \u00b6 Configuration for the type, in case it's complex. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): if not isinstance ( other , ValueSchema ): return False return ( self . type , self . default ) == ( other . type , other . default ) __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash (( self . type , self . default )) ValueSlot ( BaseModel ) pydantic-model \u00b6 Source code in kiara/data/values/__init__.py class ValueSlot ( BaseModel ): @classmethod def from_value ( cls , id : str , value : Value ) -> \"ValueSlot\" : vs = ValueSlot . from_value_schema ( id = id , value_schema = value . value_schema , kiara = value . _kiara ) vs . add_value ( value ) return vs @classmethod def from_value_schema ( cls , id : str , value_schema : ValueSchema , kiara : \"Kiara\" ) -> \"ValueSlot\" : vs = ValueSlot ( id = id , value_schema = value_schema , kiara = kiara ) return vs def __init__ ( self , ** data ): # type: ignore _kiara = data . pop ( \"kiara\" , None ) if _kiara is None : raise Exception ( \"No kiara context provided.\" ) _registry = data . pop ( \"registry\" , None ) if _registry is None : _registry = _kiara . data_registry self . _kiara = _kiara self . _registry = _registry super () . __init__ ( ** data ) _kiara : \"Kiara\" = PrivateAttr () _registry : \"DataRegistry\" = PrivateAttr () _callbacks : typing . Dict [ str , \"ValueSlotUpdateHandler\" ] = PrivateAttr ( default_factory = dict ) id : str = Field ( description = \"The id for this slot.\" ) value_schema : ValueSchema = Field ( description = \"The schema for the values of this slot.\" ) values : typing . Dict [ int , Value ] = Field ( description = \"The values of this slot, with versions as key.\" , default_factory = dict , ) tags : typing . Dict [ str , int ] = Field ( description = \"The tags for this value slot (tag name as key, linked version as value.\" , default_factory = dict , ) @property def latest_version_nr ( self ) -> int : if not self . values : return 0 return max ( self . values . keys ()) def get_latest_value ( self ) -> Value : lv = self . latest_version_nr if lv == 0 : raise Exception ( \"No value added to value slot yet.\" ) return self . values [ self . latest_version_nr ] def register_callbacks ( self , * callbacks : \"ValueSlotUpdateHandler\" ): for cb in callbacks : cb_id : typing . Optional [ str ] = None if cb_id in self . _callbacks . keys (): raise Exception ( f \"Callback with id ' { cb_id } ' already registered.\" ) if hasattr ( cb , \"id\" ): if callable ( cb . id ): # type: ignore cb_id = cb . id () # type: ignore else : cb_id = cb . id # type: ignore elif hasattr ( cb , \"get_id\" ): cb_id = cb . get_id () # type: ignore if cb_id is None : cb_id = str ( uuid . uuid4 ()) assert isinstance ( cb_id , str ) self . _callbacks [ cb_id ] = cb def add_value ( self , value : Value , trigger_callbacks : bool = True , tags : typing . Optional [ typing . Iterable [ str ]] = None , ) -> int : \"\"\"Add a value to this slot.\"\"\" if self . latest_version_nr != 0 and value . id == self . get_latest_value () . id : return self . latest_version_nr # TODO: check value data equality if self . value_schema . is_constant and self . values : if is_debug (): import traceback traceback . print_stack () raise Exception ( \"Can't add value: value slot marked as 'constant'.\" ) version = self . latest_version_nr + 1 assert version not in self . values . keys () self . values [ version ] = value if tags : for tag in tags : self . tags [ tag ] = version if trigger_callbacks : for cb in self . _callbacks . values (): cb . values_updated ( self ) return version def is_latest_value ( self , value : Value ): return value . id == self . get_latest_value () . id def find_linked_aliases ( self , value_item : typing . Union [ Value , str ] ) -> typing . List [ \"ValueAlias\" ]: if isinstance ( value_item , Value ): value_item = value_item . id result = [] for _version , _value in self . values . items (): if _value . id == value_item : va = ValueAlias ( alias = self . id , version = _version ) result . append ( va ) if _version in self . tags . values (): for _tag , _tag_version in self . tags . items (): if _tag_version == _version : va = ValueAlias ( alias = self . id , tag = _tag ) result . append ( va ) return result def __eq__ ( self , other ): if not isinstance ( other , ValueSlot ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id ) id : str pydantic-field required \u00b6 The id for this slot. tags : Dict [ str , int ] pydantic-field \u00b6 The tags for this value slot (tag name as key, linked version as value. value_schema : ValueSchema pydantic-field required \u00b6 The schema for the values of this slot. values : Dict [ int , kiara . data . values . Value ] pydantic-field \u00b6 The values of this slot, with versions as key. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): if not isinstance ( other , ValueSlot ): return False return self . id == other . id __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash ( self . id ) add_value ( self , value , trigger_callbacks = True , tags = None ) \u00b6 Add a value to this slot. Source code in kiara/data/values/__init__.py def add_value ( self , value : Value , trigger_callbacks : bool = True , tags : typing . Optional [ typing . Iterable [ str ]] = None , ) -> int : \"\"\"Add a value to this slot.\"\"\" if self . latest_version_nr != 0 and value . id == self . get_latest_value () . id : return self . latest_version_nr # TODO: check value data equality if self . value_schema . is_constant and self . values : if is_debug (): import traceback traceback . print_stack () raise Exception ( \"Can't add value: value slot marked as 'constant'.\" ) version = self . latest_version_nr + 1 assert version not in self . values . keys () self . values [ version ] = value if tags : for tag in tags : self . tags [ tag ] = version if trigger_callbacks : for cb in self . _callbacks . values (): cb . values_updated ( self ) return version value_set \u00b6 SlottedValueSet ( ValueSet ) \u00b6 Source code in kiara/data/values/value_set.py class SlottedValueSet ( ValueSet ): @classmethod def from_schemas ( cls , schemas : typing . Mapping [ str , \"ValueSchema\" ], read_only : bool = True , check_for_sameness = True , initial_values : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , title : typing . Optional [ str ] = None , default_value : typing . Any = SpecialValue . NO_VALUE , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ) -> \"SlottedValueSet\" : if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if registry is None : registry = kiara . data_registry values = {} for field_name , schema in schemas . items (): def_val = default_value if callable ( default_value ): def_val = default_value () _init_value = def_val if initial_values and initial_values . get ( field_name , None ) is not None : _init_value = initial_values [ field_name ] if not isinstance ( _init_value , Value ): value : Value = registry . register_data ( value_data = _init_value , value_schema = schema ) # value: Value = Value(value_schema=schema, value_data=_init_value, registry=registry) # type: ignore else : value = _init_value values [ field_name ] = value return cls ( items = values , title = title , read_only = read_only , check_for_sameness = check_for_sameness , kiara = kiara , registry = registry , ) def __init__ ( self , items : typing . Mapping [ str , typing . Union [ \"ValueSlot\" , \"Value\" ]], read_only : bool , check_for_sameness : bool = False , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ): \"\"\"A `ValueSet` implementation that keeps a history of each fields value. Arguments: items: the value slots read_only: whether it is allowed to set new values to fields in this set check_for_sameness: whether a check should be performed that checks for equality of the new and old values, if equal, skip the update title: An optional title for this value set kiara: the kiara context registry: the registry to use to register the values to \"\"\" if not items : raise ValueError ( \"Can't create ValueSet: no values provided\" ) self . _check_for_sameness : bool = check_for_sameness super () . __init__ ( read_only = read_only , title = title , kiara = kiara ) if registry is None : registry = self . _kiara . data_registry self . _registry : DataRegistry = registry _value_slots : typing . Dict [ str , ValueSlot ] = {} for item , value in items . items (): if value is None : raise Exception ( f \"Can't create value set, item ' { item } ' does not have a value (yet).\" ) if item . startswith ( \"_\" ): raise ValueError ( f \"Value name can't start with '_': { item } \" ) if item in INVALID_VALUE_NAMES : raise ValueError ( f \"Invalid value name ' { item } '.\" ) if isinstance ( value , Value ): slot = self . _registry . register_alias ( value ) elif isinstance ( value , ValueSlot ): slot = value else : raise TypeError ( f \"Invalid type: ' { type ( value ) } '\" ) _value_slots [ item ] = slot self . _value_slots : typing . Dict [ str , ValueSlot ] = _value_slots def get_all_field_names ( self ) -> typing . Iterable [ str ]: return self . _value_slots . keys () def _get_value_obj ( self , field_name : str ) -> \"Value\" : slot = self . _value_slots [ field_name ] return slot . get_latest_value () def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: # we want to set all registry-type values seperately, in one go, because it's more efficient registries : typing . Dict [ DataRegistry , typing . Dict [ ValueSlot , typing . Union [ \"Value\" , typing . Any ]] ] = {} field_slot_map : typing . Dict [ ValueSlot , str ] = {} result : typing . Dict [ str , typing . Union [ bool , Exception ]] = {} for field_name , value_or_data in values . items (): value_slot : ValueSlot = self . _value_slots [ field_name ] if not metadata and self . _check_for_sameness : latest_val = value_slot . get_latest_value () if isinstance ( value_or_data , Value ): if latest_val . id == value_or_data . id : result [ field_name ] = False continue else : if ( latest_val . is_set and latest_val . get_value_data () == value_or_data ): result [ field_name ] = False continue registries . setdefault ( value_slot . _registry , {})[ value_slot ] = value_or_data field_slot_map [ value_slot ] = field_name for registry , value_slots_details in registries . items (): _r = registry . update_value_slots ( value_slots_details , metadata = metadata , lineage = lineage ) # type: ignore for value_slot , details in _r . items (): result [ field_slot_map [ value_slot ]] = details return result __init__ ( self , items , read_only , check_for_sameness = False , title = None , kiara = None , registry = None ) special \u00b6 A ValueSet implementation that keeps a history of each fields value. Parameters: Name Type Description Default items Mapping[str, Union[ValueSlot, Value]] the value slots required read_only bool whether it is allowed to set new values to fields in this set required check_for_sameness bool whether a check should be performed that checks for equality of the new and old values, if equal, skip the update False title Optional[str] An optional title for this value set None kiara Optional[Kiara] the kiara context None registry Optional[kiara.data.registry.DataRegistry] the registry to use to register the values to None Source code in kiara/data/values/value_set.py def __init__ ( self , items : typing . Mapping [ str , typing . Union [ \"ValueSlot\" , \"Value\" ]], read_only : bool , check_for_sameness : bool = False , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ): \"\"\"A `ValueSet` implementation that keeps a history of each fields value. Arguments: items: the value slots read_only: whether it is allowed to set new values to fields in this set check_for_sameness: whether a check should be performed that checks for equality of the new and old values, if equal, skip the update title: An optional title for this value set kiara: the kiara context registry: the registry to use to register the values to \"\"\" if not items : raise ValueError ( \"Can't create ValueSet: no values provided\" ) self . _check_for_sameness : bool = check_for_sameness super () . __init__ ( read_only = read_only , title = title , kiara = kiara ) if registry is None : registry = self . _kiara . data_registry self . _registry : DataRegistry = registry _value_slots : typing . Dict [ str , ValueSlot ] = {} for item , value in items . items (): if value is None : raise Exception ( f \"Can't create value set, item ' { item } ' does not have a value (yet).\" ) if item . startswith ( \"_\" ): raise ValueError ( f \"Value name can't start with '_': { item } \" ) if item in INVALID_VALUE_NAMES : raise ValueError ( f \"Invalid value name ' { item } '.\" ) if isinstance ( value , Value ): slot = self . _registry . register_alias ( value ) elif isinstance ( value , ValueSlot ): slot = value else : raise TypeError ( f \"Invalid type: ' { type ( value ) } '\" ) _value_slots [ item ] = slot self . _value_slots : typing . Dict [ str , ValueSlot ] = _value_slots ValueSet ( MutableMapping , Generic ) \u00b6 Source code in kiara/data/values/value_set.py class ValueSet ( typing . MutableMapping [ str , \"Value\" ]): def __init__ ( self , read_only : bool , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): # from kiara.data.values import ValueSchema # schema = ValueSchema(type=\"any\", default=None, doc=\"-- n/a --\") # self._schema = schema if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : \"Kiara\" = kiara self . _read_only : bool = read_only if title is None : title = \"-- n/a --\" self . _title = title @abc . abstractmethod def get_all_field_names ( self ) -> typing . Iterable [ str ]: pass @abc . abstractmethod def _get_value_obj ( self , field_name : str ): pass @abc . abstractmethod def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: pass def get_value_obj ( self , field_name : str , ensure_metadata : typing . Union [ bool , typing . Iterable [ str ], str ] = False , ) -> Value : if field_name not in list ( self . get_all_field_names ()): raise KeyError ( f \"Field ' { field_name } ' not available in value set. Available fields: { ', ' . join ( self . get_all_field_names ()) } \" ) obj : Value = self . _get_value_obj ( field_name ) if ensure_metadata : if isinstance ( ensure_metadata , bool ): obj . get_metadata () elif isinstance ( ensure_metadata , str ): obj . get_metadata ( ensure_metadata ) elif isinstance ( ensure_metadata , typing . Iterable ): obj . get_metadata ( * ensure_metadata ) else : raise ValueError ( f \"Invalid type ' { type ( ensure_metadata ) } ' for 'ensure_metadata' argument.\" ) return obj def set_value ( self , key : str , value : typing . Any , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> Value : result = self . set_values ( metadata = metadata , lineage = lineage , ** { key : value }) if isinstance ( result [ key ], Exception ): raise result [ key ] # type: ignore return result [ key ] # type: ignore def set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ Value , Exception ]]: \"\"\"Batch set several values. If metadata is provided, it is added to all values. \"\"\" if self . is_read_only (): raise Exception ( \"Can't set values: this value set is read-only.\" ) if not values : return {} invalid : typing . List [ str ] = [] for k in values . keys (): if k not in self . get_all_field_names (): invalid . append ( k ) if invalid : raise ValueError ( f \"No field(s) with name(s) { ', ' . join ( invalid ) } available, valid names: { ', ' . join ( self . get_all_field_names ()) } \" ) resolved_values = {} for field_name , data in values . items (): if isinstance ( data , str ) and data . startswith ( \"value:\" ): v = self . _kiara . get_value ( data ) resolved_values [ field_name ] = v else : resolved_values [ field_name ] = data try : value_set_result = self . _set_values ( metadata = metadata , lineage = lineage , ** resolved_values ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () raise e result : typing . Dict [ str , typing . Union [ Value , Exception ]] = {} for field in values . keys (): if isinstance ( value_set_result [ field ], Exception ): result [ field ] = value_set_result [ field ] # type: ignore else : result [ field ] = self . get_value_obj ( field ) return result def is_read_only ( self ): return self . _read_only def items_are_valid ( self ) -> bool : return check_valueset_valid ( self ) # for field_name in self.get_all_field_names(): # item = self.get_value_obj(field_name) # if not item.item_is_valid(): # return False # return True def check_invalid ( self ) -> typing . Optional [ typing . Dict [ str , str ]]: \"\"\"Check whether the value set is invalid, if it is, return a description of what's wrong.\"\"\" invalid = {} for field_name , item in self . items (): if not item . item_is_valid (): if item . value_schema . is_required (): if not item . is_set : msg = \"not set\" elif item . is_none : msg = \"no value\" else : msg = \"n/a\" else : msg = \"n/a\" invalid [ field_name ] = msg return invalid def get_all_value_objects ( self ) -> typing . Mapping [ str , typing . Any ]: return { fn : self . get_value_obj ( fn ) for fn in self . get_all_field_names ()} def get_value_data_for_fields ( self , * field_names : str , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). \"\"\" result : typing . Dict [ str , typing . Any ] = {} unset : typing . List [ str ] = [] for k in field_names : v = self . get_value_obj ( k ) if not v . is_set : if raise_exception_when_unset : unset . append ( k ) else : result [ k ] = None else : data = v . get_value_data () result [ k ] = data if unset : raise Exception ( f \"Can't get data for fields, one or several of the requested fields are not set yet: { ', ' . join ( unset ) } .\" ) return result def get_value_data ( self , field_name : str , raise_exception_when_unset : bool = False ) -> typing . Any : return self . get_value_data_for_fields ( field_name , raise_exception_when_unset = raise_exception_when_unset )[ field_name ] def get_all_value_data ( self , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: return self . get_value_data_for_fields ( * self . get_all_field_names (), raise_exception_when_unset = raise_exception_when_unset , ) def save_all ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None ): if aliases : aliases = set ( aliases ) for k , v in self . items (): field_aliases = None if aliases : field_aliases = [ f \" { a } __ { k } \" for a in aliases ] v . save ( aliases = field_aliases ) def __getitem__ ( self , item : str ) -> \"Value\" : return self . get_value_obj ( item ) def __setitem__ ( self , key : str , value ): self . set_value ( key , value ) def __delitem__ ( self , key : str ): raise Exception ( f \"Removing items not supported: { key } \" ) def __iter__ ( self ) -> typing . Iterator [ str ]: return iter ( self . get_all_field_names ()) def __len__ ( self ): return len ( list ( self . get_all_field_names ())) def to_details ( self , ensure_metadata : bool = False ) -> \"PipelineValuesInfo\" : from kiara.pipeline import PipelineValueInfo , PipelineValuesInfo result = {} for name in self . get_all_field_names (): item = self . get_value_obj ( name ) result [ name ] = PipelineValueInfo . from_value_obj ( item , ensure_metadata = ensure_metadata ) return PipelineValuesInfo ( values = result ) def _create_rich_table ( self , show_headers : bool = True , ensure_metadata : bool = False ) -> Table : table = Table ( box = box . SIMPLE , show_header = show_headers ) table . add_column ( \"Field name\" , style = \"i\" ) table . add_column ( \"Type\" ) table . add_column ( \"Description\" ) table . add_column ( \"Required\" ) table . add_column ( \"Is set\" ) for k in self . get_all_field_names (): v = self . get_value_obj ( k , ensure_metadata = ensure_metadata ) t = v . value_schema . type desc = v . value_schema . doc if not v . value_schema . is_required (): req = \"no\" else : if ( v . value_schema . default and v . value_schema . default != SpecialValue . NO_VALUE and v . value_schema . default != SpecialValue . NOT_SET ): req = \"no\" else : req = \"[red]yes[/red]\" is_set = \"yes\" if v . item_is_valid () else \"no\" table . add_row ( k , t , desc , req , is_set ) return table def __repr__ ( self ): title_str = \"\" if self . _title : title_str = f \" title=' { self . _title } '\" return f \" { self . __class__ . __name__ } (field_names= { list ( self . keys ()) }{ title_str } )\" def __str__ ( self ): return self . __repr__ () def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : title = self . _title if title : postfix = f \" [b] { title } [/b]\" else : postfix = \"\" yield Panel ( self . _create_rich_table ( show_headers = True ), box = box . ROUNDED , title_align = \"left\" , title = f \"Value-Set: { postfix } \" , ) check_invalid ( self ) \u00b6 Check whether the value set is invalid, if it is, return a description of what's wrong. Source code in kiara/data/values/value_set.py def check_invalid ( self ) -> typing . Optional [ typing . Dict [ str , str ]]: \"\"\"Check whether the value set is invalid, if it is, return a description of what's wrong.\"\"\" invalid = {} for field_name , item in self . items (): if not item . item_is_valid (): if item . value_schema . is_required (): if not item . is_set : msg = \"not set\" elif item . is_none : msg = \"no value\" else : msg = \"n/a\" else : msg = \"n/a\" invalid [ field_name ] = msg return invalid get_value_data_for_fields ( self , * field_names , * , raise_exception_when_unset = False ) \u00b6 Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). Source code in kiara/data/values/value_set.py def get_value_data_for_fields ( self , * field_names : str , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). \"\"\" result : typing . Dict [ str , typing . Any ] = {} unset : typing . List [ str ] = [] for k in field_names : v = self . get_value_obj ( k ) if not v . is_set : if raise_exception_when_unset : unset . append ( k ) else : result [ k ] = None else : data = v . get_value_data () result [ k ] = data if unset : raise Exception ( f \"Can't get data for fields, one or several of the requested fields are not set yet: { ', ' . join ( unset ) } .\" ) return result set_values ( self , metadata = None , lineage = None , ** values ) \u00b6 Batch set several values. If metadata is provided, it is added to all values. Source code in kiara/data/values/value_set.py def set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ Value , Exception ]]: \"\"\"Batch set several values. If metadata is provided, it is added to all values. \"\"\" if self . is_read_only (): raise Exception ( \"Can't set values: this value set is read-only.\" ) if not values : return {} invalid : typing . List [ str ] = [] for k in values . keys (): if k not in self . get_all_field_names (): invalid . append ( k ) if invalid : raise ValueError ( f \"No field(s) with name(s) { ', ' . join ( invalid ) } available, valid names: { ', ' . join ( self . get_all_field_names ()) } \" ) resolved_values = {} for field_name , data in values . items (): if isinstance ( data , str ) and data . startswith ( \"value:\" ): v = self . _kiara . get_value ( data ) resolved_values [ field_name ] = v else : resolved_values [ field_name ] = data try : value_set_result = self . _set_values ( metadata = metadata , lineage = lineage , ** resolved_values ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () raise e result : typing . Dict [ str , typing . Union [ Value , Exception ]] = {} for field in values . keys (): if isinstance ( value_set_result [ field ], Exception ): result [ field ] = value_set_result [ field ] # type: ignore else : result [ field ] = self . get_value_obj ( field ) return result","title":"data"},{"location":"reference/kiara/data/__init__/#kiara.data.onboarding","text":"","title":"onboarding"},{"location":"reference/kiara/data/__init__/#kiara.data.onboarding.ValueStoreConfig","text":"A configuration that describes which step outputs of a pipeline to save, and how. Source code in kiara/data/onboarding/__init__.py class ValueStoreConfig ( BaseModel ): \"\"\"A configuration that describes which step outputs of a pipeline to save, and how.\"\"\" @classmethod def save_fields ( self , base_id : str , value_set : typing . Mapping [ str , Value ], matchers : typing . Optional [ typing . List [ FieldMatcher ]] = None , ) -> typing . Dict [ Value , typing . List [ str ]]: if matchers is None : matchers = FieldMatcher . create_default_matchers () for matcher in matchers : if matcher . matcher_type not in [ \"glob\" , \"regex\" ]: raise NotImplementedError ( \"Only 'glob' and 'regex' field matcher type implemented yet.\" ) if matcher . matcher_type == \"glob\" : matcher . matcher_type = \"regex\" matcher . match_expr = fnmatch . translate ( matcher . match_expr ) env = Environment () to_save : typing . Dict [ str , typing . Set [ str ]] = {} for field_name in value_set . keys (): for matcher in matchers : if not re . match ( matcher . match_expr , field_name ): continue template = env . from_string ( matcher . alias_template ) rendered = template . render ( field_name = field_name , base_id = base_id ) to_save . setdefault ( field_name , set ()) . add ( rendered ) if not matcher . check_next_on_match : break result : typing . Dict [ Value , typing . List [ str ]] = {} for field_name , aliases in to_save . items (): value = value_set [ field_name ] _v = value . save ( aliases = aliases ) result . setdefault ( _v , []) . extend ( aliases ) return result inputs : typing . List [ FieldMatcher ] = Field ( description = \"Whether and how to save inputs.\" , default_factory = list ) outputs : typing . List [ FieldMatcher ] = Field ( description = \"Whether and how to save inputs.\" , default_factory = FieldMatcher . create_default_matchers , ) steps : typing . List [ \"ValueStoreConfig\" ] = Field ( description = \"Whether and how to save step inputs and outputs.\" , default_factory = list , )","title":"ValueStoreConfig"},{"location":"reference/kiara/data/__init__/#kiara.data.onboarding.batch","text":"","title":"batch"},{"location":"reference/kiara/data/__init__/#kiara.data.registry","text":"","title":"registry"},{"location":"reference/kiara/data/__init__/#kiara.data.registry.BaseDataRegistry","text":"Base class to extend if you want to write a kiara data registry. This outlines the main methods that must be available for an entity that holds some values and their data, as well as their associated aliases. In most cases users will interact with subclasses of the more fully featured DataRegistry class, but there are cases where it will make sense to sub-class this base class directly (like for example read-only 'archive' data registries). Source code in kiara/data/registry/__init__.py class BaseDataRegistry ( abc . ABC ): \"\"\"Base class to extend if you want to write a *kiara* data registry. This outlines the main methods that must be available for an entity that holds some values and their data, as well as their associated aliases. In most cases users will interact with subclasses of the more fully featured [DataRegistry][kiara.data.registry.DataRegistry] class, but there are cases where it will make sense to sub-class this base class directly (like for example read-only 'archive' data registries). \"\"\" def __init__ ( self , kiara : \"Kiara\" ): self . _id : str = str ( uuid . uuid4 ()) self . _kiara : Kiara = kiara # self._hashes: typing.Dict[str, typing.Dict[str, str]] = {} self . _register_tokens : typing . Set = set () @property def id ( self ) -> str : return self . _id @property def value_ids ( self ) -> typing . List [ str ]: return list ( self . _get_available_value_ids ()) @property def alias_names ( self ) -> typing . List [ str ]: return sorted ( self . _get_available_aliases ()) # ====================================================================== # main abstract methods @abc . abstractmethod def _get_available_value_ids ( self ) -> typing . Iterable [ str ]: \"\"\"Return all of the registries available value ids.\"\"\" @abc . abstractmethod def _get_value_obj_for_id ( self , value_id : str ) -> Value : pass @abc . abstractmethod def _get_value_data_for_id ( self , value_item : str ) -> typing . Any : pass @abc . abstractmethod def _get_available_aliases ( self ) -> typing . Iterable [ str ]: pass @abc . abstractmethod def _get_value_slot_for_alias ( self , alias_name : str ) -> ValueSlot : pass @abc . abstractmethod def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: pass def _find_value_for_hashes ( self , * hashes : ValueHash ) -> typing . Optional [ Value ]: \"\"\"Return a value that matches one of the provided hashes. This method does not need to be overwritten, but it is recommended that it is done for performance reasons. Arguments: hashes: a collection of hashes, Returns: A (registered) value that matches one of the hashes \"\"\" return None # ----------------------------------------------------------------------- # main value retrieval methods def _create_alias_obj ( self , alias : str ) -> ValueAlias : if alias . startswith ( \"value:\" ): alias = alias [ 6 :] value_alias = ValueAlias . from_string ( value_alias = alias ) if ( value_alias . alias not in self . _get_available_value_ids () and value_alias . alias not in self . _get_available_aliases () ): raise Exception ( f \"Neither id nor alias ' { alias } ' registered with this registry.\" ) if ( value_alias . alias in self . _get_available_aliases () and value_alias . tag is None and value_alias . version is None ): value_alias . version = self . get_latest_version_for_alias ( value_alias . alias ) return value_alias def get_value_obj ( self , value_item : typing . Union [ str , Value , ValueAlias , ValueSlot ], raise_exception : bool = False , ) -> typing . Optional [ Value ]: if value_item == NO_ID_YET_MARKER : raise Exception ( \"Can't get value object: value not fully registered yet.\" ) if isinstance ( value_item , Value ): value_item = value_item . id elif isinstance ( value_item , ValueSlot ): value_item = value_item . get_latest_value () . id if isinstance ( value_item , str ): value_item = self . _create_alias_obj ( value_item ) elif not isinstance ( value_item , ValueAlias ): raise TypeError ( f \"Invalid type ' { type ( value_item ) } ' for value item parameter.\" ) if value_item . alias in self . _get_available_value_ids (): _value_id = value_item . alias _value : typing . Optional [ Value ] = self . _get_value_obj_for_id ( _value_id ) elif value_item . alias in self . _get_available_aliases (): _value = self . _resolve_alias_to_value ( value_item ) else : _value = None if _value is None and raise_exception : raise Exception ( f \"No value or alias registered in registry for: { value_item } \" ) return _value def get_value_data ( self , value_item : typing . Union [ str , Value , ValueSlot , ValueAlias ] ) -> typing . Any : value_obj = self . get_value_obj ( value_item ) if value_obj is None : raise Exception ( f \"No value registered for: { value_item } \" ) if not value_obj . is_set and value_obj . value_schema . default not in ( SpecialValue . NO_VALUE , SpecialValue . NOT_SET , None , ): return value_obj . value_schema . default elif not value_obj . is_set : raise Exception ( f \"Value not set: { value_obj . id } \" ) data = self . _get_value_data_for_id ( value_obj . id ) if data == SpecialValue . NO_VALUE : return None elif isinstance ( data , Value ): return data . get_value_data () else : return data def get_lineage ( self , value_item : typing . Union [ str , Value , ValueSlot ] ) -> typing . Optional [ ValueLineage ]: value_obj = self . get_value_obj ( value_item = value_item ) if value_obj is None : raise Exception ( f \"No value registered for: { value_item } \" ) return self . _get_value_lineage ( value_obj . id ) # def get_value_info(self, value_item: typing.Union[str, Value, ValueAlias]) -> ValueInfo: # # value_obj = self.get_value_obj(value_item=value_item) # # aliases = self.find_aliases_for_value_id(value_id=value_obj.id, include_all_versions=True, include_tags=True) # # info = ValueInfo(value_id=value_obj.id, value_type=value_obj.value_schema.type, aliases=aliases, metadata=value_obj.get_metadata()) # return info def get_value_slot ( self , alias : typing . Union [ str , ValueSlot ] ) -> typing . Optional [ ValueSlot ]: if isinstance ( alias , ValueSlot ): alias = alias . id if alias not in self . alias_names : return None return self . _get_value_slot_for_alias ( alias_name = alias ) # ----------------------------------------------------------------------- # def _resolve_hash_to_value( # self, hash_str: str, hash_type: typing.Optional[str] = None # ) -> typing.Optional[Value]: # # matches: typing.Set[str] = set() # if hash_type is None: # for hash_type, details in self._hashes.items(): # if hash_str in details.keys(): # matches.add(details[hash_str]) # else: # hashes_for_type = self._hashes.get(hash_type, {}) # if hash_str in hashes_for_type.keys(): # matches.add(hashes_for_type[hash_str]) # # if len(matches) == 0: # return None # elif len(matches) > 1: # raise Exception(f\"Multiple values found for hash '{hash_str}'.\") # # match_id = next(iter(matches)) # value = self.get_value_obj(match_id) # return value def _check_register_token ( self , register_token : uuid . UUID ): return register_token in self . _register_tokens # alias-related methods def _resolve_alias_to_value ( self , alias : ValueAlias ) -> typing . Optional [ Value ]: alias_name = alias . alias alias_version = alias . version alias_tag = alias . tag value_slot = self . get_value_slot ( alias_name ) if not value_slot : raise Exception ( f \"No alias ' { alias_name } ' registered with registry.\" ) if alias_tag : _version = value_slot . tags . get ( alias_tag ) if alias_version : if _version != alias_version : raise Exception ( f \"Value alias object contains both tag and version information, but actual version of tag resolves to different version in the registry: { _version } != { alias_version } \" ) else : alias_version = _version assert alias_version is not None value = value_slot . values . get ( alias_version , None ) return value def _find_aliases_for_value_id ( self , value_id : str ) -> typing . List [ ValueAlias ]: \"\"\"Find all aliases that point to the specified value id. Sub-classes may overwrite this method for performance reasons. \"\"\" result = [] for alias in self . _get_available_aliases (): value_slot = self . get_value_slot ( alias ) assert value_slot aliases = value_slot . find_linked_aliases ( value_id ) result . extend ( aliases ) return result def _get_tags_for_alias ( self , alias : str ) -> typing . Iterable [ str ]: value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { alias } ' registered with registry.\" ) return value_slot . tags . keys () def find_aliases_for_value ( self , value_item : typing . Union [ str , Value ], include_all_versions : bool = False , include_tags : bool = False , ) -> typing . List [ ValueAlias ]: value = self . get_value_obj ( value_item ) if value is None : raise Exception ( f \"Can't find registered value for: { value_item } \" ) aliases = self . _find_aliases_for_value_id ( value_id = value . id ) result = [] latest_cache : typing . Dict [ str , int ] = {} for alias in aliases : if alias . version is not None : if not include_all_versions : if alias . alias in latest_cache . keys (): latest_version = latest_cache [ alias . alias ] else : latest_version = self . get_latest_version_for_alias ( alias . alias ) latest_cache [ alias . alias ] = latest_version if latest_version == alias . version : result . append ( alias ) else : result . append ( alias ) if alias . tag is not None and include_tags : result . append ( alias ) return result def get_latest_version_for_alias ( self , alias : str ) -> int : versions = self . get_versions_for_alias ( alias ) if not versions : return 0 else : return max ( versions ) def get_versions_for_alias ( self , alias : str ) -> typing . Iterable [ int ]: \"\"\"Return all available versions for the specified alias.\"\"\" if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . values . keys ()) def get_tags_for_alias ( self , alias : str ) -> typing . Iterable [ str ]: if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . tags . keys ()) @deprecated ( reason = \"This will be removed soon in favor of a more generic way to query values.\" ) def find_all_values_of_type ( self , value_type : str ) -> typing . Dict [ str , Value ]: \"\"\"Find all values of a certain type.\"\"\" result = {} for value_id in self . value_ids : value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"No value registered for: { value_id } \" ) if value_obj . type_name == value_type : # type: ignore result [ value_id ] = value_obj return result def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , DataRegistry ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id ) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (id= { self . id } \" def __str__ ( self ): return self . __repr__ ()","title":"BaseDataRegistry"},{"location":"reference/kiara/data/__init__/#kiara.data.registry.DataRegistry","text":"Source code in kiara/data/registry/__init__.py class DataRegistry ( BaseDataRegistry ): def __init__ ( self , kiara : \"Kiara\" ): self . _lineages : typing . Dict [ str , typing . Optional [ ValueLineage ]] = {} super () . __init__ ( kiara = kiara ) @abc . abstractmethod def _register_value_and_data ( self , value : Value , data : typing . Any ) -> str : \"\"\"Register data into this registry. Returns the values id. In case the value already has a value set (meaning it's different from string [NO_ID_YET_MARKER][kiara.data.value.NO_ID_YET_MARKER] / '__no_id_yet__' this method must use this id or throw an exception. Otherwise, it is required that the result id is different from ``NO_ID_YET_MARKER`` and a non-empty string. \"\"\" @abc . abstractmethod def _register_remote_value ( self , value : Value ) -> Value : \"\"\"Register an existing value from a different registry into this one. Arguments: value: the original value (with the '_registry' attribute still pointing to the original registry) Returns: either None (in which case the original value object will be copied with the '_registry' and '_kiara' attributes adjusted), or a value object. \"\"\" def register_data ( self , value_data : typing . Any = SpecialValue . NOT_SET , value_schema : typing . Union [ None , ValueSchema , str ] = None , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> Value : # if lineage and not lineage.output_name: # raise Exception(f\"Value lineage for data of type '{value_data.value_schema.type}' does not have 'output_name' field set.\") value_id = str ( uuid . uuid4 ()) if value_id in self . value_ids : raise Exception ( f \"Can't register value: value id ' { value_id } ' already exists.\" ) if value_id in self . alias_names : raise Exception ( f \"Can't register value: value id ' { value_id } ' already exists as alias.\" ) if value_schema is None : if isinstance ( value_data , Value ): value_schema = value_data . value_schema else : raise Exception ( f \"No value schema provided for value: { value_data } \" ) elif isinstance ( value_schema , str ): value_schema = ValueSchema ( type = value_schema ) cls = self . _kiara . get_value_type_cls ( value_schema . type ) _type_obj = cls ( ** value_schema . type_config ) existing_value : typing . Optional [ Value ] = None if isinstance ( value_data , Value ): if value_data . id in self . value_ids and not metadata : if lineage : existing_lineage = self . _lineages . get ( value_data . id , None ) if existing_lineage : log_message ( f \"Overwriting existing value lineage for: { value_data . id } \" ) self . _lineages [ value_data . id ] = lineage # TODO: check it's really the same our_value = self . get_value_obj ( value_data . id ) if our_value is None : raise Exception ( \"Could not retrieve cloned value, this is probably a bug.\" ) return our_value existing_value = self . _find_value_for_hashes ( * value_data . get_hashes ()) if existing_value and not metadata : if lineage and self . _lineages . get ( existing_value . id , None ): log_message ( f \"Overwriting value lineage for: { existing_value . id } \" ) if lineage : self . _lineages [ existing_value . id ] = lineage return existing_value if value_data . id in self . value_ids and metadata : _metadata : typing . Optional [ typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = None if metadata : _metadata = {} for k , v in metadata . items (): _metadata [ k ] = {} _metadata [ k ][ \"metadata_item\" ] = v . dict () _metadata [ k ][ \"metadata_schema\" ] = v . schema_json () # TODO: not resolve value_data ? cloned_new_metadata = self . _create_value_obj ( value_schema = value_data . value_schema , is_set = value_data . is_set , is_none = value_data . is_none , type_obj = value_data . type_obj , metadata = _metadata , value_hashes = None , ) r = self . _register_new_value_obj ( value_obj = cloned_new_metadata , value_data = value_data . get_value_data (), lineage = lineage , # value_hashes=value_hashes, ) return r copied_value = self . _register_remote_value ( value_data ) if value_data . id not in self . value_ids : raise Exception ( f \"Value with id ' { value_data . id } ' wasn't successfully registered. This is most likely a bug.\" ) assert copied_value . _registry == self assert copied_value . _kiara == self . _kiara if value_data . id != copied_value . id : log_message ( f \"Value id for value of type { copied_value . type_name } changed when registering in other registry.\" ) raise Exception ( f \"Imported value object with id ' { value_data . id } ' resulted in copied value with different id. This is a bug.\" ) # for hash_type, value_hash in copied_value.get_hashes().items(): # self._hashes.setdefault(hash_type, {})[value_hash.hash] = value_id self . _lineages [ copied_value . id ] = value_data . get_lineage () return copied_value if value_data in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE , ] and value_schema . default not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE , ]: if metadata : raise NotImplementedError () value_data = copy . deepcopy ( value_schema . default ) if value_data not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ]: # TODO: actually implement that for hash_type in _type_obj . get_supported_hash_types (): hash_str = _type_obj . calculate_value_hash ( value = value_data , hash_type = hash_type ) existing_value = self . _find_value_for_hashes ( ValueHash ( hash_type = hash_type , hash = hash_str ) ) if existing_value : break if existing_value : if metadata : raise NotImplementedError () if lineage and self . _lineages . get ( existing_value . id , None ): log_message ( f \"Overwriting lineage for value ' { existing_value . id } '.\" ) if lineage : self . _lineages [ existing_value . id ] = lineage return existing_value if value_schema . is_constant : if ( value_data not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ] and value_data != value_schema . default ): raise NotImplementedError () value_data = value_schema . default is_set = value_data != SpecialValue . NOT_SET is_none = value_data in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ] _metadata = None if metadata : _metadata = {} for k , v in metadata . items (): _metadata [ k ] = {} _metadata [ k ][ \"metadata_item\" ] = v . dict () _metadata [ k ][ \"metadata_schema\" ] = v . schema_json () # value_hashes: typing.Dict[str] = {} value = self . _create_value_obj ( value_schema = value_schema , is_set = is_set , is_none = is_none , type_obj = _type_obj , metadata = _metadata , value_hashes = None , ) self . _register_new_value_obj ( value_obj = value , value_data = value_data , lineage = lineage , # value_hashes=value_hashes, ) # if not value.is_none: # value.get_metadata() # metadata = self._kiara.metadata_mgmt.get_value_metadata(value=value, also_return_schema=True) # print(metadata) return value def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: return self . _lineages . get ( value_id , None ) def _create_value_obj ( self , value_schema : ValueSchema , is_set : bool , is_none : bool , type_obj : typing . Optional [ ValueType ] = None , value_hashes : typing . Optional [ typing . Iterable [ ValueHash ]] = None , metadata : typing . Optional [ typing . Mapping [ str , typing . Mapping [ str , typing . Any ]] ] = None , ): # if value_schema.is_constant and value_data not in [ # SpecialValue.NO_VALUE, # SpecialValue.NOT_SET, # None, # ]: # raise Exception( # \"Can't create value. Is a constant, but value data was provided.\" # ) if type_obj is None : if value_schema . type not in self . _kiara . value_types : log_message ( f \"Value type ' { value_schema . type } ' not supported in this kiara environment, changing type to 'any'.\" ) type_cls = self . _kiara . get_value_type_cls ( \"any\" ) else : type_cls = self . _kiara . get_value_type_cls ( value_schema . type ) type_obj = type_cls ( ** value_schema . type_config ) register_token = uuid . uuid4 () self . _register_tokens . add ( register_token ) try : value = Value ( registry = self , # type: ignore value_schema = value_schema , type_obj = type_obj , # type: ignore is_set = is_set , is_none = is_none , hashes = value_hashes , metadata = metadata , register_token = register_token , # type: ignore ) return value finally : self . _register_tokens . remove ( register_token ) def _register_new_value_obj ( self , value_obj : Value , value_data : typing . Any , lineage : typing . Optional [ ValueLineage ], # value_hashes: typing.Optional[typing.Mapping[str, ValueHash]] = None, ): # assert value_obj.id == NO_ID_YET_MARKER if value_data not in [ SpecialValue . NO_VALUE , SpecialValue . NOT_SET , SpecialValue . IGNORE , None , ]: # TODO: should we keep the original value? value_data = value_obj . type_obj . import_value ( value_data ) value_id = self . _register_value_and_data ( value = value_obj , data = value_data ) if value_obj . id == NO_ID_YET_MARKER : value_obj . id = value_id if value_obj . id != value_id : raise Exception ( f \"Inconsistent id for value: { value_obj . id } != { value_id } \" ) if value_id not in self . value_ids : raise Exception ( f \"Value id ' { value_id } ' wasn't registered propertly in registry. This is most likely a bug.\" ) if lineage : self . _lineages [ value_id ] = lineage # if value_hashes is None: # value_hashes = {} # for hash_type, value_hash in value_hashes.items(): # self._hashes.setdefault(hash_type, {})[value_hash.hash] = value_id return value_obj def set_value_lineage ( self , value_item : typing . Union [ str , Value ], value_lineage : ValueLineage ): value_obj = self . get_value_obj ( value_item ) if value_obj is None : raise Exception ( f \"No value available for: { value_item } \" ) self . _lineages [ value_obj . id ] = value_lineage def link_alias ( self , value : typing . Union [ str , Value ], alias : str , register_missing_alias : bool = True , ) -> None : value_obj = self . get_value_obj ( value ) if value_obj is None : raise Exception ( f \"No value for: { value } \" ) value_slot = self . get_value_slot ( alias = alias ) if value_slot is None : if not register_missing_alias : raise Exception ( f \"Can't link value to alias ' { alias } ': alias not registered.\" ) else : self . register_alias ( value_or_schema = value_obj , alias_name = alias ) else : value_alias = ValueAlias . from_string ( alias ) if value_alias . tag : value_slot . add_value ( value_obj , tags = [ value_alias . tag ]) else : value_slot . add_value ( value_obj ) def link_aliases ( self , value : typing . Union [ str , Value ], * aliases : str , register_missing_aliases : bool = True , ) -> None : value_obj = self . get_value_obj ( value ) if value_obj is None : raise Exception ( f \"No value for: { value } \" ) if not register_missing_aliases : invalid = [] for alias in aliases : if alias not in self . alias_names : invalid . append ( alias ) if invalid : raise Exception ( f \"Can't link value(s), invalid alias(es): { ', ' . join ( invalid ) } \" ) for alias in aliases : self . link_alias ( value_obj , alias = alias ) def _check_valid_alias ( self , alias_name : str ): match = bool ( re . match ( VALID_ALIAS_PATTERN , alias_name )) return True if match else False @abc . abstractmethod def _register_alias ( self , alias_name : str , value_schema : ValueSchema ) -> ValueSlot : pass def register_aliases ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], aliases : typing . Iterable [ str ], callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> typing . Mapping [ str , ValueSlot ]: invalid = [] for alias in aliases : if not self . _check_valid_alias ( alias_name = alias ): invalid . append ( alias ) if invalid : raise Exception ( f \"Invalid alias(es), only alphanumeric characters, '-', and '_' allowed in alias name: { ', ' . join ( invalid ) } \" ) result = {} for alias in aliases : vs = self . register_alias ( value_or_schema = value_or_schema , alias_name = alias , callbacks = callbacks ) result [ alias ] = vs return result def register_alias ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], # preseed: bool = False, alias_name : typing . Optional [ str ] = None , callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> ValueSlot : \"\"\"Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Arguments: value_or_schema: a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) preseed: whether to add an empty/non-set value to the newly created slot alias_name: the alias name, will be auto-created if not provided callbacks: a list of callbacks to trigger whenever the alias is updated \"\"\" if alias_name is None : alias_name = str ( uuid . uuid4 ()) if not alias_name : raise Exception ( \"Empty alias name not allowed.\" ) match = self . _check_valid_alias ( alias_name ) if not match : raise Exception ( f \"Invalid alias ' { alias_name } ': only alphanumeric characters, '-', and '_' allowed in alias name.\" ) # if the provided value_or_schema argument is a string, it must be a value_id (for now) if isinstance ( value_or_schema , str ): if value_or_schema not in self . value_ids : raise Exception ( f \"Can't register alias ' { alias_name } ', provided string ' { value_or_schema } ' not an existing value id.\" ) _value_or_schema = self . get_value_obj ( value_or_schema ) if _value_or_schema is None : raise Exception ( f \"No value registered for: { value_or_schema } \" ) value_or_schema = _value_or_schema if isinstance ( value_or_schema , Value ): _value_schema = value_or_schema . value_schema _value : typing . Optional [ Value ] = value_or_schema elif isinstance ( value_or_schema , ValueSchema ): _value_schema = value_or_schema _value = None else : raise TypeError ( f \"Invalid value type: { type ( value_or_schema ) } \" ) if alias_name in self . _get_available_aliases (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists.\" ) elif alias_name in self . _get_available_value_ids (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists as value id.\" ) # vs = ValueSlot.from_value(id=_id, value=value_or_schema) vs = self . _register_alias ( alias_name = alias_name , value_schema = _value_schema ) # self._value_slots[vs.id] = vs # if preseed: # _v = Value(value_schema=_value_schema, kiara=self._kiara, registry=self) # vs.add_value(_v) if callbacks : vs . register_callbacks ( * callbacks ) # self.register_callbacks(vs, *callbacks) if _value is not None : vs . add_value ( _value ) return vs # def find_value_slots( # self, value_item: typing.Union[str, Value] # ) -> typing.List[ValueSlot]: # # value_item = self.get_value_obj(value_item) # result = [] # for slot_id, slot in self._value_slots.items(): # if slot.is_latest_value(value_item): # result.append(slot) # return result # def register_callbacks( # self, # alias: typing.Union[str, ValueSlot], # *callbacks: ValueSlotUpdateHandler, # ) -> None: # # _value_slot = self.get_value_slot(alias) # _value_slot.register_callbacks(*callbacks) def update_value_slot ( self , value_slot : typing . Union [ str , Value , ValueSlot ], data : typing . Any , value_lineage : typing . Optional [ ValueLineage ] = None , ) -> bool : # first, resolve a potential string into a value_slot or value if isinstance ( value_slot , str ): if value_slot in self . alias_names : _value_slot : typing . Union [ None , Value , ValueSlot , str ] = self . get_value_slot ( alias = value_slot ) elif value_slot in self . value_ids : _value_slot = self . get_value_obj ( value_slot ) else : _value_slot = value_slot if _value_slot is None : raise Exception ( f \"Can't retrieve target object for id: { value_slot } \" ) value_slot = _value_slot if isinstance ( value_slot , Value ): aliases = self . find_aliases_for_value ( value_slot . id ) # slots = self.find_value_slots(value_slot) if len ( aliases ) == 0 : raise Exception ( f \"No value slot found for value ' { value_slot . id } '.\" ) elif len ( aliases ) > 1 : raise Exception ( f \"Multiple value slots found for value ' { value_slot . id } '. This is not supported (yet).\" ) _value_slot_2 : typing . Optional [ ValueSlot ] = self . get_value_slot ( alias = aliases [ 0 ] . alias ) elif isinstance ( value_slot , ValueSlot ): _value_slot_2 = value_slot else : raise TypeError ( f \"Invalid type for value slot: { type ( value_slot ) } \" ) assert _value_slot_2 is not None if isinstance ( data , Value ): if value_lineage : raise Exception ( \"Can't update value slot with new value lineage data.\" ) _value : Value = data else : _value = self . register_data ( value_data = data , value_schema = value_slot . value_schema , lineage = value_lineage , ) # _value = self.create_value( # value_data=data, # value_schema=_value_slot.value_schema, # value_lineage=value_lineage, # ) return self . _update_value_slot ( value_slot = _value_slot_2 , new_value = _value , trigger_callbacks = True ) def update_value_slots ( self , updated_values : typing . Mapping [ typing . Union [ str , ValueSlot ], typing . Any ], metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> typing . Mapping [ ValueSlot , typing . Union [ bool , Exception ]]: updated : typing . Dict [ str , typing . List [ ValueSlot ]] = {} cb_map : typing . Dict [ str , ValueSlotUpdateHandler ] = {} result : typing . Dict [ ValueSlot , typing . Union [ bool , Exception ]] = {} invalid : typing . Set [ str ] = set () for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id _alias = self . get_value_slot ( alias ) if _alias is None : if isinstance ( alias , ValueSlot ): invalid . add ( alias . id ) else : invalid . add ( alias ) if invalid : raise Exception ( f \"Can't update value slots: invalid alias name(s) ' { ', ' . join ( invalid ) } '.\" ) for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id value_slot = self . get_value_slot ( alias ) if value_slot is None : raise Exception ( f \"Can't retrieve value slot for alias: { alias } \" ) for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id value_slot = self . get_value_slot ( alias ) if value_slot is None : raise Exception ( f \"No value slot for alias: { alias } .\" ) try : if isinstance ( value_item , Value ): _value_item : Value = value_item if _value_item . _registry != self : _value_item = self . register_data ( _value_item , metadata = metadata , lineage = lineage ) elif metadata : _value_item = self . register_data ( _value_item , metadata = metadata , lineage = lineage ) else : _value_item = self . register_data ( value_data = value_item , value_schema = value_slot . value_schema , metadata = metadata , lineage = lineage , ) updated_item = self . _update_value_slot ( value_slot = value_slot , new_value = _value_item , trigger_callbacks = False , ) result [ value_slot ] = updated_item if updated_item : for cb_id , cb in value_slot . _callbacks . items (): cb_map [ cb_id ] = cb updated . setdefault ( cb_id , []) . append ( value_slot ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () result [ value_slot ] = e for cb_id , value_slots in updated . items (): cb = cb_map [ cb_id ] cb . values_updated ( * value_slots ) return result def _update_value_slot ( self , value_slot : ValueSlot , new_value : Value , trigger_callbacks : bool = True ) -> bool : last_version = value_slot . latest_version_nr new_version = value_slot . add_value ( new_value , trigger_callbacks = trigger_callbacks ) updated = last_version != new_version return updated","title":"DataRegistry"},{"location":"reference/kiara/data/__init__/#kiara.data.registry.ValueSlotUpdateHandler","text":"The call signature for callbacks that can be registered as value update handlers. Source code in kiara/data/registry/__init__.py class ValueSlotUpdateHandler ( Protocol ): \"\"\"The call signature for callbacks that can be registered as value update handlers.\"\"\" def values_updated ( self , * items : \"ValueSlot\" ) -> typing . Any : ...","title":"ValueSlotUpdateHandler"},{"location":"reference/kiara/data/__init__/#kiara.data.registry.store","text":"","title":"store"},{"location":"reference/kiara/data/__init__/#kiara.data.types","text":"This is the base module that contains everything data type-related in kiara . I'm still not 100% sure how to best implement the kiara type system, there are several ways it could be done, for example based on Python type-hints, using JSON-schema, Avro (which is my 2nd favourite option), as well as by implementing a custom type-class hierarchy. Which is what I have choosen to try first. For now, it looks like it'll work out, but there is a chance requirements I haven't forseen will crop up that could make this become ugly. Anyway, the way it works (for now) is that kiara comes with a set of often used types (the standard set of: scalars, list, dict, table & array, etc.) which each come with 2 functions that can serialize and deserialize values of that type in a persistant fashion -- which could be storing as a file on disk, or as a cell/row in a database. Those functions will most likley be kiara modules themselves, with even more restricted input/output type options. In addition, packages that contain modules can implement their own, custom types, if suitable ones are not available in core- kiara . Those can either be 'serialized/deserialized' into kiara -native types (which in turn will serialize them using their own serializing functions), or will have to implement custom serializing functionality (which will probably be discouraged, since this might not be trivial and there are quite a few things to consider).","title":"types"},{"location":"reference/kiara/data/__init__/#kiara.data.types.ValueType","text":"Base class that all kiara types must inherit from. kiara types have 3 main responsibilities: serialize into / deserialize from persistent state data validation metadata extraction Serializing being the arguably most important of those, because without most of the data management features of kiara would be impossible. Validation should not require any explanation. Metadata extraction is important, because that metadata will be available to other components of kiara (or frontends for it), without them having to request the actual data. That will hopefully make kiara very efficient in terms of memory management, as well as data transfer and I/O. Ideally, the actual data (bytes) will only be requested at the last possible moment. For example when a module needs the input data to do processing on it -- and even then it might be that it only requests a part of the data, say a single column of a table. Or when a frontend needs to display/visualize the data. Source code in kiara/data/types/__init__.py class ValueType ( abc . ABC , typing . Generic [ TYPE_PYTHON_CLS , TYPE_CONFIG_CLS ]): \"\"\"Base class that all *kiara* types must inherit from. *kiara* types have 3 main responsibilities: - serialize into / deserialize from persistent state - data validation - metadata extraction Serializing being the arguably most important of those, because without most of the data management features of *kiara* would be impossible. Validation should not require any explanation. Metadata extraction is important, because that metadata will be available to other components of *kiara* (or frontends for it), without them having to request the actual data. That will hopefully make *kiara* very efficient in terms of memory management, as well as data transfer and I/O. Ideally, the actual data (bytes) will only be requested at the last possible moment. For example when a module needs the input data to do processing on it -- and even then it might be that it only requests a part of the data, say a single column of a table. Or when a frontend needs to display/visualize the data. \"\"\" @classmethod def get_type_metadata ( cls ) -> ValueTypeMetadata : return ValueTypeMetadata . from_value_type_class ( cls ) # @classmethod # def doc(cls) -> str: # # return extract_doc_from_cls(cls) # # @classmethod # def desc(cls) -> str: # return extract_doc_from_cls(cls, only_first_line=True) # @classmethod # def conversions( # self, # ) -> typing.Optional[typing.Mapping[str, typing.Mapping[str, typing.Any]]]: # \"\"\"Return a dictionary of configuration for modules that can transform this type. # # The name of the transformation is the key of the result dictionary, the configuration is a module configuration # (dictionary wth 'module_type' and optional 'module_config', 'input_name' and 'output_name' keys). # \"\"\" # # return {\"string\": {\"module_type\": \"string.pretty_print\", \"input_name\": \"item\"}} @classmethod def check_data ( cls , data : typing . Any ) -> typing . Optional [ \"ValueType\" ]: \"\"\"Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). \"\"\" return None @classmethod @abc . abstractmethod def backing_python_type ( cls ) -> typing . Type [ TYPE_PYTHON_CLS ]: pass @classmethod @abc . abstractmethod def type_config_cls ( cls ) -> typing . Type [ TYPE_CONFIG_CLS ]: pass @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return None @classmethod def get_supported_hash_types ( cls ) -> typing . Iterable [ str ]: return [] @classmethod def calculate_value_hash ( cls , value : typing . Any , hash_type : str ) -> str : \"\"\"Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. \"\"\" raise Exception ( f \"Value type ' { cls . _value_type_name } ' does not support hash calculation.\" ) # type: ignore def __init__ ( self , ** type_config : typing . Any ): try : self . _type_config : TYPE_CONFIG_CLS = self . __class__ . type_config_cls ( ** type_config ) # type: ignore # TODO: double-check this is only a mypy issue except ValidationError as ve : raise ValueTypeConfigException ( f \"Error creating object for value_type: { ve } \" , self . __class__ , type_config , ve , ) # self._type_config: typing.Mapping[str, typing.Any] = self # self._transformations: typing.Optional[ # typing.Mapping[str, typing.Mapping[str, typing.Any]] # ] = None @property def type_config ( self ) -> TYPE_CONFIG_CLS : return self . _type_config def import_value ( self , value : typing . Any ) -> typing . Any : assert value is not None try : parsed = self . parse_value ( value ) if parsed is None : parsed = value self . validate ( parsed ) except Exception as e : raise KiaraValueException ( value_type = self . __class__ , value_data = value , exception = e ) return parsed def parse_value ( self , value : typing . Any ) -> typing . Any : \"\"\"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Arguments: v: the value Returns: 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object \"\"\" return None def validate ( cls , value : typing . Any ) -> None : \"\"\"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type).\"\"\" def get_type_hint ( self , context : str = \"python\" ) -> typing . Optional [ typing . Type ]: \"\"\"Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. \"\"\" return None def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : raise NotImplementedError ()","title":"ValueType"},{"location":"reference/kiara/data/__init__/#kiara.data.types.ValueTypeConfigMetadata","text":"Source code in kiara/data/types/__init__.py class ValueTypeConfigMetadata ( MetadataModel ): @classmethod def from_config_class ( cls , config_cls : typing . Type [ ValueTypeConfigSchema ], ): flat_models = get_flat_models_from_model ( config_cls ) model_name_map = get_model_name_map ( flat_models ) m_schema , _ , _ = model_process_schema ( config_cls , model_name_map = model_name_map ) fields = m_schema [ \"properties\" ] config_values = {} for field_name , details in fields . items (): type_str = \"-- n/a --\" if \"type\" in details . keys (): type_str = details [ \"type\" ] desc = details . get ( \"description\" , DEFAULT_NO_DESC_VALUE ) default = config_cls . __fields__ [ field_name ] . default if default is None : if callable ( config_cls . __fields__ [ field_name ] . default_factory ): default = config_cls . __fields__ [ field_name ] . default_factory () # type: ignore req = config_cls . __fields__ [ field_name ] . required config_values [ field_name ] = ValueTypeAndDescription ( description = desc , type = type_str , value_default = default , required = req ) python_cls = PythonClassMetadata . from_class ( config_cls ) return ValueTypeConfigMetadata ( python_class = python_cls , config_values = config_values ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this configuration.\" ) config_values : typing . Dict [ str , ValueTypeAndDescription ] = Field ( description = \"The available configuration values.\" )","title":"ValueTypeConfigMetadata"},{"location":"reference/kiara/data/__init__/#kiara.data.types.ValueTypeConfigSchema","text":"Base class that describes the configuration a ValueType class accepts. This is stored in the _config_cls class attribute in each ValueType class. By default, a ValueType is not configurable, unless the _config_cls class attribute points to a sub-class of this class. Source code in kiara/data/types/__init__.py class ValueTypeConfigSchema ( BaseModel ): \"\"\"Base class that describes the configuration a [``ValueType``][kiara.data.types.ValueType] class accepts. This is stored in the ``_config_cls`` class attribute in each ``ValueType`` class. By default, a ``ValueType`` is not configurable, unless the ``_config_cls`` class attribute points to a sub-class of this class. \"\"\" @classmethod def requires_config ( cls ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : return True return False _config_hash : str = PrivateAttr ( default = None ) class Config : extra = Extra . forbid allow_mutation = False def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) @property def config_hash ( self ): if self . _config_hash is None : _d = self . dict () hashes = deepdiff . DeepHash ( _d ) self . _config_hash = hashes [ _d ] return self . _config_hash def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () def __hash__ ( self ): return hash ( self . config_hash ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : my_table = Table ( box = box . MINIMAL , show_header = False ) my_table . add_column ( \"Field name\" , style = \"i\" ) my_table . add_column ( \"Value\" ) for field in self . __fields__ : my_table . add_row ( field , getattr ( self , field )) yield my_table","title":"ValueTypeConfigSchema"},{"location":"reference/kiara/data/__init__/#kiara.data.types.get_type_name","text":"Utility function to get a pretty string from the class of an object. Source code in kiara/data/types/__init__.py def get_type_name ( obj : typing . Any ): \"\"\"Utility function to get a pretty string from the class of an object.\"\"\" if obj . __class__ . __module__ == \"builtins\" : return obj . __class__ . __name__ else : return f \" { obj . __class__ . __module__ } . { obj . __class__ . __name__ } \"","title":"get_type_name()"},{"location":"reference/kiara/data/__init__/#kiara.data.types.core","text":"","title":"core"},{"location":"reference/kiara/data/__init__/#kiara.data.types.type_mgmt","text":"","title":"type_mgmt"},{"location":"reference/kiara/data/__init__/#kiara.data.values","text":"A module that contains value-related classes for Kiara . A value in Kiara-speak is a pointer to actual data (aka 'bytes'). It contains metadata about that data (like whether it's valid/set, what type/schema it has, when it was last modified, ...), but it does not contain the data itself. The reason for that is that such data can be fairly large, and in a lot of cases it is not necessary for the code involved to have access to it, access to the metadata is enough. Each Value has a unique id, which can be used to retrieve the data (whole, or parts of it) from a DataRegistry . In addition, that id can be used to subscribe to change events for a value (published whenever the data that is associated with a value was changed).","title":"values"},{"location":"reference/kiara/data/__init__/#kiara.data.values.Value","text":"The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry. Source code in kiara/data/values/__init__.py class Value ( BaseModel , JupyterMixin ): \"\"\"The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry. \"\"\" class Config : extra = Extra . forbid use_enum_values = True def __init__ ( self , registry : \"BaseDataRegistry\" , value_schema : ValueSchema , type_obj : ValueType , is_set : bool , is_none : bool , hashes : typing . Optional [ typing . Iterable [ ValueHash ]] = None , metadata : typing . Optional [ typing . Mapping [ str , typing . Dict [ str , typing . Any ]]] = None , register_token : typing . Optional [ uuid . UUID ] = None ): # type: ignore if not register_token : raise Exception ( \"No register token provided.\" ) if not registry . _check_register_token ( register_token ): raise Exception ( f \"Value registration with token ' { register_token } ' not allowed.\" ) if value_schema is None : raise NotImplementedError () assert registry self . _registry = registry self . _kiara = self . _registry . _kiara kwargs : typing . Dict [ str , typing . Any ] = {} kwargs [ \"id\" ] = NO_ID_YET_MARKER kwargs [ \"value_schema\" ] = value_schema # if value_lineage is None: # value_lineage = ValueLineage() # # kwargs[\"value_lineage\"] = value_lineage # kwargs[\"is_streaming\"] = False # not used yet kwargs [ \"creation_date\" ] = datetime . now () kwargs [ \"is_set\" ] = is_set kwargs [ \"is_none\" ] = is_none if hashes : kwargs [ \"hashes\" ] = list ( hashes ) if metadata : kwargs [ \"metadata\" ] = dict ( metadata ) else : kwargs [ \"metadata\" ] = {} super () . __init__ ( ** kwargs ) self . _type_obj = type_obj _kiara : \"Kiara\" = PrivateAttr () _registry : \"BaseDataRegistry\" = PrivateAttr () _type_obj : ValueType = PrivateAttr () _value_info : \"ValueInfo\" = PrivateAttr ( default = None ) id : str = Field ( description = \"A unique id for this value.\" ) value_schema : ValueSchema = Field ( description = \"The schema of this value.\" ) creation_date : typing . Optional [ datetime ] = Field ( description = \"The time this value was created value happened.\" ) # is_streaming: bool = Field( # default=False, # description=\"Whether the value is currently streamed into this object.\", # ) is_set : bool = Field ( description = \"Whether the value was set (in some way: user input, default, constant...).\" , default = False , ) is_none : bool = Field ( description = \"Whether the value is 'None'.\" , default = True ) hashes : typing . List [ ValueHash ] = Field ( description = \"Available hashes relating to the actual value data. This attribute is not populated by default, use the 'get_hashes()' method to request one or several hash items, afterwards those hashes will be reflected in this attribute.\" , default_factory = list , ) metadata : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = Field ( description = \"Available metadata relating to the actual value data (size, no. of rows, etc. -- depending on data type). This attribute is not populated by default, use the 'get_metadata()' method to request one or several metadata items, afterwards those metadata items will be reflected in this attribute.\" , default_factory = dict , ) @property def type_name ( self ) -> str : return self . value_schema . type @property def type_obj ( self ): \"\"\"Return the object that contains all the type information for this value.\"\"\" return self . _type_obj def get_hash ( self , hash_type : str ) -> ValueHash : \"\"\"Calculate the hash of a specified type for this value. Hashes are cached.\"\"\" hashes = self . get_hashes ( hash_type ) return list ( hashes )[ 0 ] def get_hashes ( self , * hash_types : str ) -> typing . Iterable [ ValueHash ]: all_hash_types = self . type_obj . get_supported_hash_types () if not hash_types : try : hash_types = all_hash_types except Exception as e : log_message ( str ( e )) if not hash_types : return [] result = [] missing = list ( hash_types ) for hash_obj in self . hashes : if hash_obj . hash_type in hash_types : result . append ( hash_obj ) missing . remove ( hash_obj . hash_type ) for hash_type in missing : if hash_type not in all_hash_types : raise Exception ( f \"Hash type ' { hash_type } ' not supported for ' { self . type_name } '\" ) hash_str = self . type_obj . calculate_value_hash ( value = self . get_value_data (), hash_type = hash_type ) hash_obj = ValueHash ( hash_type = hash_type , hash = hash_str ) self . hashes . append ( hash_obj ) result . append ( hash_obj ) return result def item_is_valid ( self ) -> bool : \"\"\"Check whether the current value is valid\"\"\" if self . value_schema . optional : return True else : return self . is_set and not self . is_none def item_status ( self ) -> str : \"\"\"Print a human readable short description of this values status.\"\"\" if self . value_schema . optional : if self . is_set : if self . is_none : return \"no value (not required)\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set (not required)\" else : if self . is_set : if self . is_none : return \"no value\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set\" def get_value_data ( self ) -> typing . Any : return self . _registry . get_value_data ( self ) def get_lineage ( self ) -> typing . Optional [ ValueLineage ]: return self . _registry . get_lineage ( self ) def set_value_lineage ( self , value_lineage : ValueLineage ) -> None : if hasattr ( self . _registry , \"set_value_lineage\" ): return self . _registry . set_value_lineage ( self , value_lineage ) # type: ignore else : raise Exception ( \"Can't set value lineage: registry is read only\" ) def get_info ( self ) -> \"ValueInfo\" : if self . _value_info is None : self . _value_info = ValueInfo . from_value ( self ) return self . _value_info def create_info ( self , include_deserialization_config : bool = False ) -> \"ValueInfo\" : return ValueInfo . from_value ( self , include_deserialization_config = include_deserialization_config ) def get_metadata ( self , * metadata_keys : str , also_return_schema : bool = False ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: \"\"\"Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to `True`, the value will be a two-key map with the metadata under the ``metadata_item`` subkey, and the value schema under ``metadata_schema``. If no metadata key is specified, all available metadata for this value type will be returned. Arguments: metadata_keys: the metadata keys to retrieve metadata (empty for all available metadata) also_return_schema: whether to also return the schema for each metadata item \"\"\" if not metadata_keys : _metadata_keys = set ( self . _kiara . metadata_mgmt . get_metadata_keys_for_type ( self . type_name ) ) for key in self . metadata . keys (): _metadata_keys . add ( key ) else : _metadata_keys = set ( metadata_keys ) result = {} missing = set () for metadata_key in sorted ( _metadata_keys ): if metadata_key in self . metadata . keys (): if also_return_schema : result [ metadata_key ] = self . metadata [ metadata_key ] else : result [ metadata_key ] = self . metadata [ metadata_key ][ \"metadata_item\" ] else : missing . add ( metadata_key ) if not missing : return result _md = self . _kiara . metadata_mgmt . get_value_metadata ( self , * missing , also_return_schema = True ) for k , v in _md . items (): self . metadata [ k ] = v if also_return_schema : result [ k ] = v else : result [ k ] = v [ \"metadata_item\" ] return { k : result [ k ] for k in sorted ( result . keys ())} def save ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None , register_missing_aliases : bool = True , ) -> \"Value\" : \"\"\"Save this value, under the specified alias(es).\"\"\" # if self.get_value_lineage(): # for field_name, value in self.get_value_lineage().inputs.items(): # value_obj = self._registry.get_value_obj(value) # try: # value_obj.save() # except Exception as e: # print(e) value = self . _kiara . data_store . register_data ( self ) if aliases : self . _kiara . data_store . link_aliases ( value , * aliases , register_missing_aliases = register_missing_aliases ) return value def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , Value ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id ) def __repr__ ( self ): return f \"Value(id= { self . id } , type= { self . type_name } , is_set= { self . is_set } , is_none= { self . is_none } \" def __str__ ( self ): return self . __repr__ () def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : # table = self._create_value_table() title = \"Value\" yield Panel ( self . get_info (), box = box . ROUNDED , title_align = \"left\" , title = title )","title":"Value"},{"location":"reference/kiara/data/__init__/#kiara.data.values.ValueAlias","text":"Source code in kiara/data/values/__init__.py class ValueAlias ( BaseModel ): @classmethod def from_string ( self , value_alias : str , default_repo_name : typing . Optional [ str ] = None ) -> \"ValueAlias\" : if not isinstance ( value_alias , str ): raise Exception ( \"Invalid id_or_alias: not a string.\" ) if not value_alias : raise Exception ( \"Invalid id_or_alias: can't be empty string.\" ) _repo_name : typing . Optional [ str ] = default_repo_name _version : typing . Optional [ int ] = None _tag : typing . Optional [ str ] = None if \"#\" in value_alias : _repo_name , _value_alias = value_alias . split ( \"#\" , maxsplit = 1 ) else : _value_alias = value_alias if \"@\" in _value_alias : _alias , _postfix = _value_alias . split ( \"@\" , maxsplit = 1 ) try : _version = int ( _postfix ) except ValueError : if not _postfix . isidentifier (): raise Exception ( f \"Invalid format for version/tag element of id_or_alias: { _tag } \" ) _tag = _postfix else : _alias = _value_alias return ValueAlias ( repo_name = _repo_name , alias = _alias , version = _version , tag = _tag ) @classmethod def from_strings ( cls , * value_aliases : typing . Union [ str , \"ValueAlias\" ] ) -> typing . List [ \"ValueAlias\" ]: result = [] for va in value_aliases : if isinstance ( va , str ): result . append ( ValueAlias . from_string ( va )) elif isinstance ( va , ValueAlias ): result . append ( va ) else : raise TypeError ( f \"Invalid type ' { type ( va ) } ' for type alias, expected 'str' or 'ValueAlias'.\" ) return result repo_name : typing . Optional [ str ] = Field ( description = \"The name of the data repo the value lives in.\" , default = None ) alias : str = Field ( \"The alias name.\" ) version : typing . Optional [ int ] = Field ( description = \"The version of this alias.\" , default = None ) tag : typing . Optional [ str ] = Field ( description = \"The tag for the alias.\" , default = None ) @property def full_alias ( self ): if self . tag is not None : return f \" { self . alias } @ { self . tag } \" elif self . version is not None : return f \" { self . alias } @ { self . version } \" else : return self . alias","title":"ValueAlias"},{"location":"reference/kiara/data/__init__/#kiara.data.values.ValueHash","text":"Source code in kiara/data/values/__init__.py class ValueHash ( BaseModel ): hash : str = Field ( description = \"The value hash.\" ) hash_type : str = Field ( description = \"The value hash method.\" )","title":"ValueHash"},{"location":"reference/kiara/data/__init__/#kiara.data.values.ValueInfo","text":"Source code in kiara/data/values/__init__.py class ValueInfo ( KiaraInfoModel ): @classmethod def from_value ( cls , value : Value , include_deserialization_config : bool = False ): if value . id not in value . _registry . value_ids : raise Exception ( \"Value not registered (yet).\" ) # aliases = value._registry.find_aliases_for_value(value) hashes = value . get_hashes () metadata = value . get_metadata ( also_return_schema = True ) # metadata = value.metadata value_lineage = value . get_lineage () if include_deserialization_config : # serialize_operation: SerializeValueOperationType = ( # type: ignore # value._kiara.operation_mgmt.get_operation(\"serialize\") # type: ignore # ) raise NotImplementedError () return ValueInfo ( value_id = value . id , value_schema = value . value_schema , hashes = hashes , metadata = metadata , lineage = value_lineage , is_valid = value . item_is_valid (), ) value_id : str = Field ( description = \"The value id.\" ) value_schema : ValueSchema = Field ( description = \"The value schema.\" ) # aliases: typing.List[ValueAlias] = Field( # description=\"All aliases for this value.\", default_factory=list # ) # tags: typing.List[str] = Field( # description=\"All tags for this value.\", default_factory=list # ) # created: str = Field(description=\"The time the data was created.\") is_valid : bool = Field ( description = \"Whether the item is valid (in the context of its schema).\" ) hashes : typing . List [ ValueHash ] = Field ( description = \"All available hashes for this value.\" , default_factory = list ) metadata : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = Field ( description = \"The metadata associated with this value.\" ) lineage : typing . Optional [ ValueLineage ] = Field ( description = \"Information about how the value was created.\" , default = None ) deserialize_config : typing . Optional [ DeserializeConfig ] = Field ( description = \"The module config (incl. inputs) to deserialize the value.\" , default = None , ) def get_metadata_items ( self , * keys : str ) -> typing . Dict [ str , typing . Any ]: if not keys : _keys : typing . Iterable [ str ] = self . metadata . keys () else : _keys = keys result = {} for k in _keys : md = self . metadata . get ( k ) if md is None : raise Exception ( f \"No metadata for key ' { k } ' available.\" ) result [ k ] = md [ \"metadata_item\" ] return result def get_metadata_schemas ( self , * keys : str ) -> typing . Dict [ str , typing . Any ]: if not keys : _keys : typing . Iterable [ str ] = self . metadata . keys () else : _keys = keys result = {} for k in _keys : md = self . metadata . get ( k ) if md is None : raise Exception ( f \"No metadata for key ' { k } ' available.\" ) result [ k ] = md [ \"metadata_schema\" ] return result def create_renderable ( self , ** config : typing . Any ) -> RenderableType : padding = config . get ( \"padding\" , ( 0 , 1 )) skip_metadata = config . get ( \"skip_metadata\" , False ) skip_value_lineage = config . get ( \"skip_lineage\" , True ) include_ids = config . get ( \"include_ids\" , False ) table = Table ( box = box . SIMPLE , show_header = False , padding = padding ) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) table . add_row ( \"id\" , self . value_id ) # type: ignore table . add_row ( \"type\" , self . value_schema . type ) if self . value_schema . type_config : json_data = json . dumps ( self . value_schema . type_config ) tc_content = Syntax ( json_data , \"json\" ) table . add_row ( \"type config\" , tc_content ) table . add_row ( \"desc\" , self . value_schema . doc ) table . add_row ( \"is set\" , \"yes\" if self . is_valid else \"no\" ) # table.add_row(\"is constant\", \"yes\" if self.is_constant else \"no\") # if isinstance(self.value_hash, int): # vh = str(self.value_hash) # else: # vh = self.value_hash.value # table.add_row(\"hash\", vh) if self . hashes : hashes_dict = { hs . hash_type : hs . hash for hs in self . hashes } yaml_string = yaml . dump ( hashes_dict ) hases_str = Syntax ( yaml_string , \"yaml\" , background_color = \"default\" ) table . add_row ( \"\" , \"\" ) table . add_row ( \"hashes\" , hases_str ) if not skip_metadata : if self . metadata : yaml_string = yaml . dump ( data = self . get_metadata_items ()) # json_string = json.dumps(self.get_metadata_items(), indent=2) metadata = Syntax ( yaml_string , \"yaml\" , background_color = \"default\" ) table . add_row ( \"metadata\" , metadata ) else : table . add_row ( \"metadata\" , \"-- no metadata --\" ) if not skip_value_lineage and self . lineage : if self . metadata : table . add_row ( \"\" , \"\" ) # json_string = self.lineage.json(indent=2) # seed_content = Syntax(json_string, \"json\") table . add_row ( \"lineage\" , self . lineage . create_renderable ( include_ids = include_ids ) ) return table","title":"ValueInfo"},{"location":"reference/kiara/data/__init__/#kiara.data.values.ValueLineage","text":"Model containing the lineage of a value. Source code in kiara/data/values/__init__.py class ValueLineage ( ModuleConfig ): \"\"\"Model containing the lineage of a value.\"\"\" @classmethod def from_module_and_inputs ( cls , module : \"KiaraModule\" , output_name : str , inputs : typing . Mapping [ str , typing . Union [ \"Value\" , \"ValueInfo\" ]], ): module_type = module . _module_type_id # type: ignore module_config = module . config . dict () doc = module . get_type_metadata () . documentation _inputs = {} for field_name , value in inputs . items (): if isinstance ( value , Value ): _inputs [ field_name ] = value . get_info () else : _inputs [ field_name ] = value return ValueLineage . construct ( module_type = module_type , module_config = module_config , doc = doc , output_name = output_name , inputs = _inputs , ) @classmethod def create ( cls , module_type : str , module_config : typing . Mapping [ str , typing . Any ], module_doc : DocumentationMetadataModel , inputs : typing . Mapping [ str , typing . Union [ \"Value\" , \"ValueInfo\" ]], output_name : typing . Optional [ str ] = None , ): _inputs = {} for field_name , value in inputs . items (): if isinstance ( value , Value ): _inputs [ field_name ] = value . get_info () else : _inputs [ field_name ] = value return ValueLineage . construct ( module_type = module_type , module_config = dict ( module_config ), doc = module_doc , output_name = output_name , inputs = _inputs , ) output_name : typing . Optional [ str ] = Field ( description = \"The result field name for the value this refers to.\" ) inputs : typing . Dict [ str , \"ValueInfo\" ] = Field ( description = \"The inputs that were used to create the value this refers to.\" ) value_index : typing . Optional [ typing . Dict [ str , \"ValueInfo\" ]] = Field ( description = \"Index of all values that are associated with this value lineage.\" , default = None , ) def to_minimal_dict ( self , include_metadata : bool = False , include_module_doc : bool = False , include_module_config : bool = True , ) -> typing . Dict [ str , typing . Any ]: full_dict = self . dict ( exclude_none = True ) minimal_dict = filter_metadata_schema ( full_dict , include_metadata = include_metadata , include_module_doc = include_module_doc , include_module_config = include_module_config , ) return minimal_dict def create_renderable ( self , ** config : typing . Any ) -> RenderableType : all_ids = sorted ( find_all_ids_in_lineage ( self )) id_color_map = {} for idx , v_id in enumerate ( all_ids ): id_color_map [ v_id ] = COLOR_LIST [ idx % len ( COLOR_LIST )] show_ids = config . get ( \"include_ids\" , False ) tree = fill_lineage_tree ( self , include_ids = show_ids ) return tree def create_graph ( self ) -> nx . DiGraph : return create_lineage_graph ( self )","title":"ValueLineage"},{"location":"reference/kiara/data/__init__/#kiara.data.values.ValueSchema","text":"The schema of a value. The schema contains the ValueType of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the type_config field. Source code in kiara/data/values/__init__.py class ValueSchema ( BaseModel ): \"\"\"The schema of a value. The schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field. \"\"\" class Config : use_enum_values = True # extra = Extra.forbid type : str = Field ( description = \"The type of the value.\" ) type_config : typing . Dict [ str , typing . Any ] = Field ( description = \"Configuration for the type, in case it's complex.\" , default_factory = dict , ) default : typing . Any = Field ( description = \"A default value.\" , default = SpecialValue . NOT_SET ) optional : bool = Field ( description = \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , default = False , ) is_constant : bool = Field ( description = \"Whether the value is a constant.\" , default = False ) # required: typing.Any = Field( # description=\"Whether this value is required to be set.\", default=True # ) doc : str = Field ( default = \"-- n/a --\" , description = \"A description for the value of this input field.\" , ) def is_required ( self ): if self . optional : return False else : if self . default in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ]: return True else : return False def validate_types ( self , kiara : \"Kiara\" ): if self . type not in kiara . value_type_names : raise ValueError ( f \"Invalid value type ' { self . type } ', available types: { kiara . value_type_names } \" ) @property def desc ( self ): \"\"\"The first line of the 'doc' value.\"\"\" return self . doc . split ( \" \\n \" )[ 0 ] def __eq__ ( self , other ): if not isinstance ( other , ValueSchema ): return False return ( self . type , self . default ) == ( other . type , other . default ) def __hash__ ( self ): return hash (( self . type , self . default ))","title":"ValueSchema"},{"location":"reference/kiara/data/__init__/#kiara.data.values.ValueSlot","text":"Source code in kiara/data/values/__init__.py class ValueSlot ( BaseModel ): @classmethod def from_value ( cls , id : str , value : Value ) -> \"ValueSlot\" : vs = ValueSlot . from_value_schema ( id = id , value_schema = value . value_schema , kiara = value . _kiara ) vs . add_value ( value ) return vs @classmethod def from_value_schema ( cls , id : str , value_schema : ValueSchema , kiara : \"Kiara\" ) -> \"ValueSlot\" : vs = ValueSlot ( id = id , value_schema = value_schema , kiara = kiara ) return vs def __init__ ( self , ** data ): # type: ignore _kiara = data . pop ( \"kiara\" , None ) if _kiara is None : raise Exception ( \"No kiara context provided.\" ) _registry = data . pop ( \"registry\" , None ) if _registry is None : _registry = _kiara . data_registry self . _kiara = _kiara self . _registry = _registry super () . __init__ ( ** data ) _kiara : \"Kiara\" = PrivateAttr () _registry : \"DataRegistry\" = PrivateAttr () _callbacks : typing . Dict [ str , \"ValueSlotUpdateHandler\" ] = PrivateAttr ( default_factory = dict ) id : str = Field ( description = \"The id for this slot.\" ) value_schema : ValueSchema = Field ( description = \"The schema for the values of this slot.\" ) values : typing . Dict [ int , Value ] = Field ( description = \"The values of this slot, with versions as key.\" , default_factory = dict , ) tags : typing . Dict [ str , int ] = Field ( description = \"The tags for this value slot (tag name as key, linked version as value.\" , default_factory = dict , ) @property def latest_version_nr ( self ) -> int : if not self . values : return 0 return max ( self . values . keys ()) def get_latest_value ( self ) -> Value : lv = self . latest_version_nr if lv == 0 : raise Exception ( \"No value added to value slot yet.\" ) return self . values [ self . latest_version_nr ] def register_callbacks ( self , * callbacks : \"ValueSlotUpdateHandler\" ): for cb in callbacks : cb_id : typing . Optional [ str ] = None if cb_id in self . _callbacks . keys (): raise Exception ( f \"Callback with id ' { cb_id } ' already registered.\" ) if hasattr ( cb , \"id\" ): if callable ( cb . id ): # type: ignore cb_id = cb . id () # type: ignore else : cb_id = cb . id # type: ignore elif hasattr ( cb , \"get_id\" ): cb_id = cb . get_id () # type: ignore if cb_id is None : cb_id = str ( uuid . uuid4 ()) assert isinstance ( cb_id , str ) self . _callbacks [ cb_id ] = cb def add_value ( self , value : Value , trigger_callbacks : bool = True , tags : typing . Optional [ typing . Iterable [ str ]] = None , ) -> int : \"\"\"Add a value to this slot.\"\"\" if self . latest_version_nr != 0 and value . id == self . get_latest_value () . id : return self . latest_version_nr # TODO: check value data equality if self . value_schema . is_constant and self . values : if is_debug (): import traceback traceback . print_stack () raise Exception ( \"Can't add value: value slot marked as 'constant'.\" ) version = self . latest_version_nr + 1 assert version not in self . values . keys () self . values [ version ] = value if tags : for tag in tags : self . tags [ tag ] = version if trigger_callbacks : for cb in self . _callbacks . values (): cb . values_updated ( self ) return version def is_latest_value ( self , value : Value ): return value . id == self . get_latest_value () . id def find_linked_aliases ( self , value_item : typing . Union [ Value , str ] ) -> typing . List [ \"ValueAlias\" ]: if isinstance ( value_item , Value ): value_item = value_item . id result = [] for _version , _value in self . values . items (): if _value . id == value_item : va = ValueAlias ( alias = self . id , version = _version ) result . append ( va ) if _version in self . tags . values (): for _tag , _tag_version in self . tags . items (): if _tag_version == _version : va = ValueAlias ( alias = self . id , tag = _tag ) result . append ( va ) return result def __eq__ ( self , other ): if not isinstance ( other , ValueSlot ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id )","title":"ValueSlot"},{"location":"reference/kiara/data/__init__/#kiara.data.values.value_set","text":"","title":"value_set"},{"location":"reference/kiara/data/onboarding/__init__/","text":"ValueStoreConfig ( BaseModel ) pydantic-model \u00b6 A configuration that describes which step outputs of a pipeline to save, and how. Source code in kiara/data/onboarding/__init__.py class ValueStoreConfig ( BaseModel ): \"\"\"A configuration that describes which step outputs of a pipeline to save, and how.\"\"\" @classmethod def save_fields ( self , base_id : str , value_set : typing . Mapping [ str , Value ], matchers : typing . Optional [ typing . List [ FieldMatcher ]] = None , ) -> typing . Dict [ Value , typing . List [ str ]]: if matchers is None : matchers = FieldMatcher . create_default_matchers () for matcher in matchers : if matcher . matcher_type not in [ \"glob\" , \"regex\" ]: raise NotImplementedError ( \"Only 'glob' and 'regex' field matcher type implemented yet.\" ) if matcher . matcher_type == \"glob\" : matcher . matcher_type = \"regex\" matcher . match_expr = fnmatch . translate ( matcher . match_expr ) env = Environment () to_save : typing . Dict [ str , typing . Set [ str ]] = {} for field_name in value_set . keys (): for matcher in matchers : if not re . match ( matcher . match_expr , field_name ): continue template = env . from_string ( matcher . alias_template ) rendered = template . render ( field_name = field_name , base_id = base_id ) to_save . setdefault ( field_name , set ()) . add ( rendered ) if not matcher . check_next_on_match : break result : typing . Dict [ Value , typing . List [ str ]] = {} for field_name , aliases in to_save . items (): value = value_set [ field_name ] _v = value . save ( aliases = aliases ) result . setdefault ( _v , []) . extend ( aliases ) return result inputs : typing . List [ FieldMatcher ] = Field ( description = \"Whether and how to save inputs.\" , default_factory = list ) outputs : typing . List [ FieldMatcher ] = Field ( description = \"Whether and how to save inputs.\" , default_factory = FieldMatcher . create_default_matchers , ) steps : typing . List [ \"ValueStoreConfig\" ] = Field ( description = \"Whether and how to save step inputs and outputs.\" , default_factory = list , ) inputs : List [ kiara . data . onboarding . FieldMatcher ] pydantic-field \u00b6 Whether and how to save inputs. outputs : List [ kiara . data . onboarding . FieldMatcher ] pydantic-field \u00b6 Whether and how to save inputs. steps : List [ ValueStoreConfig ] pydantic-field \u00b6 Whether and how to save step inputs and outputs. batch \u00b6 BatchOnboard ( BaseModel ) pydantic-model \u00b6 Source code in kiara/data/onboarding/batch.py class BatchOnboard ( BaseModel ): @classmethod def create ( cls , module_type : str , inputs : typing . Mapping [ str , typing . Any ], store_config : typing . Optional [ ValueStoreConfig ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ Kiara ] = None , ): if kiara is None : kiara = Kiara . instance () operation = Operation . create_operation ( kiara = kiara , operation_id = \"_batch_onboarding_\" , config = module_type , module_config = module_config , ) if store_config is None : store_config = ValueStoreConfig () elif isinstance ( store_config , typing . Mapping ): store_config = ValueStoreConfig ( ** store_config ) return BatchOnboard ( operation = operation , inputs = inputs , store_config = store_config ) inputs : typing . Mapping [ str , typing . Any ] = Field ( description = \"The inputs.\" ) operation : Operation = Field ( description = \"The operation to use with the inputs.\" ) store_config : ValueStoreConfig = Field ( description = \"The configuration for storing operation/pipeline values.\" ) def run ( self , base_id : str ) -> typing . Mapping [ Value , typing . Iterable [ str ]]: run_result = self . operation . module . run ( ** self . inputs ) result : typing . Dict [ Value , typing . List [ str ]] = {} if self . store_config . inputs : raise NotImplementedError ( \"Storing inputs not supported yet.\" ) if self . store_config . steps : raise NotImplementedError ( \"Storing step inputs/outputs not supported yet.\" ) if self . store_config . outputs : r = ValueStoreConfig . save_fields ( base_id = base_id , value_set = run_result , matchers = self . store_config . outputs , ) for value , aliases in r . items (): result . setdefault ( value , []) . extend ( aliases ) return result inputs : Mapping [ str , Any ] pydantic-field required \u00b6 The inputs. operation : Operation pydantic-field required \u00b6 The operation to use with the inputs. store_config : ValueStoreConfig pydantic-field required \u00b6 The configuration for storing operation/pipeline values.","title":"onboarding"},{"location":"reference/kiara/data/onboarding/__init__/#kiara.data.onboarding.ValueStoreConfig","text":"A configuration that describes which step outputs of a pipeline to save, and how. Source code in kiara/data/onboarding/__init__.py class ValueStoreConfig ( BaseModel ): \"\"\"A configuration that describes which step outputs of a pipeline to save, and how.\"\"\" @classmethod def save_fields ( self , base_id : str , value_set : typing . Mapping [ str , Value ], matchers : typing . Optional [ typing . List [ FieldMatcher ]] = None , ) -> typing . Dict [ Value , typing . List [ str ]]: if matchers is None : matchers = FieldMatcher . create_default_matchers () for matcher in matchers : if matcher . matcher_type not in [ \"glob\" , \"regex\" ]: raise NotImplementedError ( \"Only 'glob' and 'regex' field matcher type implemented yet.\" ) if matcher . matcher_type == \"glob\" : matcher . matcher_type = \"regex\" matcher . match_expr = fnmatch . translate ( matcher . match_expr ) env = Environment () to_save : typing . Dict [ str , typing . Set [ str ]] = {} for field_name in value_set . keys (): for matcher in matchers : if not re . match ( matcher . match_expr , field_name ): continue template = env . from_string ( matcher . alias_template ) rendered = template . render ( field_name = field_name , base_id = base_id ) to_save . setdefault ( field_name , set ()) . add ( rendered ) if not matcher . check_next_on_match : break result : typing . Dict [ Value , typing . List [ str ]] = {} for field_name , aliases in to_save . items (): value = value_set [ field_name ] _v = value . save ( aliases = aliases ) result . setdefault ( _v , []) . extend ( aliases ) return result inputs : typing . List [ FieldMatcher ] = Field ( description = \"Whether and how to save inputs.\" , default_factory = list ) outputs : typing . List [ FieldMatcher ] = Field ( description = \"Whether and how to save inputs.\" , default_factory = FieldMatcher . create_default_matchers , ) steps : typing . List [ \"ValueStoreConfig\" ] = Field ( description = \"Whether and how to save step inputs and outputs.\" , default_factory = list , )","title":"ValueStoreConfig"},{"location":"reference/kiara/data/onboarding/__init__/#kiara.data.onboarding.ValueStoreConfig.inputs","text":"Whether and how to save inputs.","title":"inputs"},{"location":"reference/kiara/data/onboarding/__init__/#kiara.data.onboarding.ValueStoreConfig.outputs","text":"Whether and how to save inputs.","title":"outputs"},{"location":"reference/kiara/data/onboarding/__init__/#kiara.data.onboarding.ValueStoreConfig.steps","text":"Whether and how to save step inputs and outputs.","title":"steps"},{"location":"reference/kiara/data/onboarding/__init__/#kiara.data.onboarding.batch","text":"","title":"batch"},{"location":"reference/kiara/data/onboarding/__init__/#kiara.data.onboarding.batch.BatchOnboard","text":"Source code in kiara/data/onboarding/batch.py class BatchOnboard ( BaseModel ): @classmethod def create ( cls , module_type : str , inputs : typing . Mapping [ str , typing . Any ], store_config : typing . Optional [ ValueStoreConfig ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ Kiara ] = None , ): if kiara is None : kiara = Kiara . instance () operation = Operation . create_operation ( kiara = kiara , operation_id = \"_batch_onboarding_\" , config = module_type , module_config = module_config , ) if store_config is None : store_config = ValueStoreConfig () elif isinstance ( store_config , typing . Mapping ): store_config = ValueStoreConfig ( ** store_config ) return BatchOnboard ( operation = operation , inputs = inputs , store_config = store_config ) inputs : typing . Mapping [ str , typing . Any ] = Field ( description = \"The inputs.\" ) operation : Operation = Field ( description = \"The operation to use with the inputs.\" ) store_config : ValueStoreConfig = Field ( description = \"The configuration for storing operation/pipeline values.\" ) def run ( self , base_id : str ) -> typing . Mapping [ Value , typing . Iterable [ str ]]: run_result = self . operation . module . run ( ** self . inputs ) result : typing . Dict [ Value , typing . List [ str ]] = {} if self . store_config . inputs : raise NotImplementedError ( \"Storing inputs not supported yet.\" ) if self . store_config . steps : raise NotImplementedError ( \"Storing step inputs/outputs not supported yet.\" ) if self . store_config . outputs : r = ValueStoreConfig . save_fields ( base_id = base_id , value_set = run_result , matchers = self . store_config . outputs , ) for value , aliases in r . items (): result . setdefault ( value , []) . extend ( aliases ) return result","title":"BatchOnboard"},{"location":"reference/kiara/data/onboarding/batch/","text":"BatchOnboard ( BaseModel ) pydantic-model \u00b6 Source code in kiara/data/onboarding/batch.py class BatchOnboard ( BaseModel ): @classmethod def create ( cls , module_type : str , inputs : typing . Mapping [ str , typing . Any ], store_config : typing . Optional [ ValueStoreConfig ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ Kiara ] = None , ): if kiara is None : kiara = Kiara . instance () operation = Operation . create_operation ( kiara = kiara , operation_id = \"_batch_onboarding_\" , config = module_type , module_config = module_config , ) if store_config is None : store_config = ValueStoreConfig () elif isinstance ( store_config , typing . Mapping ): store_config = ValueStoreConfig ( ** store_config ) return BatchOnboard ( operation = operation , inputs = inputs , store_config = store_config ) inputs : typing . Mapping [ str , typing . Any ] = Field ( description = \"The inputs.\" ) operation : Operation = Field ( description = \"The operation to use with the inputs.\" ) store_config : ValueStoreConfig = Field ( description = \"The configuration for storing operation/pipeline values.\" ) def run ( self , base_id : str ) -> typing . Mapping [ Value , typing . Iterable [ str ]]: run_result = self . operation . module . run ( ** self . inputs ) result : typing . Dict [ Value , typing . List [ str ]] = {} if self . store_config . inputs : raise NotImplementedError ( \"Storing inputs not supported yet.\" ) if self . store_config . steps : raise NotImplementedError ( \"Storing step inputs/outputs not supported yet.\" ) if self . store_config . outputs : r = ValueStoreConfig . save_fields ( base_id = base_id , value_set = run_result , matchers = self . store_config . outputs , ) for value , aliases in r . items (): result . setdefault ( value , []) . extend ( aliases ) return result inputs : Mapping [ str , Any ] pydantic-field required \u00b6 The inputs. operation : Operation pydantic-field required \u00b6 The operation to use with the inputs. store_config : ValueStoreConfig pydantic-field required \u00b6 The configuration for storing operation/pipeline values.","title":"batch"},{"location":"reference/kiara/data/onboarding/batch/#kiara.data.onboarding.batch.BatchOnboard","text":"Source code in kiara/data/onboarding/batch.py class BatchOnboard ( BaseModel ): @classmethod def create ( cls , module_type : str , inputs : typing . Mapping [ str , typing . Any ], store_config : typing . Optional [ ValueStoreConfig ] = None , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ Kiara ] = None , ): if kiara is None : kiara = Kiara . instance () operation = Operation . create_operation ( kiara = kiara , operation_id = \"_batch_onboarding_\" , config = module_type , module_config = module_config , ) if store_config is None : store_config = ValueStoreConfig () elif isinstance ( store_config , typing . Mapping ): store_config = ValueStoreConfig ( ** store_config ) return BatchOnboard ( operation = operation , inputs = inputs , store_config = store_config ) inputs : typing . Mapping [ str , typing . Any ] = Field ( description = \"The inputs.\" ) operation : Operation = Field ( description = \"The operation to use with the inputs.\" ) store_config : ValueStoreConfig = Field ( description = \"The configuration for storing operation/pipeline values.\" ) def run ( self , base_id : str ) -> typing . Mapping [ Value , typing . Iterable [ str ]]: run_result = self . operation . module . run ( ** self . inputs ) result : typing . Dict [ Value , typing . List [ str ]] = {} if self . store_config . inputs : raise NotImplementedError ( \"Storing inputs not supported yet.\" ) if self . store_config . steps : raise NotImplementedError ( \"Storing step inputs/outputs not supported yet.\" ) if self . store_config . outputs : r = ValueStoreConfig . save_fields ( base_id = base_id , value_set = run_result , matchers = self . store_config . outputs , ) for value , aliases in r . items (): result . setdefault ( value , []) . extend ( aliases ) return result","title":"BatchOnboard"},{"location":"reference/kiara/data/onboarding/batch/#kiara.data.onboarding.batch.BatchOnboard.inputs","text":"The inputs.","title":"inputs"},{"location":"reference/kiara/data/onboarding/batch/#kiara.data.onboarding.batch.BatchOnboard.operation","text":"The operation to use with the inputs.","title":"operation"},{"location":"reference/kiara/data/onboarding/batch/#kiara.data.onboarding.batch.BatchOnboard.store_config","text":"The configuration for storing operation/pipeline values.","title":"store_config"},{"location":"reference/kiara/data/registry/__init__/","text":"BaseDataRegistry ( ABC ) \u00b6 Base class to extend if you want to write a kiara data registry. This outlines the main methods that must be available for an entity that holds some values and their data, as well as their associated aliases. In most cases users will interact with subclasses of the more fully featured DataRegistry class, but there are cases where it will make sense to sub-class this base class directly (like for example read-only 'archive' data registries). Source code in kiara/data/registry/__init__.py class BaseDataRegistry ( abc . ABC ): \"\"\"Base class to extend if you want to write a *kiara* data registry. This outlines the main methods that must be available for an entity that holds some values and their data, as well as their associated aliases. In most cases users will interact with subclasses of the more fully featured [DataRegistry][kiara.data.registry.DataRegistry] class, but there are cases where it will make sense to sub-class this base class directly (like for example read-only 'archive' data registries). \"\"\" def __init__ ( self , kiara : \"Kiara\" ): self . _id : str = str ( uuid . uuid4 ()) self . _kiara : Kiara = kiara # self._hashes: typing.Dict[str, typing.Dict[str, str]] = {} self . _register_tokens : typing . Set = set () @property def id ( self ) -> str : return self . _id @property def value_ids ( self ) -> typing . List [ str ]: return list ( self . _get_available_value_ids ()) @property def alias_names ( self ) -> typing . List [ str ]: return sorted ( self . _get_available_aliases ()) # ====================================================================== # main abstract methods @abc . abstractmethod def _get_available_value_ids ( self ) -> typing . Iterable [ str ]: \"\"\"Return all of the registries available value ids.\"\"\" @abc . abstractmethod def _get_value_obj_for_id ( self , value_id : str ) -> Value : pass @abc . abstractmethod def _get_value_data_for_id ( self , value_item : str ) -> typing . Any : pass @abc . abstractmethod def _get_available_aliases ( self ) -> typing . Iterable [ str ]: pass @abc . abstractmethod def _get_value_slot_for_alias ( self , alias_name : str ) -> ValueSlot : pass @abc . abstractmethod def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: pass def _find_value_for_hashes ( self , * hashes : ValueHash ) -> typing . Optional [ Value ]: \"\"\"Return a value that matches one of the provided hashes. This method does not need to be overwritten, but it is recommended that it is done for performance reasons. Arguments: hashes: a collection of hashes, Returns: A (registered) value that matches one of the hashes \"\"\" return None # ----------------------------------------------------------------------- # main value retrieval methods def _create_alias_obj ( self , alias : str ) -> ValueAlias : if alias . startswith ( \"value:\" ): alias = alias [ 6 :] value_alias = ValueAlias . from_string ( value_alias = alias ) if ( value_alias . alias not in self . _get_available_value_ids () and value_alias . alias not in self . _get_available_aliases () ): raise Exception ( f \"Neither id nor alias ' { alias } ' registered with this registry.\" ) if ( value_alias . alias in self . _get_available_aliases () and value_alias . tag is None and value_alias . version is None ): value_alias . version = self . get_latest_version_for_alias ( value_alias . alias ) return value_alias def get_value_obj ( self , value_item : typing . Union [ str , Value , ValueAlias , ValueSlot ], raise_exception : bool = False , ) -> typing . Optional [ Value ]: if value_item == NO_ID_YET_MARKER : raise Exception ( \"Can't get value object: value not fully registered yet.\" ) if isinstance ( value_item , Value ): value_item = value_item . id elif isinstance ( value_item , ValueSlot ): value_item = value_item . get_latest_value () . id if isinstance ( value_item , str ): value_item = self . _create_alias_obj ( value_item ) elif not isinstance ( value_item , ValueAlias ): raise TypeError ( f \"Invalid type ' { type ( value_item ) } ' for value item parameter.\" ) if value_item . alias in self . _get_available_value_ids (): _value_id = value_item . alias _value : typing . Optional [ Value ] = self . _get_value_obj_for_id ( _value_id ) elif value_item . alias in self . _get_available_aliases (): _value = self . _resolve_alias_to_value ( value_item ) else : _value = None if _value is None and raise_exception : raise Exception ( f \"No value or alias registered in registry for: { value_item } \" ) return _value def get_value_data ( self , value_item : typing . Union [ str , Value , ValueSlot , ValueAlias ] ) -> typing . Any : value_obj = self . get_value_obj ( value_item ) if value_obj is None : raise Exception ( f \"No value registered for: { value_item } \" ) if not value_obj . is_set and value_obj . value_schema . default not in ( SpecialValue . NO_VALUE , SpecialValue . NOT_SET , None , ): return value_obj . value_schema . default elif not value_obj . is_set : raise Exception ( f \"Value not set: { value_obj . id } \" ) data = self . _get_value_data_for_id ( value_obj . id ) if data == SpecialValue . NO_VALUE : return None elif isinstance ( data , Value ): return data . get_value_data () else : return data def get_lineage ( self , value_item : typing . Union [ str , Value , ValueSlot ] ) -> typing . Optional [ ValueLineage ]: value_obj = self . get_value_obj ( value_item = value_item ) if value_obj is None : raise Exception ( f \"No value registered for: { value_item } \" ) return self . _get_value_lineage ( value_obj . id ) # def get_value_info(self, value_item: typing.Union[str, Value, ValueAlias]) -> ValueInfo: # # value_obj = self.get_value_obj(value_item=value_item) # # aliases = self.find_aliases_for_value_id(value_id=value_obj.id, include_all_versions=True, include_tags=True) # # info = ValueInfo(value_id=value_obj.id, value_type=value_obj.value_schema.type, aliases=aliases, metadata=value_obj.get_metadata()) # return info def get_value_slot ( self , alias : typing . Union [ str , ValueSlot ] ) -> typing . Optional [ ValueSlot ]: if isinstance ( alias , ValueSlot ): alias = alias . id if alias not in self . alias_names : return None return self . _get_value_slot_for_alias ( alias_name = alias ) # ----------------------------------------------------------------------- # def _resolve_hash_to_value( # self, hash_str: str, hash_type: typing.Optional[str] = None # ) -> typing.Optional[Value]: # # matches: typing.Set[str] = set() # if hash_type is None: # for hash_type, details in self._hashes.items(): # if hash_str in details.keys(): # matches.add(details[hash_str]) # else: # hashes_for_type = self._hashes.get(hash_type, {}) # if hash_str in hashes_for_type.keys(): # matches.add(hashes_for_type[hash_str]) # # if len(matches) == 0: # return None # elif len(matches) > 1: # raise Exception(f\"Multiple values found for hash '{hash_str}'.\") # # match_id = next(iter(matches)) # value = self.get_value_obj(match_id) # return value def _check_register_token ( self , register_token : uuid . UUID ): return register_token in self . _register_tokens # alias-related methods def _resolve_alias_to_value ( self , alias : ValueAlias ) -> typing . Optional [ Value ]: alias_name = alias . alias alias_version = alias . version alias_tag = alias . tag value_slot = self . get_value_slot ( alias_name ) if not value_slot : raise Exception ( f \"No alias ' { alias_name } ' registered with registry.\" ) if alias_tag : _version = value_slot . tags . get ( alias_tag ) if alias_version : if _version != alias_version : raise Exception ( f \"Value alias object contains both tag and version information, but actual version of tag resolves to different version in the registry: { _version } != { alias_version } \" ) else : alias_version = _version assert alias_version is not None value = value_slot . values . get ( alias_version , None ) return value def _find_aliases_for_value_id ( self , value_id : str ) -> typing . List [ ValueAlias ]: \"\"\"Find all aliases that point to the specified value id. Sub-classes may overwrite this method for performance reasons. \"\"\" result = [] for alias in self . _get_available_aliases (): value_slot = self . get_value_slot ( alias ) assert value_slot aliases = value_slot . find_linked_aliases ( value_id ) result . extend ( aliases ) return result def _get_tags_for_alias ( self , alias : str ) -> typing . Iterable [ str ]: value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { alias } ' registered with registry.\" ) return value_slot . tags . keys () def find_aliases_for_value ( self , value_item : typing . Union [ str , Value ], include_all_versions : bool = False , include_tags : bool = False , ) -> typing . List [ ValueAlias ]: value = self . get_value_obj ( value_item ) if value is None : raise Exception ( f \"Can't find registered value for: { value_item } \" ) aliases = self . _find_aliases_for_value_id ( value_id = value . id ) result = [] latest_cache : typing . Dict [ str , int ] = {} for alias in aliases : if alias . version is not None : if not include_all_versions : if alias . alias in latest_cache . keys (): latest_version = latest_cache [ alias . alias ] else : latest_version = self . get_latest_version_for_alias ( alias . alias ) latest_cache [ alias . alias ] = latest_version if latest_version == alias . version : result . append ( alias ) else : result . append ( alias ) if alias . tag is not None and include_tags : result . append ( alias ) return result def get_latest_version_for_alias ( self , alias : str ) -> int : versions = self . get_versions_for_alias ( alias ) if not versions : return 0 else : return max ( versions ) def get_versions_for_alias ( self , alias : str ) -> typing . Iterable [ int ]: \"\"\"Return all available versions for the specified alias.\"\"\" if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . values . keys ()) def get_tags_for_alias ( self , alias : str ) -> typing . Iterable [ str ]: if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . tags . keys ()) @deprecated ( reason = \"This will be removed soon in favor of a more generic way to query values.\" ) def find_all_values_of_type ( self , value_type : str ) -> typing . Dict [ str , Value ]: \"\"\"Find all values of a certain type.\"\"\" result = {} for value_id in self . value_ids : value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"No value registered for: { value_id } \" ) if value_obj . type_name == value_type : # type: ignore result [ value_id ] = value_obj return result def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , DataRegistry ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id ) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (id= { self . id } \" def __str__ ( self ): return self . __repr__ () find_all_values_of_type ( self , value_type ) \u00b6 Find all values of a certain type. Source code in kiara/data/registry/__init__.py @deprecated ( reason = \"This will be removed soon in favor of a more generic way to query values.\" ) def find_all_values_of_type ( self , value_type : str ) -> typing . Dict [ str , Value ]: \"\"\"Find all values of a certain type.\"\"\" result = {} for value_id in self . value_ids : value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"No value registered for: { value_id } \" ) if value_obj . type_name == value_type : # type: ignore result [ value_id ] = value_obj return result get_versions_for_alias ( self , alias ) \u00b6 Return all available versions for the specified alias. Source code in kiara/data/registry/__init__.py def get_versions_for_alias ( self , alias : str ) -> typing . Iterable [ int ]: \"\"\"Return all available versions for the specified alias.\"\"\" if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . values . keys ()) DataRegistry ( BaseDataRegistry ) \u00b6 Source code in kiara/data/registry/__init__.py class DataRegistry ( BaseDataRegistry ): def __init__ ( self , kiara : \"Kiara\" ): self . _lineages : typing . Dict [ str , typing . Optional [ ValueLineage ]] = {} super () . __init__ ( kiara = kiara ) @abc . abstractmethod def _register_value_and_data ( self , value : Value , data : typing . Any ) -> str : \"\"\"Register data into this registry. Returns the values id. In case the value already has a value set (meaning it's different from string [NO_ID_YET_MARKER][kiara.data.value.NO_ID_YET_MARKER] / '__no_id_yet__' this method must use this id or throw an exception. Otherwise, it is required that the result id is different from ``NO_ID_YET_MARKER`` and a non-empty string. \"\"\" @abc . abstractmethod def _register_remote_value ( self , value : Value ) -> Value : \"\"\"Register an existing value from a different registry into this one. Arguments: value: the original value (with the '_registry' attribute still pointing to the original registry) Returns: either None (in which case the original value object will be copied with the '_registry' and '_kiara' attributes adjusted), or a value object. \"\"\" def register_data ( self , value_data : typing . Any = SpecialValue . NOT_SET , value_schema : typing . Union [ None , ValueSchema , str ] = None , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> Value : # if lineage and not lineage.output_name: # raise Exception(f\"Value lineage for data of type '{value_data.value_schema.type}' does not have 'output_name' field set.\") value_id = str ( uuid . uuid4 ()) if value_id in self . value_ids : raise Exception ( f \"Can't register value: value id ' { value_id } ' already exists.\" ) if value_id in self . alias_names : raise Exception ( f \"Can't register value: value id ' { value_id } ' already exists as alias.\" ) if value_schema is None : if isinstance ( value_data , Value ): value_schema = value_data . value_schema else : raise Exception ( f \"No value schema provided for value: { value_data } \" ) elif isinstance ( value_schema , str ): value_schema = ValueSchema ( type = value_schema ) cls = self . _kiara . get_value_type_cls ( value_schema . type ) _type_obj = cls ( ** value_schema . type_config ) existing_value : typing . Optional [ Value ] = None if isinstance ( value_data , Value ): if value_data . id in self . value_ids and not metadata : if lineage : existing_lineage = self . _lineages . get ( value_data . id , None ) if existing_lineage : log_message ( f \"Overwriting existing value lineage for: { value_data . id } \" ) self . _lineages [ value_data . id ] = lineage # TODO: check it's really the same our_value = self . get_value_obj ( value_data . id ) if our_value is None : raise Exception ( \"Could not retrieve cloned value, this is probably a bug.\" ) return our_value existing_value = self . _find_value_for_hashes ( * value_data . get_hashes ()) if existing_value and not metadata : if lineage and self . _lineages . get ( existing_value . id , None ): log_message ( f \"Overwriting value lineage for: { existing_value . id } \" ) if lineage : self . _lineages [ existing_value . id ] = lineage return existing_value if value_data . id in self . value_ids and metadata : _metadata : typing . Optional [ typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = None if metadata : _metadata = {} for k , v in metadata . items (): _metadata [ k ] = {} _metadata [ k ][ \"metadata_item\" ] = v . dict () _metadata [ k ][ \"metadata_schema\" ] = v . schema_json () # TODO: not resolve value_data ? cloned_new_metadata = self . _create_value_obj ( value_schema = value_data . value_schema , is_set = value_data . is_set , is_none = value_data . is_none , type_obj = value_data . type_obj , metadata = _metadata , value_hashes = None , ) r = self . _register_new_value_obj ( value_obj = cloned_new_metadata , value_data = value_data . get_value_data (), lineage = lineage , # value_hashes=value_hashes, ) return r copied_value = self . _register_remote_value ( value_data ) if value_data . id not in self . value_ids : raise Exception ( f \"Value with id ' { value_data . id } ' wasn't successfully registered. This is most likely a bug.\" ) assert copied_value . _registry == self assert copied_value . _kiara == self . _kiara if value_data . id != copied_value . id : log_message ( f \"Value id for value of type { copied_value . type_name } changed when registering in other registry.\" ) raise Exception ( f \"Imported value object with id ' { value_data . id } ' resulted in copied value with different id. This is a bug.\" ) # for hash_type, value_hash in copied_value.get_hashes().items(): # self._hashes.setdefault(hash_type, {})[value_hash.hash] = value_id self . _lineages [ copied_value . id ] = value_data . get_lineage () return copied_value if value_data in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE , ] and value_schema . default not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE , ]: if metadata : raise NotImplementedError () value_data = copy . deepcopy ( value_schema . default ) if value_data not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ]: # TODO: actually implement that for hash_type in _type_obj . get_supported_hash_types (): hash_str = _type_obj . calculate_value_hash ( value = value_data , hash_type = hash_type ) existing_value = self . _find_value_for_hashes ( ValueHash ( hash_type = hash_type , hash = hash_str ) ) if existing_value : break if existing_value : if metadata : raise NotImplementedError () if lineage and self . _lineages . get ( existing_value . id , None ): log_message ( f \"Overwriting lineage for value ' { existing_value . id } '.\" ) if lineage : self . _lineages [ existing_value . id ] = lineage return existing_value if value_schema . is_constant : if ( value_data not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ] and value_data != value_schema . default ): raise NotImplementedError () value_data = value_schema . default is_set = value_data != SpecialValue . NOT_SET is_none = value_data in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ] _metadata = None if metadata : _metadata = {} for k , v in metadata . items (): _metadata [ k ] = {} _metadata [ k ][ \"metadata_item\" ] = v . dict () _metadata [ k ][ \"metadata_schema\" ] = v . schema_json () # value_hashes: typing.Dict[str] = {} value = self . _create_value_obj ( value_schema = value_schema , is_set = is_set , is_none = is_none , type_obj = _type_obj , metadata = _metadata , value_hashes = None , ) self . _register_new_value_obj ( value_obj = value , value_data = value_data , lineage = lineage , # value_hashes=value_hashes, ) # if not value.is_none: # value.get_metadata() # metadata = self._kiara.metadata_mgmt.get_value_metadata(value=value, also_return_schema=True) # print(metadata) return value def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: return self . _lineages . get ( value_id , None ) def _create_value_obj ( self , value_schema : ValueSchema , is_set : bool , is_none : bool , type_obj : typing . Optional [ ValueType ] = None , value_hashes : typing . Optional [ typing . Iterable [ ValueHash ]] = None , metadata : typing . Optional [ typing . Mapping [ str , typing . Mapping [ str , typing . Any ]] ] = None , ): # if value_schema.is_constant and value_data not in [ # SpecialValue.NO_VALUE, # SpecialValue.NOT_SET, # None, # ]: # raise Exception( # \"Can't create value. Is a constant, but value data was provided.\" # ) if type_obj is None : if value_schema . type not in self . _kiara . value_types : log_message ( f \"Value type ' { value_schema . type } ' not supported in this kiara environment, changing type to 'any'.\" ) type_cls = self . _kiara . get_value_type_cls ( \"any\" ) else : type_cls = self . _kiara . get_value_type_cls ( value_schema . type ) type_obj = type_cls ( ** value_schema . type_config ) register_token = uuid . uuid4 () self . _register_tokens . add ( register_token ) try : value = Value ( registry = self , # type: ignore value_schema = value_schema , type_obj = type_obj , # type: ignore is_set = is_set , is_none = is_none , hashes = value_hashes , metadata = metadata , register_token = register_token , # type: ignore ) return value finally : self . _register_tokens . remove ( register_token ) def _register_new_value_obj ( self , value_obj : Value , value_data : typing . Any , lineage : typing . Optional [ ValueLineage ], # value_hashes: typing.Optional[typing.Mapping[str, ValueHash]] = None, ): # assert value_obj.id == NO_ID_YET_MARKER if value_data not in [ SpecialValue . NO_VALUE , SpecialValue . NOT_SET , SpecialValue . IGNORE , None , ]: # TODO: should we keep the original value? value_data = value_obj . type_obj . import_value ( value_data ) value_id = self . _register_value_and_data ( value = value_obj , data = value_data ) if value_obj . id == NO_ID_YET_MARKER : value_obj . id = value_id if value_obj . id != value_id : raise Exception ( f \"Inconsistent id for value: { value_obj . id } != { value_id } \" ) if value_id not in self . value_ids : raise Exception ( f \"Value id ' { value_id } ' wasn't registered propertly in registry. This is most likely a bug.\" ) if lineage : self . _lineages [ value_id ] = lineage # if value_hashes is None: # value_hashes = {} # for hash_type, value_hash in value_hashes.items(): # self._hashes.setdefault(hash_type, {})[value_hash.hash] = value_id return value_obj def set_value_lineage ( self , value_item : typing . Union [ str , Value ], value_lineage : ValueLineage ): value_obj = self . get_value_obj ( value_item ) if value_obj is None : raise Exception ( f \"No value available for: { value_item } \" ) self . _lineages [ value_obj . id ] = value_lineage def link_alias ( self , value : typing . Union [ str , Value ], alias : str , register_missing_alias : bool = True , ) -> None : value_obj = self . get_value_obj ( value ) if value_obj is None : raise Exception ( f \"No value for: { value } \" ) value_slot = self . get_value_slot ( alias = alias ) if value_slot is None : if not register_missing_alias : raise Exception ( f \"Can't link value to alias ' { alias } ': alias not registered.\" ) else : self . register_alias ( value_or_schema = value_obj , alias_name = alias ) else : value_alias = ValueAlias . from_string ( alias ) if value_alias . tag : value_slot . add_value ( value_obj , tags = [ value_alias . tag ]) else : value_slot . add_value ( value_obj ) def link_aliases ( self , value : typing . Union [ str , Value ], * aliases : str , register_missing_aliases : bool = True , ) -> None : value_obj = self . get_value_obj ( value ) if value_obj is None : raise Exception ( f \"No value for: { value } \" ) if not register_missing_aliases : invalid = [] for alias in aliases : if alias not in self . alias_names : invalid . append ( alias ) if invalid : raise Exception ( f \"Can't link value(s), invalid alias(es): { ', ' . join ( invalid ) } \" ) for alias in aliases : self . link_alias ( value_obj , alias = alias ) def _check_valid_alias ( self , alias_name : str ): match = bool ( re . match ( VALID_ALIAS_PATTERN , alias_name )) return True if match else False @abc . abstractmethod def _register_alias ( self , alias_name : str , value_schema : ValueSchema ) -> ValueSlot : pass def register_aliases ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], aliases : typing . Iterable [ str ], callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> typing . Mapping [ str , ValueSlot ]: invalid = [] for alias in aliases : if not self . _check_valid_alias ( alias_name = alias ): invalid . append ( alias ) if invalid : raise Exception ( f \"Invalid alias(es), only alphanumeric characters, '-', and '_' allowed in alias name: { ', ' . join ( invalid ) } \" ) result = {} for alias in aliases : vs = self . register_alias ( value_or_schema = value_or_schema , alias_name = alias , callbacks = callbacks ) result [ alias ] = vs return result def register_alias ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], # preseed: bool = False, alias_name : typing . Optional [ str ] = None , callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> ValueSlot : \"\"\"Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Arguments: value_or_schema: a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) preseed: whether to add an empty/non-set value to the newly created slot alias_name: the alias name, will be auto-created if not provided callbacks: a list of callbacks to trigger whenever the alias is updated \"\"\" if alias_name is None : alias_name = str ( uuid . uuid4 ()) if not alias_name : raise Exception ( \"Empty alias name not allowed.\" ) match = self . _check_valid_alias ( alias_name ) if not match : raise Exception ( f \"Invalid alias ' { alias_name } ': only alphanumeric characters, '-', and '_' allowed in alias name.\" ) # if the provided value_or_schema argument is a string, it must be a value_id (for now) if isinstance ( value_or_schema , str ): if value_or_schema not in self . value_ids : raise Exception ( f \"Can't register alias ' { alias_name } ', provided string ' { value_or_schema } ' not an existing value id.\" ) _value_or_schema = self . get_value_obj ( value_or_schema ) if _value_or_schema is None : raise Exception ( f \"No value registered for: { value_or_schema } \" ) value_or_schema = _value_or_schema if isinstance ( value_or_schema , Value ): _value_schema = value_or_schema . value_schema _value : typing . Optional [ Value ] = value_or_schema elif isinstance ( value_or_schema , ValueSchema ): _value_schema = value_or_schema _value = None else : raise TypeError ( f \"Invalid value type: { type ( value_or_schema ) } \" ) if alias_name in self . _get_available_aliases (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists.\" ) elif alias_name in self . _get_available_value_ids (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists as value id.\" ) # vs = ValueSlot.from_value(id=_id, value=value_or_schema) vs = self . _register_alias ( alias_name = alias_name , value_schema = _value_schema ) # self._value_slots[vs.id] = vs # if preseed: # _v = Value(value_schema=_value_schema, kiara=self._kiara, registry=self) # vs.add_value(_v) if callbacks : vs . register_callbacks ( * callbacks ) # self.register_callbacks(vs, *callbacks) if _value is not None : vs . add_value ( _value ) return vs # def find_value_slots( # self, value_item: typing.Union[str, Value] # ) -> typing.List[ValueSlot]: # # value_item = self.get_value_obj(value_item) # result = [] # for slot_id, slot in self._value_slots.items(): # if slot.is_latest_value(value_item): # result.append(slot) # return result # def register_callbacks( # self, # alias: typing.Union[str, ValueSlot], # *callbacks: ValueSlotUpdateHandler, # ) -> None: # # _value_slot = self.get_value_slot(alias) # _value_slot.register_callbacks(*callbacks) def update_value_slot ( self , value_slot : typing . Union [ str , Value , ValueSlot ], data : typing . Any , value_lineage : typing . Optional [ ValueLineage ] = None , ) -> bool : # first, resolve a potential string into a value_slot or value if isinstance ( value_slot , str ): if value_slot in self . alias_names : _value_slot : typing . Union [ None , Value , ValueSlot , str ] = self . get_value_slot ( alias = value_slot ) elif value_slot in self . value_ids : _value_slot = self . get_value_obj ( value_slot ) else : _value_slot = value_slot if _value_slot is None : raise Exception ( f \"Can't retrieve target object for id: { value_slot } \" ) value_slot = _value_slot if isinstance ( value_slot , Value ): aliases = self . find_aliases_for_value ( value_slot . id ) # slots = self.find_value_slots(value_slot) if len ( aliases ) == 0 : raise Exception ( f \"No value slot found for value ' { value_slot . id } '.\" ) elif len ( aliases ) > 1 : raise Exception ( f \"Multiple value slots found for value ' { value_slot . id } '. This is not supported (yet).\" ) _value_slot_2 : typing . Optional [ ValueSlot ] = self . get_value_slot ( alias = aliases [ 0 ] . alias ) elif isinstance ( value_slot , ValueSlot ): _value_slot_2 = value_slot else : raise TypeError ( f \"Invalid type for value slot: { type ( value_slot ) } \" ) assert _value_slot_2 is not None if isinstance ( data , Value ): if value_lineage : raise Exception ( \"Can't update value slot with new value lineage data.\" ) _value : Value = data else : _value = self . register_data ( value_data = data , value_schema = value_slot . value_schema , lineage = value_lineage , ) # _value = self.create_value( # value_data=data, # value_schema=_value_slot.value_schema, # value_lineage=value_lineage, # ) return self . _update_value_slot ( value_slot = _value_slot_2 , new_value = _value , trigger_callbacks = True ) def update_value_slots ( self , updated_values : typing . Mapping [ typing . Union [ str , ValueSlot ], typing . Any ], metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> typing . Mapping [ ValueSlot , typing . Union [ bool , Exception ]]: updated : typing . Dict [ str , typing . List [ ValueSlot ]] = {} cb_map : typing . Dict [ str , ValueSlotUpdateHandler ] = {} result : typing . Dict [ ValueSlot , typing . Union [ bool , Exception ]] = {} invalid : typing . Set [ str ] = set () for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id _alias = self . get_value_slot ( alias ) if _alias is None : if isinstance ( alias , ValueSlot ): invalid . add ( alias . id ) else : invalid . add ( alias ) if invalid : raise Exception ( f \"Can't update value slots: invalid alias name(s) ' { ', ' . join ( invalid ) } '.\" ) for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id value_slot = self . get_value_slot ( alias ) if value_slot is None : raise Exception ( f \"Can't retrieve value slot for alias: { alias } \" ) for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id value_slot = self . get_value_slot ( alias ) if value_slot is None : raise Exception ( f \"No value slot for alias: { alias } .\" ) try : if isinstance ( value_item , Value ): _value_item : Value = value_item if _value_item . _registry != self : _value_item = self . register_data ( _value_item , metadata = metadata , lineage = lineage ) elif metadata : _value_item = self . register_data ( _value_item , metadata = metadata , lineage = lineage ) else : _value_item = self . register_data ( value_data = value_item , value_schema = value_slot . value_schema , metadata = metadata , lineage = lineage , ) updated_item = self . _update_value_slot ( value_slot = value_slot , new_value = _value_item , trigger_callbacks = False , ) result [ value_slot ] = updated_item if updated_item : for cb_id , cb in value_slot . _callbacks . items (): cb_map [ cb_id ] = cb updated . setdefault ( cb_id , []) . append ( value_slot ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () result [ value_slot ] = e for cb_id , value_slots in updated . items (): cb = cb_map [ cb_id ] cb . values_updated ( * value_slots ) return result def _update_value_slot ( self , value_slot : ValueSlot , new_value : Value , trigger_callbacks : bool = True ) -> bool : last_version = value_slot . latest_version_nr new_version = value_slot . add_value ( new_value , trigger_callbacks = trigger_callbacks ) updated = last_version != new_version return updated register_alias ( self , value_or_schema , alias_name = None , callbacks = None ) \u00b6 Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Parameters: Name Type Description Default value_or_schema Union[kiara.data.values.Value, kiara.data.values.ValueSchema, str] a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) required preseed whether to add an empty/non-set value to the newly created slot required alias_name Optional[str] the alias name, will be auto-created if not provided None callbacks Optional[Iterable[kiara.data.registry.ValueSlotUpdateHandler]] a list of callbacks to trigger whenever the alias is updated None Source code in kiara/data/registry/__init__.py def register_alias ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], # preseed: bool = False, alias_name : typing . Optional [ str ] = None , callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> ValueSlot : \"\"\"Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Arguments: value_or_schema: a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) preseed: whether to add an empty/non-set value to the newly created slot alias_name: the alias name, will be auto-created if not provided callbacks: a list of callbacks to trigger whenever the alias is updated \"\"\" if alias_name is None : alias_name = str ( uuid . uuid4 ()) if not alias_name : raise Exception ( \"Empty alias name not allowed.\" ) match = self . _check_valid_alias ( alias_name ) if not match : raise Exception ( f \"Invalid alias ' { alias_name } ': only alphanumeric characters, '-', and '_' allowed in alias name.\" ) # if the provided value_or_schema argument is a string, it must be a value_id (for now) if isinstance ( value_or_schema , str ): if value_or_schema not in self . value_ids : raise Exception ( f \"Can't register alias ' { alias_name } ', provided string ' { value_or_schema } ' not an existing value id.\" ) _value_or_schema = self . get_value_obj ( value_or_schema ) if _value_or_schema is None : raise Exception ( f \"No value registered for: { value_or_schema } \" ) value_or_schema = _value_or_schema if isinstance ( value_or_schema , Value ): _value_schema = value_or_schema . value_schema _value : typing . Optional [ Value ] = value_or_schema elif isinstance ( value_or_schema , ValueSchema ): _value_schema = value_or_schema _value = None else : raise TypeError ( f \"Invalid value type: { type ( value_or_schema ) } \" ) if alias_name in self . _get_available_aliases (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists.\" ) elif alias_name in self . _get_available_value_ids (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists as value id.\" ) # vs = ValueSlot.from_value(id=_id, value=value_or_schema) vs = self . _register_alias ( alias_name = alias_name , value_schema = _value_schema ) # self._value_slots[vs.id] = vs # if preseed: # _v = Value(value_schema=_value_schema, kiara=self._kiara, registry=self) # vs.add_value(_v) if callbacks : vs . register_callbacks ( * callbacks ) # self.register_callbacks(vs, *callbacks) if _value is not None : vs . add_value ( _value ) return vs ValueSlotUpdateHandler ( Protocol ) \u00b6 The call signature for callbacks that can be registered as value update handlers. Source code in kiara/data/registry/__init__.py class ValueSlotUpdateHandler ( Protocol ): \"\"\"The call signature for callbacks that can be registered as value update handlers.\"\"\" def values_updated ( self , * items : \"ValueSlot\" ) -> typing . Any : ... store \u00b6 LocalDataStore ( DataRegistry ) \u00b6 An implementation of a kiara data registry that stores data locally, so it can be accessed across sessions. Data is stored under the value for the configured 'base_path' attribute, along with LoadConfig data that describes how the data can be re-loaded into memory. Source code in kiara/data/registry/store.py class LocalDataStore ( DataRegistry ): \"\"\"An implementation of a *kiara* data registry that stores data locally, so it can be accessed across sessions. Data is stored under the value for the configured 'base_path' attribute, along with [LoadConfig][kiara.metadata.data.LoadConfig] data that describes how the data can be re-loaded into memory. \"\"\" def __init__ ( self , kiara : \"Kiara\" , base_path : typing . Union [ str , Path ]): if isinstance ( base_path , str ): base_path = Path ( base_path ) self . _base_path : Path = base_path self . _value_obj_cache : typing . Dict [ str , Value ] = {} self . _value_info_cache : typing . Dict [ str , SavedValueInfo ] = {} self . _data_cache : typing . Dict [ str , Value ] = {} self . _metadata_cache : typing . Dict [ str , typing . Any ] = {} self . _value_slots : typing . Dict [ str , ValueSlot ] = {} super () . __init__ ( kiara = kiara ) @property def base_path ( self ) -> Path : return self . _base_path def _get_saved_value_info ( self , value_id : str ) -> SavedValueInfo : if value_id in self . _value_info_cache . keys (): return self . _value_info_cache [ value_id ] metadata_path = self . get_metadata_path ( value_id = value_id ) if not metadata_path . exists (): raise Exception ( f \"No value with id ' { value_id } ' registered.\" ) info_content = metadata_path . read_text () info_dict = json . loads ( info_content ) value_info = SavedValueInfo ( ** info_dict ) assert value_info . value_id == value_id self . _value_info_cache [ value_id ] = value_info return value_info def _get_value_obj_for_id ( self , value_id : str ) -> Value : if value_id in self . _value_obj_cache . keys (): return self . _value_obj_cache [ value_id ] value_info : SavedValueInfo = self . _get_saved_value_info ( value_id ) value_schema = value_info . value_schema # value_schema = ValueSchema( # type=value_info.value_type, type_config=value_info.value_type_config # ) value_obj = self . _create_value_obj ( value_schema = value_schema , is_set = True , is_none = False , value_hashes = value_info . hashes , metadata = value_info . metadata , ) value_obj . id = value_info . value_id self . _register_new_value_obj ( value_obj = value_obj , value_data = SpecialValue . IGNORE , lineage = value_info . lineage , # value_hashes=value_info.hashes, ) self . _value_obj_cache [ value_obj . id ] = value_obj return value_obj def _get_value_data_for_id ( self , value_id : str ) -> typing . Any : if value_id in self . _data_cache . keys (): return self . _data_cache [ value_id ] . get_value_data () # # value_obj = self.get_value_obj(value_item=value_id) # metadata_path = self.get_metadata_path(value_id=value_id) # if not metadata_path.exists(): # raise Exception(f\"No value with id '{value_id}' registered.\") # # info_content = metadata_path.read_text() # info_dict = json.loads(info_content) # value_info = SavedValueInfo(**info_dict) value_info = self . _get_saved_value_info ( value_id = value_id ) assert value_info . value_id == value_id load_config = value_info . load_config value : Value = self . _kiara . run ( ** load_config . dict ( exclude = { \"value_id\" , \"base_path_input_name\" , \"doc\" })) # type: ignore self . _data_cache [ value_id ] = value return value . get_value_data () def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: return self . _get_saved_value_info ( value_id ) . lineage def _register_value_and_data ( self , value : Value , data : typing . Any ) -> str : if data == SpecialValue . IGNORE : return value . id raise NotImplementedError () # # new_value_id = str(uuid.uuid4()) # # value.id = new_value_id # value_type = value.type_name # # # run the type and repo type specific save module, and retrieve the load_config that is needed to retrieve it again later # save_config = self._prepare_save_config( # value_id=new_value_id, value_type=value_type, value=value # ) # # save_module = save_config.create_module(kiara=self._kiara) # result = save_module.run(**save_config.inputs) # load_config_value: Value = result.get_value_obj(\"load_config\") # # # assemble the value metadata # metadata: typing.Dict[str, typing.Mapping[str, typing.Any]] = dict( # value.get_metadata(also_return_schema=True) # ) def _register_remote_value ( self , value : Value ) -> Value : value_metadata_path = self . get_metadata_path ( value . id ) if value_metadata_path . exists (): raise Exception ( f \"Can't register remote value with id ' { value . id } '. Load config for this id already exists: { value_metadata_path . as_posix () } \" ) value_type = value . type_name # run the type and repo type specific save module, and retrieve the load_config that is needed to retrieve it again later save_config = self . _prepare_save_config ( value_id = value . id , value_type = value_type , value = value ) save_module = save_config . create_module ( kiara = self . _kiara ) result = save_module . run ( ** save_config . inputs ) load_config_value : Value = result . get_value_obj ( \"load_config\" ) field_name = value_type if field_name == \"any\" : field_name = \"value_item\" saved_value : Value = result . get_value_obj ( field_name ) load_config_data = load_config_value . get_value_data () metadata = saved_value . get_metadata ( also_return_schema = True ) value_info = SavedValueInfo ( value_id = value . id , value_schema = saved_value . value_schema , is_valid = saved_value . item_is_valid (), hashes = saved_value . get_hashes (), lineage = value . get_lineage (), save_lineage = saved_value . get_lineage (), metadata = metadata , load_config = load_config_data , ) value_metadata_path . parent . mkdir ( parents = True , exist_ok = True ) value_metadata_path . write_text ( value_info . json ()) copied_value = saved_value . copy () copied_value . _registry = self copied_value . _kiara = self . _kiara if copied_value . id != value . id : log_message ( f \"Saved value id different to original id ' { value . id } .\" ) copied_value . id = value . id self . _value_obj_cache [ copied_value . id ] = copied_value return copied_value def _prepare_save_config ( self , value_id : str , value_type : str , value : \"Value\" ) -> SaveConfig : store_operations : StoreOperationType = self . _kiara . operation_mgmt . get_operations ( \"store_value\" ) # type: ignore op_config = store_operations . get_store_operation_for_type ( value_type ) if not op_config : raise Exception ( f \"Can't save value: no save operation found for value type ' { value_type } '\" ) target_path = self . get_data_path ( value_id = value_id ) metadata_path = self . get_metadata_path ( value_id = value_id ) if os . path . exists ( metadata_path ): raise Exception ( f \"Can't save value, metadata file already exists: { metadata_path } \" ) input_name = value . type_name if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : value . get_value_data (), \"base_path\" : target_path . as_posix (), \"value_id\" : value_id , } # type: ignore save_config = SaveConfig ( module_type = op_config . module_type , module_config = op_config . module_config , inputs = inputs , load_config_output = \"load_config\" , ) return save_config def get_data_path ( self , value_id : str ) -> Path : return self . _base_path / f \"value_ { value_id } \" / \"data\" def get_metadata_path ( self , value_id : str ) -> Path : path = self . _base_path / f \"value_ { value_id } \" / \"value_metadata.json\" return path # def get_load_config_path(self, value_id: str) -> Path: # path = self._base_path / f\"value_{value_id}\" / \"load_config.json\" # return path def _get_value_slot_for_alias ( self , alias_name : str ) -> ValueSlot : if alias_name in self . _value_slots . keys (): return self . _value_slots [ alias_name ] alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" if not alias_file . is_file (): raise Exception ( f \"No alias with name ' { alias_name } ' registered.\" ) file_content = alias_file . read_text () alias_data = json . loads ( file_content ) value_schema = ValueSchema ( ** alias_data [ \"value_schema\" ]) value_slot = ValueSlot ( id = alias_name , value_schema = value_schema , kiara = self . _kiara , registry = self ) value_slot . register_callbacks ( self ) for version in sorted (( int ( x ) for x in alias_data [ \"versions\" ] . keys ())): value_id = alias_data [ \"versions\" ][ str ( version )] value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"Can't find value with id: { value_id } \" ) tags : typing . Optional [ typing . Iterable [ str ]] = None if value_id in alias_data [ \"tags\" ] . values (): tags = [ tag for tag , version in alias_data [ \"tags\" ] . items () if version == version ] new_version = value_slot . add_value ( value_obj , trigger_callbacks = False , tags = tags ) if new_version != int ( version ): raise Exception ( f \"New version different to stored version ( { new_version } != { version } ). This is a bug.\" ) self . _value_slots [ alias_name ] = value_slot return self . _value_slots [ alias_name ] def _get_available_aliases ( self ) -> typing . Iterable [ str ]: alias_path = self . _base_path / \"aliases\" alias_file = alias_path . glob ( \"alias_*.json\" ) result = [] for af in alias_file : if not af . is_file (): log_message ( f \"Ignoring non-file: { af . as_posix () } \" ) continue alias_name = af . name [ 6 : - 5 ] result . append ( alias_name ) return result def _get_available_value_ids ( self ) -> typing . Iterable [ str ]: value_ids = [ x . parent . name [ 6 :] for x in self . _base_path . glob ( \"value_*/value_metadata.json\" ) ] return value_ids def _register_alias ( self , alias_name : str , value_schema : ValueSchema ) -> ValueSlot : alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" if alias_file . exists (): raise Exception ( f \"Alias ' { alias_name } ' already registered.\" ) if alias_name in self . _value_slots . keys (): raise Exception ( f \"Value slot for alias ' { alias_name } ' already exists.\" ) alias_file . parent . mkdir ( parents = True , exist_ok = True ) vs = ValueSlot ( id = alias_name , value_schema = value_schema , kiara = self . _kiara , registry = self ) self . _value_slots [ alias_name ] = vs self . _write_alias_file ( alias_name ) vs . register_callbacks ( self ) return vs def _write_alias_file ( self , alias_name ): alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" value_slot = self . _value_slots [ alias_name ] alias_dict = { \"value_schema\" : value_slot . value_schema . dict (), \"versions\" : {}, \"tags\" : {}, } for version , value in value_slot . values . items (): alias_dict [ \"versions\" ][ version ] = value . id for tag_name , version in value_slot . tags . items (): alias_dict [ \"tags\" ][ tag_name ] = version alias_file . write_text ( json . dumps ( alias_dict )) def values_updated ( self , * items : \"ValueSlot\" ): invalid = [] for item in items : if item . id not in self . _value_slots . keys (): invalid . append ( item . id ) if invalid : raise Exception ( f \"Can't update value(s), invalid aliases: { ', ' . join ( invalid ) } \" ) for item in items : self . _write_alias_file ( alias_name = item . id ) SavedValueInfo ( ValueInfo ) pydantic-model \u00b6 Source code in kiara/data/registry/store.py class SavedValueInfo ( ValueInfo ): load_config : LoadConfig = Field ( description = \"The configuration to load this value from disk (or however it is stored).\" ) save_lineage : ValueLineage = Field ( description = \"Information about how the value was saved.\" ) load_config : LoadConfig pydantic-field required \u00b6 The configuration to load this value from disk (or however it is stored). save_lineage : ValueLineage pydantic-field required \u00b6 Information about how the value was saved.","title":"registry"},{"location":"reference/kiara/data/registry/__init__/#kiara.data.registry.BaseDataRegistry","text":"Base class to extend if you want to write a kiara data registry. This outlines the main methods that must be available for an entity that holds some values and their data, as well as their associated aliases. In most cases users will interact with subclasses of the more fully featured DataRegistry class, but there are cases where it will make sense to sub-class this base class directly (like for example read-only 'archive' data registries). Source code in kiara/data/registry/__init__.py class BaseDataRegistry ( abc . ABC ): \"\"\"Base class to extend if you want to write a *kiara* data registry. This outlines the main methods that must be available for an entity that holds some values and their data, as well as their associated aliases. In most cases users will interact with subclasses of the more fully featured [DataRegistry][kiara.data.registry.DataRegistry] class, but there are cases where it will make sense to sub-class this base class directly (like for example read-only 'archive' data registries). \"\"\" def __init__ ( self , kiara : \"Kiara\" ): self . _id : str = str ( uuid . uuid4 ()) self . _kiara : Kiara = kiara # self._hashes: typing.Dict[str, typing.Dict[str, str]] = {} self . _register_tokens : typing . Set = set () @property def id ( self ) -> str : return self . _id @property def value_ids ( self ) -> typing . List [ str ]: return list ( self . _get_available_value_ids ()) @property def alias_names ( self ) -> typing . List [ str ]: return sorted ( self . _get_available_aliases ()) # ====================================================================== # main abstract methods @abc . abstractmethod def _get_available_value_ids ( self ) -> typing . Iterable [ str ]: \"\"\"Return all of the registries available value ids.\"\"\" @abc . abstractmethod def _get_value_obj_for_id ( self , value_id : str ) -> Value : pass @abc . abstractmethod def _get_value_data_for_id ( self , value_item : str ) -> typing . Any : pass @abc . abstractmethod def _get_available_aliases ( self ) -> typing . Iterable [ str ]: pass @abc . abstractmethod def _get_value_slot_for_alias ( self , alias_name : str ) -> ValueSlot : pass @abc . abstractmethod def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: pass def _find_value_for_hashes ( self , * hashes : ValueHash ) -> typing . Optional [ Value ]: \"\"\"Return a value that matches one of the provided hashes. This method does not need to be overwritten, but it is recommended that it is done for performance reasons. Arguments: hashes: a collection of hashes, Returns: A (registered) value that matches one of the hashes \"\"\" return None # ----------------------------------------------------------------------- # main value retrieval methods def _create_alias_obj ( self , alias : str ) -> ValueAlias : if alias . startswith ( \"value:\" ): alias = alias [ 6 :] value_alias = ValueAlias . from_string ( value_alias = alias ) if ( value_alias . alias not in self . _get_available_value_ids () and value_alias . alias not in self . _get_available_aliases () ): raise Exception ( f \"Neither id nor alias ' { alias } ' registered with this registry.\" ) if ( value_alias . alias in self . _get_available_aliases () and value_alias . tag is None and value_alias . version is None ): value_alias . version = self . get_latest_version_for_alias ( value_alias . alias ) return value_alias def get_value_obj ( self , value_item : typing . Union [ str , Value , ValueAlias , ValueSlot ], raise_exception : bool = False , ) -> typing . Optional [ Value ]: if value_item == NO_ID_YET_MARKER : raise Exception ( \"Can't get value object: value not fully registered yet.\" ) if isinstance ( value_item , Value ): value_item = value_item . id elif isinstance ( value_item , ValueSlot ): value_item = value_item . get_latest_value () . id if isinstance ( value_item , str ): value_item = self . _create_alias_obj ( value_item ) elif not isinstance ( value_item , ValueAlias ): raise TypeError ( f \"Invalid type ' { type ( value_item ) } ' for value item parameter.\" ) if value_item . alias in self . _get_available_value_ids (): _value_id = value_item . alias _value : typing . Optional [ Value ] = self . _get_value_obj_for_id ( _value_id ) elif value_item . alias in self . _get_available_aliases (): _value = self . _resolve_alias_to_value ( value_item ) else : _value = None if _value is None and raise_exception : raise Exception ( f \"No value or alias registered in registry for: { value_item } \" ) return _value def get_value_data ( self , value_item : typing . Union [ str , Value , ValueSlot , ValueAlias ] ) -> typing . Any : value_obj = self . get_value_obj ( value_item ) if value_obj is None : raise Exception ( f \"No value registered for: { value_item } \" ) if not value_obj . is_set and value_obj . value_schema . default not in ( SpecialValue . NO_VALUE , SpecialValue . NOT_SET , None , ): return value_obj . value_schema . default elif not value_obj . is_set : raise Exception ( f \"Value not set: { value_obj . id } \" ) data = self . _get_value_data_for_id ( value_obj . id ) if data == SpecialValue . NO_VALUE : return None elif isinstance ( data , Value ): return data . get_value_data () else : return data def get_lineage ( self , value_item : typing . Union [ str , Value , ValueSlot ] ) -> typing . Optional [ ValueLineage ]: value_obj = self . get_value_obj ( value_item = value_item ) if value_obj is None : raise Exception ( f \"No value registered for: { value_item } \" ) return self . _get_value_lineage ( value_obj . id ) # def get_value_info(self, value_item: typing.Union[str, Value, ValueAlias]) -> ValueInfo: # # value_obj = self.get_value_obj(value_item=value_item) # # aliases = self.find_aliases_for_value_id(value_id=value_obj.id, include_all_versions=True, include_tags=True) # # info = ValueInfo(value_id=value_obj.id, value_type=value_obj.value_schema.type, aliases=aliases, metadata=value_obj.get_metadata()) # return info def get_value_slot ( self , alias : typing . Union [ str , ValueSlot ] ) -> typing . Optional [ ValueSlot ]: if isinstance ( alias , ValueSlot ): alias = alias . id if alias not in self . alias_names : return None return self . _get_value_slot_for_alias ( alias_name = alias ) # ----------------------------------------------------------------------- # def _resolve_hash_to_value( # self, hash_str: str, hash_type: typing.Optional[str] = None # ) -> typing.Optional[Value]: # # matches: typing.Set[str] = set() # if hash_type is None: # for hash_type, details in self._hashes.items(): # if hash_str in details.keys(): # matches.add(details[hash_str]) # else: # hashes_for_type = self._hashes.get(hash_type, {}) # if hash_str in hashes_for_type.keys(): # matches.add(hashes_for_type[hash_str]) # # if len(matches) == 0: # return None # elif len(matches) > 1: # raise Exception(f\"Multiple values found for hash '{hash_str}'.\") # # match_id = next(iter(matches)) # value = self.get_value_obj(match_id) # return value def _check_register_token ( self , register_token : uuid . UUID ): return register_token in self . _register_tokens # alias-related methods def _resolve_alias_to_value ( self , alias : ValueAlias ) -> typing . Optional [ Value ]: alias_name = alias . alias alias_version = alias . version alias_tag = alias . tag value_slot = self . get_value_slot ( alias_name ) if not value_slot : raise Exception ( f \"No alias ' { alias_name } ' registered with registry.\" ) if alias_tag : _version = value_slot . tags . get ( alias_tag ) if alias_version : if _version != alias_version : raise Exception ( f \"Value alias object contains both tag and version information, but actual version of tag resolves to different version in the registry: { _version } != { alias_version } \" ) else : alias_version = _version assert alias_version is not None value = value_slot . values . get ( alias_version , None ) return value def _find_aliases_for_value_id ( self , value_id : str ) -> typing . List [ ValueAlias ]: \"\"\"Find all aliases that point to the specified value id. Sub-classes may overwrite this method for performance reasons. \"\"\" result = [] for alias in self . _get_available_aliases (): value_slot = self . get_value_slot ( alias ) assert value_slot aliases = value_slot . find_linked_aliases ( value_id ) result . extend ( aliases ) return result def _get_tags_for_alias ( self , alias : str ) -> typing . Iterable [ str ]: value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { alias } ' registered with registry.\" ) return value_slot . tags . keys () def find_aliases_for_value ( self , value_item : typing . Union [ str , Value ], include_all_versions : bool = False , include_tags : bool = False , ) -> typing . List [ ValueAlias ]: value = self . get_value_obj ( value_item ) if value is None : raise Exception ( f \"Can't find registered value for: { value_item } \" ) aliases = self . _find_aliases_for_value_id ( value_id = value . id ) result = [] latest_cache : typing . Dict [ str , int ] = {} for alias in aliases : if alias . version is not None : if not include_all_versions : if alias . alias in latest_cache . keys (): latest_version = latest_cache [ alias . alias ] else : latest_version = self . get_latest_version_for_alias ( alias . alias ) latest_cache [ alias . alias ] = latest_version if latest_version == alias . version : result . append ( alias ) else : result . append ( alias ) if alias . tag is not None and include_tags : result . append ( alias ) return result def get_latest_version_for_alias ( self , alias : str ) -> int : versions = self . get_versions_for_alias ( alias ) if not versions : return 0 else : return max ( versions ) def get_versions_for_alias ( self , alias : str ) -> typing . Iterable [ int ]: \"\"\"Return all available versions for the specified alias.\"\"\" if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . values . keys ()) def get_tags_for_alias ( self , alias : str ) -> typing . Iterable [ str ]: if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . tags . keys ()) @deprecated ( reason = \"This will be removed soon in favor of a more generic way to query values.\" ) def find_all_values_of_type ( self , value_type : str ) -> typing . Dict [ str , Value ]: \"\"\"Find all values of a certain type.\"\"\" result = {} for value_id in self . value_ids : value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"No value registered for: { value_id } \" ) if value_obj . type_name == value_type : # type: ignore result [ value_id ] = value_obj return result def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , DataRegistry ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id ) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (id= { self . id } \" def __str__ ( self ): return self . __repr__ ()","title":"BaseDataRegistry"},{"location":"reference/kiara/data/registry/__init__/#kiara.data.registry.BaseDataRegistry.find_all_values_of_type","text":"Find all values of a certain type. Source code in kiara/data/registry/__init__.py @deprecated ( reason = \"This will be removed soon in favor of a more generic way to query values.\" ) def find_all_values_of_type ( self , value_type : str ) -> typing . Dict [ str , Value ]: \"\"\"Find all values of a certain type.\"\"\" result = {} for value_id in self . value_ids : value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"No value registered for: { value_id } \" ) if value_obj . type_name == value_type : # type: ignore result [ value_id ] = value_obj return result","title":"find_all_values_of_type()"},{"location":"reference/kiara/data/registry/__init__/#kiara.data.registry.BaseDataRegistry.get_versions_for_alias","text":"Return all available versions for the specified alias. Source code in kiara/data/registry/__init__.py def get_versions_for_alias ( self , alias : str ) -> typing . Iterable [ int ]: \"\"\"Return all available versions for the specified alias.\"\"\" if isinstance ( alias , str ): _alias = ValueAlias . from_string ( alias ) elif not isinstance ( alias , ValueAlias ): raise TypeError ( f \"Invalid type for alias: { type ( alias ) } \" ) else : _alias = alias value_slot = self . get_value_slot ( alias ) if not value_slot : raise Exception ( f \"No alias ' { _alias . alias } ' registered with registry.\" ) return sorted ( value_slot . values . keys ())","title":"get_versions_for_alias()"},{"location":"reference/kiara/data/registry/__init__/#kiara.data.registry.DataRegistry","text":"Source code in kiara/data/registry/__init__.py class DataRegistry ( BaseDataRegistry ): def __init__ ( self , kiara : \"Kiara\" ): self . _lineages : typing . Dict [ str , typing . Optional [ ValueLineage ]] = {} super () . __init__ ( kiara = kiara ) @abc . abstractmethod def _register_value_and_data ( self , value : Value , data : typing . Any ) -> str : \"\"\"Register data into this registry. Returns the values id. In case the value already has a value set (meaning it's different from string [NO_ID_YET_MARKER][kiara.data.value.NO_ID_YET_MARKER] / '__no_id_yet__' this method must use this id or throw an exception. Otherwise, it is required that the result id is different from ``NO_ID_YET_MARKER`` and a non-empty string. \"\"\" @abc . abstractmethod def _register_remote_value ( self , value : Value ) -> Value : \"\"\"Register an existing value from a different registry into this one. Arguments: value: the original value (with the '_registry' attribute still pointing to the original registry) Returns: either None (in which case the original value object will be copied with the '_registry' and '_kiara' attributes adjusted), or a value object. \"\"\" def register_data ( self , value_data : typing . Any = SpecialValue . NOT_SET , value_schema : typing . Union [ None , ValueSchema , str ] = None , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> Value : # if lineage and not lineage.output_name: # raise Exception(f\"Value lineage for data of type '{value_data.value_schema.type}' does not have 'output_name' field set.\") value_id = str ( uuid . uuid4 ()) if value_id in self . value_ids : raise Exception ( f \"Can't register value: value id ' { value_id } ' already exists.\" ) if value_id in self . alias_names : raise Exception ( f \"Can't register value: value id ' { value_id } ' already exists as alias.\" ) if value_schema is None : if isinstance ( value_data , Value ): value_schema = value_data . value_schema else : raise Exception ( f \"No value schema provided for value: { value_data } \" ) elif isinstance ( value_schema , str ): value_schema = ValueSchema ( type = value_schema ) cls = self . _kiara . get_value_type_cls ( value_schema . type ) _type_obj = cls ( ** value_schema . type_config ) existing_value : typing . Optional [ Value ] = None if isinstance ( value_data , Value ): if value_data . id in self . value_ids and not metadata : if lineage : existing_lineage = self . _lineages . get ( value_data . id , None ) if existing_lineage : log_message ( f \"Overwriting existing value lineage for: { value_data . id } \" ) self . _lineages [ value_data . id ] = lineage # TODO: check it's really the same our_value = self . get_value_obj ( value_data . id ) if our_value is None : raise Exception ( \"Could not retrieve cloned value, this is probably a bug.\" ) return our_value existing_value = self . _find_value_for_hashes ( * value_data . get_hashes ()) if existing_value and not metadata : if lineage and self . _lineages . get ( existing_value . id , None ): log_message ( f \"Overwriting value lineage for: { existing_value . id } \" ) if lineage : self . _lineages [ existing_value . id ] = lineage return existing_value if value_data . id in self . value_ids and metadata : _metadata : typing . Optional [ typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = None if metadata : _metadata = {} for k , v in metadata . items (): _metadata [ k ] = {} _metadata [ k ][ \"metadata_item\" ] = v . dict () _metadata [ k ][ \"metadata_schema\" ] = v . schema_json () # TODO: not resolve value_data ? cloned_new_metadata = self . _create_value_obj ( value_schema = value_data . value_schema , is_set = value_data . is_set , is_none = value_data . is_none , type_obj = value_data . type_obj , metadata = _metadata , value_hashes = None , ) r = self . _register_new_value_obj ( value_obj = cloned_new_metadata , value_data = value_data . get_value_data (), lineage = lineage , # value_hashes=value_hashes, ) return r copied_value = self . _register_remote_value ( value_data ) if value_data . id not in self . value_ids : raise Exception ( f \"Value with id ' { value_data . id } ' wasn't successfully registered. This is most likely a bug.\" ) assert copied_value . _registry == self assert copied_value . _kiara == self . _kiara if value_data . id != copied_value . id : log_message ( f \"Value id for value of type { copied_value . type_name } changed when registering in other registry.\" ) raise Exception ( f \"Imported value object with id ' { value_data . id } ' resulted in copied value with different id. This is a bug.\" ) # for hash_type, value_hash in copied_value.get_hashes().items(): # self._hashes.setdefault(hash_type, {})[value_hash.hash] = value_id self . _lineages [ copied_value . id ] = value_data . get_lineage () return copied_value if value_data in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE , ] and value_schema . default not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE , ]: if metadata : raise NotImplementedError () value_data = copy . deepcopy ( value_schema . default ) if value_data not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ]: # TODO: actually implement that for hash_type in _type_obj . get_supported_hash_types (): hash_str = _type_obj . calculate_value_hash ( value = value_data , hash_type = hash_type ) existing_value = self . _find_value_for_hashes ( ValueHash ( hash_type = hash_type , hash = hash_str ) ) if existing_value : break if existing_value : if metadata : raise NotImplementedError () if lineage and self . _lineages . get ( existing_value . id , None ): log_message ( f \"Overwriting lineage for value ' { existing_value . id } '.\" ) if lineage : self . _lineages [ existing_value . id ] = lineage return existing_value if value_schema . is_constant : if ( value_data not in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ] and value_data != value_schema . default ): raise NotImplementedError () value_data = value_schema . default is_set = value_data != SpecialValue . NOT_SET is_none = value_data in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ] _metadata = None if metadata : _metadata = {} for k , v in metadata . items (): _metadata [ k ] = {} _metadata [ k ][ \"metadata_item\" ] = v . dict () _metadata [ k ][ \"metadata_schema\" ] = v . schema_json () # value_hashes: typing.Dict[str] = {} value = self . _create_value_obj ( value_schema = value_schema , is_set = is_set , is_none = is_none , type_obj = _type_obj , metadata = _metadata , value_hashes = None , ) self . _register_new_value_obj ( value_obj = value , value_data = value_data , lineage = lineage , # value_hashes=value_hashes, ) # if not value.is_none: # value.get_metadata() # metadata = self._kiara.metadata_mgmt.get_value_metadata(value=value, also_return_schema=True) # print(metadata) return value def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: return self . _lineages . get ( value_id , None ) def _create_value_obj ( self , value_schema : ValueSchema , is_set : bool , is_none : bool , type_obj : typing . Optional [ ValueType ] = None , value_hashes : typing . Optional [ typing . Iterable [ ValueHash ]] = None , metadata : typing . Optional [ typing . Mapping [ str , typing . Mapping [ str , typing . Any ]] ] = None , ): # if value_schema.is_constant and value_data not in [ # SpecialValue.NO_VALUE, # SpecialValue.NOT_SET, # None, # ]: # raise Exception( # \"Can't create value. Is a constant, but value data was provided.\" # ) if type_obj is None : if value_schema . type not in self . _kiara . value_types : log_message ( f \"Value type ' { value_schema . type } ' not supported in this kiara environment, changing type to 'any'.\" ) type_cls = self . _kiara . get_value_type_cls ( \"any\" ) else : type_cls = self . _kiara . get_value_type_cls ( value_schema . type ) type_obj = type_cls ( ** value_schema . type_config ) register_token = uuid . uuid4 () self . _register_tokens . add ( register_token ) try : value = Value ( registry = self , # type: ignore value_schema = value_schema , type_obj = type_obj , # type: ignore is_set = is_set , is_none = is_none , hashes = value_hashes , metadata = metadata , register_token = register_token , # type: ignore ) return value finally : self . _register_tokens . remove ( register_token ) def _register_new_value_obj ( self , value_obj : Value , value_data : typing . Any , lineage : typing . Optional [ ValueLineage ], # value_hashes: typing.Optional[typing.Mapping[str, ValueHash]] = None, ): # assert value_obj.id == NO_ID_YET_MARKER if value_data not in [ SpecialValue . NO_VALUE , SpecialValue . NOT_SET , SpecialValue . IGNORE , None , ]: # TODO: should we keep the original value? value_data = value_obj . type_obj . import_value ( value_data ) value_id = self . _register_value_and_data ( value = value_obj , data = value_data ) if value_obj . id == NO_ID_YET_MARKER : value_obj . id = value_id if value_obj . id != value_id : raise Exception ( f \"Inconsistent id for value: { value_obj . id } != { value_id } \" ) if value_id not in self . value_ids : raise Exception ( f \"Value id ' { value_id } ' wasn't registered propertly in registry. This is most likely a bug.\" ) if lineage : self . _lineages [ value_id ] = lineage # if value_hashes is None: # value_hashes = {} # for hash_type, value_hash in value_hashes.items(): # self._hashes.setdefault(hash_type, {})[value_hash.hash] = value_id return value_obj def set_value_lineage ( self , value_item : typing . Union [ str , Value ], value_lineage : ValueLineage ): value_obj = self . get_value_obj ( value_item ) if value_obj is None : raise Exception ( f \"No value available for: { value_item } \" ) self . _lineages [ value_obj . id ] = value_lineage def link_alias ( self , value : typing . Union [ str , Value ], alias : str , register_missing_alias : bool = True , ) -> None : value_obj = self . get_value_obj ( value ) if value_obj is None : raise Exception ( f \"No value for: { value } \" ) value_slot = self . get_value_slot ( alias = alias ) if value_slot is None : if not register_missing_alias : raise Exception ( f \"Can't link value to alias ' { alias } ': alias not registered.\" ) else : self . register_alias ( value_or_schema = value_obj , alias_name = alias ) else : value_alias = ValueAlias . from_string ( alias ) if value_alias . tag : value_slot . add_value ( value_obj , tags = [ value_alias . tag ]) else : value_slot . add_value ( value_obj ) def link_aliases ( self , value : typing . Union [ str , Value ], * aliases : str , register_missing_aliases : bool = True , ) -> None : value_obj = self . get_value_obj ( value ) if value_obj is None : raise Exception ( f \"No value for: { value } \" ) if not register_missing_aliases : invalid = [] for alias in aliases : if alias not in self . alias_names : invalid . append ( alias ) if invalid : raise Exception ( f \"Can't link value(s), invalid alias(es): { ', ' . join ( invalid ) } \" ) for alias in aliases : self . link_alias ( value_obj , alias = alias ) def _check_valid_alias ( self , alias_name : str ): match = bool ( re . match ( VALID_ALIAS_PATTERN , alias_name )) return True if match else False @abc . abstractmethod def _register_alias ( self , alias_name : str , value_schema : ValueSchema ) -> ValueSlot : pass def register_aliases ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], aliases : typing . Iterable [ str ], callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> typing . Mapping [ str , ValueSlot ]: invalid = [] for alias in aliases : if not self . _check_valid_alias ( alias_name = alias ): invalid . append ( alias ) if invalid : raise Exception ( f \"Invalid alias(es), only alphanumeric characters, '-', and '_' allowed in alias name: { ', ' . join ( invalid ) } \" ) result = {} for alias in aliases : vs = self . register_alias ( value_or_schema = value_or_schema , alias_name = alias , callbacks = callbacks ) result [ alias ] = vs return result def register_alias ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], # preseed: bool = False, alias_name : typing . Optional [ str ] = None , callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> ValueSlot : \"\"\"Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Arguments: value_or_schema: a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) preseed: whether to add an empty/non-set value to the newly created slot alias_name: the alias name, will be auto-created if not provided callbacks: a list of callbacks to trigger whenever the alias is updated \"\"\" if alias_name is None : alias_name = str ( uuid . uuid4 ()) if not alias_name : raise Exception ( \"Empty alias name not allowed.\" ) match = self . _check_valid_alias ( alias_name ) if not match : raise Exception ( f \"Invalid alias ' { alias_name } ': only alphanumeric characters, '-', and '_' allowed in alias name.\" ) # if the provided value_or_schema argument is a string, it must be a value_id (for now) if isinstance ( value_or_schema , str ): if value_or_schema not in self . value_ids : raise Exception ( f \"Can't register alias ' { alias_name } ', provided string ' { value_or_schema } ' not an existing value id.\" ) _value_or_schema = self . get_value_obj ( value_or_schema ) if _value_or_schema is None : raise Exception ( f \"No value registered for: { value_or_schema } \" ) value_or_schema = _value_or_schema if isinstance ( value_or_schema , Value ): _value_schema = value_or_schema . value_schema _value : typing . Optional [ Value ] = value_or_schema elif isinstance ( value_or_schema , ValueSchema ): _value_schema = value_or_schema _value = None else : raise TypeError ( f \"Invalid value type: { type ( value_or_schema ) } \" ) if alias_name in self . _get_available_aliases (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists.\" ) elif alias_name in self . _get_available_value_ids (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists as value id.\" ) # vs = ValueSlot.from_value(id=_id, value=value_or_schema) vs = self . _register_alias ( alias_name = alias_name , value_schema = _value_schema ) # self._value_slots[vs.id] = vs # if preseed: # _v = Value(value_schema=_value_schema, kiara=self._kiara, registry=self) # vs.add_value(_v) if callbacks : vs . register_callbacks ( * callbacks ) # self.register_callbacks(vs, *callbacks) if _value is not None : vs . add_value ( _value ) return vs # def find_value_slots( # self, value_item: typing.Union[str, Value] # ) -> typing.List[ValueSlot]: # # value_item = self.get_value_obj(value_item) # result = [] # for slot_id, slot in self._value_slots.items(): # if slot.is_latest_value(value_item): # result.append(slot) # return result # def register_callbacks( # self, # alias: typing.Union[str, ValueSlot], # *callbacks: ValueSlotUpdateHandler, # ) -> None: # # _value_slot = self.get_value_slot(alias) # _value_slot.register_callbacks(*callbacks) def update_value_slot ( self , value_slot : typing . Union [ str , Value , ValueSlot ], data : typing . Any , value_lineage : typing . Optional [ ValueLineage ] = None , ) -> bool : # first, resolve a potential string into a value_slot or value if isinstance ( value_slot , str ): if value_slot in self . alias_names : _value_slot : typing . Union [ None , Value , ValueSlot , str ] = self . get_value_slot ( alias = value_slot ) elif value_slot in self . value_ids : _value_slot = self . get_value_obj ( value_slot ) else : _value_slot = value_slot if _value_slot is None : raise Exception ( f \"Can't retrieve target object for id: { value_slot } \" ) value_slot = _value_slot if isinstance ( value_slot , Value ): aliases = self . find_aliases_for_value ( value_slot . id ) # slots = self.find_value_slots(value_slot) if len ( aliases ) == 0 : raise Exception ( f \"No value slot found for value ' { value_slot . id } '.\" ) elif len ( aliases ) > 1 : raise Exception ( f \"Multiple value slots found for value ' { value_slot . id } '. This is not supported (yet).\" ) _value_slot_2 : typing . Optional [ ValueSlot ] = self . get_value_slot ( alias = aliases [ 0 ] . alias ) elif isinstance ( value_slot , ValueSlot ): _value_slot_2 = value_slot else : raise TypeError ( f \"Invalid type for value slot: { type ( value_slot ) } \" ) assert _value_slot_2 is not None if isinstance ( data , Value ): if value_lineage : raise Exception ( \"Can't update value slot with new value lineage data.\" ) _value : Value = data else : _value = self . register_data ( value_data = data , value_schema = value_slot . value_schema , lineage = value_lineage , ) # _value = self.create_value( # value_data=data, # value_schema=_value_slot.value_schema, # value_lineage=value_lineage, # ) return self . _update_value_slot ( value_slot = _value_slot_2 , new_value = _value , trigger_callbacks = True ) def update_value_slots ( self , updated_values : typing . Mapping [ typing . Union [ str , ValueSlot ], typing . Any ], metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> typing . Mapping [ ValueSlot , typing . Union [ bool , Exception ]]: updated : typing . Dict [ str , typing . List [ ValueSlot ]] = {} cb_map : typing . Dict [ str , ValueSlotUpdateHandler ] = {} result : typing . Dict [ ValueSlot , typing . Union [ bool , Exception ]] = {} invalid : typing . Set [ str ] = set () for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id _alias = self . get_value_slot ( alias ) if _alias is None : if isinstance ( alias , ValueSlot ): invalid . add ( alias . id ) else : invalid . add ( alias ) if invalid : raise Exception ( f \"Can't update value slots: invalid alias name(s) ' { ', ' . join ( invalid ) } '.\" ) for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id value_slot = self . get_value_slot ( alias ) if value_slot is None : raise Exception ( f \"Can't retrieve value slot for alias: { alias } \" ) for alias , value_item in updated_values . items (): if isinstance ( alias , ValueSlot ): alias = alias . id value_slot = self . get_value_slot ( alias ) if value_slot is None : raise Exception ( f \"No value slot for alias: { alias } .\" ) try : if isinstance ( value_item , Value ): _value_item : Value = value_item if _value_item . _registry != self : _value_item = self . register_data ( _value_item , metadata = metadata , lineage = lineage ) elif metadata : _value_item = self . register_data ( _value_item , metadata = metadata , lineage = lineage ) else : _value_item = self . register_data ( value_data = value_item , value_schema = value_slot . value_schema , metadata = metadata , lineage = lineage , ) updated_item = self . _update_value_slot ( value_slot = value_slot , new_value = _value_item , trigger_callbacks = False , ) result [ value_slot ] = updated_item if updated_item : for cb_id , cb in value_slot . _callbacks . items (): cb_map [ cb_id ] = cb updated . setdefault ( cb_id , []) . append ( value_slot ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () result [ value_slot ] = e for cb_id , value_slots in updated . items (): cb = cb_map [ cb_id ] cb . values_updated ( * value_slots ) return result def _update_value_slot ( self , value_slot : ValueSlot , new_value : Value , trigger_callbacks : bool = True ) -> bool : last_version = value_slot . latest_version_nr new_version = value_slot . add_value ( new_value , trigger_callbacks = trigger_callbacks ) updated = last_version != new_version return updated","title":"DataRegistry"},{"location":"reference/kiara/data/registry/__init__/#kiara.data.registry.DataRegistry.register_alias","text":"Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Parameters: Name Type Description Default value_or_schema Union[kiara.data.values.Value, kiara.data.values.ValueSchema, str] a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) required preseed whether to add an empty/non-set value to the newly created slot required alias_name Optional[str] the alias name, will be auto-created if not provided None callbacks Optional[Iterable[kiara.data.registry.ValueSlotUpdateHandler]] a list of callbacks to trigger whenever the alias is updated None Source code in kiara/data/registry/__init__.py def register_alias ( self , value_or_schema : typing . Union [ Value , ValueSchema , str ], # preseed: bool = False, alias_name : typing . Optional [ str ] = None , callbacks : typing . Optional [ typing . Iterable [ ValueSlotUpdateHandler ]] = None , ) -> ValueSlot : \"\"\"Register a value slot. A value slot is an object that holds multiple versions of values that all use the same schema. Arguments: value_or_schema: a value, value_id, or schema, if value, it is added to the newly created slot (after preseed, if selected) preseed: whether to add an empty/non-set value to the newly created slot alias_name: the alias name, will be auto-created if not provided callbacks: a list of callbacks to trigger whenever the alias is updated \"\"\" if alias_name is None : alias_name = str ( uuid . uuid4 ()) if not alias_name : raise Exception ( \"Empty alias name not allowed.\" ) match = self . _check_valid_alias ( alias_name ) if not match : raise Exception ( f \"Invalid alias ' { alias_name } ': only alphanumeric characters, '-', and '_' allowed in alias name.\" ) # if the provided value_or_schema argument is a string, it must be a value_id (for now) if isinstance ( value_or_schema , str ): if value_or_schema not in self . value_ids : raise Exception ( f \"Can't register alias ' { alias_name } ', provided string ' { value_or_schema } ' not an existing value id.\" ) _value_or_schema = self . get_value_obj ( value_or_schema ) if _value_or_schema is None : raise Exception ( f \"No value registered for: { value_or_schema } \" ) value_or_schema = _value_or_schema if isinstance ( value_or_schema , Value ): _value_schema = value_or_schema . value_schema _value : typing . Optional [ Value ] = value_or_schema elif isinstance ( value_or_schema , ValueSchema ): _value_schema = value_or_schema _value = None else : raise TypeError ( f \"Invalid value type: { type ( value_or_schema ) } \" ) if alias_name in self . _get_available_aliases (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists.\" ) elif alias_name in self . _get_available_value_ids (): raise Exception ( f \"Can't register alias: alias ' { alias_name } ' already exists as value id.\" ) # vs = ValueSlot.from_value(id=_id, value=value_or_schema) vs = self . _register_alias ( alias_name = alias_name , value_schema = _value_schema ) # self._value_slots[vs.id] = vs # if preseed: # _v = Value(value_schema=_value_schema, kiara=self._kiara, registry=self) # vs.add_value(_v) if callbacks : vs . register_callbacks ( * callbacks ) # self.register_callbacks(vs, *callbacks) if _value is not None : vs . add_value ( _value ) return vs","title":"register_alias()"},{"location":"reference/kiara/data/registry/__init__/#kiara.data.registry.ValueSlotUpdateHandler","text":"The call signature for callbacks that can be registered as value update handlers. Source code in kiara/data/registry/__init__.py class ValueSlotUpdateHandler ( Protocol ): \"\"\"The call signature for callbacks that can be registered as value update handlers.\"\"\" def values_updated ( self , * items : \"ValueSlot\" ) -> typing . Any : ...","title":"ValueSlotUpdateHandler"},{"location":"reference/kiara/data/registry/__init__/#kiara.data.registry.store","text":"","title":"store"},{"location":"reference/kiara/data/registry/__init__/#kiara.data.registry.store.LocalDataStore","text":"An implementation of a kiara data registry that stores data locally, so it can be accessed across sessions. Data is stored under the value for the configured 'base_path' attribute, along with LoadConfig data that describes how the data can be re-loaded into memory. Source code in kiara/data/registry/store.py class LocalDataStore ( DataRegistry ): \"\"\"An implementation of a *kiara* data registry that stores data locally, so it can be accessed across sessions. Data is stored under the value for the configured 'base_path' attribute, along with [LoadConfig][kiara.metadata.data.LoadConfig] data that describes how the data can be re-loaded into memory. \"\"\" def __init__ ( self , kiara : \"Kiara\" , base_path : typing . Union [ str , Path ]): if isinstance ( base_path , str ): base_path = Path ( base_path ) self . _base_path : Path = base_path self . _value_obj_cache : typing . Dict [ str , Value ] = {} self . _value_info_cache : typing . Dict [ str , SavedValueInfo ] = {} self . _data_cache : typing . Dict [ str , Value ] = {} self . _metadata_cache : typing . Dict [ str , typing . Any ] = {} self . _value_slots : typing . Dict [ str , ValueSlot ] = {} super () . __init__ ( kiara = kiara ) @property def base_path ( self ) -> Path : return self . _base_path def _get_saved_value_info ( self , value_id : str ) -> SavedValueInfo : if value_id in self . _value_info_cache . keys (): return self . _value_info_cache [ value_id ] metadata_path = self . get_metadata_path ( value_id = value_id ) if not metadata_path . exists (): raise Exception ( f \"No value with id ' { value_id } ' registered.\" ) info_content = metadata_path . read_text () info_dict = json . loads ( info_content ) value_info = SavedValueInfo ( ** info_dict ) assert value_info . value_id == value_id self . _value_info_cache [ value_id ] = value_info return value_info def _get_value_obj_for_id ( self , value_id : str ) -> Value : if value_id in self . _value_obj_cache . keys (): return self . _value_obj_cache [ value_id ] value_info : SavedValueInfo = self . _get_saved_value_info ( value_id ) value_schema = value_info . value_schema # value_schema = ValueSchema( # type=value_info.value_type, type_config=value_info.value_type_config # ) value_obj = self . _create_value_obj ( value_schema = value_schema , is_set = True , is_none = False , value_hashes = value_info . hashes , metadata = value_info . metadata , ) value_obj . id = value_info . value_id self . _register_new_value_obj ( value_obj = value_obj , value_data = SpecialValue . IGNORE , lineage = value_info . lineage , # value_hashes=value_info.hashes, ) self . _value_obj_cache [ value_obj . id ] = value_obj return value_obj def _get_value_data_for_id ( self , value_id : str ) -> typing . Any : if value_id in self . _data_cache . keys (): return self . _data_cache [ value_id ] . get_value_data () # # value_obj = self.get_value_obj(value_item=value_id) # metadata_path = self.get_metadata_path(value_id=value_id) # if not metadata_path.exists(): # raise Exception(f\"No value with id '{value_id}' registered.\") # # info_content = metadata_path.read_text() # info_dict = json.loads(info_content) # value_info = SavedValueInfo(**info_dict) value_info = self . _get_saved_value_info ( value_id = value_id ) assert value_info . value_id == value_id load_config = value_info . load_config value : Value = self . _kiara . run ( ** load_config . dict ( exclude = { \"value_id\" , \"base_path_input_name\" , \"doc\" })) # type: ignore self . _data_cache [ value_id ] = value return value . get_value_data () def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: return self . _get_saved_value_info ( value_id ) . lineage def _register_value_and_data ( self , value : Value , data : typing . Any ) -> str : if data == SpecialValue . IGNORE : return value . id raise NotImplementedError () # # new_value_id = str(uuid.uuid4()) # # value.id = new_value_id # value_type = value.type_name # # # run the type and repo type specific save module, and retrieve the load_config that is needed to retrieve it again later # save_config = self._prepare_save_config( # value_id=new_value_id, value_type=value_type, value=value # ) # # save_module = save_config.create_module(kiara=self._kiara) # result = save_module.run(**save_config.inputs) # load_config_value: Value = result.get_value_obj(\"load_config\") # # # assemble the value metadata # metadata: typing.Dict[str, typing.Mapping[str, typing.Any]] = dict( # value.get_metadata(also_return_schema=True) # ) def _register_remote_value ( self , value : Value ) -> Value : value_metadata_path = self . get_metadata_path ( value . id ) if value_metadata_path . exists (): raise Exception ( f \"Can't register remote value with id ' { value . id } '. Load config for this id already exists: { value_metadata_path . as_posix () } \" ) value_type = value . type_name # run the type and repo type specific save module, and retrieve the load_config that is needed to retrieve it again later save_config = self . _prepare_save_config ( value_id = value . id , value_type = value_type , value = value ) save_module = save_config . create_module ( kiara = self . _kiara ) result = save_module . run ( ** save_config . inputs ) load_config_value : Value = result . get_value_obj ( \"load_config\" ) field_name = value_type if field_name == \"any\" : field_name = \"value_item\" saved_value : Value = result . get_value_obj ( field_name ) load_config_data = load_config_value . get_value_data () metadata = saved_value . get_metadata ( also_return_schema = True ) value_info = SavedValueInfo ( value_id = value . id , value_schema = saved_value . value_schema , is_valid = saved_value . item_is_valid (), hashes = saved_value . get_hashes (), lineage = value . get_lineage (), save_lineage = saved_value . get_lineage (), metadata = metadata , load_config = load_config_data , ) value_metadata_path . parent . mkdir ( parents = True , exist_ok = True ) value_metadata_path . write_text ( value_info . json ()) copied_value = saved_value . copy () copied_value . _registry = self copied_value . _kiara = self . _kiara if copied_value . id != value . id : log_message ( f \"Saved value id different to original id ' { value . id } .\" ) copied_value . id = value . id self . _value_obj_cache [ copied_value . id ] = copied_value return copied_value def _prepare_save_config ( self , value_id : str , value_type : str , value : \"Value\" ) -> SaveConfig : store_operations : StoreOperationType = self . _kiara . operation_mgmt . get_operations ( \"store_value\" ) # type: ignore op_config = store_operations . get_store_operation_for_type ( value_type ) if not op_config : raise Exception ( f \"Can't save value: no save operation found for value type ' { value_type } '\" ) target_path = self . get_data_path ( value_id = value_id ) metadata_path = self . get_metadata_path ( value_id = value_id ) if os . path . exists ( metadata_path ): raise Exception ( f \"Can't save value, metadata file already exists: { metadata_path } \" ) input_name = value . type_name if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : value . get_value_data (), \"base_path\" : target_path . as_posix (), \"value_id\" : value_id , } # type: ignore save_config = SaveConfig ( module_type = op_config . module_type , module_config = op_config . module_config , inputs = inputs , load_config_output = \"load_config\" , ) return save_config def get_data_path ( self , value_id : str ) -> Path : return self . _base_path / f \"value_ { value_id } \" / \"data\" def get_metadata_path ( self , value_id : str ) -> Path : path = self . _base_path / f \"value_ { value_id } \" / \"value_metadata.json\" return path # def get_load_config_path(self, value_id: str) -> Path: # path = self._base_path / f\"value_{value_id}\" / \"load_config.json\" # return path def _get_value_slot_for_alias ( self , alias_name : str ) -> ValueSlot : if alias_name in self . _value_slots . keys (): return self . _value_slots [ alias_name ] alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" if not alias_file . is_file (): raise Exception ( f \"No alias with name ' { alias_name } ' registered.\" ) file_content = alias_file . read_text () alias_data = json . loads ( file_content ) value_schema = ValueSchema ( ** alias_data [ \"value_schema\" ]) value_slot = ValueSlot ( id = alias_name , value_schema = value_schema , kiara = self . _kiara , registry = self ) value_slot . register_callbacks ( self ) for version in sorted (( int ( x ) for x in alias_data [ \"versions\" ] . keys ())): value_id = alias_data [ \"versions\" ][ str ( version )] value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"Can't find value with id: { value_id } \" ) tags : typing . Optional [ typing . Iterable [ str ]] = None if value_id in alias_data [ \"tags\" ] . values (): tags = [ tag for tag , version in alias_data [ \"tags\" ] . items () if version == version ] new_version = value_slot . add_value ( value_obj , trigger_callbacks = False , tags = tags ) if new_version != int ( version ): raise Exception ( f \"New version different to stored version ( { new_version } != { version } ). This is a bug.\" ) self . _value_slots [ alias_name ] = value_slot return self . _value_slots [ alias_name ] def _get_available_aliases ( self ) -> typing . Iterable [ str ]: alias_path = self . _base_path / \"aliases\" alias_file = alias_path . glob ( \"alias_*.json\" ) result = [] for af in alias_file : if not af . is_file (): log_message ( f \"Ignoring non-file: { af . as_posix () } \" ) continue alias_name = af . name [ 6 : - 5 ] result . append ( alias_name ) return result def _get_available_value_ids ( self ) -> typing . Iterable [ str ]: value_ids = [ x . parent . name [ 6 :] for x in self . _base_path . glob ( \"value_*/value_metadata.json\" ) ] return value_ids def _register_alias ( self , alias_name : str , value_schema : ValueSchema ) -> ValueSlot : alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" if alias_file . exists (): raise Exception ( f \"Alias ' { alias_name } ' already registered.\" ) if alias_name in self . _value_slots . keys (): raise Exception ( f \"Value slot for alias ' { alias_name } ' already exists.\" ) alias_file . parent . mkdir ( parents = True , exist_ok = True ) vs = ValueSlot ( id = alias_name , value_schema = value_schema , kiara = self . _kiara , registry = self ) self . _value_slots [ alias_name ] = vs self . _write_alias_file ( alias_name ) vs . register_callbacks ( self ) return vs def _write_alias_file ( self , alias_name ): alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" value_slot = self . _value_slots [ alias_name ] alias_dict = { \"value_schema\" : value_slot . value_schema . dict (), \"versions\" : {}, \"tags\" : {}, } for version , value in value_slot . values . items (): alias_dict [ \"versions\" ][ version ] = value . id for tag_name , version in value_slot . tags . items (): alias_dict [ \"tags\" ][ tag_name ] = version alias_file . write_text ( json . dumps ( alias_dict )) def values_updated ( self , * items : \"ValueSlot\" ): invalid = [] for item in items : if item . id not in self . _value_slots . keys (): invalid . append ( item . id ) if invalid : raise Exception ( f \"Can't update value(s), invalid aliases: { ', ' . join ( invalid ) } \" ) for item in items : self . _write_alias_file ( alias_name = item . id )","title":"LocalDataStore"},{"location":"reference/kiara/data/registry/__init__/#kiara.data.registry.store.SavedValueInfo","text":"Source code in kiara/data/registry/store.py class SavedValueInfo ( ValueInfo ): load_config : LoadConfig = Field ( description = \"The configuration to load this value from disk (or however it is stored).\" ) save_lineage : ValueLineage = Field ( description = \"Information about how the value was saved.\" )","title":"SavedValueInfo"},{"location":"reference/kiara/data/registry/store/","text":"LocalDataStore ( DataRegistry ) \u00b6 An implementation of a kiara data registry that stores data locally, so it can be accessed across sessions. Data is stored under the value for the configured 'base_path' attribute, along with LoadConfig data that describes how the data can be re-loaded into memory. Source code in kiara/data/registry/store.py class LocalDataStore ( DataRegistry ): \"\"\"An implementation of a *kiara* data registry that stores data locally, so it can be accessed across sessions. Data is stored under the value for the configured 'base_path' attribute, along with [LoadConfig][kiara.metadata.data.LoadConfig] data that describes how the data can be re-loaded into memory. \"\"\" def __init__ ( self , kiara : \"Kiara\" , base_path : typing . Union [ str , Path ]): if isinstance ( base_path , str ): base_path = Path ( base_path ) self . _base_path : Path = base_path self . _value_obj_cache : typing . Dict [ str , Value ] = {} self . _value_info_cache : typing . Dict [ str , SavedValueInfo ] = {} self . _data_cache : typing . Dict [ str , Value ] = {} self . _metadata_cache : typing . Dict [ str , typing . Any ] = {} self . _value_slots : typing . Dict [ str , ValueSlot ] = {} super () . __init__ ( kiara = kiara ) @property def base_path ( self ) -> Path : return self . _base_path def _get_saved_value_info ( self , value_id : str ) -> SavedValueInfo : if value_id in self . _value_info_cache . keys (): return self . _value_info_cache [ value_id ] metadata_path = self . get_metadata_path ( value_id = value_id ) if not metadata_path . exists (): raise Exception ( f \"No value with id ' { value_id } ' registered.\" ) info_content = metadata_path . read_text () info_dict = json . loads ( info_content ) value_info = SavedValueInfo ( ** info_dict ) assert value_info . value_id == value_id self . _value_info_cache [ value_id ] = value_info return value_info def _get_value_obj_for_id ( self , value_id : str ) -> Value : if value_id in self . _value_obj_cache . keys (): return self . _value_obj_cache [ value_id ] value_info : SavedValueInfo = self . _get_saved_value_info ( value_id ) value_schema = value_info . value_schema # value_schema = ValueSchema( # type=value_info.value_type, type_config=value_info.value_type_config # ) value_obj = self . _create_value_obj ( value_schema = value_schema , is_set = True , is_none = False , value_hashes = value_info . hashes , metadata = value_info . metadata , ) value_obj . id = value_info . value_id self . _register_new_value_obj ( value_obj = value_obj , value_data = SpecialValue . IGNORE , lineage = value_info . lineage , # value_hashes=value_info.hashes, ) self . _value_obj_cache [ value_obj . id ] = value_obj return value_obj def _get_value_data_for_id ( self , value_id : str ) -> typing . Any : if value_id in self . _data_cache . keys (): return self . _data_cache [ value_id ] . get_value_data () # # value_obj = self.get_value_obj(value_item=value_id) # metadata_path = self.get_metadata_path(value_id=value_id) # if not metadata_path.exists(): # raise Exception(f\"No value with id '{value_id}' registered.\") # # info_content = metadata_path.read_text() # info_dict = json.loads(info_content) # value_info = SavedValueInfo(**info_dict) value_info = self . _get_saved_value_info ( value_id = value_id ) assert value_info . value_id == value_id load_config = value_info . load_config value : Value = self . _kiara . run ( ** load_config . dict ( exclude = { \"value_id\" , \"base_path_input_name\" , \"doc\" })) # type: ignore self . _data_cache [ value_id ] = value return value . get_value_data () def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: return self . _get_saved_value_info ( value_id ) . lineage def _register_value_and_data ( self , value : Value , data : typing . Any ) -> str : if data == SpecialValue . IGNORE : return value . id raise NotImplementedError () # # new_value_id = str(uuid.uuid4()) # # value.id = new_value_id # value_type = value.type_name # # # run the type and repo type specific save module, and retrieve the load_config that is needed to retrieve it again later # save_config = self._prepare_save_config( # value_id=new_value_id, value_type=value_type, value=value # ) # # save_module = save_config.create_module(kiara=self._kiara) # result = save_module.run(**save_config.inputs) # load_config_value: Value = result.get_value_obj(\"load_config\") # # # assemble the value metadata # metadata: typing.Dict[str, typing.Mapping[str, typing.Any]] = dict( # value.get_metadata(also_return_schema=True) # ) def _register_remote_value ( self , value : Value ) -> Value : value_metadata_path = self . get_metadata_path ( value . id ) if value_metadata_path . exists (): raise Exception ( f \"Can't register remote value with id ' { value . id } '. Load config for this id already exists: { value_metadata_path . as_posix () } \" ) value_type = value . type_name # run the type and repo type specific save module, and retrieve the load_config that is needed to retrieve it again later save_config = self . _prepare_save_config ( value_id = value . id , value_type = value_type , value = value ) save_module = save_config . create_module ( kiara = self . _kiara ) result = save_module . run ( ** save_config . inputs ) load_config_value : Value = result . get_value_obj ( \"load_config\" ) field_name = value_type if field_name == \"any\" : field_name = \"value_item\" saved_value : Value = result . get_value_obj ( field_name ) load_config_data = load_config_value . get_value_data () metadata = saved_value . get_metadata ( also_return_schema = True ) value_info = SavedValueInfo ( value_id = value . id , value_schema = saved_value . value_schema , is_valid = saved_value . item_is_valid (), hashes = saved_value . get_hashes (), lineage = value . get_lineage (), save_lineage = saved_value . get_lineage (), metadata = metadata , load_config = load_config_data , ) value_metadata_path . parent . mkdir ( parents = True , exist_ok = True ) value_metadata_path . write_text ( value_info . json ()) copied_value = saved_value . copy () copied_value . _registry = self copied_value . _kiara = self . _kiara if copied_value . id != value . id : log_message ( f \"Saved value id different to original id ' { value . id } .\" ) copied_value . id = value . id self . _value_obj_cache [ copied_value . id ] = copied_value return copied_value def _prepare_save_config ( self , value_id : str , value_type : str , value : \"Value\" ) -> SaveConfig : store_operations : StoreOperationType = self . _kiara . operation_mgmt . get_operations ( \"store_value\" ) # type: ignore op_config = store_operations . get_store_operation_for_type ( value_type ) if not op_config : raise Exception ( f \"Can't save value: no save operation found for value type ' { value_type } '\" ) target_path = self . get_data_path ( value_id = value_id ) metadata_path = self . get_metadata_path ( value_id = value_id ) if os . path . exists ( metadata_path ): raise Exception ( f \"Can't save value, metadata file already exists: { metadata_path } \" ) input_name = value . type_name if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : value . get_value_data (), \"base_path\" : target_path . as_posix (), \"value_id\" : value_id , } # type: ignore save_config = SaveConfig ( module_type = op_config . module_type , module_config = op_config . module_config , inputs = inputs , load_config_output = \"load_config\" , ) return save_config def get_data_path ( self , value_id : str ) -> Path : return self . _base_path / f \"value_ { value_id } \" / \"data\" def get_metadata_path ( self , value_id : str ) -> Path : path = self . _base_path / f \"value_ { value_id } \" / \"value_metadata.json\" return path # def get_load_config_path(self, value_id: str) -> Path: # path = self._base_path / f\"value_{value_id}\" / \"load_config.json\" # return path def _get_value_slot_for_alias ( self , alias_name : str ) -> ValueSlot : if alias_name in self . _value_slots . keys (): return self . _value_slots [ alias_name ] alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" if not alias_file . is_file (): raise Exception ( f \"No alias with name ' { alias_name } ' registered.\" ) file_content = alias_file . read_text () alias_data = json . loads ( file_content ) value_schema = ValueSchema ( ** alias_data [ \"value_schema\" ]) value_slot = ValueSlot ( id = alias_name , value_schema = value_schema , kiara = self . _kiara , registry = self ) value_slot . register_callbacks ( self ) for version in sorted (( int ( x ) for x in alias_data [ \"versions\" ] . keys ())): value_id = alias_data [ \"versions\" ][ str ( version )] value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"Can't find value with id: { value_id } \" ) tags : typing . Optional [ typing . Iterable [ str ]] = None if value_id in alias_data [ \"tags\" ] . values (): tags = [ tag for tag , version in alias_data [ \"tags\" ] . items () if version == version ] new_version = value_slot . add_value ( value_obj , trigger_callbacks = False , tags = tags ) if new_version != int ( version ): raise Exception ( f \"New version different to stored version ( { new_version } != { version } ). This is a bug.\" ) self . _value_slots [ alias_name ] = value_slot return self . _value_slots [ alias_name ] def _get_available_aliases ( self ) -> typing . Iterable [ str ]: alias_path = self . _base_path / \"aliases\" alias_file = alias_path . glob ( \"alias_*.json\" ) result = [] for af in alias_file : if not af . is_file (): log_message ( f \"Ignoring non-file: { af . as_posix () } \" ) continue alias_name = af . name [ 6 : - 5 ] result . append ( alias_name ) return result def _get_available_value_ids ( self ) -> typing . Iterable [ str ]: value_ids = [ x . parent . name [ 6 :] for x in self . _base_path . glob ( \"value_*/value_metadata.json\" ) ] return value_ids def _register_alias ( self , alias_name : str , value_schema : ValueSchema ) -> ValueSlot : alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" if alias_file . exists (): raise Exception ( f \"Alias ' { alias_name } ' already registered.\" ) if alias_name in self . _value_slots . keys (): raise Exception ( f \"Value slot for alias ' { alias_name } ' already exists.\" ) alias_file . parent . mkdir ( parents = True , exist_ok = True ) vs = ValueSlot ( id = alias_name , value_schema = value_schema , kiara = self . _kiara , registry = self ) self . _value_slots [ alias_name ] = vs self . _write_alias_file ( alias_name ) vs . register_callbacks ( self ) return vs def _write_alias_file ( self , alias_name ): alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" value_slot = self . _value_slots [ alias_name ] alias_dict = { \"value_schema\" : value_slot . value_schema . dict (), \"versions\" : {}, \"tags\" : {}, } for version , value in value_slot . values . items (): alias_dict [ \"versions\" ][ version ] = value . id for tag_name , version in value_slot . tags . items (): alias_dict [ \"tags\" ][ tag_name ] = version alias_file . write_text ( json . dumps ( alias_dict )) def values_updated ( self , * items : \"ValueSlot\" ): invalid = [] for item in items : if item . id not in self . _value_slots . keys (): invalid . append ( item . id ) if invalid : raise Exception ( f \"Can't update value(s), invalid aliases: { ', ' . join ( invalid ) } \" ) for item in items : self . _write_alias_file ( alias_name = item . id ) SavedValueInfo ( ValueInfo ) pydantic-model \u00b6 Source code in kiara/data/registry/store.py class SavedValueInfo ( ValueInfo ): load_config : LoadConfig = Field ( description = \"The configuration to load this value from disk (or however it is stored).\" ) save_lineage : ValueLineage = Field ( description = \"Information about how the value was saved.\" ) load_config : LoadConfig pydantic-field required \u00b6 The configuration to load this value from disk (or however it is stored). save_lineage : ValueLineage pydantic-field required \u00b6 Information about how the value was saved.","title":"store"},{"location":"reference/kiara/data/registry/store/#kiara.data.registry.store.LocalDataStore","text":"An implementation of a kiara data registry that stores data locally, so it can be accessed across sessions. Data is stored under the value for the configured 'base_path' attribute, along with LoadConfig data that describes how the data can be re-loaded into memory. Source code in kiara/data/registry/store.py class LocalDataStore ( DataRegistry ): \"\"\"An implementation of a *kiara* data registry that stores data locally, so it can be accessed across sessions. Data is stored under the value for the configured 'base_path' attribute, along with [LoadConfig][kiara.metadata.data.LoadConfig] data that describes how the data can be re-loaded into memory. \"\"\" def __init__ ( self , kiara : \"Kiara\" , base_path : typing . Union [ str , Path ]): if isinstance ( base_path , str ): base_path = Path ( base_path ) self . _base_path : Path = base_path self . _value_obj_cache : typing . Dict [ str , Value ] = {} self . _value_info_cache : typing . Dict [ str , SavedValueInfo ] = {} self . _data_cache : typing . Dict [ str , Value ] = {} self . _metadata_cache : typing . Dict [ str , typing . Any ] = {} self . _value_slots : typing . Dict [ str , ValueSlot ] = {} super () . __init__ ( kiara = kiara ) @property def base_path ( self ) -> Path : return self . _base_path def _get_saved_value_info ( self , value_id : str ) -> SavedValueInfo : if value_id in self . _value_info_cache . keys (): return self . _value_info_cache [ value_id ] metadata_path = self . get_metadata_path ( value_id = value_id ) if not metadata_path . exists (): raise Exception ( f \"No value with id ' { value_id } ' registered.\" ) info_content = metadata_path . read_text () info_dict = json . loads ( info_content ) value_info = SavedValueInfo ( ** info_dict ) assert value_info . value_id == value_id self . _value_info_cache [ value_id ] = value_info return value_info def _get_value_obj_for_id ( self , value_id : str ) -> Value : if value_id in self . _value_obj_cache . keys (): return self . _value_obj_cache [ value_id ] value_info : SavedValueInfo = self . _get_saved_value_info ( value_id ) value_schema = value_info . value_schema # value_schema = ValueSchema( # type=value_info.value_type, type_config=value_info.value_type_config # ) value_obj = self . _create_value_obj ( value_schema = value_schema , is_set = True , is_none = False , value_hashes = value_info . hashes , metadata = value_info . metadata , ) value_obj . id = value_info . value_id self . _register_new_value_obj ( value_obj = value_obj , value_data = SpecialValue . IGNORE , lineage = value_info . lineage , # value_hashes=value_info.hashes, ) self . _value_obj_cache [ value_obj . id ] = value_obj return value_obj def _get_value_data_for_id ( self , value_id : str ) -> typing . Any : if value_id in self . _data_cache . keys (): return self . _data_cache [ value_id ] . get_value_data () # # value_obj = self.get_value_obj(value_item=value_id) # metadata_path = self.get_metadata_path(value_id=value_id) # if not metadata_path.exists(): # raise Exception(f\"No value with id '{value_id}' registered.\") # # info_content = metadata_path.read_text() # info_dict = json.loads(info_content) # value_info = SavedValueInfo(**info_dict) value_info = self . _get_saved_value_info ( value_id = value_id ) assert value_info . value_id == value_id load_config = value_info . load_config value : Value = self . _kiara . run ( ** load_config . dict ( exclude = { \"value_id\" , \"base_path_input_name\" , \"doc\" })) # type: ignore self . _data_cache [ value_id ] = value return value . get_value_data () def _get_value_lineage ( self , value_id : str ) -> typing . Optional [ ValueLineage ]: return self . _get_saved_value_info ( value_id ) . lineage def _register_value_and_data ( self , value : Value , data : typing . Any ) -> str : if data == SpecialValue . IGNORE : return value . id raise NotImplementedError () # # new_value_id = str(uuid.uuid4()) # # value.id = new_value_id # value_type = value.type_name # # # run the type and repo type specific save module, and retrieve the load_config that is needed to retrieve it again later # save_config = self._prepare_save_config( # value_id=new_value_id, value_type=value_type, value=value # ) # # save_module = save_config.create_module(kiara=self._kiara) # result = save_module.run(**save_config.inputs) # load_config_value: Value = result.get_value_obj(\"load_config\") # # # assemble the value metadata # metadata: typing.Dict[str, typing.Mapping[str, typing.Any]] = dict( # value.get_metadata(also_return_schema=True) # ) def _register_remote_value ( self , value : Value ) -> Value : value_metadata_path = self . get_metadata_path ( value . id ) if value_metadata_path . exists (): raise Exception ( f \"Can't register remote value with id ' { value . id } '. Load config for this id already exists: { value_metadata_path . as_posix () } \" ) value_type = value . type_name # run the type and repo type specific save module, and retrieve the load_config that is needed to retrieve it again later save_config = self . _prepare_save_config ( value_id = value . id , value_type = value_type , value = value ) save_module = save_config . create_module ( kiara = self . _kiara ) result = save_module . run ( ** save_config . inputs ) load_config_value : Value = result . get_value_obj ( \"load_config\" ) field_name = value_type if field_name == \"any\" : field_name = \"value_item\" saved_value : Value = result . get_value_obj ( field_name ) load_config_data = load_config_value . get_value_data () metadata = saved_value . get_metadata ( also_return_schema = True ) value_info = SavedValueInfo ( value_id = value . id , value_schema = saved_value . value_schema , is_valid = saved_value . item_is_valid (), hashes = saved_value . get_hashes (), lineage = value . get_lineage (), save_lineage = saved_value . get_lineage (), metadata = metadata , load_config = load_config_data , ) value_metadata_path . parent . mkdir ( parents = True , exist_ok = True ) value_metadata_path . write_text ( value_info . json ()) copied_value = saved_value . copy () copied_value . _registry = self copied_value . _kiara = self . _kiara if copied_value . id != value . id : log_message ( f \"Saved value id different to original id ' { value . id } .\" ) copied_value . id = value . id self . _value_obj_cache [ copied_value . id ] = copied_value return copied_value def _prepare_save_config ( self , value_id : str , value_type : str , value : \"Value\" ) -> SaveConfig : store_operations : StoreOperationType = self . _kiara . operation_mgmt . get_operations ( \"store_value\" ) # type: ignore op_config = store_operations . get_store_operation_for_type ( value_type ) if not op_config : raise Exception ( f \"Can't save value: no save operation found for value type ' { value_type } '\" ) target_path = self . get_data_path ( value_id = value_id ) metadata_path = self . get_metadata_path ( value_id = value_id ) if os . path . exists ( metadata_path ): raise Exception ( f \"Can't save value, metadata file already exists: { metadata_path } \" ) input_name = value . type_name if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : value . get_value_data (), \"base_path\" : target_path . as_posix (), \"value_id\" : value_id , } # type: ignore save_config = SaveConfig ( module_type = op_config . module_type , module_config = op_config . module_config , inputs = inputs , load_config_output = \"load_config\" , ) return save_config def get_data_path ( self , value_id : str ) -> Path : return self . _base_path / f \"value_ { value_id } \" / \"data\" def get_metadata_path ( self , value_id : str ) -> Path : path = self . _base_path / f \"value_ { value_id } \" / \"value_metadata.json\" return path # def get_load_config_path(self, value_id: str) -> Path: # path = self._base_path / f\"value_{value_id}\" / \"load_config.json\" # return path def _get_value_slot_for_alias ( self , alias_name : str ) -> ValueSlot : if alias_name in self . _value_slots . keys (): return self . _value_slots [ alias_name ] alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" if not alias_file . is_file (): raise Exception ( f \"No alias with name ' { alias_name } ' registered.\" ) file_content = alias_file . read_text () alias_data = json . loads ( file_content ) value_schema = ValueSchema ( ** alias_data [ \"value_schema\" ]) value_slot = ValueSlot ( id = alias_name , value_schema = value_schema , kiara = self . _kiara , registry = self ) value_slot . register_callbacks ( self ) for version in sorted (( int ( x ) for x in alias_data [ \"versions\" ] . keys ())): value_id = alias_data [ \"versions\" ][ str ( version )] value_obj = self . get_value_obj ( value_item = value_id ) if value_obj is None : raise Exception ( f \"Can't find value with id: { value_id } \" ) tags : typing . Optional [ typing . Iterable [ str ]] = None if value_id in alias_data [ \"tags\" ] . values (): tags = [ tag for tag , version in alias_data [ \"tags\" ] . items () if version == version ] new_version = value_slot . add_value ( value_obj , trigger_callbacks = False , tags = tags ) if new_version != int ( version ): raise Exception ( f \"New version different to stored version ( { new_version } != { version } ). This is a bug.\" ) self . _value_slots [ alias_name ] = value_slot return self . _value_slots [ alias_name ] def _get_available_aliases ( self ) -> typing . Iterable [ str ]: alias_path = self . _base_path / \"aliases\" alias_file = alias_path . glob ( \"alias_*.json\" ) result = [] for af in alias_file : if not af . is_file (): log_message ( f \"Ignoring non-file: { af . as_posix () } \" ) continue alias_name = af . name [ 6 : - 5 ] result . append ( alias_name ) return result def _get_available_value_ids ( self ) -> typing . Iterable [ str ]: value_ids = [ x . parent . name [ 6 :] for x in self . _base_path . glob ( \"value_*/value_metadata.json\" ) ] return value_ids def _register_alias ( self , alias_name : str , value_schema : ValueSchema ) -> ValueSlot : alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" if alias_file . exists (): raise Exception ( f \"Alias ' { alias_name } ' already registered.\" ) if alias_name in self . _value_slots . keys (): raise Exception ( f \"Value slot for alias ' { alias_name } ' already exists.\" ) alias_file . parent . mkdir ( parents = True , exist_ok = True ) vs = ValueSlot ( id = alias_name , value_schema = value_schema , kiara = self . _kiara , registry = self ) self . _value_slots [ alias_name ] = vs self . _write_alias_file ( alias_name ) vs . register_callbacks ( self ) return vs def _write_alias_file ( self , alias_name ): alias_file = self . _base_path / \"aliases\" / f \"alias_ { alias_name } .json\" value_slot = self . _value_slots [ alias_name ] alias_dict = { \"value_schema\" : value_slot . value_schema . dict (), \"versions\" : {}, \"tags\" : {}, } for version , value in value_slot . values . items (): alias_dict [ \"versions\" ][ version ] = value . id for tag_name , version in value_slot . tags . items (): alias_dict [ \"tags\" ][ tag_name ] = version alias_file . write_text ( json . dumps ( alias_dict )) def values_updated ( self , * items : \"ValueSlot\" ): invalid = [] for item in items : if item . id not in self . _value_slots . keys (): invalid . append ( item . id ) if invalid : raise Exception ( f \"Can't update value(s), invalid aliases: { ', ' . join ( invalid ) } \" ) for item in items : self . _write_alias_file ( alias_name = item . id )","title":"LocalDataStore"},{"location":"reference/kiara/data/registry/store/#kiara.data.registry.store.SavedValueInfo","text":"Source code in kiara/data/registry/store.py class SavedValueInfo ( ValueInfo ): load_config : LoadConfig = Field ( description = \"The configuration to load this value from disk (or however it is stored).\" ) save_lineage : ValueLineage = Field ( description = \"Information about how the value was saved.\" )","title":"SavedValueInfo"},{"location":"reference/kiara/data/registry/store/#kiara.data.registry.store.SavedValueInfo.load_config","text":"The configuration to load this value from disk (or however it is stored).","title":"load_config"},{"location":"reference/kiara/data/registry/store/#kiara.data.registry.store.SavedValueInfo.save_lineage","text":"Information about how the value was saved.","title":"save_lineage"},{"location":"reference/kiara/data/types/__init__/","text":"This is the base module that contains everything data type-related in kiara . I'm still not 100% sure how to best implement the kiara type system, there are several ways it could be done, for example based on Python type-hints, using JSON-schema, Avro (which is my 2nd favourite option), as well as by implementing a custom type-class hierarchy. Which is what I have choosen to try first. For now, it looks like it'll work out, but there is a chance requirements I haven't forseen will crop up that could make this become ugly. Anyway, the way it works (for now) is that kiara comes with a set of often used types (the standard set of: scalars, list, dict, table & array, etc.) which each come with 2 functions that can serialize and deserialize values of that type in a persistant fashion -- which could be storing as a file on disk, or as a cell/row in a database. Those functions will most likley be kiara modules themselves, with even more restricted input/output type options. In addition, packages that contain modules can implement their own, custom types, if suitable ones are not available in core- kiara . Those can either be 'serialized/deserialized' into kiara -native types (which in turn will serialize them using their own serializing functions), or will have to implement custom serializing functionality (which will probably be discouraged, since this might not be trivial and there are quite a few things to consider). ValueType ( ABC , Generic ) \u00b6 Base class that all kiara types must inherit from. kiara types have 3 main responsibilities: serialize into / deserialize from persistent state data validation metadata extraction Serializing being the arguably most important of those, because without most of the data management features of kiara would be impossible. Validation should not require any explanation. Metadata extraction is important, because that metadata will be available to other components of kiara (or frontends for it), without them having to request the actual data. That will hopefully make kiara very efficient in terms of memory management, as well as data transfer and I/O. Ideally, the actual data (bytes) will only be requested at the last possible moment. For example when a module needs the input data to do processing on it -- and even then it might be that it only requests a part of the data, say a single column of a table. Or when a frontend needs to display/visualize the data. Source code in kiara/data/types/__init__.py class ValueType ( abc . ABC , typing . Generic [ TYPE_PYTHON_CLS , TYPE_CONFIG_CLS ]): \"\"\"Base class that all *kiara* types must inherit from. *kiara* types have 3 main responsibilities: - serialize into / deserialize from persistent state - data validation - metadata extraction Serializing being the arguably most important of those, because without most of the data management features of *kiara* would be impossible. Validation should not require any explanation. Metadata extraction is important, because that metadata will be available to other components of *kiara* (or frontends for it), without them having to request the actual data. That will hopefully make *kiara* very efficient in terms of memory management, as well as data transfer and I/O. Ideally, the actual data (bytes) will only be requested at the last possible moment. For example when a module needs the input data to do processing on it -- and even then it might be that it only requests a part of the data, say a single column of a table. Or when a frontend needs to display/visualize the data. \"\"\" @classmethod def get_type_metadata ( cls ) -> ValueTypeMetadata : return ValueTypeMetadata . from_value_type_class ( cls ) # @classmethod # def doc(cls) -> str: # # return extract_doc_from_cls(cls) # # @classmethod # def desc(cls) -> str: # return extract_doc_from_cls(cls, only_first_line=True) # @classmethod # def conversions( # self, # ) -> typing.Optional[typing.Mapping[str, typing.Mapping[str, typing.Any]]]: # \"\"\"Return a dictionary of configuration for modules that can transform this type. # # The name of the transformation is the key of the result dictionary, the configuration is a module configuration # (dictionary wth 'module_type' and optional 'module_config', 'input_name' and 'output_name' keys). # \"\"\" # # return {\"string\": {\"module_type\": \"string.pretty_print\", \"input_name\": \"item\"}} @classmethod def check_data ( cls , data : typing . Any ) -> typing . Optional [ \"ValueType\" ]: \"\"\"Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). \"\"\" return None @classmethod @abc . abstractmethod def backing_python_type ( cls ) -> typing . Type [ TYPE_PYTHON_CLS ]: pass @classmethod @abc . abstractmethod def type_config_cls ( cls ) -> typing . Type [ TYPE_CONFIG_CLS ]: pass @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return None @classmethod def get_supported_hash_types ( cls ) -> typing . Iterable [ str ]: return [] @classmethod def calculate_value_hash ( cls , value : typing . Any , hash_type : str ) -> str : \"\"\"Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. \"\"\" raise Exception ( f \"Value type ' { cls . _value_type_name } ' does not support hash calculation.\" ) # type: ignore def __init__ ( self , ** type_config : typing . Any ): try : self . _type_config : TYPE_CONFIG_CLS = self . __class__ . type_config_cls ( ** type_config ) # type: ignore # TODO: double-check this is only a mypy issue except ValidationError as ve : raise ValueTypeConfigException ( f \"Error creating object for value_type: { ve } \" , self . __class__ , type_config , ve , ) # self._type_config: typing.Mapping[str, typing.Any] = self # self._transformations: typing.Optional[ # typing.Mapping[str, typing.Mapping[str, typing.Any]] # ] = None @property def type_config ( self ) -> TYPE_CONFIG_CLS : return self . _type_config def import_value ( self , value : typing . Any ) -> typing . Any : assert value is not None try : parsed = self . parse_value ( value ) if parsed is None : parsed = value self . validate ( parsed ) except Exception as e : raise KiaraValueException ( value_type = self . __class__ , value_data = value , exception = e ) return parsed def parse_value ( self , value : typing . Any ) -> typing . Any : \"\"\"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Arguments: v: the value Returns: 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object \"\"\" return None def validate ( cls , value : typing . Any ) -> None : \"\"\"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type).\"\"\" def get_type_hint ( self , context : str = \"python\" ) -> typing . Optional [ typing . Type ]: \"\"\"Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. \"\"\" return None def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : raise NotImplementedError () calculate_value_hash ( value , hash_type ) classmethod \u00b6 Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. Source code in kiara/data/types/__init__.py @classmethod def calculate_value_hash ( cls , value : typing . Any , hash_type : str ) -> str : \"\"\"Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. \"\"\" raise Exception ( f \"Value type ' { cls . _value_type_name } ' does not support hash calculation.\" ) # type: ignore check_data ( data ) classmethod \u00b6 Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). Source code in kiara/data/types/__init__.py @classmethod def check_data ( cls , data : typing . Any ) -> typing . Optional [ \"ValueType\" ]: \"\"\"Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). \"\"\" return None get_type_hint ( self , context = 'python' ) \u00b6 Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. Source code in kiara/data/types/__init__.py def get_type_hint ( self , context : str = \"python\" ) -> typing . Optional [ typing . Type ]: \"\"\"Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. \"\"\" return None parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/__init__.py def parse_value ( self , value : typing . Any ) -> typing . Any : \"\"\"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Arguments: v: the value Returns: 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object \"\"\" return None validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/__init__.py def validate ( cls , value : typing . Any ) -> None : \"\"\"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type).\"\"\" ValueTypeConfigMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/data/types/__init__.py class ValueTypeConfigMetadata ( MetadataModel ): @classmethod def from_config_class ( cls , config_cls : typing . Type [ ValueTypeConfigSchema ], ): flat_models = get_flat_models_from_model ( config_cls ) model_name_map = get_model_name_map ( flat_models ) m_schema , _ , _ = model_process_schema ( config_cls , model_name_map = model_name_map ) fields = m_schema [ \"properties\" ] config_values = {} for field_name , details in fields . items (): type_str = \"-- n/a --\" if \"type\" in details . keys (): type_str = details [ \"type\" ] desc = details . get ( \"description\" , DEFAULT_NO_DESC_VALUE ) default = config_cls . __fields__ [ field_name ] . default if default is None : if callable ( config_cls . __fields__ [ field_name ] . default_factory ): default = config_cls . __fields__ [ field_name ] . default_factory () # type: ignore req = config_cls . __fields__ [ field_name ] . required config_values [ field_name ] = ValueTypeAndDescription ( description = desc , type = type_str , value_default = default , required = req ) python_cls = PythonClassMetadata . from_class ( config_cls ) return ValueTypeConfigMetadata ( python_class = python_cls , config_values = config_values ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this configuration.\" ) config_values : typing . Dict [ str , ValueTypeAndDescription ] = Field ( description = \"The available configuration values.\" ) config_values : Dict [ str , kiara . metadata . ValueTypeAndDescription ] pydantic-field required \u00b6 The available configuration values. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this configuration. ValueTypeConfigSchema ( BaseModel ) pydantic-model \u00b6 Base class that describes the configuration a ValueType class accepts. This is stored in the _config_cls class attribute in each ValueType class. By default, a ValueType is not configurable, unless the _config_cls class attribute points to a sub-class of this class. Source code in kiara/data/types/__init__.py class ValueTypeConfigSchema ( BaseModel ): \"\"\"Base class that describes the configuration a [``ValueType``][kiara.data.types.ValueType] class accepts. This is stored in the ``_config_cls`` class attribute in each ``ValueType`` class. By default, a ``ValueType`` is not configurable, unless the ``_config_cls`` class attribute points to a sub-class of this class. \"\"\" @classmethod def requires_config ( cls ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : return True return False _config_hash : str = PrivateAttr ( default = None ) class Config : extra = Extra . forbid allow_mutation = False def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) @property def config_hash ( self ): if self . _config_hash is None : _d = self . dict () hashes = deepdiff . DeepHash ( _d ) self . _config_hash = hashes [ _d ] return self . _config_hash def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () def __hash__ ( self ): return hash ( self . config_hash ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : my_table = Table ( box = box . MINIMAL , show_header = False ) my_table . add_column ( \"Field name\" , style = \"i\" ) my_table . add_column ( \"Value\" ) for field in self . __fields__ : my_table . add_row ( field , getattr ( self , field )) yield my_table __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/types/__init__.py def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/types/__init__.py def __hash__ ( self ): return hash ( self . config_hash ) get ( self , key ) \u00b6 Get the value for the specified configuation key. Source code in kiara/data/types/__init__.py def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) requires_config () classmethod \u00b6 Return whether this class can be used as-is, or requires configuration before an instance can be created. Source code in kiara/data/types/__init__.py @classmethod def requires_config ( cls ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : return True return False get_type_name ( obj ) \u00b6 Utility function to get a pretty string from the class of an object. Source code in kiara/data/types/__init__.py def get_type_name ( obj : typing . Any ): \"\"\"Utility function to get a pretty string from the class of an object.\"\"\" if obj . __class__ . __module__ == \"builtins\" : return obj . __class__ . __name__ else : return f \" { obj . __class__ . __module__ } . { obj . __class__ . __name__ } \" core \u00b6 AnyType ( ValueType ) \u00b6 Any type / No type information. Source code in kiara/data/types/core.py class AnyType ( ValueType [ object , ValueTypeConfigSchema ]): \"\"\"Any type / No type information.\"\"\" _value_type_name = \"any\" @classmethod def backing_python_type ( cls ) -> typing . Type : return object @classmethod def type_config_cls ( cls ) -> typing . Type [ ValueTypeConfigSchema ]: return ValueTypeConfigSchema def pretty_print_as_renderables ( self , value : \"Value\" , print_config : typing . Mapping [ str , typing . Any ] ) -> typing . Any : data = value . get_value_data () return [ str ( data )] DeserializeConfigData ( KiaraInternalValueType ) \u00b6 A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. Source code in kiara/data/types/core.py class DeserializeConfigData ( KiaraInternalValueType ): \"\"\"A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. \"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , DeserializeConfig ] @classmethod def type_name ( cls ): return \"deserialize_config\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = DeserializeConfig ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , DeserializeConfig ): raise Exception ( f \"Invalid type for deserialize config: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = DeserializeConfig ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , DeserializeConfig ): raise Exception ( f \"Invalid type for deserialize config: { type ( value ) } .\" ) LoadConfigData ( KiaraInternalValueType ) \u00b6 A dictionary representing load config for a kiara value. The load config schema itself can be looked up via the wrapper class ' LoadConfig '. Source code in kiara/data/types/core.py class LoadConfigData ( KiaraInternalValueType ): \"\"\"A dictionary representing load config for a *kiara* value. The load config schema itself can be looked up via the wrapper class '[LoadConfig][kiara.data.persistence.LoadConfig]'. \"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , LoadConfig ] @classmethod def type_name ( cls ): return \"load_config\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = LoadConfig ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , LoadConfig ): raise Exception ( f \"Invalid type for load config: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = LoadConfig ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , LoadConfig ): raise Exception ( f \"Invalid type for load config: { type ( value ) } .\" ) ValueInfoData ( KiaraInternalValueType ) \u00b6 A dictionary representing a kiara ValueInfo object. Source code in kiara/data/types/core.py class ValueInfoData ( KiaraInternalValueType ): \"\"\"A dictionary representing a kiara ValueInfo object.\"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , ValueInfo ] @classmethod def type_name ( cls ): return \"value_info\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueInfo ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueInfo ): raise Exception ( f \"Invalid type for value info: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueInfo ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueInfo ): raise Exception ( f \"Invalid type for value info: { type ( value ) } .\" ) ValueLineageData ( KiaraInternalValueType ) \u00b6 A dictionary representing a kiara ValueLineage. Source code in kiara/data/types/core.py class ValueLineageData ( KiaraInternalValueType ): \"\"\"A dictionary representing a kiara ValueLineage.\"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , ValueLineage ] @classmethod def type_name ( cls ): return \"value_lineage\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueLineage ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueLineage ): raise Exception ( f \"Invalid type for value seed: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueLineage ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueLineage ): raise Exception ( f \"Invalid type for value seed: { type ( value ) } .\" ) type_mgmt \u00b6 TypeMgmt \u00b6 Source code in kiara/data/types/type_mgmt.py class TypeMgmt ( object ): def __init__ ( self , kiara : \"Kiara\" ): self . _kiara : Kiara = kiara self . _value_types : typing . Optional [ bidict [ str , typing . Type [ ValueType ]]] = None self . _value_type_transformations : typing . Dict [ str , typing . Dict [ str , typing . Any ] ] = {} self . _registered_python_classes : typing . Dict [ typing . Type , typing . List [ str ]] = None # type: ignore self . _type_hierarchy : typing . Optional [ nx . DiGraph ] = None def invalidate_types ( self ): self . _value_types = None self . _value_type_transformations . clear () self . _registered_python_classes = None @property def value_types ( self ) -> bidict [ str , typing . Type [ ValueType ]]: if self . _value_types is not None : return self . _value_types self . _value_types = bidict ( find_all_value_types ()) return self . _value_types @property def value_type_hierarchy ( self ) -> \"nx.DiGraph\" : if self . _type_hierarchy is not None : return self . _type_hierarchy def recursive_base_find ( cls : typing . Type , current : typing . Optional [ typing . List [ str ]] = None ): if current is None : current = [] for base in cls . __bases__ : if base in self . value_types . values (): current . append ( self . value_types . inverse [ base ]) recursive_base_find ( base , current = current ) return current bases = {} for name , cls in self . value_types . items (): bases [ name ] = recursive_base_find ( cls ) import networkx as nx hierarchy = nx . DiGraph () for name , _bases in bases . items (): hierarchy . add_node ( name , cls = self . value_types [ name ]) if not _bases : continue # we only need the first parent, all others will be taken care of by the parent of the parent hierarchy . add_edge ( _bases [ 0 ], name ) self . _type_hierarchy = hierarchy return self . _type_hierarchy def get_type_lineage ( self , value_type : str ) -> typing . Iterable [ str ]: \"\"\"Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type.\"\"\" if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx path = nx . shortest_path ( self . value_type_hierarchy , \"any\" , value_type ) return reversed ( path ) def get_sub_types ( self , value_type : str ) -> typing . Set [ str ]: if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx desc = nx . descendants ( self . value_type_hierarchy , value_type ) return desc @property def value_type_names ( self ) -> typing . List [ str ]: return list ( self . value_types . keys ()) @property def registered_python_classes ( self , ) -> typing . Mapping [ typing . Type , typing . Iterable [ str ]]: if self . _registered_python_classes is not None : return self . _registered_python_classes registered_types = {} for name , v_type in self . value_types . items (): rel = v_type . candidate_python_types () if rel : for cls in rel : registered_types . setdefault ( cls , []) . append ( name ) self . _registered_python_classes = registered_types return self . _registered_python_classes def get_type_config_for_data_profile ( self , profile_name : str ) -> typing . Mapping [ str , typing . Any ]: type_name = TYPE_PROFILE_MAP [ profile_name ] return { \"type\" : type_name , \"type_config\" : {}} def determine_type ( self , data : typing . Any ) -> typing . Optional [ ValueType ]: if isinstance ( data , Value ): data = data . get_value_data () result : typing . List [ ValueType ] = [] registered_types = set ( self . registered_python_classes . get ( data . __class__ , [])) for cls in data . __class__ . __bases__ : reg = self . registered_python_classes . get ( cls ) if reg : registered_types . update ( reg ) if registered_types : for rt in registered_types : _cls : typing . Type [ ValueType ] = self . get_value_type_cls ( rt ) match = _cls . check_data ( data ) if match : result . append ( match ) # TODO: re-run all checks on all modules, not just the ones that registered interest in the class if len ( result ) == 0 : return None elif len ( result ) > 1 : result_str = [ x . _value_type_name for x in result ] # type: ignore raise Exception ( f \"Multiple value types found for value: { ', ' . join ( result_str ) } .\" ) else : return result [ 0 ] def get_value_type_cls ( self , type_name : str ) -> typing . Type [ ValueType ]: t = self . value_types . get ( type_name , None ) if t is None : raise Exception ( f \"No value type ' { type_name } ', available types: { ', ' . join ( self . value_types . keys ()) } \" ) return t # def get_value_type_transformations( # self, value_type_name: str # ) -> typing.Mapping[str, typing.Mapping[str, typing.Any]]: # \"\"\"Return available transform pipelines for value types.\"\"\" # # if value_type_name in self._value_type_transformations.keys(): # return self._value_type_transformations[value_type_name] # # type_cls = self.get_value_type_cls(type_name=value_type_name) # _configs = type_cls.conversions() # if _configs is None: # module_configs = {} # else: # module_configs = dict(_configs) # for base in type_cls.__bases__: # if hasattr(base, \"conversions\"): # _b_configs = base.conversions() # type: ignore # if not _b_configs: # continue # for k, v in _b_configs.items(): # if k not in module_configs.keys(): # module_configs[k] = v # # # TODO: check input type compatibility? # result: typing.Dict[str, typing.Dict[str, typing.Any]] = {} # for name, config in module_configs.items(): # config = dict(config) # module_type = config.pop(\"module_type\", None) # if not module_type: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}', no module type specified in config: {config}\" # ) # module_config = config.pop(\"module_config\", {}) # module = self._kiara.create_module( # id=f\"_transform_{value_type_name}_{name}\", # module_type=module_type, # module_config=module_config, # ) # # if \"input_name\" not in config.keys(): # # if len(module.input_schemas) == 1: # config[\"input_name\"] = next(iter(module.input_schemas.keys())) # else: # required_inputs = [ # inp # for inp, schema in module.input_schemas.items() # if schema.is_required() # ] # if len(required_inputs) == 1: # config[\"input_name\"] = required_inputs[0] # else: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}': can't determine input name between those options: '{', '.join(required_inputs)}'\" # ) # # if \"output_name\" not in config.keys(): # # if len(module.input_schemas) == 1: # config[\"output_name\"] = next(iter(module.output_schemas.keys())) # else: # required_outputs = [ # inp # for inp, schema in module.output_schemas.items() # if schema.is_required() # ] # if len(required_outputs) == 1: # config[\"output_name\"] = required_outputs[0] # else: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}': can't determine output name between those options: '{', '.join(required_outputs)}'\" # ) # # result[name] = { # \"module\": module, # \"module_type\": module_type, # \"module_config\": module_config, # \"transformation_config\": config, # } # # self._value_type_transformations[value_type_name] = result # return self._value_type_transformations[value_type_name] def find_value_types_for_package ( self , package_name : str ) -> typing . Dict [ str , typing . Type [ ValueType ]]: result = {} for value_type_name , value_type in self . value_types . items (): value_md = value_type . get_type_metadata () package = value_md . context . labels . get ( \"package\" ) if package == package_name : result [ value_type_name ] = value_type return result # def get_available_transformations_for_type( # self, value_type_name: str # ) -> typing.Iterable[str]: # # return self.get_value_type_transformations(value_type_name=value_type_name) # def transform_value( # self, # transformation_alias: str, # value: Value, # other_inputs: typing.Optional[typing.Mapping[str, typing.Any]] = None, # ) -> Value: # # transformations = self.get_value_type_transformations(value.value_schema.type) # # if transformation_alias not in transformations.keys(): # raise Exception( # f\"Can't transform value of type '{value.value_schema.type}', transformation '{transformation_alias}' not available for this type. Available: {', '.join(transformations.keys())}\" # ) # # config = transformations[transformation_alias] # # transformation_config = config[\"transformation_config\"] # input_name = transformation_config[\"input_name\"] # # module: KiaraModule = config[\"module\"] # # constants = module.get_config_value(\"constants\") # inputs = dict(constants) # # if other_inputs: # # for k, v in other_inputs.items(): # if k in constants.keys(): # raise Exception(f\"Invalid value '{k}' for 'other_inputs', this is a constant that can't be overwrittern.\") # inputs[k] = v # # defaults = transformation_config.get(\"defaults\", None) # if defaults: # for k, v in defaults.items(): # if k in constants.keys(): # raise Exception(f\"Invalid default value '{k}', this is a constant that can't be overwrittern.\") # if k not in inputs.keys(): # inputs[k] = v # # if input_name in inputs.keys(): # raise Exception( # f\"Invalid value for inputs in transform arguments, can't contain the main input key '{input_name}'.\" # ) # # inputs[input_name] = value # # result = module.run(**inputs) # output_name = transformation_config[\"output_name\"] # # result_value = result.get_value_obj(output_name) # return result_value get_type_lineage ( self , value_type ) \u00b6 Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type. Source code in kiara/data/types/type_mgmt.py def get_type_lineage ( self , value_type : str ) -> typing . Iterable [ str ]: \"\"\"Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type.\"\"\" if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx path = nx . shortest_path ( self . value_type_hierarchy , \"any\" , value_type ) return reversed ( path )","title":"types"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueType","text":"Base class that all kiara types must inherit from. kiara types have 3 main responsibilities: serialize into / deserialize from persistent state data validation metadata extraction Serializing being the arguably most important of those, because without most of the data management features of kiara would be impossible. Validation should not require any explanation. Metadata extraction is important, because that metadata will be available to other components of kiara (or frontends for it), without them having to request the actual data. That will hopefully make kiara very efficient in terms of memory management, as well as data transfer and I/O. Ideally, the actual data (bytes) will only be requested at the last possible moment. For example when a module needs the input data to do processing on it -- and even then it might be that it only requests a part of the data, say a single column of a table. Or when a frontend needs to display/visualize the data. Source code in kiara/data/types/__init__.py class ValueType ( abc . ABC , typing . Generic [ TYPE_PYTHON_CLS , TYPE_CONFIG_CLS ]): \"\"\"Base class that all *kiara* types must inherit from. *kiara* types have 3 main responsibilities: - serialize into / deserialize from persistent state - data validation - metadata extraction Serializing being the arguably most important of those, because without most of the data management features of *kiara* would be impossible. Validation should not require any explanation. Metadata extraction is important, because that metadata will be available to other components of *kiara* (or frontends for it), without them having to request the actual data. That will hopefully make *kiara* very efficient in terms of memory management, as well as data transfer and I/O. Ideally, the actual data (bytes) will only be requested at the last possible moment. For example when a module needs the input data to do processing on it -- and even then it might be that it only requests a part of the data, say a single column of a table. Or when a frontend needs to display/visualize the data. \"\"\" @classmethod def get_type_metadata ( cls ) -> ValueTypeMetadata : return ValueTypeMetadata . from_value_type_class ( cls ) # @classmethod # def doc(cls) -> str: # # return extract_doc_from_cls(cls) # # @classmethod # def desc(cls) -> str: # return extract_doc_from_cls(cls, only_first_line=True) # @classmethod # def conversions( # self, # ) -> typing.Optional[typing.Mapping[str, typing.Mapping[str, typing.Any]]]: # \"\"\"Return a dictionary of configuration for modules that can transform this type. # # The name of the transformation is the key of the result dictionary, the configuration is a module configuration # (dictionary wth 'module_type' and optional 'module_config', 'input_name' and 'output_name' keys). # \"\"\" # # return {\"string\": {\"module_type\": \"string.pretty_print\", \"input_name\": \"item\"}} @classmethod def check_data ( cls , data : typing . Any ) -> typing . Optional [ \"ValueType\" ]: \"\"\"Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). \"\"\" return None @classmethod @abc . abstractmethod def backing_python_type ( cls ) -> typing . Type [ TYPE_PYTHON_CLS ]: pass @classmethod @abc . abstractmethod def type_config_cls ( cls ) -> typing . Type [ TYPE_CONFIG_CLS ]: pass @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return None @classmethod def get_supported_hash_types ( cls ) -> typing . Iterable [ str ]: return [] @classmethod def calculate_value_hash ( cls , value : typing . Any , hash_type : str ) -> str : \"\"\"Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. \"\"\" raise Exception ( f \"Value type ' { cls . _value_type_name } ' does not support hash calculation.\" ) # type: ignore def __init__ ( self , ** type_config : typing . Any ): try : self . _type_config : TYPE_CONFIG_CLS = self . __class__ . type_config_cls ( ** type_config ) # type: ignore # TODO: double-check this is only a mypy issue except ValidationError as ve : raise ValueTypeConfigException ( f \"Error creating object for value_type: { ve } \" , self . __class__ , type_config , ve , ) # self._type_config: typing.Mapping[str, typing.Any] = self # self._transformations: typing.Optional[ # typing.Mapping[str, typing.Mapping[str, typing.Any]] # ] = None @property def type_config ( self ) -> TYPE_CONFIG_CLS : return self . _type_config def import_value ( self , value : typing . Any ) -> typing . Any : assert value is not None try : parsed = self . parse_value ( value ) if parsed is None : parsed = value self . validate ( parsed ) except Exception as e : raise KiaraValueException ( value_type = self . __class__ , value_data = value , exception = e ) return parsed def parse_value ( self , value : typing . Any ) -> typing . Any : \"\"\"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Arguments: v: the value Returns: 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object \"\"\" return None def validate ( cls , value : typing . Any ) -> None : \"\"\"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type).\"\"\" def get_type_hint ( self , context : str = \"python\" ) -> typing . Optional [ typing . Type ]: \"\"\"Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. \"\"\" return None def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : raise NotImplementedError ()","title":"ValueType"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueType.calculate_value_hash","text":"Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. Source code in kiara/data/types/__init__.py @classmethod def calculate_value_hash ( cls , value : typing . Any , hash_type : str ) -> str : \"\"\"Calculate the hash of this value. If a hash can't be calculated, or the calculation of a type is not implemented (yet), this will return None. \"\"\" raise Exception ( f \"Value type ' { cls . _value_type_name } ' does not support hash calculation.\" ) # type: ignore","title":"calculate_value_hash()"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueType.check_data","text":"Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). Source code in kiara/data/types/__init__.py @classmethod def check_data ( cls , data : typing . Any ) -> typing . Optional [ \"ValueType\" ]: \"\"\"Check whether the provided input matches this value type. If it does, return a ValueType object (with the appropriate type configuration). \"\"\" return None","title":"check_data()"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueType.get_type_hint","text":"Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. Source code in kiara/data/types/__init__.py def get_type_hint ( self , context : str = \"python\" ) -> typing . Optional [ typing . Type ]: \"\"\"Return a type hint for this value type object. This can be used by kiara interfaces to document/validate user input. For now, only 'python' type hints are expected to be implemented, but in theory this could also return type hints for other contexts. \"\"\" return None","title":"get_type_hint()"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueType.parse_value","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/__init__.py def parse_value ( self , value : typing . Any ) -> typing . Any : \"\"\"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Arguments: v: the value Returns: 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object \"\"\" return None","title":"parse_value()"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueType.validate","text":"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/__init__.py def validate ( cls , value : typing . Any ) -> None : \"\"\"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type).\"\"\"","title":"validate()"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueTypeConfigMetadata","text":"Source code in kiara/data/types/__init__.py class ValueTypeConfigMetadata ( MetadataModel ): @classmethod def from_config_class ( cls , config_cls : typing . Type [ ValueTypeConfigSchema ], ): flat_models = get_flat_models_from_model ( config_cls ) model_name_map = get_model_name_map ( flat_models ) m_schema , _ , _ = model_process_schema ( config_cls , model_name_map = model_name_map ) fields = m_schema [ \"properties\" ] config_values = {} for field_name , details in fields . items (): type_str = \"-- n/a --\" if \"type\" in details . keys (): type_str = details [ \"type\" ] desc = details . get ( \"description\" , DEFAULT_NO_DESC_VALUE ) default = config_cls . __fields__ [ field_name ] . default if default is None : if callable ( config_cls . __fields__ [ field_name ] . default_factory ): default = config_cls . __fields__ [ field_name ] . default_factory () # type: ignore req = config_cls . __fields__ [ field_name ] . required config_values [ field_name ] = ValueTypeAndDescription ( description = desc , type = type_str , value_default = default , required = req ) python_cls = PythonClassMetadata . from_class ( config_cls ) return ValueTypeConfigMetadata ( python_class = python_cls , config_values = config_values ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this configuration.\" ) config_values : typing . Dict [ str , ValueTypeAndDescription ] = Field ( description = \"The available configuration values.\" )","title":"ValueTypeConfigMetadata"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueTypeConfigMetadata.config_values","text":"The available configuration values.","title":"config_values"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueTypeConfigMetadata.python_class","text":"The Python class for this configuration.","title":"python_class"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueTypeConfigSchema","text":"Base class that describes the configuration a ValueType class accepts. This is stored in the _config_cls class attribute in each ValueType class. By default, a ValueType is not configurable, unless the _config_cls class attribute points to a sub-class of this class. Source code in kiara/data/types/__init__.py class ValueTypeConfigSchema ( BaseModel ): \"\"\"Base class that describes the configuration a [``ValueType``][kiara.data.types.ValueType] class accepts. This is stored in the ``_config_cls`` class attribute in each ``ValueType`` class. By default, a ``ValueType`` is not configurable, unless the ``_config_cls`` class attribute points to a sub-class of this class. \"\"\" @classmethod def requires_config ( cls ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : return True return False _config_hash : str = PrivateAttr ( default = None ) class Config : extra = Extra . forbid allow_mutation = False def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key ) @property def config_hash ( self ): if self . _config_hash is None : _d = self . dict () hashes = deepdiff . DeepHash ( _d ) self . _config_hash = hashes [ _d ] return self . _config_hash def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict () def __hash__ ( self ): return hash ( self . config_hash ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : my_table = Table ( box = box . MINIMAL , show_header = False ) my_table . add_column ( \"Field name\" , style = \"i\" ) my_table . add_column ( \"Value\" ) for field in self . __fields__ : my_table . add_row ( field , getattr ( self , field )) yield my_table","title":"ValueTypeConfigSchema"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueTypeConfigSchema.__eq__","text":"Return self==value. Source code in kiara/data/types/__init__.py def __eq__ ( self , other ): if self . __class__ != other . __class__ : return False return self . dict () == other . dict ()","title":"__eq__()"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueTypeConfigSchema.__hash__","text":"Return hash(self). Source code in kiara/data/types/__init__.py def __hash__ ( self ): return hash ( self . config_hash )","title":"__hash__()"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueTypeConfigSchema.get","text":"Get the value for the specified configuation key. Source code in kiara/data/types/__init__.py def get ( self , key : str ) -> typing . Any : \"\"\"Get the value for the specified configuation key.\"\"\" if key not in self . __fields__ : raise Exception ( f \"No config value ' { key } ' in module config class ' { self . __class__ . __name__ } '.\" ) return getattr ( self , key )","title":"get()"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.ValueTypeConfigSchema.requires_config","text":"Return whether this class can be used as-is, or requires configuration before an instance can be created. Source code in kiara/data/types/__init__.py @classmethod def requires_config ( cls ) -> bool : \"\"\"Return whether this class can be used as-is, or requires configuration before an instance can be created.\"\"\" for field_name , field in cls . __fields__ . items (): if field . required and field . default is None : return True return False","title":"requires_config()"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.get_type_name","text":"Utility function to get a pretty string from the class of an object. Source code in kiara/data/types/__init__.py def get_type_name ( obj : typing . Any ): \"\"\"Utility function to get a pretty string from the class of an object.\"\"\" if obj . __class__ . __module__ == \"builtins\" : return obj . __class__ . __name__ else : return f \" { obj . __class__ . __module__ } . { obj . __class__ . __name__ } \"","title":"get_type_name()"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.core","text":"","title":"core"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.core.AnyType","text":"Any type / No type information. Source code in kiara/data/types/core.py class AnyType ( ValueType [ object , ValueTypeConfigSchema ]): \"\"\"Any type / No type information.\"\"\" _value_type_name = \"any\" @classmethod def backing_python_type ( cls ) -> typing . Type : return object @classmethod def type_config_cls ( cls ) -> typing . Type [ ValueTypeConfigSchema ]: return ValueTypeConfigSchema def pretty_print_as_renderables ( self , value : \"Value\" , print_config : typing . Mapping [ str , typing . Any ] ) -> typing . Any : data = value . get_value_data () return [ str ( data )]","title":"AnyType"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.core.DeserializeConfigData","text":"A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. Source code in kiara/data/types/core.py class DeserializeConfigData ( KiaraInternalValueType ): \"\"\"A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. \"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , DeserializeConfig ] @classmethod def type_name ( cls ): return \"deserialize_config\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = DeserializeConfig ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , DeserializeConfig ): raise Exception ( f \"Invalid type for deserialize config: { type ( value ) } .\" )","title":"DeserializeConfigData"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.core.LoadConfigData","text":"A dictionary representing load config for a kiara value. The load config schema itself can be looked up via the wrapper class ' LoadConfig '. Source code in kiara/data/types/core.py class LoadConfigData ( KiaraInternalValueType ): \"\"\"A dictionary representing load config for a *kiara* value. The load config schema itself can be looked up via the wrapper class '[LoadConfig][kiara.data.persistence.LoadConfig]'. \"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , LoadConfig ] @classmethod def type_name ( cls ): return \"load_config\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = LoadConfig ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , LoadConfig ): raise Exception ( f \"Invalid type for load config: { type ( value ) } .\" )","title":"LoadConfigData"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.core.ValueInfoData","text":"A dictionary representing a kiara ValueInfo object. Source code in kiara/data/types/core.py class ValueInfoData ( KiaraInternalValueType ): \"\"\"A dictionary representing a kiara ValueInfo object.\"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , ValueInfo ] @classmethod def type_name ( cls ): return \"value_info\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueInfo ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueInfo ): raise Exception ( f \"Invalid type for value info: { type ( value ) } .\" )","title":"ValueInfoData"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.core.ValueLineageData","text":"A dictionary representing a kiara ValueLineage. Source code in kiara/data/types/core.py class ValueLineageData ( KiaraInternalValueType ): \"\"\"A dictionary representing a kiara ValueLineage.\"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , ValueLineage ] @classmethod def type_name ( cls ): return \"value_lineage\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueLineage ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueLineage ): raise Exception ( f \"Invalid type for value seed: { type ( value ) } .\" )","title":"ValueLineageData"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.type_mgmt","text":"","title":"type_mgmt"},{"location":"reference/kiara/data/types/__init__/#kiara.data.types.type_mgmt.TypeMgmt","text":"Source code in kiara/data/types/type_mgmt.py class TypeMgmt ( object ): def __init__ ( self , kiara : \"Kiara\" ): self . _kiara : Kiara = kiara self . _value_types : typing . Optional [ bidict [ str , typing . Type [ ValueType ]]] = None self . _value_type_transformations : typing . Dict [ str , typing . Dict [ str , typing . Any ] ] = {} self . _registered_python_classes : typing . Dict [ typing . Type , typing . List [ str ]] = None # type: ignore self . _type_hierarchy : typing . Optional [ nx . DiGraph ] = None def invalidate_types ( self ): self . _value_types = None self . _value_type_transformations . clear () self . _registered_python_classes = None @property def value_types ( self ) -> bidict [ str , typing . Type [ ValueType ]]: if self . _value_types is not None : return self . _value_types self . _value_types = bidict ( find_all_value_types ()) return self . _value_types @property def value_type_hierarchy ( self ) -> \"nx.DiGraph\" : if self . _type_hierarchy is not None : return self . _type_hierarchy def recursive_base_find ( cls : typing . Type , current : typing . Optional [ typing . List [ str ]] = None ): if current is None : current = [] for base in cls . __bases__ : if base in self . value_types . values (): current . append ( self . value_types . inverse [ base ]) recursive_base_find ( base , current = current ) return current bases = {} for name , cls in self . value_types . items (): bases [ name ] = recursive_base_find ( cls ) import networkx as nx hierarchy = nx . DiGraph () for name , _bases in bases . items (): hierarchy . add_node ( name , cls = self . value_types [ name ]) if not _bases : continue # we only need the first parent, all others will be taken care of by the parent of the parent hierarchy . add_edge ( _bases [ 0 ], name ) self . _type_hierarchy = hierarchy return self . _type_hierarchy def get_type_lineage ( self , value_type : str ) -> typing . Iterable [ str ]: \"\"\"Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type.\"\"\" if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx path = nx . shortest_path ( self . value_type_hierarchy , \"any\" , value_type ) return reversed ( path ) def get_sub_types ( self , value_type : str ) -> typing . Set [ str ]: if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx desc = nx . descendants ( self . value_type_hierarchy , value_type ) return desc @property def value_type_names ( self ) -> typing . List [ str ]: return list ( self . value_types . keys ()) @property def registered_python_classes ( self , ) -> typing . Mapping [ typing . Type , typing . Iterable [ str ]]: if self . _registered_python_classes is not None : return self . _registered_python_classes registered_types = {} for name , v_type in self . value_types . items (): rel = v_type . candidate_python_types () if rel : for cls in rel : registered_types . setdefault ( cls , []) . append ( name ) self . _registered_python_classes = registered_types return self . _registered_python_classes def get_type_config_for_data_profile ( self , profile_name : str ) -> typing . Mapping [ str , typing . Any ]: type_name = TYPE_PROFILE_MAP [ profile_name ] return { \"type\" : type_name , \"type_config\" : {}} def determine_type ( self , data : typing . Any ) -> typing . Optional [ ValueType ]: if isinstance ( data , Value ): data = data . get_value_data () result : typing . List [ ValueType ] = [] registered_types = set ( self . registered_python_classes . get ( data . __class__ , [])) for cls in data . __class__ . __bases__ : reg = self . registered_python_classes . get ( cls ) if reg : registered_types . update ( reg ) if registered_types : for rt in registered_types : _cls : typing . Type [ ValueType ] = self . get_value_type_cls ( rt ) match = _cls . check_data ( data ) if match : result . append ( match ) # TODO: re-run all checks on all modules, not just the ones that registered interest in the class if len ( result ) == 0 : return None elif len ( result ) > 1 : result_str = [ x . _value_type_name for x in result ] # type: ignore raise Exception ( f \"Multiple value types found for value: { ', ' . join ( result_str ) } .\" ) else : return result [ 0 ] def get_value_type_cls ( self , type_name : str ) -> typing . Type [ ValueType ]: t = self . value_types . get ( type_name , None ) if t is None : raise Exception ( f \"No value type ' { type_name } ', available types: { ', ' . join ( self . value_types . keys ()) } \" ) return t # def get_value_type_transformations( # self, value_type_name: str # ) -> typing.Mapping[str, typing.Mapping[str, typing.Any]]: # \"\"\"Return available transform pipelines for value types.\"\"\" # # if value_type_name in self._value_type_transformations.keys(): # return self._value_type_transformations[value_type_name] # # type_cls = self.get_value_type_cls(type_name=value_type_name) # _configs = type_cls.conversions() # if _configs is None: # module_configs = {} # else: # module_configs = dict(_configs) # for base in type_cls.__bases__: # if hasattr(base, \"conversions\"): # _b_configs = base.conversions() # type: ignore # if not _b_configs: # continue # for k, v in _b_configs.items(): # if k not in module_configs.keys(): # module_configs[k] = v # # # TODO: check input type compatibility? # result: typing.Dict[str, typing.Dict[str, typing.Any]] = {} # for name, config in module_configs.items(): # config = dict(config) # module_type = config.pop(\"module_type\", None) # if not module_type: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}', no module type specified in config: {config}\" # ) # module_config = config.pop(\"module_config\", {}) # module = self._kiara.create_module( # id=f\"_transform_{value_type_name}_{name}\", # module_type=module_type, # module_config=module_config, # ) # # if \"input_name\" not in config.keys(): # # if len(module.input_schemas) == 1: # config[\"input_name\"] = next(iter(module.input_schemas.keys())) # else: # required_inputs = [ # inp # for inp, schema in module.input_schemas.items() # if schema.is_required() # ] # if len(required_inputs) == 1: # config[\"input_name\"] = required_inputs[0] # else: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}': can't determine input name between those options: '{', '.join(required_inputs)}'\" # ) # # if \"output_name\" not in config.keys(): # # if len(module.input_schemas) == 1: # config[\"output_name\"] = next(iter(module.output_schemas.keys())) # else: # required_outputs = [ # inp # for inp, schema in module.output_schemas.items() # if schema.is_required() # ] # if len(required_outputs) == 1: # config[\"output_name\"] = required_outputs[0] # else: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}': can't determine output name between those options: '{', '.join(required_outputs)}'\" # ) # # result[name] = { # \"module\": module, # \"module_type\": module_type, # \"module_config\": module_config, # \"transformation_config\": config, # } # # self._value_type_transformations[value_type_name] = result # return self._value_type_transformations[value_type_name] def find_value_types_for_package ( self , package_name : str ) -> typing . Dict [ str , typing . Type [ ValueType ]]: result = {} for value_type_name , value_type in self . value_types . items (): value_md = value_type . get_type_metadata () package = value_md . context . labels . get ( \"package\" ) if package == package_name : result [ value_type_name ] = value_type return result # def get_available_transformations_for_type( # self, value_type_name: str # ) -> typing.Iterable[str]: # # return self.get_value_type_transformations(value_type_name=value_type_name) # def transform_value( # self, # transformation_alias: str, # value: Value, # other_inputs: typing.Optional[typing.Mapping[str, typing.Any]] = None, # ) -> Value: # # transformations = self.get_value_type_transformations(value.value_schema.type) # # if transformation_alias not in transformations.keys(): # raise Exception( # f\"Can't transform value of type '{value.value_schema.type}', transformation '{transformation_alias}' not available for this type. Available: {', '.join(transformations.keys())}\" # ) # # config = transformations[transformation_alias] # # transformation_config = config[\"transformation_config\"] # input_name = transformation_config[\"input_name\"] # # module: KiaraModule = config[\"module\"] # # constants = module.get_config_value(\"constants\") # inputs = dict(constants) # # if other_inputs: # # for k, v in other_inputs.items(): # if k in constants.keys(): # raise Exception(f\"Invalid value '{k}' for 'other_inputs', this is a constant that can't be overwrittern.\") # inputs[k] = v # # defaults = transformation_config.get(\"defaults\", None) # if defaults: # for k, v in defaults.items(): # if k in constants.keys(): # raise Exception(f\"Invalid default value '{k}', this is a constant that can't be overwrittern.\") # if k not in inputs.keys(): # inputs[k] = v # # if input_name in inputs.keys(): # raise Exception( # f\"Invalid value for inputs in transform arguments, can't contain the main input key '{input_name}'.\" # ) # # inputs[input_name] = value # # result = module.run(**inputs) # output_name = transformation_config[\"output_name\"] # # result_value = result.get_value_obj(output_name) # return result_value","title":"TypeMgmt"},{"location":"reference/kiara/data/types/core/","text":"AnyType ( ValueType ) \u00b6 Any type / No type information. Source code in kiara/data/types/core.py class AnyType ( ValueType [ object , ValueTypeConfigSchema ]): \"\"\"Any type / No type information.\"\"\" _value_type_name = \"any\" @classmethod def backing_python_type ( cls ) -> typing . Type : return object @classmethod def type_config_cls ( cls ) -> typing . Type [ ValueTypeConfigSchema ]: return ValueTypeConfigSchema def pretty_print_as_renderables ( self , value : \"Value\" , print_config : typing . Mapping [ str , typing . Any ] ) -> typing . Any : data = value . get_value_data () return [ str ( data )] DeserializeConfigData ( KiaraInternalValueType ) \u00b6 A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. Source code in kiara/data/types/core.py class DeserializeConfigData ( KiaraInternalValueType ): \"\"\"A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. \"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , DeserializeConfig ] @classmethod def type_name ( cls ): return \"deserialize_config\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = DeserializeConfig ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , DeserializeConfig ): raise Exception ( f \"Invalid type for deserialize config: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = DeserializeConfig ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , DeserializeConfig ): raise Exception ( f \"Invalid type for deserialize config: { type ( value ) } .\" ) LoadConfigData ( KiaraInternalValueType ) \u00b6 A dictionary representing load config for a kiara value. The load config schema itself can be looked up via the wrapper class ' LoadConfig '. Source code in kiara/data/types/core.py class LoadConfigData ( KiaraInternalValueType ): \"\"\"A dictionary representing load config for a *kiara* value. The load config schema itself can be looked up via the wrapper class '[LoadConfig][kiara.data.persistence.LoadConfig]'. \"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , LoadConfig ] @classmethod def type_name ( cls ): return \"load_config\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = LoadConfig ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , LoadConfig ): raise Exception ( f \"Invalid type for load config: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = LoadConfig ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , LoadConfig ): raise Exception ( f \"Invalid type for load config: { type ( value ) } .\" ) ValueInfoData ( KiaraInternalValueType ) \u00b6 A dictionary representing a kiara ValueInfo object. Source code in kiara/data/types/core.py class ValueInfoData ( KiaraInternalValueType ): \"\"\"A dictionary representing a kiara ValueInfo object.\"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , ValueInfo ] @classmethod def type_name ( cls ): return \"value_info\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueInfo ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueInfo ): raise Exception ( f \"Invalid type for value info: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueInfo ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueInfo ): raise Exception ( f \"Invalid type for value info: { type ( value ) } .\" ) ValueLineageData ( KiaraInternalValueType ) \u00b6 A dictionary representing a kiara ValueLineage. Source code in kiara/data/types/core.py class ValueLineageData ( KiaraInternalValueType ): \"\"\"A dictionary representing a kiara ValueLineage.\"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , ValueLineage ] @classmethod def type_name ( cls ): return \"value_lineage\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueLineage ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueLineage ): raise Exception ( f \"Invalid type for value seed: { type ( value ) } .\" ) parse_value ( self , value ) \u00b6 Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueLineage ( ** value ) return _value validate ( cls , value ) \u00b6 Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueLineage ): raise Exception ( f \"Invalid type for value seed: { type ( value ) } .\" )","title":"core"},{"location":"reference/kiara/data/types/core/#kiara.data.types.core.AnyType","text":"Any type / No type information. Source code in kiara/data/types/core.py class AnyType ( ValueType [ object , ValueTypeConfigSchema ]): \"\"\"Any type / No type information.\"\"\" _value_type_name = \"any\" @classmethod def backing_python_type ( cls ) -> typing . Type : return object @classmethod def type_config_cls ( cls ) -> typing . Type [ ValueTypeConfigSchema ]: return ValueTypeConfigSchema def pretty_print_as_renderables ( self , value : \"Value\" , print_config : typing . Mapping [ str , typing . Any ] ) -> typing . Any : data = value . get_value_data () return [ str ( data )]","title":"AnyType"},{"location":"reference/kiara/data/types/core/#kiara.data.types.core.DeserializeConfigData","text":"A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. Source code in kiara/data/types/core.py class DeserializeConfigData ( KiaraInternalValueType ): \"\"\"A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. \"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , DeserializeConfig ] @classmethod def type_name ( cls ): return \"deserialize_config\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = DeserializeConfig ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , DeserializeConfig ): raise Exception ( f \"Invalid type for deserialize config: { type ( value ) } .\" )","title":"DeserializeConfigData"},{"location":"reference/kiara/data/types/core/#kiara.data.types.core.DeserializeConfigData.parse_value","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = DeserializeConfig ( ** value ) return _value","title":"parse_value()"},{"location":"reference/kiara/data/types/core/#kiara.data.types.core.DeserializeConfigData.validate","text":"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , DeserializeConfig ): raise Exception ( f \"Invalid type for deserialize config: { type ( value ) } .\" )","title":"validate()"},{"location":"reference/kiara/data/types/core/#kiara.data.types.core.LoadConfigData","text":"A dictionary representing load config for a kiara value. The load config schema itself can be looked up via the wrapper class ' LoadConfig '. Source code in kiara/data/types/core.py class LoadConfigData ( KiaraInternalValueType ): \"\"\"A dictionary representing load config for a *kiara* value. The load config schema itself can be looked up via the wrapper class '[LoadConfig][kiara.data.persistence.LoadConfig]'. \"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , LoadConfig ] @classmethod def type_name ( cls ): return \"load_config\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = LoadConfig ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , LoadConfig ): raise Exception ( f \"Invalid type for load config: { type ( value ) } .\" )","title":"LoadConfigData"},{"location":"reference/kiara/data/types/core/#kiara.data.types.core.LoadConfigData.parse_value","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = LoadConfig ( ** value ) return _value","title":"parse_value()"},{"location":"reference/kiara/data/types/core/#kiara.data.types.core.LoadConfigData.validate","text":"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , LoadConfig ): raise Exception ( f \"Invalid type for load config: { type ( value ) } .\" )","title":"validate()"},{"location":"reference/kiara/data/types/core/#kiara.data.types.core.ValueInfoData","text":"A dictionary representing a kiara ValueInfo object. Source code in kiara/data/types/core.py class ValueInfoData ( KiaraInternalValueType ): \"\"\"A dictionary representing a kiara ValueInfo object.\"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , ValueInfo ] @classmethod def type_name ( cls ): return \"value_info\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueInfo ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueInfo ): raise Exception ( f \"Invalid type for value info: { type ( value ) } .\" )","title":"ValueInfoData"},{"location":"reference/kiara/data/types/core/#kiara.data.types.core.ValueInfoData.parse_value","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueInfo ( ** value ) return _value","title":"parse_value()"},{"location":"reference/kiara/data/types/core/#kiara.data.types.core.ValueInfoData.validate","text":"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueInfo ): raise Exception ( f \"Invalid type for value info: { type ( value ) } .\" )","title":"validate()"},{"location":"reference/kiara/data/types/core/#kiara.data.types.core.ValueLineageData","text":"A dictionary representing a kiara ValueLineage. Source code in kiara/data/types/core.py class ValueLineageData ( KiaraInternalValueType ): \"\"\"A dictionary representing a kiara ValueLineage.\"\"\" @classmethod def candidate_python_types ( cls ) -> typing . Optional [ typing . Iterable [ typing . Type ]]: return [ typing . Mapping , ValueLineage ] @classmethod def type_name ( cls ): return \"value_lineage\" def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueLineage ( ** value ) return _value def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueLineage ): raise Exception ( f \"Invalid type for value seed: { type ( value ) } .\" )","title":"ValueLineageData"},{"location":"reference/kiara/data/types/core/#kiara.data.types.core.ValueLineageData.parse_value","text":"Parse a value into a supported python type. This exists to make it easier to do trivial conversions (e.g. from a date string to a datetime object). If you choose to overwrite this method, make 100% sure that you don't change the meaning of the value, and try to avoid adding or removing information from the data (e.g. by changing the resolution of a date). Parameters: Name Type Description Default v the value required Returns: Type Description Any 'None', if no parsing was done and the original value should be used, otherwise return the parsed Python object Source code in kiara/data/types/core.py def parse_value ( self , value : typing . Any ) -> typing . Any : if isinstance ( value , typing . Mapping ): _value = ValueLineage ( ** value ) return _value","title":"parse_value()"},{"location":"reference/kiara/data/types/core/#kiara.data.types.core.ValueLineageData.validate","text":"Validate the value. This expects an instance of the defined Python class (from 'backing_python_type). Source code in kiara/data/types/core.py def validate ( cls , value : typing . Any ) -> None : if not isinstance ( value , ValueLineage ): raise Exception ( f \"Invalid type for value seed: { type ( value ) } .\" )","title":"validate()"},{"location":"reference/kiara/data/types/type_mgmt/","text":"TypeMgmt \u00b6 Source code in kiara/data/types/type_mgmt.py class TypeMgmt ( object ): def __init__ ( self , kiara : \"Kiara\" ): self . _kiara : Kiara = kiara self . _value_types : typing . Optional [ bidict [ str , typing . Type [ ValueType ]]] = None self . _value_type_transformations : typing . Dict [ str , typing . Dict [ str , typing . Any ] ] = {} self . _registered_python_classes : typing . Dict [ typing . Type , typing . List [ str ]] = None # type: ignore self . _type_hierarchy : typing . Optional [ nx . DiGraph ] = None def invalidate_types ( self ): self . _value_types = None self . _value_type_transformations . clear () self . _registered_python_classes = None @property def value_types ( self ) -> bidict [ str , typing . Type [ ValueType ]]: if self . _value_types is not None : return self . _value_types self . _value_types = bidict ( find_all_value_types ()) return self . _value_types @property def value_type_hierarchy ( self ) -> \"nx.DiGraph\" : if self . _type_hierarchy is not None : return self . _type_hierarchy def recursive_base_find ( cls : typing . Type , current : typing . Optional [ typing . List [ str ]] = None ): if current is None : current = [] for base in cls . __bases__ : if base in self . value_types . values (): current . append ( self . value_types . inverse [ base ]) recursive_base_find ( base , current = current ) return current bases = {} for name , cls in self . value_types . items (): bases [ name ] = recursive_base_find ( cls ) import networkx as nx hierarchy = nx . DiGraph () for name , _bases in bases . items (): hierarchy . add_node ( name , cls = self . value_types [ name ]) if not _bases : continue # we only need the first parent, all others will be taken care of by the parent of the parent hierarchy . add_edge ( _bases [ 0 ], name ) self . _type_hierarchy = hierarchy return self . _type_hierarchy def get_type_lineage ( self , value_type : str ) -> typing . Iterable [ str ]: \"\"\"Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type.\"\"\" if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx path = nx . shortest_path ( self . value_type_hierarchy , \"any\" , value_type ) return reversed ( path ) def get_sub_types ( self , value_type : str ) -> typing . Set [ str ]: if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx desc = nx . descendants ( self . value_type_hierarchy , value_type ) return desc @property def value_type_names ( self ) -> typing . List [ str ]: return list ( self . value_types . keys ()) @property def registered_python_classes ( self , ) -> typing . Mapping [ typing . Type , typing . Iterable [ str ]]: if self . _registered_python_classes is not None : return self . _registered_python_classes registered_types = {} for name , v_type in self . value_types . items (): rel = v_type . candidate_python_types () if rel : for cls in rel : registered_types . setdefault ( cls , []) . append ( name ) self . _registered_python_classes = registered_types return self . _registered_python_classes def get_type_config_for_data_profile ( self , profile_name : str ) -> typing . Mapping [ str , typing . Any ]: type_name = TYPE_PROFILE_MAP [ profile_name ] return { \"type\" : type_name , \"type_config\" : {}} def determine_type ( self , data : typing . Any ) -> typing . Optional [ ValueType ]: if isinstance ( data , Value ): data = data . get_value_data () result : typing . List [ ValueType ] = [] registered_types = set ( self . registered_python_classes . get ( data . __class__ , [])) for cls in data . __class__ . __bases__ : reg = self . registered_python_classes . get ( cls ) if reg : registered_types . update ( reg ) if registered_types : for rt in registered_types : _cls : typing . Type [ ValueType ] = self . get_value_type_cls ( rt ) match = _cls . check_data ( data ) if match : result . append ( match ) # TODO: re-run all checks on all modules, not just the ones that registered interest in the class if len ( result ) == 0 : return None elif len ( result ) > 1 : result_str = [ x . _value_type_name for x in result ] # type: ignore raise Exception ( f \"Multiple value types found for value: { ', ' . join ( result_str ) } .\" ) else : return result [ 0 ] def get_value_type_cls ( self , type_name : str ) -> typing . Type [ ValueType ]: t = self . value_types . get ( type_name , None ) if t is None : raise Exception ( f \"No value type ' { type_name } ', available types: { ', ' . join ( self . value_types . keys ()) } \" ) return t # def get_value_type_transformations( # self, value_type_name: str # ) -> typing.Mapping[str, typing.Mapping[str, typing.Any]]: # \"\"\"Return available transform pipelines for value types.\"\"\" # # if value_type_name in self._value_type_transformations.keys(): # return self._value_type_transformations[value_type_name] # # type_cls = self.get_value_type_cls(type_name=value_type_name) # _configs = type_cls.conversions() # if _configs is None: # module_configs = {} # else: # module_configs = dict(_configs) # for base in type_cls.__bases__: # if hasattr(base, \"conversions\"): # _b_configs = base.conversions() # type: ignore # if not _b_configs: # continue # for k, v in _b_configs.items(): # if k not in module_configs.keys(): # module_configs[k] = v # # # TODO: check input type compatibility? # result: typing.Dict[str, typing.Dict[str, typing.Any]] = {} # for name, config in module_configs.items(): # config = dict(config) # module_type = config.pop(\"module_type\", None) # if not module_type: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}', no module type specified in config: {config}\" # ) # module_config = config.pop(\"module_config\", {}) # module = self._kiara.create_module( # id=f\"_transform_{value_type_name}_{name}\", # module_type=module_type, # module_config=module_config, # ) # # if \"input_name\" not in config.keys(): # # if len(module.input_schemas) == 1: # config[\"input_name\"] = next(iter(module.input_schemas.keys())) # else: # required_inputs = [ # inp # for inp, schema in module.input_schemas.items() # if schema.is_required() # ] # if len(required_inputs) == 1: # config[\"input_name\"] = required_inputs[0] # else: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}': can't determine input name between those options: '{', '.join(required_inputs)}'\" # ) # # if \"output_name\" not in config.keys(): # # if len(module.input_schemas) == 1: # config[\"output_name\"] = next(iter(module.output_schemas.keys())) # else: # required_outputs = [ # inp # for inp, schema in module.output_schemas.items() # if schema.is_required() # ] # if len(required_outputs) == 1: # config[\"output_name\"] = required_outputs[0] # else: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}': can't determine output name between those options: '{', '.join(required_outputs)}'\" # ) # # result[name] = { # \"module\": module, # \"module_type\": module_type, # \"module_config\": module_config, # \"transformation_config\": config, # } # # self._value_type_transformations[value_type_name] = result # return self._value_type_transformations[value_type_name] def find_value_types_for_package ( self , package_name : str ) -> typing . Dict [ str , typing . Type [ ValueType ]]: result = {} for value_type_name , value_type in self . value_types . items (): value_md = value_type . get_type_metadata () package = value_md . context . labels . get ( \"package\" ) if package == package_name : result [ value_type_name ] = value_type return result # def get_available_transformations_for_type( # self, value_type_name: str # ) -> typing.Iterable[str]: # # return self.get_value_type_transformations(value_type_name=value_type_name) # def transform_value( # self, # transformation_alias: str, # value: Value, # other_inputs: typing.Optional[typing.Mapping[str, typing.Any]] = None, # ) -> Value: # # transformations = self.get_value_type_transformations(value.value_schema.type) # # if transformation_alias not in transformations.keys(): # raise Exception( # f\"Can't transform value of type '{value.value_schema.type}', transformation '{transformation_alias}' not available for this type. Available: {', '.join(transformations.keys())}\" # ) # # config = transformations[transformation_alias] # # transformation_config = config[\"transformation_config\"] # input_name = transformation_config[\"input_name\"] # # module: KiaraModule = config[\"module\"] # # constants = module.get_config_value(\"constants\") # inputs = dict(constants) # # if other_inputs: # # for k, v in other_inputs.items(): # if k in constants.keys(): # raise Exception(f\"Invalid value '{k}' for 'other_inputs', this is a constant that can't be overwrittern.\") # inputs[k] = v # # defaults = transformation_config.get(\"defaults\", None) # if defaults: # for k, v in defaults.items(): # if k in constants.keys(): # raise Exception(f\"Invalid default value '{k}', this is a constant that can't be overwrittern.\") # if k not in inputs.keys(): # inputs[k] = v # # if input_name in inputs.keys(): # raise Exception( # f\"Invalid value for inputs in transform arguments, can't contain the main input key '{input_name}'.\" # ) # # inputs[input_name] = value # # result = module.run(**inputs) # output_name = transformation_config[\"output_name\"] # # result_value = result.get_value_obj(output_name) # return result_value get_type_lineage ( self , value_type ) \u00b6 Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type. Source code in kiara/data/types/type_mgmt.py def get_type_lineage ( self , value_type : str ) -> typing . Iterable [ str ]: \"\"\"Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type.\"\"\" if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx path = nx . shortest_path ( self . value_type_hierarchy , \"any\" , value_type ) return reversed ( path )","title":"type_mgmt"},{"location":"reference/kiara/data/types/type_mgmt/#kiara.data.types.type_mgmt.TypeMgmt","text":"Source code in kiara/data/types/type_mgmt.py class TypeMgmt ( object ): def __init__ ( self , kiara : \"Kiara\" ): self . _kiara : Kiara = kiara self . _value_types : typing . Optional [ bidict [ str , typing . Type [ ValueType ]]] = None self . _value_type_transformations : typing . Dict [ str , typing . Dict [ str , typing . Any ] ] = {} self . _registered_python_classes : typing . Dict [ typing . Type , typing . List [ str ]] = None # type: ignore self . _type_hierarchy : typing . Optional [ nx . DiGraph ] = None def invalidate_types ( self ): self . _value_types = None self . _value_type_transformations . clear () self . _registered_python_classes = None @property def value_types ( self ) -> bidict [ str , typing . Type [ ValueType ]]: if self . _value_types is not None : return self . _value_types self . _value_types = bidict ( find_all_value_types ()) return self . _value_types @property def value_type_hierarchy ( self ) -> \"nx.DiGraph\" : if self . _type_hierarchy is not None : return self . _type_hierarchy def recursive_base_find ( cls : typing . Type , current : typing . Optional [ typing . List [ str ]] = None ): if current is None : current = [] for base in cls . __bases__ : if base in self . value_types . values (): current . append ( self . value_types . inverse [ base ]) recursive_base_find ( base , current = current ) return current bases = {} for name , cls in self . value_types . items (): bases [ name ] = recursive_base_find ( cls ) import networkx as nx hierarchy = nx . DiGraph () for name , _bases in bases . items (): hierarchy . add_node ( name , cls = self . value_types [ name ]) if not _bases : continue # we only need the first parent, all others will be taken care of by the parent of the parent hierarchy . add_edge ( _bases [ 0 ], name ) self . _type_hierarchy = hierarchy return self . _type_hierarchy def get_type_lineage ( self , value_type : str ) -> typing . Iterable [ str ]: \"\"\"Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type.\"\"\" if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx path = nx . shortest_path ( self . value_type_hierarchy , \"any\" , value_type ) return reversed ( path ) def get_sub_types ( self , value_type : str ) -> typing . Set [ str ]: if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx desc = nx . descendants ( self . value_type_hierarchy , value_type ) return desc @property def value_type_names ( self ) -> typing . List [ str ]: return list ( self . value_types . keys ()) @property def registered_python_classes ( self , ) -> typing . Mapping [ typing . Type , typing . Iterable [ str ]]: if self . _registered_python_classes is not None : return self . _registered_python_classes registered_types = {} for name , v_type in self . value_types . items (): rel = v_type . candidate_python_types () if rel : for cls in rel : registered_types . setdefault ( cls , []) . append ( name ) self . _registered_python_classes = registered_types return self . _registered_python_classes def get_type_config_for_data_profile ( self , profile_name : str ) -> typing . Mapping [ str , typing . Any ]: type_name = TYPE_PROFILE_MAP [ profile_name ] return { \"type\" : type_name , \"type_config\" : {}} def determine_type ( self , data : typing . Any ) -> typing . Optional [ ValueType ]: if isinstance ( data , Value ): data = data . get_value_data () result : typing . List [ ValueType ] = [] registered_types = set ( self . registered_python_classes . get ( data . __class__ , [])) for cls in data . __class__ . __bases__ : reg = self . registered_python_classes . get ( cls ) if reg : registered_types . update ( reg ) if registered_types : for rt in registered_types : _cls : typing . Type [ ValueType ] = self . get_value_type_cls ( rt ) match = _cls . check_data ( data ) if match : result . append ( match ) # TODO: re-run all checks on all modules, not just the ones that registered interest in the class if len ( result ) == 0 : return None elif len ( result ) > 1 : result_str = [ x . _value_type_name for x in result ] # type: ignore raise Exception ( f \"Multiple value types found for value: { ', ' . join ( result_str ) } .\" ) else : return result [ 0 ] def get_value_type_cls ( self , type_name : str ) -> typing . Type [ ValueType ]: t = self . value_types . get ( type_name , None ) if t is None : raise Exception ( f \"No value type ' { type_name } ', available types: { ', ' . join ( self . value_types . keys ()) } \" ) return t # def get_value_type_transformations( # self, value_type_name: str # ) -> typing.Mapping[str, typing.Mapping[str, typing.Any]]: # \"\"\"Return available transform pipelines for value types.\"\"\" # # if value_type_name in self._value_type_transformations.keys(): # return self._value_type_transformations[value_type_name] # # type_cls = self.get_value_type_cls(type_name=value_type_name) # _configs = type_cls.conversions() # if _configs is None: # module_configs = {} # else: # module_configs = dict(_configs) # for base in type_cls.__bases__: # if hasattr(base, \"conversions\"): # _b_configs = base.conversions() # type: ignore # if not _b_configs: # continue # for k, v in _b_configs.items(): # if k not in module_configs.keys(): # module_configs[k] = v # # # TODO: check input type compatibility? # result: typing.Dict[str, typing.Dict[str, typing.Any]] = {} # for name, config in module_configs.items(): # config = dict(config) # module_type = config.pop(\"module_type\", None) # if not module_type: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}', no module type specified in config: {config}\" # ) # module_config = config.pop(\"module_config\", {}) # module = self._kiara.create_module( # id=f\"_transform_{value_type_name}_{name}\", # module_type=module_type, # module_config=module_config, # ) # # if \"input_name\" not in config.keys(): # # if len(module.input_schemas) == 1: # config[\"input_name\"] = next(iter(module.input_schemas.keys())) # else: # required_inputs = [ # inp # for inp, schema in module.input_schemas.items() # if schema.is_required() # ] # if len(required_inputs) == 1: # config[\"input_name\"] = required_inputs[0] # else: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}': can't determine input name between those options: '{', '.join(required_inputs)}'\" # ) # # if \"output_name\" not in config.keys(): # # if len(module.input_schemas) == 1: # config[\"output_name\"] = next(iter(module.output_schemas.keys())) # else: # required_outputs = [ # inp # for inp, schema in module.output_schemas.items() # if schema.is_required() # ] # if len(required_outputs) == 1: # config[\"output_name\"] = required_outputs[0] # else: # raise Exception( # f\"Can't create transformation '{name}' for type '{value_type_name}': can't determine output name between those options: '{', '.join(required_outputs)}'\" # ) # # result[name] = { # \"module\": module, # \"module_type\": module_type, # \"module_config\": module_config, # \"transformation_config\": config, # } # # self._value_type_transformations[value_type_name] = result # return self._value_type_transformations[value_type_name] def find_value_types_for_package ( self , package_name : str ) -> typing . Dict [ str , typing . Type [ ValueType ]]: result = {} for value_type_name , value_type in self . value_types . items (): value_md = value_type . get_type_metadata () package = value_md . context . labels . get ( \"package\" ) if package == package_name : result [ value_type_name ] = value_type return result # def get_available_transformations_for_type( # self, value_type_name: str # ) -> typing.Iterable[str]: # # return self.get_value_type_transformations(value_type_name=value_type_name) # def transform_value( # self, # transformation_alias: str, # value: Value, # other_inputs: typing.Optional[typing.Mapping[str, typing.Any]] = None, # ) -> Value: # # transformations = self.get_value_type_transformations(value.value_schema.type) # # if transformation_alias not in transformations.keys(): # raise Exception( # f\"Can't transform value of type '{value.value_schema.type}', transformation '{transformation_alias}' not available for this type. Available: {', '.join(transformations.keys())}\" # ) # # config = transformations[transformation_alias] # # transformation_config = config[\"transformation_config\"] # input_name = transformation_config[\"input_name\"] # # module: KiaraModule = config[\"module\"] # # constants = module.get_config_value(\"constants\") # inputs = dict(constants) # # if other_inputs: # # for k, v in other_inputs.items(): # if k in constants.keys(): # raise Exception(f\"Invalid value '{k}' for 'other_inputs', this is a constant that can't be overwrittern.\") # inputs[k] = v # # defaults = transformation_config.get(\"defaults\", None) # if defaults: # for k, v in defaults.items(): # if k in constants.keys(): # raise Exception(f\"Invalid default value '{k}', this is a constant that can't be overwrittern.\") # if k not in inputs.keys(): # inputs[k] = v # # if input_name in inputs.keys(): # raise Exception( # f\"Invalid value for inputs in transform arguments, can't contain the main input key '{input_name}'.\" # ) # # inputs[input_name] = value # # result = module.run(**inputs) # output_name = transformation_config[\"output_name\"] # # result_value = result.get_value_obj(output_name) # return result_value","title":"TypeMgmt"},{"location":"reference/kiara/data/types/type_mgmt/#kiara.data.types.type_mgmt.TypeMgmt.get_type_lineage","text":"Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type. Source code in kiara/data/types/type_mgmt.py def get_type_lineage ( self , value_type : str ) -> typing . Iterable [ str ]: \"\"\"Returns the shortest path between the specified type and the 'any' type, in reverse direction starting from the specified type.\"\"\" if value_type not in self . value_types . keys (): raise Exception ( f \"No value type ' { value_type } ' registered.\" ) import networkx as nx path = nx . shortest_path ( self . value_type_hierarchy , \"any\" , value_type ) return reversed ( path )","title":"get_type_lineage()"},{"location":"reference/kiara/data/values/__init__/","text":"A module that contains value-related classes for Kiara . A value in Kiara-speak is a pointer to actual data (aka 'bytes'). It contains metadata about that data (like whether it's valid/set, what type/schema it has, when it was last modified, ...), but it does not contain the data itself. The reason for that is that such data can be fairly large, and in a lot of cases it is not necessary for the code involved to have access to it, access to the metadata is enough. Each Value has a unique id, which can be used to retrieve the data (whole, or parts of it) from a DataRegistry . In addition, that id can be used to subscribe to change events for a value (published whenever the data that is associated with a value was changed). Value ( BaseModel , JupyterMixin ) pydantic-model \u00b6 The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry. Source code in kiara/data/values/__init__.py class Value ( BaseModel , JupyterMixin ): \"\"\"The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry. \"\"\" class Config : extra = Extra . forbid use_enum_values = True def __init__ ( self , registry : \"BaseDataRegistry\" , value_schema : ValueSchema , type_obj : ValueType , is_set : bool , is_none : bool , hashes : typing . Optional [ typing . Iterable [ ValueHash ]] = None , metadata : typing . Optional [ typing . Mapping [ str , typing . Dict [ str , typing . Any ]]] = None , register_token : typing . Optional [ uuid . UUID ] = None ): # type: ignore if not register_token : raise Exception ( \"No register token provided.\" ) if not registry . _check_register_token ( register_token ): raise Exception ( f \"Value registration with token ' { register_token } ' not allowed.\" ) if value_schema is None : raise NotImplementedError () assert registry self . _registry = registry self . _kiara = self . _registry . _kiara kwargs : typing . Dict [ str , typing . Any ] = {} kwargs [ \"id\" ] = NO_ID_YET_MARKER kwargs [ \"value_schema\" ] = value_schema # if value_lineage is None: # value_lineage = ValueLineage() # # kwargs[\"value_lineage\"] = value_lineage # kwargs[\"is_streaming\"] = False # not used yet kwargs [ \"creation_date\" ] = datetime . now () kwargs [ \"is_set\" ] = is_set kwargs [ \"is_none\" ] = is_none if hashes : kwargs [ \"hashes\" ] = list ( hashes ) if metadata : kwargs [ \"metadata\" ] = dict ( metadata ) else : kwargs [ \"metadata\" ] = {} super () . __init__ ( ** kwargs ) self . _type_obj = type_obj _kiara : \"Kiara\" = PrivateAttr () _registry : \"BaseDataRegistry\" = PrivateAttr () _type_obj : ValueType = PrivateAttr () _value_info : \"ValueInfo\" = PrivateAttr ( default = None ) id : str = Field ( description = \"A unique id for this value.\" ) value_schema : ValueSchema = Field ( description = \"The schema of this value.\" ) creation_date : typing . Optional [ datetime ] = Field ( description = \"The time this value was created value happened.\" ) # is_streaming: bool = Field( # default=False, # description=\"Whether the value is currently streamed into this object.\", # ) is_set : bool = Field ( description = \"Whether the value was set (in some way: user input, default, constant...).\" , default = False , ) is_none : bool = Field ( description = \"Whether the value is 'None'.\" , default = True ) hashes : typing . List [ ValueHash ] = Field ( description = \"Available hashes relating to the actual value data. This attribute is not populated by default, use the 'get_hashes()' method to request one or several hash items, afterwards those hashes will be reflected in this attribute.\" , default_factory = list , ) metadata : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = Field ( description = \"Available metadata relating to the actual value data (size, no. of rows, etc. -- depending on data type). This attribute is not populated by default, use the 'get_metadata()' method to request one or several metadata items, afterwards those metadata items will be reflected in this attribute.\" , default_factory = dict , ) @property def type_name ( self ) -> str : return self . value_schema . type @property def type_obj ( self ): \"\"\"Return the object that contains all the type information for this value.\"\"\" return self . _type_obj def get_hash ( self , hash_type : str ) -> ValueHash : \"\"\"Calculate the hash of a specified type for this value. Hashes are cached.\"\"\" hashes = self . get_hashes ( hash_type ) return list ( hashes )[ 0 ] def get_hashes ( self , * hash_types : str ) -> typing . Iterable [ ValueHash ]: all_hash_types = self . type_obj . get_supported_hash_types () if not hash_types : try : hash_types = all_hash_types except Exception as e : log_message ( str ( e )) if not hash_types : return [] result = [] missing = list ( hash_types ) for hash_obj in self . hashes : if hash_obj . hash_type in hash_types : result . append ( hash_obj ) missing . remove ( hash_obj . hash_type ) for hash_type in missing : if hash_type not in all_hash_types : raise Exception ( f \"Hash type ' { hash_type } ' not supported for ' { self . type_name } '\" ) hash_str = self . type_obj . calculate_value_hash ( value = self . get_value_data (), hash_type = hash_type ) hash_obj = ValueHash ( hash_type = hash_type , hash = hash_str ) self . hashes . append ( hash_obj ) result . append ( hash_obj ) return result def item_is_valid ( self ) -> bool : \"\"\"Check whether the current value is valid\"\"\" if self . value_schema . optional : return True else : return self . is_set and not self . is_none def item_status ( self ) -> str : \"\"\"Print a human readable short description of this values status.\"\"\" if self . value_schema . optional : if self . is_set : if self . is_none : return \"no value (not required)\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set (not required)\" else : if self . is_set : if self . is_none : return \"no value\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set\" def get_value_data ( self ) -> typing . Any : return self . _registry . get_value_data ( self ) def get_lineage ( self ) -> typing . Optional [ ValueLineage ]: return self . _registry . get_lineage ( self ) def set_value_lineage ( self , value_lineage : ValueLineage ) -> None : if hasattr ( self . _registry , \"set_value_lineage\" ): return self . _registry . set_value_lineage ( self , value_lineage ) # type: ignore else : raise Exception ( \"Can't set value lineage: registry is read only\" ) def get_info ( self ) -> \"ValueInfo\" : if self . _value_info is None : self . _value_info = ValueInfo . from_value ( self ) return self . _value_info def create_info ( self , include_deserialization_config : bool = False ) -> \"ValueInfo\" : return ValueInfo . from_value ( self , include_deserialization_config = include_deserialization_config ) def get_metadata ( self , * metadata_keys : str , also_return_schema : bool = False ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: \"\"\"Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to `True`, the value will be a two-key map with the metadata under the ``metadata_item`` subkey, and the value schema under ``metadata_schema``. If no metadata key is specified, all available metadata for this value type will be returned. Arguments: metadata_keys: the metadata keys to retrieve metadata (empty for all available metadata) also_return_schema: whether to also return the schema for each metadata item \"\"\" if not metadata_keys : _metadata_keys = set ( self . _kiara . metadata_mgmt . get_metadata_keys_for_type ( self . type_name ) ) for key in self . metadata . keys (): _metadata_keys . add ( key ) else : _metadata_keys = set ( metadata_keys ) result = {} missing = set () for metadata_key in sorted ( _metadata_keys ): if metadata_key in self . metadata . keys (): if also_return_schema : result [ metadata_key ] = self . metadata [ metadata_key ] else : result [ metadata_key ] = self . metadata [ metadata_key ][ \"metadata_item\" ] else : missing . add ( metadata_key ) if not missing : return result _md = self . _kiara . metadata_mgmt . get_value_metadata ( self , * missing , also_return_schema = True ) for k , v in _md . items (): self . metadata [ k ] = v if also_return_schema : result [ k ] = v else : result [ k ] = v [ \"metadata_item\" ] return { k : result [ k ] for k in sorted ( result . keys ())} def save ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None , register_missing_aliases : bool = True , ) -> \"Value\" : \"\"\"Save this value, under the specified alias(es).\"\"\" # if self.get_value_lineage(): # for field_name, value in self.get_value_lineage().inputs.items(): # value_obj = self._registry.get_value_obj(value) # try: # value_obj.save() # except Exception as e: # print(e) value = self . _kiara . data_store . register_data ( self ) if aliases : self . _kiara . data_store . link_aliases ( value , * aliases , register_missing_aliases = register_missing_aliases ) return value def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , Value ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id ) def __repr__ ( self ): return f \"Value(id= { self . id } , type= { self . type_name } , is_set= { self . is_set } , is_none= { self . is_none } \" def __str__ ( self ): return self . __repr__ () def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : # table = self._create_value_table() title = \"Value\" yield Panel ( self . get_info (), box = box . ROUNDED , title_align = \"left\" , title = title ) creation_date : datetime pydantic-field \u00b6 The time this value was created value happened. hashes : List [ kiara . data . values . ValueHash ] pydantic-field \u00b6 Available hashes relating to the actual value data. This attribute is not populated by default, use the 'get_hashes()' method to request one or several hash items, afterwards those hashes will be reflected in this attribute. id : str pydantic-field required \u00b6 A unique id for this value. is_none : bool pydantic-field \u00b6 Whether the value is 'None'. is_set : bool pydantic-field \u00b6 Whether the value was set (in some way: user input, default, constant...). metadata : Dict [ str , Dict [ str , Any ]] pydantic-field \u00b6 Available metadata relating to the actual value data (size, no. of rows, etc. -- depending on data type). This attribute is not populated by default, use the 'get_metadata()' method to request one or several metadata items, afterwards those metadata items will be reflected in this attribute. type_obj property readonly \u00b6 Return the object that contains all the type information for this value. value_schema : ValueSchema pydantic-field required \u00b6 The schema of this value. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , Value ): return False return self . id == other . id __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash ( self . id ) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/data/values/__init__.py def __repr__ ( self ): return f \"Value(id= { self . id } , type= { self . type_name } , is_set= { self . is_set } , is_none= { self . is_none } \" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/data/values/__init__.py def __str__ ( self ): return self . __repr__ () get_hash ( self , hash_type ) \u00b6 Calculate the hash of a specified type for this value. Hashes are cached. Source code in kiara/data/values/__init__.py def get_hash ( self , hash_type : str ) -> ValueHash : \"\"\"Calculate the hash of a specified type for this value. Hashes are cached.\"\"\" hashes = self . get_hashes ( hash_type ) return list ( hashes )[ 0 ] get_metadata ( self , * metadata_keys , * , also_return_schema = False ) \u00b6 Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to True , the value will be a two-key map with the metadata under the metadata_item subkey, and the value schema under metadata_schema . If no metadata key is specified, all available metadata for this value type will be returned. Parameters: Name Type Description Default metadata_keys str the metadata keys to retrieve metadata (empty for all available metadata) () also_return_schema bool whether to also return the schema for each metadata item False Source code in kiara/data/values/__init__.py def get_metadata ( self , * metadata_keys : str , also_return_schema : bool = False ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: \"\"\"Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to `True`, the value will be a two-key map with the metadata under the ``metadata_item`` subkey, and the value schema under ``metadata_schema``. If no metadata key is specified, all available metadata for this value type will be returned. Arguments: metadata_keys: the metadata keys to retrieve metadata (empty for all available metadata) also_return_schema: whether to also return the schema for each metadata item \"\"\" if not metadata_keys : _metadata_keys = set ( self . _kiara . metadata_mgmt . get_metadata_keys_for_type ( self . type_name ) ) for key in self . metadata . keys (): _metadata_keys . add ( key ) else : _metadata_keys = set ( metadata_keys ) result = {} missing = set () for metadata_key in sorted ( _metadata_keys ): if metadata_key in self . metadata . keys (): if also_return_schema : result [ metadata_key ] = self . metadata [ metadata_key ] else : result [ metadata_key ] = self . metadata [ metadata_key ][ \"metadata_item\" ] else : missing . add ( metadata_key ) if not missing : return result _md = self . _kiara . metadata_mgmt . get_value_metadata ( self , * missing , also_return_schema = True ) for k , v in _md . items (): self . metadata [ k ] = v if also_return_schema : result [ k ] = v else : result [ k ] = v [ \"metadata_item\" ] return { k : result [ k ] for k in sorted ( result . keys ())} item_is_valid ( self ) \u00b6 Check whether the current value is valid Source code in kiara/data/values/__init__.py def item_is_valid ( self ) -> bool : \"\"\"Check whether the current value is valid\"\"\" if self . value_schema . optional : return True else : return self . is_set and not self . is_none item_status ( self ) \u00b6 Print a human readable short description of this values status. Source code in kiara/data/values/__init__.py def item_status ( self ) -> str : \"\"\"Print a human readable short description of this values status.\"\"\" if self . value_schema . optional : if self . is_set : if self . is_none : return \"no value (not required)\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set (not required)\" else : if self . is_set : if self . is_none : return \"no value\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set\" save ( self , aliases = None , register_missing_aliases = True ) \u00b6 Save this value, under the specified alias(es). Source code in kiara/data/values/__init__.py def save ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None , register_missing_aliases : bool = True , ) -> \"Value\" : \"\"\"Save this value, under the specified alias(es).\"\"\" # if self.get_value_lineage(): # for field_name, value in self.get_value_lineage().inputs.items(): # value_obj = self._registry.get_value_obj(value) # try: # value_obj.save() # except Exception as e: # print(e) value = self . _kiara . data_store . register_data ( self ) if aliases : self . _kiara . data_store . link_aliases ( value , * aliases , register_missing_aliases = register_missing_aliases ) return value ValueAlias ( BaseModel ) pydantic-model \u00b6 Source code in kiara/data/values/__init__.py class ValueAlias ( BaseModel ): @classmethod def from_string ( self , value_alias : str , default_repo_name : typing . Optional [ str ] = None ) -> \"ValueAlias\" : if not isinstance ( value_alias , str ): raise Exception ( \"Invalid id_or_alias: not a string.\" ) if not value_alias : raise Exception ( \"Invalid id_or_alias: can't be empty string.\" ) _repo_name : typing . Optional [ str ] = default_repo_name _version : typing . Optional [ int ] = None _tag : typing . Optional [ str ] = None if \"#\" in value_alias : _repo_name , _value_alias = value_alias . split ( \"#\" , maxsplit = 1 ) else : _value_alias = value_alias if \"@\" in _value_alias : _alias , _postfix = _value_alias . split ( \"@\" , maxsplit = 1 ) try : _version = int ( _postfix ) except ValueError : if not _postfix . isidentifier (): raise Exception ( f \"Invalid format for version/tag element of id_or_alias: { _tag } \" ) _tag = _postfix else : _alias = _value_alias return ValueAlias ( repo_name = _repo_name , alias = _alias , version = _version , tag = _tag ) @classmethod def from_strings ( cls , * value_aliases : typing . Union [ str , \"ValueAlias\" ] ) -> typing . List [ \"ValueAlias\" ]: result = [] for va in value_aliases : if isinstance ( va , str ): result . append ( ValueAlias . from_string ( va )) elif isinstance ( va , ValueAlias ): result . append ( va ) else : raise TypeError ( f \"Invalid type ' { type ( va ) } ' for type alias, expected 'str' or 'ValueAlias'.\" ) return result repo_name : typing . Optional [ str ] = Field ( description = \"The name of the data repo the value lives in.\" , default = None ) alias : str = Field ( \"The alias name.\" ) version : typing . Optional [ int ] = Field ( description = \"The version of this alias.\" , default = None ) tag : typing . Optional [ str ] = Field ( description = \"The tag for the alias.\" , default = None ) @property def full_alias ( self ): if self . tag is not None : return f \" { self . alias } @ { self . tag } \" elif self . version is not None : return f \" { self . alias } @ { self . version } \" else : return self . alias repo_name : str pydantic-field \u00b6 The name of the data repo the value lives in. tag : str pydantic-field \u00b6 The tag for the alias. version : int pydantic-field \u00b6 The version of this alias. ValueHash ( BaseModel ) pydantic-model \u00b6 Source code in kiara/data/values/__init__.py class ValueHash ( BaseModel ): hash : str = Field ( description = \"The value hash.\" ) hash_type : str = Field ( description = \"The value hash method.\" ) hash : str pydantic-field required \u00b6 The value hash. hash_type : str pydantic-field required \u00b6 The value hash method. ValueInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/data/values/__init__.py class ValueInfo ( KiaraInfoModel ): @classmethod def from_value ( cls , value : Value , include_deserialization_config : bool = False ): if value . id not in value . _registry . value_ids : raise Exception ( \"Value not registered (yet).\" ) # aliases = value._registry.find_aliases_for_value(value) hashes = value . get_hashes () metadata = value . get_metadata ( also_return_schema = True ) # metadata = value.metadata value_lineage = value . get_lineage () if include_deserialization_config : # serialize_operation: SerializeValueOperationType = ( # type: ignore # value._kiara.operation_mgmt.get_operation(\"serialize\") # type: ignore # ) raise NotImplementedError () return ValueInfo ( value_id = value . id , value_schema = value . value_schema , hashes = hashes , metadata = metadata , lineage = value_lineage , is_valid = value . item_is_valid (), ) value_id : str = Field ( description = \"The value id.\" ) value_schema : ValueSchema = Field ( description = \"The value schema.\" ) # aliases: typing.List[ValueAlias] = Field( # description=\"All aliases for this value.\", default_factory=list # ) # tags: typing.List[str] = Field( # description=\"All tags for this value.\", default_factory=list # ) # created: str = Field(description=\"The time the data was created.\") is_valid : bool = Field ( description = \"Whether the item is valid (in the context of its schema).\" ) hashes : typing . List [ ValueHash ] = Field ( description = \"All available hashes for this value.\" , default_factory = list ) metadata : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = Field ( description = \"The metadata associated with this value.\" ) lineage : typing . Optional [ ValueLineage ] = Field ( description = \"Information about how the value was created.\" , default = None ) deserialize_config : typing . Optional [ DeserializeConfig ] = Field ( description = \"The module config (incl. inputs) to deserialize the value.\" , default = None , ) def get_metadata_items ( self , * keys : str ) -> typing . Dict [ str , typing . Any ]: if not keys : _keys : typing . Iterable [ str ] = self . metadata . keys () else : _keys = keys result = {} for k in _keys : md = self . metadata . get ( k ) if md is None : raise Exception ( f \"No metadata for key ' { k } ' available.\" ) result [ k ] = md [ \"metadata_item\" ] return result def get_metadata_schemas ( self , * keys : str ) -> typing . Dict [ str , typing . Any ]: if not keys : _keys : typing . Iterable [ str ] = self . metadata . keys () else : _keys = keys result = {} for k in _keys : md = self . metadata . get ( k ) if md is None : raise Exception ( f \"No metadata for key ' { k } ' available.\" ) result [ k ] = md [ \"metadata_schema\" ] return result def create_renderable ( self , ** config : typing . Any ) -> RenderableType : padding = config . get ( \"padding\" , ( 0 , 1 )) skip_metadata = config . get ( \"skip_metadata\" , False ) skip_value_lineage = config . get ( \"skip_lineage\" , True ) include_ids = config . get ( \"include_ids\" , False ) table = Table ( box = box . SIMPLE , show_header = False , padding = padding ) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) table . add_row ( \"id\" , self . value_id ) # type: ignore table . add_row ( \"type\" , self . value_schema . type ) if self . value_schema . type_config : json_data = json . dumps ( self . value_schema . type_config ) tc_content = Syntax ( json_data , \"json\" ) table . add_row ( \"type config\" , tc_content ) table . add_row ( \"desc\" , self . value_schema . doc ) table . add_row ( \"is set\" , \"yes\" if self . is_valid else \"no\" ) # table.add_row(\"is constant\", \"yes\" if self.is_constant else \"no\") # if isinstance(self.value_hash, int): # vh = str(self.value_hash) # else: # vh = self.value_hash.value # table.add_row(\"hash\", vh) if self . hashes : hashes_dict = { hs . hash_type : hs . hash for hs in self . hashes } yaml_string = yaml . dump ( hashes_dict ) hases_str = Syntax ( yaml_string , \"yaml\" , background_color = \"default\" ) table . add_row ( \"\" , \"\" ) table . add_row ( \"hashes\" , hases_str ) if not skip_metadata : if self . metadata : yaml_string = yaml . dump ( data = self . get_metadata_items ()) # json_string = json.dumps(self.get_metadata_items(), indent=2) metadata = Syntax ( yaml_string , \"yaml\" , background_color = \"default\" ) table . add_row ( \"metadata\" , metadata ) else : table . add_row ( \"metadata\" , \"-- no metadata --\" ) if not skip_value_lineage and self . lineage : if self . metadata : table . add_row ( \"\" , \"\" ) # json_string = self.lineage.json(indent=2) # seed_content = Syntax(json_string, \"json\") table . add_row ( \"lineage\" , self . lineage . create_renderable ( include_ids = include_ids ) ) return table deserialize_config : DeserializeConfig pydantic-field \u00b6 The module config (incl. inputs) to deserialize the value. hashes : List [ kiara . data . values . ValueHash ] pydantic-field \u00b6 All available hashes for this value. is_valid : bool pydantic-field required \u00b6 Whether the item is valid (in the context of its schema). lineage : ValueLineage pydantic-field \u00b6 Information about how the value was created. metadata : Dict [ str , Dict [ str , Any ]] pydantic-field required \u00b6 The metadata associated with this value. value_id : str pydantic-field required \u00b6 The value id. value_schema : ValueSchema pydantic-field required \u00b6 The value schema. ValueLineage ( ModuleConfig ) pydantic-model \u00b6 Model containing the lineage of a value. Source code in kiara/data/values/__init__.py class ValueLineage ( ModuleConfig ): \"\"\"Model containing the lineage of a value.\"\"\" @classmethod def from_module_and_inputs ( cls , module : \"KiaraModule\" , output_name : str , inputs : typing . Mapping [ str , typing . Union [ \"Value\" , \"ValueInfo\" ]], ): module_type = module . _module_type_id # type: ignore module_config = module . config . dict () doc = module . get_type_metadata () . documentation _inputs = {} for field_name , value in inputs . items (): if isinstance ( value , Value ): _inputs [ field_name ] = value . get_info () else : _inputs [ field_name ] = value return ValueLineage . construct ( module_type = module_type , module_config = module_config , doc = doc , output_name = output_name , inputs = _inputs , ) @classmethod def create ( cls , module_type : str , module_config : typing . Mapping [ str , typing . Any ], module_doc : DocumentationMetadataModel , inputs : typing . Mapping [ str , typing . Union [ \"Value\" , \"ValueInfo\" ]], output_name : typing . Optional [ str ] = None , ): _inputs = {} for field_name , value in inputs . items (): if isinstance ( value , Value ): _inputs [ field_name ] = value . get_info () else : _inputs [ field_name ] = value return ValueLineage . construct ( module_type = module_type , module_config = dict ( module_config ), doc = module_doc , output_name = output_name , inputs = _inputs , ) output_name : typing . Optional [ str ] = Field ( description = \"The result field name for the value this refers to.\" ) inputs : typing . Dict [ str , \"ValueInfo\" ] = Field ( description = \"The inputs that were used to create the value this refers to.\" ) value_index : typing . Optional [ typing . Dict [ str , \"ValueInfo\" ]] = Field ( description = \"Index of all values that are associated with this value lineage.\" , default = None , ) def to_minimal_dict ( self , include_metadata : bool = False , include_module_doc : bool = False , include_module_config : bool = True , ) -> typing . Dict [ str , typing . Any ]: full_dict = self . dict ( exclude_none = True ) minimal_dict = filter_metadata_schema ( full_dict , include_metadata = include_metadata , include_module_doc = include_module_doc , include_module_config = include_module_config , ) return minimal_dict def create_renderable ( self , ** config : typing . Any ) -> RenderableType : all_ids = sorted ( find_all_ids_in_lineage ( self )) id_color_map = {} for idx , v_id in enumerate ( all_ids ): id_color_map [ v_id ] = COLOR_LIST [ idx % len ( COLOR_LIST )] show_ids = config . get ( \"include_ids\" , False ) tree = fill_lineage_tree ( self , include_ids = show_ids ) return tree def create_graph ( self ) -> nx . DiGraph : return create_lineage_graph ( self ) inputs : Dict [ str , ValueInfo ] pydantic-field required \u00b6 The inputs that were used to create the value this refers to. output_name : str pydantic-field \u00b6 The result field name for the value this refers to. value_index : Dict [ str , ValueInfo ] pydantic-field \u00b6 Index of all values that are associated with this value lineage. create_renderable ( self , ** config ) \u00b6 Create a renderable for this module configuration. Source code in kiara/data/values/__init__.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : all_ids = sorted ( find_all_ids_in_lineage ( self )) id_color_map = {} for idx , v_id in enumerate ( all_ids ): id_color_map [ v_id ] = COLOR_LIST [ idx % len ( COLOR_LIST )] show_ids = config . get ( \"include_ids\" , False ) tree = fill_lineage_tree ( self , include_ids = show_ids ) return tree ValueSchema ( BaseModel ) pydantic-model \u00b6 The schema of a value. The schema contains the ValueType of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the type_config field. Source code in kiara/data/values/__init__.py class ValueSchema ( BaseModel ): \"\"\"The schema of a value. The schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field. \"\"\" class Config : use_enum_values = True # extra = Extra.forbid type : str = Field ( description = \"The type of the value.\" ) type_config : typing . Dict [ str , typing . Any ] = Field ( description = \"Configuration for the type, in case it's complex.\" , default_factory = dict , ) default : typing . Any = Field ( description = \"A default value.\" , default = SpecialValue . NOT_SET ) optional : bool = Field ( description = \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , default = False , ) is_constant : bool = Field ( description = \"Whether the value is a constant.\" , default = False ) # required: typing.Any = Field( # description=\"Whether this value is required to be set.\", default=True # ) doc : str = Field ( default = \"-- n/a --\" , description = \"A description for the value of this input field.\" , ) def is_required ( self ): if self . optional : return False else : if self . default in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ]: return True else : return False def validate_types ( self , kiara : \"Kiara\" ): if self . type not in kiara . value_type_names : raise ValueError ( f \"Invalid value type ' { self . type } ', available types: { kiara . value_type_names } \" ) @property def desc ( self ): \"\"\"The first line of the 'doc' value.\"\"\" return self . doc . split ( \" \\n \" )[ 0 ] def __eq__ ( self , other ): if not isinstance ( other , ValueSchema ): return False return ( self . type , self . default ) == ( other . type , other . default ) def __hash__ ( self ): return hash (( self . type , self . default )) default : Any pydantic-field \u00b6 A default value. desc property readonly \u00b6 The first line of the 'doc' value. doc : str pydantic-field \u00b6 A description for the value of this input field. is_constant : bool pydantic-field \u00b6 Whether the value is a constant. optional : bool pydantic-field \u00b6 Whether this value is required (True), or whether 'None' value is allowed (False). type : str pydantic-field required \u00b6 The type of the value. type_config : Dict [ str , Any ] pydantic-field \u00b6 Configuration for the type, in case it's complex. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): if not isinstance ( other , ValueSchema ): return False return ( self . type , self . default ) == ( other . type , other . default ) __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash (( self . type , self . default )) ValueSlot ( BaseModel ) pydantic-model \u00b6 Source code in kiara/data/values/__init__.py class ValueSlot ( BaseModel ): @classmethod def from_value ( cls , id : str , value : Value ) -> \"ValueSlot\" : vs = ValueSlot . from_value_schema ( id = id , value_schema = value . value_schema , kiara = value . _kiara ) vs . add_value ( value ) return vs @classmethod def from_value_schema ( cls , id : str , value_schema : ValueSchema , kiara : \"Kiara\" ) -> \"ValueSlot\" : vs = ValueSlot ( id = id , value_schema = value_schema , kiara = kiara ) return vs def __init__ ( self , ** data ): # type: ignore _kiara = data . pop ( \"kiara\" , None ) if _kiara is None : raise Exception ( \"No kiara context provided.\" ) _registry = data . pop ( \"registry\" , None ) if _registry is None : _registry = _kiara . data_registry self . _kiara = _kiara self . _registry = _registry super () . __init__ ( ** data ) _kiara : \"Kiara\" = PrivateAttr () _registry : \"DataRegistry\" = PrivateAttr () _callbacks : typing . Dict [ str , \"ValueSlotUpdateHandler\" ] = PrivateAttr ( default_factory = dict ) id : str = Field ( description = \"The id for this slot.\" ) value_schema : ValueSchema = Field ( description = \"The schema for the values of this slot.\" ) values : typing . Dict [ int , Value ] = Field ( description = \"The values of this slot, with versions as key.\" , default_factory = dict , ) tags : typing . Dict [ str , int ] = Field ( description = \"The tags for this value slot (tag name as key, linked version as value.\" , default_factory = dict , ) @property def latest_version_nr ( self ) -> int : if not self . values : return 0 return max ( self . values . keys ()) def get_latest_value ( self ) -> Value : lv = self . latest_version_nr if lv == 0 : raise Exception ( \"No value added to value slot yet.\" ) return self . values [ self . latest_version_nr ] def register_callbacks ( self , * callbacks : \"ValueSlotUpdateHandler\" ): for cb in callbacks : cb_id : typing . Optional [ str ] = None if cb_id in self . _callbacks . keys (): raise Exception ( f \"Callback with id ' { cb_id } ' already registered.\" ) if hasattr ( cb , \"id\" ): if callable ( cb . id ): # type: ignore cb_id = cb . id () # type: ignore else : cb_id = cb . id # type: ignore elif hasattr ( cb , \"get_id\" ): cb_id = cb . get_id () # type: ignore if cb_id is None : cb_id = str ( uuid . uuid4 ()) assert isinstance ( cb_id , str ) self . _callbacks [ cb_id ] = cb def add_value ( self , value : Value , trigger_callbacks : bool = True , tags : typing . Optional [ typing . Iterable [ str ]] = None , ) -> int : \"\"\"Add a value to this slot.\"\"\" if self . latest_version_nr != 0 and value . id == self . get_latest_value () . id : return self . latest_version_nr # TODO: check value data equality if self . value_schema . is_constant and self . values : if is_debug (): import traceback traceback . print_stack () raise Exception ( \"Can't add value: value slot marked as 'constant'.\" ) version = self . latest_version_nr + 1 assert version not in self . values . keys () self . values [ version ] = value if tags : for tag in tags : self . tags [ tag ] = version if trigger_callbacks : for cb in self . _callbacks . values (): cb . values_updated ( self ) return version def is_latest_value ( self , value : Value ): return value . id == self . get_latest_value () . id def find_linked_aliases ( self , value_item : typing . Union [ Value , str ] ) -> typing . List [ \"ValueAlias\" ]: if isinstance ( value_item , Value ): value_item = value_item . id result = [] for _version , _value in self . values . items (): if _value . id == value_item : va = ValueAlias ( alias = self . id , version = _version ) result . append ( va ) if _version in self . tags . values (): for _tag , _tag_version in self . tags . items (): if _tag_version == _version : va = ValueAlias ( alias = self . id , tag = _tag ) result . append ( va ) return result def __eq__ ( self , other ): if not isinstance ( other , ValueSlot ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id ) id : str pydantic-field required \u00b6 The id for this slot. tags : Dict [ str , int ] pydantic-field \u00b6 The tags for this value slot (tag name as key, linked version as value. value_schema : ValueSchema pydantic-field required \u00b6 The schema for the values of this slot. values : Dict [ int , kiara . data . values . Value ] pydantic-field \u00b6 The values of this slot, with versions as key. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): if not isinstance ( other , ValueSlot ): return False return self . id == other . id __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash ( self . id ) add_value ( self , value , trigger_callbacks = True , tags = None ) \u00b6 Add a value to this slot. Source code in kiara/data/values/__init__.py def add_value ( self , value : Value , trigger_callbacks : bool = True , tags : typing . Optional [ typing . Iterable [ str ]] = None , ) -> int : \"\"\"Add a value to this slot.\"\"\" if self . latest_version_nr != 0 and value . id == self . get_latest_value () . id : return self . latest_version_nr # TODO: check value data equality if self . value_schema . is_constant and self . values : if is_debug (): import traceback traceback . print_stack () raise Exception ( \"Can't add value: value slot marked as 'constant'.\" ) version = self . latest_version_nr + 1 assert version not in self . values . keys () self . values [ version ] = value if tags : for tag in tags : self . tags [ tag ] = version if trigger_callbacks : for cb in self . _callbacks . values (): cb . values_updated ( self ) return version value_set \u00b6 SlottedValueSet ( ValueSet ) \u00b6 Source code in kiara/data/values/value_set.py class SlottedValueSet ( ValueSet ): @classmethod def from_schemas ( cls , schemas : typing . Mapping [ str , \"ValueSchema\" ], read_only : bool = True , check_for_sameness = True , initial_values : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , title : typing . Optional [ str ] = None , default_value : typing . Any = SpecialValue . NO_VALUE , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ) -> \"SlottedValueSet\" : if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if registry is None : registry = kiara . data_registry values = {} for field_name , schema in schemas . items (): def_val = default_value if callable ( default_value ): def_val = default_value () _init_value = def_val if initial_values and initial_values . get ( field_name , None ) is not None : _init_value = initial_values [ field_name ] if not isinstance ( _init_value , Value ): value : Value = registry . register_data ( value_data = _init_value , value_schema = schema ) # value: Value = Value(value_schema=schema, value_data=_init_value, registry=registry) # type: ignore else : value = _init_value values [ field_name ] = value return cls ( items = values , title = title , read_only = read_only , check_for_sameness = check_for_sameness , kiara = kiara , registry = registry , ) def __init__ ( self , items : typing . Mapping [ str , typing . Union [ \"ValueSlot\" , \"Value\" ]], read_only : bool , check_for_sameness : bool = False , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ): \"\"\"A `ValueSet` implementation that keeps a history of each fields value. Arguments: items: the value slots read_only: whether it is allowed to set new values to fields in this set check_for_sameness: whether a check should be performed that checks for equality of the new and old values, if equal, skip the update title: An optional title for this value set kiara: the kiara context registry: the registry to use to register the values to \"\"\" if not items : raise ValueError ( \"Can't create ValueSet: no values provided\" ) self . _check_for_sameness : bool = check_for_sameness super () . __init__ ( read_only = read_only , title = title , kiara = kiara ) if registry is None : registry = self . _kiara . data_registry self . _registry : DataRegistry = registry _value_slots : typing . Dict [ str , ValueSlot ] = {} for item , value in items . items (): if value is None : raise Exception ( f \"Can't create value set, item ' { item } ' does not have a value (yet).\" ) if item . startswith ( \"_\" ): raise ValueError ( f \"Value name can't start with '_': { item } \" ) if item in INVALID_VALUE_NAMES : raise ValueError ( f \"Invalid value name ' { item } '.\" ) if isinstance ( value , Value ): slot = self . _registry . register_alias ( value ) elif isinstance ( value , ValueSlot ): slot = value else : raise TypeError ( f \"Invalid type: ' { type ( value ) } '\" ) _value_slots [ item ] = slot self . _value_slots : typing . Dict [ str , ValueSlot ] = _value_slots def get_all_field_names ( self ) -> typing . Iterable [ str ]: return self . _value_slots . keys () def _get_value_obj ( self , field_name : str ) -> \"Value\" : slot = self . _value_slots [ field_name ] return slot . get_latest_value () def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: # we want to set all registry-type values seperately, in one go, because it's more efficient registries : typing . Dict [ DataRegistry , typing . Dict [ ValueSlot , typing . Union [ \"Value\" , typing . Any ]] ] = {} field_slot_map : typing . Dict [ ValueSlot , str ] = {} result : typing . Dict [ str , typing . Union [ bool , Exception ]] = {} for field_name , value_or_data in values . items (): value_slot : ValueSlot = self . _value_slots [ field_name ] if not metadata and self . _check_for_sameness : latest_val = value_slot . get_latest_value () if isinstance ( value_or_data , Value ): if latest_val . id == value_or_data . id : result [ field_name ] = False continue else : if ( latest_val . is_set and latest_val . get_value_data () == value_or_data ): result [ field_name ] = False continue registries . setdefault ( value_slot . _registry , {})[ value_slot ] = value_or_data field_slot_map [ value_slot ] = field_name for registry , value_slots_details in registries . items (): _r = registry . update_value_slots ( value_slots_details , metadata = metadata , lineage = lineage ) # type: ignore for value_slot , details in _r . items (): result [ field_slot_map [ value_slot ]] = details return result __init__ ( self , items , read_only , check_for_sameness = False , title = None , kiara = None , registry = None ) special \u00b6 A ValueSet implementation that keeps a history of each fields value. Parameters: Name Type Description Default items Mapping[str, Union[ValueSlot, Value]] the value slots required read_only bool whether it is allowed to set new values to fields in this set required check_for_sameness bool whether a check should be performed that checks for equality of the new and old values, if equal, skip the update False title Optional[str] An optional title for this value set None kiara Optional[Kiara] the kiara context None registry Optional[kiara.data.registry.DataRegistry] the registry to use to register the values to None Source code in kiara/data/values/value_set.py def __init__ ( self , items : typing . Mapping [ str , typing . Union [ \"ValueSlot\" , \"Value\" ]], read_only : bool , check_for_sameness : bool = False , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ): \"\"\"A `ValueSet` implementation that keeps a history of each fields value. Arguments: items: the value slots read_only: whether it is allowed to set new values to fields in this set check_for_sameness: whether a check should be performed that checks for equality of the new and old values, if equal, skip the update title: An optional title for this value set kiara: the kiara context registry: the registry to use to register the values to \"\"\" if not items : raise ValueError ( \"Can't create ValueSet: no values provided\" ) self . _check_for_sameness : bool = check_for_sameness super () . __init__ ( read_only = read_only , title = title , kiara = kiara ) if registry is None : registry = self . _kiara . data_registry self . _registry : DataRegistry = registry _value_slots : typing . Dict [ str , ValueSlot ] = {} for item , value in items . items (): if value is None : raise Exception ( f \"Can't create value set, item ' { item } ' does not have a value (yet).\" ) if item . startswith ( \"_\" ): raise ValueError ( f \"Value name can't start with '_': { item } \" ) if item in INVALID_VALUE_NAMES : raise ValueError ( f \"Invalid value name ' { item } '.\" ) if isinstance ( value , Value ): slot = self . _registry . register_alias ( value ) elif isinstance ( value , ValueSlot ): slot = value else : raise TypeError ( f \"Invalid type: ' { type ( value ) } '\" ) _value_slots [ item ] = slot self . _value_slots : typing . Dict [ str , ValueSlot ] = _value_slots ValueSet ( MutableMapping , Generic ) \u00b6 Source code in kiara/data/values/value_set.py class ValueSet ( typing . MutableMapping [ str , \"Value\" ]): def __init__ ( self , read_only : bool , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): # from kiara.data.values import ValueSchema # schema = ValueSchema(type=\"any\", default=None, doc=\"-- n/a --\") # self._schema = schema if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : \"Kiara\" = kiara self . _read_only : bool = read_only if title is None : title = \"-- n/a --\" self . _title = title @abc . abstractmethod def get_all_field_names ( self ) -> typing . Iterable [ str ]: pass @abc . abstractmethod def _get_value_obj ( self , field_name : str ): pass @abc . abstractmethod def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: pass def get_value_obj ( self , field_name : str , ensure_metadata : typing . Union [ bool , typing . Iterable [ str ], str ] = False , ) -> Value : if field_name not in list ( self . get_all_field_names ()): raise KeyError ( f \"Field ' { field_name } ' not available in value set. Available fields: { ', ' . join ( self . get_all_field_names ()) } \" ) obj : Value = self . _get_value_obj ( field_name ) if ensure_metadata : if isinstance ( ensure_metadata , bool ): obj . get_metadata () elif isinstance ( ensure_metadata , str ): obj . get_metadata ( ensure_metadata ) elif isinstance ( ensure_metadata , typing . Iterable ): obj . get_metadata ( * ensure_metadata ) else : raise ValueError ( f \"Invalid type ' { type ( ensure_metadata ) } ' for 'ensure_metadata' argument.\" ) return obj def set_value ( self , key : str , value : typing . Any , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> Value : result = self . set_values ( metadata = metadata , lineage = lineage , ** { key : value }) if isinstance ( result [ key ], Exception ): raise result [ key ] # type: ignore return result [ key ] # type: ignore def set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ Value , Exception ]]: \"\"\"Batch set several values. If metadata is provided, it is added to all values. \"\"\" if self . is_read_only (): raise Exception ( \"Can't set values: this value set is read-only.\" ) if not values : return {} invalid : typing . List [ str ] = [] for k in values . keys (): if k not in self . get_all_field_names (): invalid . append ( k ) if invalid : raise ValueError ( f \"No field(s) with name(s) { ', ' . join ( invalid ) } available, valid names: { ', ' . join ( self . get_all_field_names ()) } \" ) resolved_values = {} for field_name , data in values . items (): if isinstance ( data , str ) and data . startswith ( \"value:\" ): v = self . _kiara . get_value ( data ) resolved_values [ field_name ] = v else : resolved_values [ field_name ] = data try : value_set_result = self . _set_values ( metadata = metadata , lineage = lineage , ** resolved_values ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () raise e result : typing . Dict [ str , typing . Union [ Value , Exception ]] = {} for field in values . keys (): if isinstance ( value_set_result [ field ], Exception ): result [ field ] = value_set_result [ field ] # type: ignore else : result [ field ] = self . get_value_obj ( field ) return result def is_read_only ( self ): return self . _read_only def items_are_valid ( self ) -> bool : return check_valueset_valid ( self ) # for field_name in self.get_all_field_names(): # item = self.get_value_obj(field_name) # if not item.item_is_valid(): # return False # return True def check_invalid ( self ) -> typing . Optional [ typing . Dict [ str , str ]]: \"\"\"Check whether the value set is invalid, if it is, return a description of what's wrong.\"\"\" invalid = {} for field_name , item in self . items (): if not item . item_is_valid (): if item . value_schema . is_required (): if not item . is_set : msg = \"not set\" elif item . is_none : msg = \"no value\" else : msg = \"n/a\" else : msg = \"n/a\" invalid [ field_name ] = msg return invalid def get_all_value_objects ( self ) -> typing . Mapping [ str , typing . Any ]: return { fn : self . get_value_obj ( fn ) for fn in self . get_all_field_names ()} def get_value_data_for_fields ( self , * field_names : str , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). \"\"\" result : typing . Dict [ str , typing . Any ] = {} unset : typing . List [ str ] = [] for k in field_names : v = self . get_value_obj ( k ) if not v . is_set : if raise_exception_when_unset : unset . append ( k ) else : result [ k ] = None else : data = v . get_value_data () result [ k ] = data if unset : raise Exception ( f \"Can't get data for fields, one or several of the requested fields are not set yet: { ', ' . join ( unset ) } .\" ) return result def get_value_data ( self , field_name : str , raise_exception_when_unset : bool = False ) -> typing . Any : return self . get_value_data_for_fields ( field_name , raise_exception_when_unset = raise_exception_when_unset )[ field_name ] def get_all_value_data ( self , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: return self . get_value_data_for_fields ( * self . get_all_field_names (), raise_exception_when_unset = raise_exception_when_unset , ) def save_all ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None ): if aliases : aliases = set ( aliases ) for k , v in self . items (): field_aliases = None if aliases : field_aliases = [ f \" { a } __ { k } \" for a in aliases ] v . save ( aliases = field_aliases ) def __getitem__ ( self , item : str ) -> \"Value\" : return self . get_value_obj ( item ) def __setitem__ ( self , key : str , value ): self . set_value ( key , value ) def __delitem__ ( self , key : str ): raise Exception ( f \"Removing items not supported: { key } \" ) def __iter__ ( self ) -> typing . Iterator [ str ]: return iter ( self . get_all_field_names ()) def __len__ ( self ): return len ( list ( self . get_all_field_names ())) def to_details ( self , ensure_metadata : bool = False ) -> \"PipelineValuesInfo\" : from kiara.pipeline import PipelineValueInfo , PipelineValuesInfo result = {} for name in self . get_all_field_names (): item = self . get_value_obj ( name ) result [ name ] = PipelineValueInfo . from_value_obj ( item , ensure_metadata = ensure_metadata ) return PipelineValuesInfo ( values = result ) def _create_rich_table ( self , show_headers : bool = True , ensure_metadata : bool = False ) -> Table : table = Table ( box = box . SIMPLE , show_header = show_headers ) table . add_column ( \"Field name\" , style = \"i\" ) table . add_column ( \"Type\" ) table . add_column ( \"Description\" ) table . add_column ( \"Required\" ) table . add_column ( \"Is set\" ) for k in self . get_all_field_names (): v = self . get_value_obj ( k , ensure_metadata = ensure_metadata ) t = v . value_schema . type desc = v . value_schema . doc if not v . value_schema . is_required (): req = \"no\" else : if ( v . value_schema . default and v . value_schema . default != SpecialValue . NO_VALUE and v . value_schema . default != SpecialValue . NOT_SET ): req = \"no\" else : req = \"[red]yes[/red]\" is_set = \"yes\" if v . item_is_valid () else \"no\" table . add_row ( k , t , desc , req , is_set ) return table def __repr__ ( self ): title_str = \"\" if self . _title : title_str = f \" title=' { self . _title } '\" return f \" { self . __class__ . __name__ } (field_names= { list ( self . keys ()) }{ title_str } )\" def __str__ ( self ): return self . __repr__ () def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : title = self . _title if title : postfix = f \" [b] { title } [/b]\" else : postfix = \"\" yield Panel ( self . _create_rich_table ( show_headers = True ), box = box . ROUNDED , title_align = \"left\" , title = f \"Value-Set: { postfix } \" , ) check_invalid ( self ) \u00b6 Check whether the value set is invalid, if it is, return a description of what's wrong. Source code in kiara/data/values/value_set.py def check_invalid ( self ) -> typing . Optional [ typing . Dict [ str , str ]]: \"\"\"Check whether the value set is invalid, if it is, return a description of what's wrong.\"\"\" invalid = {} for field_name , item in self . items (): if not item . item_is_valid (): if item . value_schema . is_required (): if not item . is_set : msg = \"not set\" elif item . is_none : msg = \"no value\" else : msg = \"n/a\" else : msg = \"n/a\" invalid [ field_name ] = msg return invalid get_value_data_for_fields ( self , * field_names , * , raise_exception_when_unset = False ) \u00b6 Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). Source code in kiara/data/values/value_set.py def get_value_data_for_fields ( self , * field_names : str , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). \"\"\" result : typing . Dict [ str , typing . Any ] = {} unset : typing . List [ str ] = [] for k in field_names : v = self . get_value_obj ( k ) if not v . is_set : if raise_exception_when_unset : unset . append ( k ) else : result [ k ] = None else : data = v . get_value_data () result [ k ] = data if unset : raise Exception ( f \"Can't get data for fields, one or several of the requested fields are not set yet: { ', ' . join ( unset ) } .\" ) return result set_values ( self , metadata = None , lineage = None , ** values ) \u00b6 Batch set several values. If metadata is provided, it is added to all values. Source code in kiara/data/values/value_set.py def set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ Value , Exception ]]: \"\"\"Batch set several values. If metadata is provided, it is added to all values. \"\"\" if self . is_read_only (): raise Exception ( \"Can't set values: this value set is read-only.\" ) if not values : return {} invalid : typing . List [ str ] = [] for k in values . keys (): if k not in self . get_all_field_names (): invalid . append ( k ) if invalid : raise ValueError ( f \"No field(s) with name(s) { ', ' . join ( invalid ) } available, valid names: { ', ' . join ( self . get_all_field_names ()) } \" ) resolved_values = {} for field_name , data in values . items (): if isinstance ( data , str ) and data . startswith ( \"value:\" ): v = self . _kiara . get_value ( data ) resolved_values [ field_name ] = v else : resolved_values [ field_name ] = data try : value_set_result = self . _set_values ( metadata = metadata , lineage = lineage , ** resolved_values ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () raise e result : typing . Dict [ str , typing . Union [ Value , Exception ]] = {} for field in values . keys (): if isinstance ( value_set_result [ field ], Exception ): result [ field ] = value_set_result [ field ] # type: ignore else : result [ field ] = self . get_value_obj ( field ) return result","title":"values"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value","text":"The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry. Source code in kiara/data/values/__init__.py class Value ( BaseModel , JupyterMixin ): \"\"\"The underlying base class for all values. The important subclasses here are the ones inheriting from 'PipelineValue', as those are registered in the data registry. \"\"\" class Config : extra = Extra . forbid use_enum_values = True def __init__ ( self , registry : \"BaseDataRegistry\" , value_schema : ValueSchema , type_obj : ValueType , is_set : bool , is_none : bool , hashes : typing . Optional [ typing . Iterable [ ValueHash ]] = None , metadata : typing . Optional [ typing . Mapping [ str , typing . Dict [ str , typing . Any ]]] = None , register_token : typing . Optional [ uuid . UUID ] = None ): # type: ignore if not register_token : raise Exception ( \"No register token provided.\" ) if not registry . _check_register_token ( register_token ): raise Exception ( f \"Value registration with token ' { register_token } ' not allowed.\" ) if value_schema is None : raise NotImplementedError () assert registry self . _registry = registry self . _kiara = self . _registry . _kiara kwargs : typing . Dict [ str , typing . Any ] = {} kwargs [ \"id\" ] = NO_ID_YET_MARKER kwargs [ \"value_schema\" ] = value_schema # if value_lineage is None: # value_lineage = ValueLineage() # # kwargs[\"value_lineage\"] = value_lineage # kwargs[\"is_streaming\"] = False # not used yet kwargs [ \"creation_date\" ] = datetime . now () kwargs [ \"is_set\" ] = is_set kwargs [ \"is_none\" ] = is_none if hashes : kwargs [ \"hashes\" ] = list ( hashes ) if metadata : kwargs [ \"metadata\" ] = dict ( metadata ) else : kwargs [ \"metadata\" ] = {} super () . __init__ ( ** kwargs ) self . _type_obj = type_obj _kiara : \"Kiara\" = PrivateAttr () _registry : \"BaseDataRegistry\" = PrivateAttr () _type_obj : ValueType = PrivateAttr () _value_info : \"ValueInfo\" = PrivateAttr ( default = None ) id : str = Field ( description = \"A unique id for this value.\" ) value_schema : ValueSchema = Field ( description = \"The schema of this value.\" ) creation_date : typing . Optional [ datetime ] = Field ( description = \"The time this value was created value happened.\" ) # is_streaming: bool = Field( # default=False, # description=\"Whether the value is currently streamed into this object.\", # ) is_set : bool = Field ( description = \"Whether the value was set (in some way: user input, default, constant...).\" , default = False , ) is_none : bool = Field ( description = \"Whether the value is 'None'.\" , default = True ) hashes : typing . List [ ValueHash ] = Field ( description = \"Available hashes relating to the actual value data. This attribute is not populated by default, use the 'get_hashes()' method to request one or several hash items, afterwards those hashes will be reflected in this attribute.\" , default_factory = list , ) metadata : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = Field ( description = \"Available metadata relating to the actual value data (size, no. of rows, etc. -- depending on data type). This attribute is not populated by default, use the 'get_metadata()' method to request one or several metadata items, afterwards those metadata items will be reflected in this attribute.\" , default_factory = dict , ) @property def type_name ( self ) -> str : return self . value_schema . type @property def type_obj ( self ): \"\"\"Return the object that contains all the type information for this value.\"\"\" return self . _type_obj def get_hash ( self , hash_type : str ) -> ValueHash : \"\"\"Calculate the hash of a specified type for this value. Hashes are cached.\"\"\" hashes = self . get_hashes ( hash_type ) return list ( hashes )[ 0 ] def get_hashes ( self , * hash_types : str ) -> typing . Iterable [ ValueHash ]: all_hash_types = self . type_obj . get_supported_hash_types () if not hash_types : try : hash_types = all_hash_types except Exception as e : log_message ( str ( e )) if not hash_types : return [] result = [] missing = list ( hash_types ) for hash_obj in self . hashes : if hash_obj . hash_type in hash_types : result . append ( hash_obj ) missing . remove ( hash_obj . hash_type ) for hash_type in missing : if hash_type not in all_hash_types : raise Exception ( f \"Hash type ' { hash_type } ' not supported for ' { self . type_name } '\" ) hash_str = self . type_obj . calculate_value_hash ( value = self . get_value_data (), hash_type = hash_type ) hash_obj = ValueHash ( hash_type = hash_type , hash = hash_str ) self . hashes . append ( hash_obj ) result . append ( hash_obj ) return result def item_is_valid ( self ) -> bool : \"\"\"Check whether the current value is valid\"\"\" if self . value_schema . optional : return True else : return self . is_set and not self . is_none def item_status ( self ) -> str : \"\"\"Print a human readable short description of this values status.\"\"\" if self . value_schema . optional : if self . is_set : if self . is_none : return \"no value (not required)\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set (not required)\" else : if self . is_set : if self . is_none : return \"no value\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set\" def get_value_data ( self ) -> typing . Any : return self . _registry . get_value_data ( self ) def get_lineage ( self ) -> typing . Optional [ ValueLineage ]: return self . _registry . get_lineage ( self ) def set_value_lineage ( self , value_lineage : ValueLineage ) -> None : if hasattr ( self . _registry , \"set_value_lineage\" ): return self . _registry . set_value_lineage ( self , value_lineage ) # type: ignore else : raise Exception ( \"Can't set value lineage: registry is read only\" ) def get_info ( self ) -> \"ValueInfo\" : if self . _value_info is None : self . _value_info = ValueInfo . from_value ( self ) return self . _value_info def create_info ( self , include_deserialization_config : bool = False ) -> \"ValueInfo\" : return ValueInfo . from_value ( self , include_deserialization_config = include_deserialization_config ) def get_metadata ( self , * metadata_keys : str , also_return_schema : bool = False ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: \"\"\"Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to `True`, the value will be a two-key map with the metadata under the ``metadata_item`` subkey, and the value schema under ``metadata_schema``. If no metadata key is specified, all available metadata for this value type will be returned. Arguments: metadata_keys: the metadata keys to retrieve metadata (empty for all available metadata) also_return_schema: whether to also return the schema for each metadata item \"\"\" if not metadata_keys : _metadata_keys = set ( self . _kiara . metadata_mgmt . get_metadata_keys_for_type ( self . type_name ) ) for key in self . metadata . keys (): _metadata_keys . add ( key ) else : _metadata_keys = set ( metadata_keys ) result = {} missing = set () for metadata_key in sorted ( _metadata_keys ): if metadata_key in self . metadata . keys (): if also_return_schema : result [ metadata_key ] = self . metadata [ metadata_key ] else : result [ metadata_key ] = self . metadata [ metadata_key ][ \"metadata_item\" ] else : missing . add ( metadata_key ) if not missing : return result _md = self . _kiara . metadata_mgmt . get_value_metadata ( self , * missing , also_return_schema = True ) for k , v in _md . items (): self . metadata [ k ] = v if also_return_schema : result [ k ] = v else : result [ k ] = v [ \"metadata_item\" ] return { k : result [ k ] for k in sorted ( result . keys ())} def save ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None , register_missing_aliases : bool = True , ) -> \"Value\" : \"\"\"Save this value, under the specified alias(es).\"\"\" # if self.get_value_lineage(): # for field_name, value in self.get_value_lineage().inputs.items(): # value_obj = self._registry.get_value_obj(value) # try: # value_obj.save() # except Exception as e: # print(e) value = self . _kiara . data_store . register_data ( self ) if aliases : self . _kiara . data_store . link_aliases ( value , * aliases , register_missing_aliases = register_missing_aliases ) return value def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , Value ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id ) def __repr__ ( self ): return f \"Value(id= { self . id } , type= { self . type_name } , is_set= { self . is_set } , is_none= { self . is_none } \" def __str__ ( self ): return self . __repr__ () def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : # table = self._create_value_table() title = \"Value\" yield Panel ( self . get_info (), box = box . ROUNDED , title_align = \"left\" , title = title )","title":"Value"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.creation_date","text":"The time this value was created value happened.","title":"creation_date"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.hashes","text":"Available hashes relating to the actual value data. This attribute is not populated by default, use the 'get_hashes()' method to request one or several hash items, afterwards those hashes will be reflected in this attribute.","title":"hashes"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.id","text":"A unique id for this value.","title":"id"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.is_none","text":"Whether the value is 'None'.","title":"is_none"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.is_set","text":"Whether the value was set (in some way: user input, default, constant...).","title":"is_set"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.metadata","text":"Available metadata relating to the actual value data (size, no. of rows, etc. -- depending on data type). This attribute is not populated by default, use the 'get_metadata()' method to request one or several metadata items, afterwards those metadata items will be reflected in this attribute.","title":"metadata"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.type_obj","text":"Return the object that contains all the type information for this value.","title":"type_obj"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.value_schema","text":"The schema of this value.","title":"value_schema"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.__eq__","text":"Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): # TODO: compare all attributes if id is equal, just to make sure... if not isinstance ( other , Value ): return False return self . id == other . id","title":"__eq__()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.__hash__","text":"Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash ( self . id )","title":"__hash__()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.__repr__","text":"Return repr(self). Source code in kiara/data/values/__init__.py def __repr__ ( self ): return f \"Value(id= { self . id } , type= { self . type_name } , is_set= { self . is_set } , is_none= { self . is_none } \"","title":"__repr__()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.__str__","text":"Return str(self). Source code in kiara/data/values/__init__.py def __str__ ( self ): return self . __repr__ ()","title":"__str__()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.get_hash","text":"Calculate the hash of a specified type for this value. Hashes are cached. Source code in kiara/data/values/__init__.py def get_hash ( self , hash_type : str ) -> ValueHash : \"\"\"Calculate the hash of a specified type for this value. Hashes are cached.\"\"\" hashes = self . get_hashes ( hash_type ) return list ( hashes )[ 0 ]","title":"get_hash()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.get_metadata","text":"Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to True , the value will be a two-key map with the metadata under the metadata_item subkey, and the value schema under metadata_schema . If no metadata key is specified, all available metadata for this value type will be returned. Parameters: Name Type Description Default metadata_keys str the metadata keys to retrieve metadata (empty for all available metadata) () also_return_schema bool whether to also return the schema for each metadata item False Source code in kiara/data/values/__init__.py def get_metadata ( self , * metadata_keys : str , also_return_schema : bool = False ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: \"\"\"Retrieve (if necessary) and return metadata for the specified keys. By default, the metadata is returned as a map (in case of a single key: single-item map) with metadata key as dict key, and the metadata as value. If 'also_return_schema' was set to `True`, the value will be a two-key map with the metadata under the ``metadata_item`` subkey, and the value schema under ``metadata_schema``. If no metadata key is specified, all available metadata for this value type will be returned. Arguments: metadata_keys: the metadata keys to retrieve metadata (empty for all available metadata) also_return_schema: whether to also return the schema for each metadata item \"\"\" if not metadata_keys : _metadata_keys = set ( self . _kiara . metadata_mgmt . get_metadata_keys_for_type ( self . type_name ) ) for key in self . metadata . keys (): _metadata_keys . add ( key ) else : _metadata_keys = set ( metadata_keys ) result = {} missing = set () for metadata_key in sorted ( _metadata_keys ): if metadata_key in self . metadata . keys (): if also_return_schema : result [ metadata_key ] = self . metadata [ metadata_key ] else : result [ metadata_key ] = self . metadata [ metadata_key ][ \"metadata_item\" ] else : missing . add ( metadata_key ) if not missing : return result _md = self . _kiara . metadata_mgmt . get_value_metadata ( self , * missing , also_return_schema = True ) for k , v in _md . items (): self . metadata [ k ] = v if also_return_schema : result [ k ] = v else : result [ k ] = v [ \"metadata_item\" ] return { k : result [ k ] for k in sorted ( result . keys ())}","title":"get_metadata()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.item_is_valid","text":"Check whether the current value is valid Source code in kiara/data/values/__init__.py def item_is_valid ( self ) -> bool : \"\"\"Check whether the current value is valid\"\"\" if self . value_schema . optional : return True else : return self . is_set and not self . is_none","title":"item_is_valid()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.item_status","text":"Print a human readable short description of this values status. Source code in kiara/data/values/__init__.py def item_status ( self ) -> str : \"\"\"Print a human readable short description of this values status.\"\"\" if self . value_schema . optional : if self . is_set : if self . is_none : return \"no value (not required)\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set (not required)\" else : if self . is_set : if self . is_none : return \"no value\" else : if self . value_schema . default and self . type_name in [ \"string\" , \"integer\" , \"boolean\" , ]: if self . get_value_data () == self . value_schema . default : return \"set (default)\" return \"set\" else : return \"not set\"","title":"item_status()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.Value.save","text":"Save this value, under the specified alias(es). Source code in kiara/data/values/__init__.py def save ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None , register_missing_aliases : bool = True , ) -> \"Value\" : \"\"\"Save this value, under the specified alias(es).\"\"\" # if self.get_value_lineage(): # for field_name, value in self.get_value_lineage().inputs.items(): # value_obj = self._registry.get_value_obj(value) # try: # value_obj.save() # except Exception as e: # print(e) value = self . _kiara . data_store . register_data ( self ) if aliases : self . _kiara . data_store . link_aliases ( value , * aliases , register_missing_aliases = register_missing_aliases ) return value","title":"save()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueAlias","text":"Source code in kiara/data/values/__init__.py class ValueAlias ( BaseModel ): @classmethod def from_string ( self , value_alias : str , default_repo_name : typing . Optional [ str ] = None ) -> \"ValueAlias\" : if not isinstance ( value_alias , str ): raise Exception ( \"Invalid id_or_alias: not a string.\" ) if not value_alias : raise Exception ( \"Invalid id_or_alias: can't be empty string.\" ) _repo_name : typing . Optional [ str ] = default_repo_name _version : typing . Optional [ int ] = None _tag : typing . Optional [ str ] = None if \"#\" in value_alias : _repo_name , _value_alias = value_alias . split ( \"#\" , maxsplit = 1 ) else : _value_alias = value_alias if \"@\" in _value_alias : _alias , _postfix = _value_alias . split ( \"@\" , maxsplit = 1 ) try : _version = int ( _postfix ) except ValueError : if not _postfix . isidentifier (): raise Exception ( f \"Invalid format for version/tag element of id_or_alias: { _tag } \" ) _tag = _postfix else : _alias = _value_alias return ValueAlias ( repo_name = _repo_name , alias = _alias , version = _version , tag = _tag ) @classmethod def from_strings ( cls , * value_aliases : typing . Union [ str , \"ValueAlias\" ] ) -> typing . List [ \"ValueAlias\" ]: result = [] for va in value_aliases : if isinstance ( va , str ): result . append ( ValueAlias . from_string ( va )) elif isinstance ( va , ValueAlias ): result . append ( va ) else : raise TypeError ( f \"Invalid type ' { type ( va ) } ' for type alias, expected 'str' or 'ValueAlias'.\" ) return result repo_name : typing . Optional [ str ] = Field ( description = \"The name of the data repo the value lives in.\" , default = None ) alias : str = Field ( \"The alias name.\" ) version : typing . Optional [ int ] = Field ( description = \"The version of this alias.\" , default = None ) tag : typing . Optional [ str ] = Field ( description = \"The tag for the alias.\" , default = None ) @property def full_alias ( self ): if self . tag is not None : return f \" { self . alias } @ { self . tag } \" elif self . version is not None : return f \" { self . alias } @ { self . version } \" else : return self . alias","title":"ValueAlias"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueAlias.repo_name","text":"The name of the data repo the value lives in.","title":"repo_name"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueAlias.tag","text":"The tag for the alias.","title":"tag"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueAlias.version","text":"The version of this alias.","title":"version"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueHash","text":"Source code in kiara/data/values/__init__.py class ValueHash ( BaseModel ): hash : str = Field ( description = \"The value hash.\" ) hash_type : str = Field ( description = \"The value hash method.\" )","title":"ValueHash"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueHash.hash","text":"The value hash.","title":"hash"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueHash.hash_type","text":"The value hash method.","title":"hash_type"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueInfo","text":"Source code in kiara/data/values/__init__.py class ValueInfo ( KiaraInfoModel ): @classmethod def from_value ( cls , value : Value , include_deserialization_config : bool = False ): if value . id not in value . _registry . value_ids : raise Exception ( \"Value not registered (yet).\" ) # aliases = value._registry.find_aliases_for_value(value) hashes = value . get_hashes () metadata = value . get_metadata ( also_return_schema = True ) # metadata = value.metadata value_lineage = value . get_lineage () if include_deserialization_config : # serialize_operation: SerializeValueOperationType = ( # type: ignore # value._kiara.operation_mgmt.get_operation(\"serialize\") # type: ignore # ) raise NotImplementedError () return ValueInfo ( value_id = value . id , value_schema = value . value_schema , hashes = hashes , metadata = metadata , lineage = value_lineage , is_valid = value . item_is_valid (), ) value_id : str = Field ( description = \"The value id.\" ) value_schema : ValueSchema = Field ( description = \"The value schema.\" ) # aliases: typing.List[ValueAlias] = Field( # description=\"All aliases for this value.\", default_factory=list # ) # tags: typing.List[str] = Field( # description=\"All tags for this value.\", default_factory=list # ) # created: str = Field(description=\"The time the data was created.\") is_valid : bool = Field ( description = \"Whether the item is valid (in the context of its schema).\" ) hashes : typing . List [ ValueHash ] = Field ( description = \"All available hashes for this value.\" , default_factory = list ) metadata : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = Field ( description = \"The metadata associated with this value.\" ) lineage : typing . Optional [ ValueLineage ] = Field ( description = \"Information about how the value was created.\" , default = None ) deserialize_config : typing . Optional [ DeserializeConfig ] = Field ( description = \"The module config (incl. inputs) to deserialize the value.\" , default = None , ) def get_metadata_items ( self , * keys : str ) -> typing . Dict [ str , typing . Any ]: if not keys : _keys : typing . Iterable [ str ] = self . metadata . keys () else : _keys = keys result = {} for k in _keys : md = self . metadata . get ( k ) if md is None : raise Exception ( f \"No metadata for key ' { k } ' available.\" ) result [ k ] = md [ \"metadata_item\" ] return result def get_metadata_schemas ( self , * keys : str ) -> typing . Dict [ str , typing . Any ]: if not keys : _keys : typing . Iterable [ str ] = self . metadata . keys () else : _keys = keys result = {} for k in _keys : md = self . metadata . get ( k ) if md is None : raise Exception ( f \"No metadata for key ' { k } ' available.\" ) result [ k ] = md [ \"metadata_schema\" ] return result def create_renderable ( self , ** config : typing . Any ) -> RenderableType : padding = config . get ( \"padding\" , ( 0 , 1 )) skip_metadata = config . get ( \"skip_metadata\" , False ) skip_value_lineage = config . get ( \"skip_lineage\" , True ) include_ids = config . get ( \"include_ids\" , False ) table = Table ( box = box . SIMPLE , show_header = False , padding = padding ) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) table . add_row ( \"id\" , self . value_id ) # type: ignore table . add_row ( \"type\" , self . value_schema . type ) if self . value_schema . type_config : json_data = json . dumps ( self . value_schema . type_config ) tc_content = Syntax ( json_data , \"json\" ) table . add_row ( \"type config\" , tc_content ) table . add_row ( \"desc\" , self . value_schema . doc ) table . add_row ( \"is set\" , \"yes\" if self . is_valid else \"no\" ) # table.add_row(\"is constant\", \"yes\" if self.is_constant else \"no\") # if isinstance(self.value_hash, int): # vh = str(self.value_hash) # else: # vh = self.value_hash.value # table.add_row(\"hash\", vh) if self . hashes : hashes_dict = { hs . hash_type : hs . hash for hs in self . hashes } yaml_string = yaml . dump ( hashes_dict ) hases_str = Syntax ( yaml_string , \"yaml\" , background_color = \"default\" ) table . add_row ( \"\" , \"\" ) table . add_row ( \"hashes\" , hases_str ) if not skip_metadata : if self . metadata : yaml_string = yaml . dump ( data = self . get_metadata_items ()) # json_string = json.dumps(self.get_metadata_items(), indent=2) metadata = Syntax ( yaml_string , \"yaml\" , background_color = \"default\" ) table . add_row ( \"metadata\" , metadata ) else : table . add_row ( \"metadata\" , \"-- no metadata --\" ) if not skip_value_lineage and self . lineage : if self . metadata : table . add_row ( \"\" , \"\" ) # json_string = self.lineage.json(indent=2) # seed_content = Syntax(json_string, \"json\") table . add_row ( \"lineage\" , self . lineage . create_renderable ( include_ids = include_ids ) ) return table","title":"ValueInfo"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueInfo.deserialize_config","text":"The module config (incl. inputs) to deserialize the value.","title":"deserialize_config"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueInfo.hashes","text":"All available hashes for this value.","title":"hashes"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueInfo.is_valid","text":"Whether the item is valid (in the context of its schema).","title":"is_valid"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueInfo.lineage","text":"Information about how the value was created.","title":"lineage"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueInfo.metadata","text":"The metadata associated with this value.","title":"metadata"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueInfo.value_id","text":"The value id.","title":"value_id"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueInfo.value_schema","text":"The value schema.","title":"value_schema"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueLineage","text":"Model containing the lineage of a value. Source code in kiara/data/values/__init__.py class ValueLineage ( ModuleConfig ): \"\"\"Model containing the lineage of a value.\"\"\" @classmethod def from_module_and_inputs ( cls , module : \"KiaraModule\" , output_name : str , inputs : typing . Mapping [ str , typing . Union [ \"Value\" , \"ValueInfo\" ]], ): module_type = module . _module_type_id # type: ignore module_config = module . config . dict () doc = module . get_type_metadata () . documentation _inputs = {} for field_name , value in inputs . items (): if isinstance ( value , Value ): _inputs [ field_name ] = value . get_info () else : _inputs [ field_name ] = value return ValueLineage . construct ( module_type = module_type , module_config = module_config , doc = doc , output_name = output_name , inputs = _inputs , ) @classmethod def create ( cls , module_type : str , module_config : typing . Mapping [ str , typing . Any ], module_doc : DocumentationMetadataModel , inputs : typing . Mapping [ str , typing . Union [ \"Value\" , \"ValueInfo\" ]], output_name : typing . Optional [ str ] = None , ): _inputs = {} for field_name , value in inputs . items (): if isinstance ( value , Value ): _inputs [ field_name ] = value . get_info () else : _inputs [ field_name ] = value return ValueLineage . construct ( module_type = module_type , module_config = dict ( module_config ), doc = module_doc , output_name = output_name , inputs = _inputs , ) output_name : typing . Optional [ str ] = Field ( description = \"The result field name for the value this refers to.\" ) inputs : typing . Dict [ str , \"ValueInfo\" ] = Field ( description = \"The inputs that were used to create the value this refers to.\" ) value_index : typing . Optional [ typing . Dict [ str , \"ValueInfo\" ]] = Field ( description = \"Index of all values that are associated with this value lineage.\" , default = None , ) def to_minimal_dict ( self , include_metadata : bool = False , include_module_doc : bool = False , include_module_config : bool = True , ) -> typing . Dict [ str , typing . Any ]: full_dict = self . dict ( exclude_none = True ) minimal_dict = filter_metadata_schema ( full_dict , include_metadata = include_metadata , include_module_doc = include_module_doc , include_module_config = include_module_config , ) return minimal_dict def create_renderable ( self , ** config : typing . Any ) -> RenderableType : all_ids = sorted ( find_all_ids_in_lineage ( self )) id_color_map = {} for idx , v_id in enumerate ( all_ids ): id_color_map [ v_id ] = COLOR_LIST [ idx % len ( COLOR_LIST )] show_ids = config . get ( \"include_ids\" , False ) tree = fill_lineage_tree ( self , include_ids = show_ids ) return tree def create_graph ( self ) -> nx . DiGraph : return create_lineage_graph ( self )","title":"ValueLineage"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueLineage.inputs","text":"The inputs that were used to create the value this refers to.","title":"inputs"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueLineage.output_name","text":"The result field name for the value this refers to.","title":"output_name"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueLineage.value_index","text":"Index of all values that are associated with this value lineage.","title":"value_index"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueLineage.create_renderable","text":"Create a renderable for this module configuration. Source code in kiara/data/values/__init__.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : all_ids = sorted ( find_all_ids_in_lineage ( self )) id_color_map = {} for idx , v_id in enumerate ( all_ids ): id_color_map [ v_id ] = COLOR_LIST [ idx % len ( COLOR_LIST )] show_ids = config . get ( \"include_ids\" , False ) tree = fill_lineage_tree ( self , include_ids = show_ids ) return tree","title":"create_renderable()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSchema","text":"The schema of a value. The schema contains the ValueType of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the type_config field. Source code in kiara/data/values/__init__.py class ValueSchema ( BaseModel ): \"\"\"The schema of a value. The schema contains the [ValueType][kiara.data.values.ValueType] of a value, as well as an optional default that will be used if no user input was given (yet) for a value. For more complex container types like array, tables, unions etc, types can also be configured with values from the ``type_config`` field. \"\"\" class Config : use_enum_values = True # extra = Extra.forbid type : str = Field ( description = \"The type of the value.\" ) type_config : typing . Dict [ str , typing . Any ] = Field ( description = \"Configuration for the type, in case it's complex.\" , default_factory = dict , ) default : typing . Any = Field ( description = \"A default value.\" , default = SpecialValue . NOT_SET ) optional : bool = Field ( description = \"Whether this value is required (True), or whether 'None' value is allowed (False).\" , default = False , ) is_constant : bool = Field ( description = \"Whether the value is a constant.\" , default = False ) # required: typing.Any = Field( # description=\"Whether this value is required to be set.\", default=True # ) doc : str = Field ( default = \"-- n/a --\" , description = \"A description for the value of this input field.\" , ) def is_required ( self ): if self . optional : return False else : if self . default in [ None , SpecialValue . NOT_SET , SpecialValue . NO_VALUE ]: return True else : return False def validate_types ( self , kiara : \"Kiara\" ): if self . type not in kiara . value_type_names : raise ValueError ( f \"Invalid value type ' { self . type } ', available types: { kiara . value_type_names } \" ) @property def desc ( self ): \"\"\"The first line of the 'doc' value.\"\"\" return self . doc . split ( \" \\n \" )[ 0 ] def __eq__ ( self , other ): if not isinstance ( other , ValueSchema ): return False return ( self . type , self . default ) == ( other . type , other . default ) def __hash__ ( self ): return hash (( self . type , self . default ))","title":"ValueSchema"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSchema.default","text":"A default value.","title":"default"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSchema.desc","text":"The first line of the 'doc' value.","title":"desc"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSchema.doc","text":"A description for the value of this input field.","title":"doc"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSchema.is_constant","text":"Whether the value is a constant.","title":"is_constant"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSchema.optional","text":"Whether this value is required (True), or whether 'None' value is allowed (False).","title":"optional"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSchema.type","text":"The type of the value.","title":"type"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSchema.type_config","text":"Configuration for the type, in case it's complex.","title":"type_config"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSchema.__eq__","text":"Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): if not isinstance ( other , ValueSchema ): return False return ( self . type , self . default ) == ( other . type , other . default )","title":"__eq__()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSchema.__hash__","text":"Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash (( self . type , self . default ))","title":"__hash__()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSlot","text":"Source code in kiara/data/values/__init__.py class ValueSlot ( BaseModel ): @classmethod def from_value ( cls , id : str , value : Value ) -> \"ValueSlot\" : vs = ValueSlot . from_value_schema ( id = id , value_schema = value . value_schema , kiara = value . _kiara ) vs . add_value ( value ) return vs @classmethod def from_value_schema ( cls , id : str , value_schema : ValueSchema , kiara : \"Kiara\" ) -> \"ValueSlot\" : vs = ValueSlot ( id = id , value_schema = value_schema , kiara = kiara ) return vs def __init__ ( self , ** data ): # type: ignore _kiara = data . pop ( \"kiara\" , None ) if _kiara is None : raise Exception ( \"No kiara context provided.\" ) _registry = data . pop ( \"registry\" , None ) if _registry is None : _registry = _kiara . data_registry self . _kiara = _kiara self . _registry = _registry super () . __init__ ( ** data ) _kiara : \"Kiara\" = PrivateAttr () _registry : \"DataRegistry\" = PrivateAttr () _callbacks : typing . Dict [ str , \"ValueSlotUpdateHandler\" ] = PrivateAttr ( default_factory = dict ) id : str = Field ( description = \"The id for this slot.\" ) value_schema : ValueSchema = Field ( description = \"The schema for the values of this slot.\" ) values : typing . Dict [ int , Value ] = Field ( description = \"The values of this slot, with versions as key.\" , default_factory = dict , ) tags : typing . Dict [ str , int ] = Field ( description = \"The tags for this value slot (tag name as key, linked version as value.\" , default_factory = dict , ) @property def latest_version_nr ( self ) -> int : if not self . values : return 0 return max ( self . values . keys ()) def get_latest_value ( self ) -> Value : lv = self . latest_version_nr if lv == 0 : raise Exception ( \"No value added to value slot yet.\" ) return self . values [ self . latest_version_nr ] def register_callbacks ( self , * callbacks : \"ValueSlotUpdateHandler\" ): for cb in callbacks : cb_id : typing . Optional [ str ] = None if cb_id in self . _callbacks . keys (): raise Exception ( f \"Callback with id ' { cb_id } ' already registered.\" ) if hasattr ( cb , \"id\" ): if callable ( cb . id ): # type: ignore cb_id = cb . id () # type: ignore else : cb_id = cb . id # type: ignore elif hasattr ( cb , \"get_id\" ): cb_id = cb . get_id () # type: ignore if cb_id is None : cb_id = str ( uuid . uuid4 ()) assert isinstance ( cb_id , str ) self . _callbacks [ cb_id ] = cb def add_value ( self , value : Value , trigger_callbacks : bool = True , tags : typing . Optional [ typing . Iterable [ str ]] = None , ) -> int : \"\"\"Add a value to this slot.\"\"\" if self . latest_version_nr != 0 and value . id == self . get_latest_value () . id : return self . latest_version_nr # TODO: check value data equality if self . value_schema . is_constant and self . values : if is_debug (): import traceback traceback . print_stack () raise Exception ( \"Can't add value: value slot marked as 'constant'.\" ) version = self . latest_version_nr + 1 assert version not in self . values . keys () self . values [ version ] = value if tags : for tag in tags : self . tags [ tag ] = version if trigger_callbacks : for cb in self . _callbacks . values (): cb . values_updated ( self ) return version def is_latest_value ( self , value : Value ): return value . id == self . get_latest_value () . id def find_linked_aliases ( self , value_item : typing . Union [ Value , str ] ) -> typing . List [ \"ValueAlias\" ]: if isinstance ( value_item , Value ): value_item = value_item . id result = [] for _version , _value in self . values . items (): if _value . id == value_item : va = ValueAlias ( alias = self . id , version = _version ) result . append ( va ) if _version in self . tags . values (): for _tag , _tag_version in self . tags . items (): if _tag_version == _version : va = ValueAlias ( alias = self . id , tag = _tag ) result . append ( va ) return result def __eq__ ( self , other ): if not isinstance ( other , ValueSlot ): return False return self . id == other . id def __hash__ ( self ): return hash ( self . id )","title":"ValueSlot"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSlot.id","text":"The id for this slot.","title":"id"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSlot.tags","text":"The tags for this value slot (tag name as key, linked version as value.","title":"tags"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSlot.value_schema","text":"The schema for the values of this slot.","title":"value_schema"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSlot.values","text":"The values of this slot, with versions as key.","title":"values"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSlot.__eq__","text":"Return self==value. Source code in kiara/data/values/__init__.py def __eq__ ( self , other ): if not isinstance ( other , ValueSlot ): return False return self . id == other . id","title":"__eq__()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSlot.__hash__","text":"Return hash(self). Source code in kiara/data/values/__init__.py def __hash__ ( self ): return hash ( self . id )","title":"__hash__()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.ValueSlot.add_value","text":"Add a value to this slot. Source code in kiara/data/values/__init__.py def add_value ( self , value : Value , trigger_callbacks : bool = True , tags : typing . Optional [ typing . Iterable [ str ]] = None , ) -> int : \"\"\"Add a value to this slot.\"\"\" if self . latest_version_nr != 0 and value . id == self . get_latest_value () . id : return self . latest_version_nr # TODO: check value data equality if self . value_schema . is_constant and self . values : if is_debug (): import traceback traceback . print_stack () raise Exception ( \"Can't add value: value slot marked as 'constant'.\" ) version = self . latest_version_nr + 1 assert version not in self . values . keys () self . values [ version ] = value if tags : for tag in tags : self . tags [ tag ] = version if trigger_callbacks : for cb in self . _callbacks . values (): cb . values_updated ( self ) return version","title":"add_value()"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.value_set","text":"","title":"value_set"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.value_set.SlottedValueSet","text":"Source code in kiara/data/values/value_set.py class SlottedValueSet ( ValueSet ): @classmethod def from_schemas ( cls , schemas : typing . Mapping [ str , \"ValueSchema\" ], read_only : bool = True , check_for_sameness = True , initial_values : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , title : typing . Optional [ str ] = None , default_value : typing . Any = SpecialValue . NO_VALUE , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ) -> \"SlottedValueSet\" : if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if registry is None : registry = kiara . data_registry values = {} for field_name , schema in schemas . items (): def_val = default_value if callable ( default_value ): def_val = default_value () _init_value = def_val if initial_values and initial_values . get ( field_name , None ) is not None : _init_value = initial_values [ field_name ] if not isinstance ( _init_value , Value ): value : Value = registry . register_data ( value_data = _init_value , value_schema = schema ) # value: Value = Value(value_schema=schema, value_data=_init_value, registry=registry) # type: ignore else : value = _init_value values [ field_name ] = value return cls ( items = values , title = title , read_only = read_only , check_for_sameness = check_for_sameness , kiara = kiara , registry = registry , ) def __init__ ( self , items : typing . Mapping [ str , typing . Union [ \"ValueSlot\" , \"Value\" ]], read_only : bool , check_for_sameness : bool = False , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ): \"\"\"A `ValueSet` implementation that keeps a history of each fields value. Arguments: items: the value slots read_only: whether it is allowed to set new values to fields in this set check_for_sameness: whether a check should be performed that checks for equality of the new and old values, if equal, skip the update title: An optional title for this value set kiara: the kiara context registry: the registry to use to register the values to \"\"\" if not items : raise ValueError ( \"Can't create ValueSet: no values provided\" ) self . _check_for_sameness : bool = check_for_sameness super () . __init__ ( read_only = read_only , title = title , kiara = kiara ) if registry is None : registry = self . _kiara . data_registry self . _registry : DataRegistry = registry _value_slots : typing . Dict [ str , ValueSlot ] = {} for item , value in items . items (): if value is None : raise Exception ( f \"Can't create value set, item ' { item } ' does not have a value (yet).\" ) if item . startswith ( \"_\" ): raise ValueError ( f \"Value name can't start with '_': { item } \" ) if item in INVALID_VALUE_NAMES : raise ValueError ( f \"Invalid value name ' { item } '.\" ) if isinstance ( value , Value ): slot = self . _registry . register_alias ( value ) elif isinstance ( value , ValueSlot ): slot = value else : raise TypeError ( f \"Invalid type: ' { type ( value ) } '\" ) _value_slots [ item ] = slot self . _value_slots : typing . Dict [ str , ValueSlot ] = _value_slots def get_all_field_names ( self ) -> typing . Iterable [ str ]: return self . _value_slots . keys () def _get_value_obj ( self , field_name : str ) -> \"Value\" : slot = self . _value_slots [ field_name ] return slot . get_latest_value () def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: # we want to set all registry-type values seperately, in one go, because it's more efficient registries : typing . Dict [ DataRegistry , typing . Dict [ ValueSlot , typing . Union [ \"Value\" , typing . Any ]] ] = {} field_slot_map : typing . Dict [ ValueSlot , str ] = {} result : typing . Dict [ str , typing . Union [ bool , Exception ]] = {} for field_name , value_or_data in values . items (): value_slot : ValueSlot = self . _value_slots [ field_name ] if not metadata and self . _check_for_sameness : latest_val = value_slot . get_latest_value () if isinstance ( value_or_data , Value ): if latest_val . id == value_or_data . id : result [ field_name ] = False continue else : if ( latest_val . is_set and latest_val . get_value_data () == value_or_data ): result [ field_name ] = False continue registries . setdefault ( value_slot . _registry , {})[ value_slot ] = value_or_data field_slot_map [ value_slot ] = field_name for registry , value_slots_details in registries . items (): _r = registry . update_value_slots ( value_slots_details , metadata = metadata , lineage = lineage ) # type: ignore for value_slot , details in _r . items (): result [ field_slot_map [ value_slot ]] = details return result","title":"SlottedValueSet"},{"location":"reference/kiara/data/values/__init__/#kiara.data.values.value_set.ValueSet","text":"Source code in kiara/data/values/value_set.py class ValueSet ( typing . MutableMapping [ str , \"Value\" ]): def __init__ ( self , read_only : bool , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): # from kiara.data.values import ValueSchema # schema = ValueSchema(type=\"any\", default=None, doc=\"-- n/a --\") # self._schema = schema if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : \"Kiara\" = kiara self . _read_only : bool = read_only if title is None : title = \"-- n/a --\" self . _title = title @abc . abstractmethod def get_all_field_names ( self ) -> typing . Iterable [ str ]: pass @abc . abstractmethod def _get_value_obj ( self , field_name : str ): pass @abc . abstractmethod def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: pass def get_value_obj ( self , field_name : str , ensure_metadata : typing . Union [ bool , typing . Iterable [ str ], str ] = False , ) -> Value : if field_name not in list ( self . get_all_field_names ()): raise KeyError ( f \"Field ' { field_name } ' not available in value set. Available fields: { ', ' . join ( self . get_all_field_names ()) } \" ) obj : Value = self . _get_value_obj ( field_name ) if ensure_metadata : if isinstance ( ensure_metadata , bool ): obj . get_metadata () elif isinstance ( ensure_metadata , str ): obj . get_metadata ( ensure_metadata ) elif isinstance ( ensure_metadata , typing . Iterable ): obj . get_metadata ( * ensure_metadata ) else : raise ValueError ( f \"Invalid type ' { type ( ensure_metadata ) } ' for 'ensure_metadata' argument.\" ) return obj def set_value ( self , key : str , value : typing . Any , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> Value : result = self . set_values ( metadata = metadata , lineage = lineage , ** { key : value }) if isinstance ( result [ key ], Exception ): raise result [ key ] # type: ignore return result [ key ] # type: ignore def set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ Value , Exception ]]: \"\"\"Batch set several values. If metadata is provided, it is added to all values. \"\"\" if self . is_read_only (): raise Exception ( \"Can't set values: this value set is read-only.\" ) if not values : return {} invalid : typing . List [ str ] = [] for k in values . keys (): if k not in self . get_all_field_names (): invalid . append ( k ) if invalid : raise ValueError ( f \"No field(s) with name(s) { ', ' . join ( invalid ) } available, valid names: { ', ' . join ( self . get_all_field_names ()) } \" ) resolved_values = {} for field_name , data in values . items (): if isinstance ( data , str ) and data . startswith ( \"value:\" ): v = self . _kiara . get_value ( data ) resolved_values [ field_name ] = v else : resolved_values [ field_name ] = data try : value_set_result = self . _set_values ( metadata = metadata , lineage = lineage , ** resolved_values ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () raise e result : typing . Dict [ str , typing . Union [ Value , Exception ]] = {} for field in values . keys (): if isinstance ( value_set_result [ field ], Exception ): result [ field ] = value_set_result [ field ] # type: ignore else : result [ field ] = self . get_value_obj ( field ) return result def is_read_only ( self ): return self . _read_only def items_are_valid ( self ) -> bool : return check_valueset_valid ( self ) # for field_name in self.get_all_field_names(): # item = self.get_value_obj(field_name) # if not item.item_is_valid(): # return False # return True def check_invalid ( self ) -> typing . Optional [ typing . Dict [ str , str ]]: \"\"\"Check whether the value set is invalid, if it is, return a description of what's wrong.\"\"\" invalid = {} for field_name , item in self . items (): if not item . item_is_valid (): if item . value_schema . is_required (): if not item . is_set : msg = \"not set\" elif item . is_none : msg = \"no value\" else : msg = \"n/a\" else : msg = \"n/a\" invalid [ field_name ] = msg return invalid def get_all_value_objects ( self ) -> typing . Mapping [ str , typing . Any ]: return { fn : self . get_value_obj ( fn ) for fn in self . get_all_field_names ()} def get_value_data_for_fields ( self , * field_names : str , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). \"\"\" result : typing . Dict [ str , typing . Any ] = {} unset : typing . List [ str ] = [] for k in field_names : v = self . get_value_obj ( k ) if not v . is_set : if raise_exception_when_unset : unset . append ( k ) else : result [ k ] = None else : data = v . get_value_data () result [ k ] = data if unset : raise Exception ( f \"Can't get data for fields, one or several of the requested fields are not set yet: { ', ' . join ( unset ) } .\" ) return result def get_value_data ( self , field_name : str , raise_exception_when_unset : bool = False ) -> typing . Any : return self . get_value_data_for_fields ( field_name , raise_exception_when_unset = raise_exception_when_unset )[ field_name ] def get_all_value_data ( self , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: return self . get_value_data_for_fields ( * self . get_all_field_names (), raise_exception_when_unset = raise_exception_when_unset , ) def save_all ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None ): if aliases : aliases = set ( aliases ) for k , v in self . items (): field_aliases = None if aliases : field_aliases = [ f \" { a } __ { k } \" for a in aliases ] v . save ( aliases = field_aliases ) def __getitem__ ( self , item : str ) -> \"Value\" : return self . get_value_obj ( item ) def __setitem__ ( self , key : str , value ): self . set_value ( key , value ) def __delitem__ ( self , key : str ): raise Exception ( f \"Removing items not supported: { key } \" ) def __iter__ ( self ) -> typing . Iterator [ str ]: return iter ( self . get_all_field_names ()) def __len__ ( self ): return len ( list ( self . get_all_field_names ())) def to_details ( self , ensure_metadata : bool = False ) -> \"PipelineValuesInfo\" : from kiara.pipeline import PipelineValueInfo , PipelineValuesInfo result = {} for name in self . get_all_field_names (): item = self . get_value_obj ( name ) result [ name ] = PipelineValueInfo . from_value_obj ( item , ensure_metadata = ensure_metadata ) return PipelineValuesInfo ( values = result ) def _create_rich_table ( self , show_headers : bool = True , ensure_metadata : bool = False ) -> Table : table = Table ( box = box . SIMPLE , show_header = show_headers ) table . add_column ( \"Field name\" , style = \"i\" ) table . add_column ( \"Type\" ) table . add_column ( \"Description\" ) table . add_column ( \"Required\" ) table . add_column ( \"Is set\" ) for k in self . get_all_field_names (): v = self . get_value_obj ( k , ensure_metadata = ensure_metadata ) t = v . value_schema . type desc = v . value_schema . doc if not v . value_schema . is_required (): req = \"no\" else : if ( v . value_schema . default and v . value_schema . default != SpecialValue . NO_VALUE and v . value_schema . default != SpecialValue . NOT_SET ): req = \"no\" else : req = \"[red]yes[/red]\" is_set = \"yes\" if v . item_is_valid () else \"no\" table . add_row ( k , t , desc , req , is_set ) return table def __repr__ ( self ): title_str = \"\" if self . _title : title_str = f \" title=' { self . _title } '\" return f \" { self . __class__ . __name__ } (field_names= { list ( self . keys ()) }{ title_str } )\" def __str__ ( self ): return self . __repr__ () def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : title = self . _title if title : postfix = f \" [b] { title } [/b]\" else : postfix = \"\" yield Panel ( self . _create_rich_table ( show_headers = True ), box = box . ROUNDED , title_align = \"left\" , title = f \"Value-Set: { postfix } \" , )","title":"ValueSet"},{"location":"reference/kiara/data/values/value_set/","text":"SlottedValueSet ( ValueSet ) \u00b6 Source code in kiara/data/values/value_set.py class SlottedValueSet ( ValueSet ): @classmethod def from_schemas ( cls , schemas : typing . Mapping [ str , \"ValueSchema\" ], read_only : bool = True , check_for_sameness = True , initial_values : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , title : typing . Optional [ str ] = None , default_value : typing . Any = SpecialValue . NO_VALUE , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ) -> \"SlottedValueSet\" : if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if registry is None : registry = kiara . data_registry values = {} for field_name , schema in schemas . items (): def_val = default_value if callable ( default_value ): def_val = default_value () _init_value = def_val if initial_values and initial_values . get ( field_name , None ) is not None : _init_value = initial_values [ field_name ] if not isinstance ( _init_value , Value ): value : Value = registry . register_data ( value_data = _init_value , value_schema = schema ) # value: Value = Value(value_schema=schema, value_data=_init_value, registry=registry) # type: ignore else : value = _init_value values [ field_name ] = value return cls ( items = values , title = title , read_only = read_only , check_for_sameness = check_for_sameness , kiara = kiara , registry = registry , ) def __init__ ( self , items : typing . Mapping [ str , typing . Union [ \"ValueSlot\" , \"Value\" ]], read_only : bool , check_for_sameness : bool = False , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ): \"\"\"A `ValueSet` implementation that keeps a history of each fields value. Arguments: items: the value slots read_only: whether it is allowed to set new values to fields in this set check_for_sameness: whether a check should be performed that checks for equality of the new and old values, if equal, skip the update title: An optional title for this value set kiara: the kiara context registry: the registry to use to register the values to \"\"\" if not items : raise ValueError ( \"Can't create ValueSet: no values provided\" ) self . _check_for_sameness : bool = check_for_sameness super () . __init__ ( read_only = read_only , title = title , kiara = kiara ) if registry is None : registry = self . _kiara . data_registry self . _registry : DataRegistry = registry _value_slots : typing . Dict [ str , ValueSlot ] = {} for item , value in items . items (): if value is None : raise Exception ( f \"Can't create value set, item ' { item } ' does not have a value (yet).\" ) if item . startswith ( \"_\" ): raise ValueError ( f \"Value name can't start with '_': { item } \" ) if item in INVALID_VALUE_NAMES : raise ValueError ( f \"Invalid value name ' { item } '.\" ) if isinstance ( value , Value ): slot = self . _registry . register_alias ( value ) elif isinstance ( value , ValueSlot ): slot = value else : raise TypeError ( f \"Invalid type: ' { type ( value ) } '\" ) _value_slots [ item ] = slot self . _value_slots : typing . Dict [ str , ValueSlot ] = _value_slots def get_all_field_names ( self ) -> typing . Iterable [ str ]: return self . _value_slots . keys () def _get_value_obj ( self , field_name : str ) -> \"Value\" : slot = self . _value_slots [ field_name ] return slot . get_latest_value () def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: # we want to set all registry-type values seperately, in one go, because it's more efficient registries : typing . Dict [ DataRegistry , typing . Dict [ ValueSlot , typing . Union [ \"Value\" , typing . Any ]] ] = {} field_slot_map : typing . Dict [ ValueSlot , str ] = {} result : typing . Dict [ str , typing . Union [ bool , Exception ]] = {} for field_name , value_or_data in values . items (): value_slot : ValueSlot = self . _value_slots [ field_name ] if not metadata and self . _check_for_sameness : latest_val = value_slot . get_latest_value () if isinstance ( value_or_data , Value ): if latest_val . id == value_or_data . id : result [ field_name ] = False continue else : if ( latest_val . is_set and latest_val . get_value_data () == value_or_data ): result [ field_name ] = False continue registries . setdefault ( value_slot . _registry , {})[ value_slot ] = value_or_data field_slot_map [ value_slot ] = field_name for registry , value_slots_details in registries . items (): _r = registry . update_value_slots ( value_slots_details , metadata = metadata , lineage = lineage ) # type: ignore for value_slot , details in _r . items (): result [ field_slot_map [ value_slot ]] = details return result __init__ ( self , items , read_only , check_for_sameness = False , title = None , kiara = None , registry = None ) special \u00b6 A ValueSet implementation that keeps a history of each fields value. Parameters: Name Type Description Default items Mapping[str, Union[ValueSlot, Value]] the value slots required read_only bool whether it is allowed to set new values to fields in this set required check_for_sameness bool whether a check should be performed that checks for equality of the new and old values, if equal, skip the update False title Optional[str] An optional title for this value set None kiara Optional[Kiara] the kiara context None registry Optional[kiara.data.registry.DataRegistry] the registry to use to register the values to None Source code in kiara/data/values/value_set.py def __init__ ( self , items : typing . Mapping [ str , typing . Union [ \"ValueSlot\" , \"Value\" ]], read_only : bool , check_for_sameness : bool = False , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ): \"\"\"A `ValueSet` implementation that keeps a history of each fields value. Arguments: items: the value slots read_only: whether it is allowed to set new values to fields in this set check_for_sameness: whether a check should be performed that checks for equality of the new and old values, if equal, skip the update title: An optional title for this value set kiara: the kiara context registry: the registry to use to register the values to \"\"\" if not items : raise ValueError ( \"Can't create ValueSet: no values provided\" ) self . _check_for_sameness : bool = check_for_sameness super () . __init__ ( read_only = read_only , title = title , kiara = kiara ) if registry is None : registry = self . _kiara . data_registry self . _registry : DataRegistry = registry _value_slots : typing . Dict [ str , ValueSlot ] = {} for item , value in items . items (): if value is None : raise Exception ( f \"Can't create value set, item ' { item } ' does not have a value (yet).\" ) if item . startswith ( \"_\" ): raise ValueError ( f \"Value name can't start with '_': { item } \" ) if item in INVALID_VALUE_NAMES : raise ValueError ( f \"Invalid value name ' { item } '.\" ) if isinstance ( value , Value ): slot = self . _registry . register_alias ( value ) elif isinstance ( value , ValueSlot ): slot = value else : raise TypeError ( f \"Invalid type: ' { type ( value ) } '\" ) _value_slots [ item ] = slot self . _value_slots : typing . Dict [ str , ValueSlot ] = _value_slots ValueSet ( MutableMapping , Generic ) \u00b6 Source code in kiara/data/values/value_set.py class ValueSet ( typing . MutableMapping [ str , \"Value\" ]): def __init__ ( self , read_only : bool , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): # from kiara.data.values import ValueSchema # schema = ValueSchema(type=\"any\", default=None, doc=\"-- n/a --\") # self._schema = schema if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : \"Kiara\" = kiara self . _read_only : bool = read_only if title is None : title = \"-- n/a --\" self . _title = title @abc . abstractmethod def get_all_field_names ( self ) -> typing . Iterable [ str ]: pass @abc . abstractmethod def _get_value_obj ( self , field_name : str ): pass @abc . abstractmethod def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: pass def get_value_obj ( self , field_name : str , ensure_metadata : typing . Union [ bool , typing . Iterable [ str ], str ] = False , ) -> Value : if field_name not in list ( self . get_all_field_names ()): raise KeyError ( f \"Field ' { field_name } ' not available in value set. Available fields: { ', ' . join ( self . get_all_field_names ()) } \" ) obj : Value = self . _get_value_obj ( field_name ) if ensure_metadata : if isinstance ( ensure_metadata , bool ): obj . get_metadata () elif isinstance ( ensure_metadata , str ): obj . get_metadata ( ensure_metadata ) elif isinstance ( ensure_metadata , typing . Iterable ): obj . get_metadata ( * ensure_metadata ) else : raise ValueError ( f \"Invalid type ' { type ( ensure_metadata ) } ' for 'ensure_metadata' argument.\" ) return obj def set_value ( self , key : str , value : typing . Any , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> Value : result = self . set_values ( metadata = metadata , lineage = lineage , ** { key : value }) if isinstance ( result [ key ], Exception ): raise result [ key ] # type: ignore return result [ key ] # type: ignore def set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ Value , Exception ]]: \"\"\"Batch set several values. If metadata is provided, it is added to all values. \"\"\" if self . is_read_only (): raise Exception ( \"Can't set values: this value set is read-only.\" ) if not values : return {} invalid : typing . List [ str ] = [] for k in values . keys (): if k not in self . get_all_field_names (): invalid . append ( k ) if invalid : raise ValueError ( f \"No field(s) with name(s) { ', ' . join ( invalid ) } available, valid names: { ', ' . join ( self . get_all_field_names ()) } \" ) resolved_values = {} for field_name , data in values . items (): if isinstance ( data , str ) and data . startswith ( \"value:\" ): v = self . _kiara . get_value ( data ) resolved_values [ field_name ] = v else : resolved_values [ field_name ] = data try : value_set_result = self . _set_values ( metadata = metadata , lineage = lineage , ** resolved_values ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () raise e result : typing . Dict [ str , typing . Union [ Value , Exception ]] = {} for field in values . keys (): if isinstance ( value_set_result [ field ], Exception ): result [ field ] = value_set_result [ field ] # type: ignore else : result [ field ] = self . get_value_obj ( field ) return result def is_read_only ( self ): return self . _read_only def items_are_valid ( self ) -> bool : return check_valueset_valid ( self ) # for field_name in self.get_all_field_names(): # item = self.get_value_obj(field_name) # if not item.item_is_valid(): # return False # return True def check_invalid ( self ) -> typing . Optional [ typing . Dict [ str , str ]]: \"\"\"Check whether the value set is invalid, if it is, return a description of what's wrong.\"\"\" invalid = {} for field_name , item in self . items (): if not item . item_is_valid (): if item . value_schema . is_required (): if not item . is_set : msg = \"not set\" elif item . is_none : msg = \"no value\" else : msg = \"n/a\" else : msg = \"n/a\" invalid [ field_name ] = msg return invalid def get_all_value_objects ( self ) -> typing . Mapping [ str , typing . Any ]: return { fn : self . get_value_obj ( fn ) for fn in self . get_all_field_names ()} def get_value_data_for_fields ( self , * field_names : str , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). \"\"\" result : typing . Dict [ str , typing . Any ] = {} unset : typing . List [ str ] = [] for k in field_names : v = self . get_value_obj ( k ) if not v . is_set : if raise_exception_when_unset : unset . append ( k ) else : result [ k ] = None else : data = v . get_value_data () result [ k ] = data if unset : raise Exception ( f \"Can't get data for fields, one or several of the requested fields are not set yet: { ', ' . join ( unset ) } .\" ) return result def get_value_data ( self , field_name : str , raise_exception_when_unset : bool = False ) -> typing . Any : return self . get_value_data_for_fields ( field_name , raise_exception_when_unset = raise_exception_when_unset )[ field_name ] def get_all_value_data ( self , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: return self . get_value_data_for_fields ( * self . get_all_field_names (), raise_exception_when_unset = raise_exception_when_unset , ) def save_all ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None ): if aliases : aliases = set ( aliases ) for k , v in self . items (): field_aliases = None if aliases : field_aliases = [ f \" { a } __ { k } \" for a in aliases ] v . save ( aliases = field_aliases ) def __getitem__ ( self , item : str ) -> \"Value\" : return self . get_value_obj ( item ) def __setitem__ ( self , key : str , value ): self . set_value ( key , value ) def __delitem__ ( self , key : str ): raise Exception ( f \"Removing items not supported: { key } \" ) def __iter__ ( self ) -> typing . Iterator [ str ]: return iter ( self . get_all_field_names ()) def __len__ ( self ): return len ( list ( self . get_all_field_names ())) def to_details ( self , ensure_metadata : bool = False ) -> \"PipelineValuesInfo\" : from kiara.pipeline import PipelineValueInfo , PipelineValuesInfo result = {} for name in self . get_all_field_names (): item = self . get_value_obj ( name ) result [ name ] = PipelineValueInfo . from_value_obj ( item , ensure_metadata = ensure_metadata ) return PipelineValuesInfo ( values = result ) def _create_rich_table ( self , show_headers : bool = True , ensure_metadata : bool = False ) -> Table : table = Table ( box = box . SIMPLE , show_header = show_headers ) table . add_column ( \"Field name\" , style = \"i\" ) table . add_column ( \"Type\" ) table . add_column ( \"Description\" ) table . add_column ( \"Required\" ) table . add_column ( \"Is set\" ) for k in self . get_all_field_names (): v = self . get_value_obj ( k , ensure_metadata = ensure_metadata ) t = v . value_schema . type desc = v . value_schema . doc if not v . value_schema . is_required (): req = \"no\" else : if ( v . value_schema . default and v . value_schema . default != SpecialValue . NO_VALUE and v . value_schema . default != SpecialValue . NOT_SET ): req = \"no\" else : req = \"[red]yes[/red]\" is_set = \"yes\" if v . item_is_valid () else \"no\" table . add_row ( k , t , desc , req , is_set ) return table def __repr__ ( self ): title_str = \"\" if self . _title : title_str = f \" title=' { self . _title } '\" return f \" { self . __class__ . __name__ } (field_names= { list ( self . keys ()) }{ title_str } )\" def __str__ ( self ): return self . __repr__ () def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : title = self . _title if title : postfix = f \" [b] { title } [/b]\" else : postfix = \"\" yield Panel ( self . _create_rich_table ( show_headers = True ), box = box . ROUNDED , title_align = \"left\" , title = f \"Value-Set: { postfix } \" , ) check_invalid ( self ) \u00b6 Check whether the value set is invalid, if it is, return a description of what's wrong. Source code in kiara/data/values/value_set.py def check_invalid ( self ) -> typing . Optional [ typing . Dict [ str , str ]]: \"\"\"Check whether the value set is invalid, if it is, return a description of what's wrong.\"\"\" invalid = {} for field_name , item in self . items (): if not item . item_is_valid (): if item . value_schema . is_required (): if not item . is_set : msg = \"not set\" elif item . is_none : msg = \"no value\" else : msg = \"n/a\" else : msg = \"n/a\" invalid [ field_name ] = msg return invalid get_value_data_for_fields ( self , * field_names , * , raise_exception_when_unset = False ) \u00b6 Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). Source code in kiara/data/values/value_set.py def get_value_data_for_fields ( self , * field_names : str , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). \"\"\" result : typing . Dict [ str , typing . Any ] = {} unset : typing . List [ str ] = [] for k in field_names : v = self . get_value_obj ( k ) if not v . is_set : if raise_exception_when_unset : unset . append ( k ) else : result [ k ] = None else : data = v . get_value_data () result [ k ] = data if unset : raise Exception ( f \"Can't get data for fields, one or several of the requested fields are not set yet: { ', ' . join ( unset ) } .\" ) return result set_values ( self , metadata = None , lineage = None , ** values ) \u00b6 Batch set several values. If metadata is provided, it is added to all values. Source code in kiara/data/values/value_set.py def set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ Value , Exception ]]: \"\"\"Batch set several values. If metadata is provided, it is added to all values. \"\"\" if self . is_read_only (): raise Exception ( \"Can't set values: this value set is read-only.\" ) if not values : return {} invalid : typing . List [ str ] = [] for k in values . keys (): if k not in self . get_all_field_names (): invalid . append ( k ) if invalid : raise ValueError ( f \"No field(s) with name(s) { ', ' . join ( invalid ) } available, valid names: { ', ' . join ( self . get_all_field_names ()) } \" ) resolved_values = {} for field_name , data in values . items (): if isinstance ( data , str ) and data . startswith ( \"value:\" ): v = self . _kiara . get_value ( data ) resolved_values [ field_name ] = v else : resolved_values [ field_name ] = data try : value_set_result = self . _set_values ( metadata = metadata , lineage = lineage , ** resolved_values ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () raise e result : typing . Dict [ str , typing . Union [ Value , Exception ]] = {} for field in values . keys (): if isinstance ( value_set_result [ field ], Exception ): result [ field ] = value_set_result [ field ] # type: ignore else : result [ field ] = self . get_value_obj ( field ) return result","title":"value_set"},{"location":"reference/kiara/data/values/value_set/#kiara.data.values.value_set.SlottedValueSet","text":"Source code in kiara/data/values/value_set.py class SlottedValueSet ( ValueSet ): @classmethod def from_schemas ( cls , schemas : typing . Mapping [ str , \"ValueSchema\" ], read_only : bool = True , check_for_sameness = True , initial_values : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , title : typing . Optional [ str ] = None , default_value : typing . Any = SpecialValue . NO_VALUE , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ) -> \"SlottedValueSet\" : if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () if registry is None : registry = kiara . data_registry values = {} for field_name , schema in schemas . items (): def_val = default_value if callable ( default_value ): def_val = default_value () _init_value = def_val if initial_values and initial_values . get ( field_name , None ) is not None : _init_value = initial_values [ field_name ] if not isinstance ( _init_value , Value ): value : Value = registry . register_data ( value_data = _init_value , value_schema = schema ) # value: Value = Value(value_schema=schema, value_data=_init_value, registry=registry) # type: ignore else : value = _init_value values [ field_name ] = value return cls ( items = values , title = title , read_only = read_only , check_for_sameness = check_for_sameness , kiara = kiara , registry = registry , ) def __init__ ( self , items : typing . Mapping [ str , typing . Union [ \"ValueSlot\" , \"Value\" ]], read_only : bool , check_for_sameness : bool = False , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ): \"\"\"A `ValueSet` implementation that keeps a history of each fields value. Arguments: items: the value slots read_only: whether it is allowed to set new values to fields in this set check_for_sameness: whether a check should be performed that checks for equality of the new and old values, if equal, skip the update title: An optional title for this value set kiara: the kiara context registry: the registry to use to register the values to \"\"\" if not items : raise ValueError ( \"Can't create ValueSet: no values provided\" ) self . _check_for_sameness : bool = check_for_sameness super () . __init__ ( read_only = read_only , title = title , kiara = kiara ) if registry is None : registry = self . _kiara . data_registry self . _registry : DataRegistry = registry _value_slots : typing . Dict [ str , ValueSlot ] = {} for item , value in items . items (): if value is None : raise Exception ( f \"Can't create value set, item ' { item } ' does not have a value (yet).\" ) if item . startswith ( \"_\" ): raise ValueError ( f \"Value name can't start with '_': { item } \" ) if item in INVALID_VALUE_NAMES : raise ValueError ( f \"Invalid value name ' { item } '.\" ) if isinstance ( value , Value ): slot = self . _registry . register_alias ( value ) elif isinstance ( value , ValueSlot ): slot = value else : raise TypeError ( f \"Invalid type: ' { type ( value ) } '\" ) _value_slots [ item ] = slot self . _value_slots : typing . Dict [ str , ValueSlot ] = _value_slots def get_all_field_names ( self ) -> typing . Iterable [ str ]: return self . _value_slots . keys () def _get_value_obj ( self , field_name : str ) -> \"Value\" : slot = self . _value_slots [ field_name ] return slot . get_latest_value () def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: # we want to set all registry-type values seperately, in one go, because it's more efficient registries : typing . Dict [ DataRegistry , typing . Dict [ ValueSlot , typing . Union [ \"Value\" , typing . Any ]] ] = {} field_slot_map : typing . Dict [ ValueSlot , str ] = {} result : typing . Dict [ str , typing . Union [ bool , Exception ]] = {} for field_name , value_or_data in values . items (): value_slot : ValueSlot = self . _value_slots [ field_name ] if not metadata and self . _check_for_sameness : latest_val = value_slot . get_latest_value () if isinstance ( value_or_data , Value ): if latest_val . id == value_or_data . id : result [ field_name ] = False continue else : if ( latest_val . is_set and latest_val . get_value_data () == value_or_data ): result [ field_name ] = False continue registries . setdefault ( value_slot . _registry , {})[ value_slot ] = value_or_data field_slot_map [ value_slot ] = field_name for registry , value_slots_details in registries . items (): _r = registry . update_value_slots ( value_slots_details , metadata = metadata , lineage = lineage ) # type: ignore for value_slot , details in _r . items (): result [ field_slot_map [ value_slot ]] = details return result","title":"SlottedValueSet"},{"location":"reference/kiara/data/values/value_set/#kiara.data.values.value_set.SlottedValueSet.__init__","text":"A ValueSet implementation that keeps a history of each fields value. Parameters: Name Type Description Default items Mapping[str, Union[ValueSlot, Value]] the value slots required read_only bool whether it is allowed to set new values to fields in this set required check_for_sameness bool whether a check should be performed that checks for equality of the new and old values, if equal, skip the update False title Optional[str] An optional title for this value set None kiara Optional[Kiara] the kiara context None registry Optional[kiara.data.registry.DataRegistry] the registry to use to register the values to None Source code in kiara/data/values/value_set.py def __init__ ( self , items : typing . Mapping [ str , typing . Union [ \"ValueSlot\" , \"Value\" ]], read_only : bool , check_for_sameness : bool = False , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , registry : typing . Optional [ DataRegistry ] = None , ): \"\"\"A `ValueSet` implementation that keeps a history of each fields value. Arguments: items: the value slots read_only: whether it is allowed to set new values to fields in this set check_for_sameness: whether a check should be performed that checks for equality of the new and old values, if equal, skip the update title: An optional title for this value set kiara: the kiara context registry: the registry to use to register the values to \"\"\" if not items : raise ValueError ( \"Can't create ValueSet: no values provided\" ) self . _check_for_sameness : bool = check_for_sameness super () . __init__ ( read_only = read_only , title = title , kiara = kiara ) if registry is None : registry = self . _kiara . data_registry self . _registry : DataRegistry = registry _value_slots : typing . Dict [ str , ValueSlot ] = {} for item , value in items . items (): if value is None : raise Exception ( f \"Can't create value set, item ' { item } ' does not have a value (yet).\" ) if item . startswith ( \"_\" ): raise ValueError ( f \"Value name can't start with '_': { item } \" ) if item in INVALID_VALUE_NAMES : raise ValueError ( f \"Invalid value name ' { item } '.\" ) if isinstance ( value , Value ): slot = self . _registry . register_alias ( value ) elif isinstance ( value , ValueSlot ): slot = value else : raise TypeError ( f \"Invalid type: ' { type ( value ) } '\" ) _value_slots [ item ] = slot self . _value_slots : typing . Dict [ str , ValueSlot ] = _value_slots","title":"__init__()"},{"location":"reference/kiara/data/values/value_set/#kiara.data.values.value_set.ValueSet","text":"Source code in kiara/data/values/value_set.py class ValueSet ( typing . MutableMapping [ str , \"Value\" ]): def __init__ ( self , read_only : bool , title : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): # from kiara.data.values import ValueSchema # schema = ValueSchema(type=\"any\", default=None, doc=\"-- n/a --\") # self._schema = schema if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : \"Kiara\" = kiara self . _read_only : bool = read_only if title is None : title = \"-- n/a --\" self . _title = title @abc . abstractmethod def get_all_field_names ( self ) -> typing . Iterable [ str ]: pass @abc . abstractmethod def _get_value_obj ( self , field_name : str ): pass @abc . abstractmethod def _set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ bool , Exception ]]: pass def get_value_obj ( self , field_name : str , ensure_metadata : typing . Union [ bool , typing . Iterable [ str ], str ] = False , ) -> Value : if field_name not in list ( self . get_all_field_names ()): raise KeyError ( f \"Field ' { field_name } ' not available in value set. Available fields: { ', ' . join ( self . get_all_field_names ()) } \" ) obj : Value = self . _get_value_obj ( field_name ) if ensure_metadata : if isinstance ( ensure_metadata , bool ): obj . get_metadata () elif isinstance ( ensure_metadata , str ): obj . get_metadata ( ensure_metadata ) elif isinstance ( ensure_metadata , typing . Iterable ): obj . get_metadata ( * ensure_metadata ) else : raise ValueError ( f \"Invalid type ' { type ( ensure_metadata ) } ' for 'ensure_metadata' argument.\" ) return obj def set_value ( self , key : str , value : typing . Any , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ) -> Value : result = self . set_values ( metadata = metadata , lineage = lineage , ** { key : value }) if isinstance ( result [ key ], Exception ): raise result [ key ] # type: ignore return result [ key ] # type: ignore def set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ Value , Exception ]]: \"\"\"Batch set several values. If metadata is provided, it is added to all values. \"\"\" if self . is_read_only (): raise Exception ( \"Can't set values: this value set is read-only.\" ) if not values : return {} invalid : typing . List [ str ] = [] for k in values . keys (): if k not in self . get_all_field_names (): invalid . append ( k ) if invalid : raise ValueError ( f \"No field(s) with name(s) { ', ' . join ( invalid ) } available, valid names: { ', ' . join ( self . get_all_field_names ()) } \" ) resolved_values = {} for field_name , data in values . items (): if isinstance ( data , str ) and data . startswith ( \"value:\" ): v = self . _kiara . get_value ( data ) resolved_values [ field_name ] = v else : resolved_values [ field_name ] = data try : value_set_result = self . _set_values ( metadata = metadata , lineage = lineage , ** resolved_values ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () raise e result : typing . Dict [ str , typing . Union [ Value , Exception ]] = {} for field in values . keys (): if isinstance ( value_set_result [ field ], Exception ): result [ field ] = value_set_result [ field ] # type: ignore else : result [ field ] = self . get_value_obj ( field ) return result def is_read_only ( self ): return self . _read_only def items_are_valid ( self ) -> bool : return check_valueset_valid ( self ) # for field_name in self.get_all_field_names(): # item = self.get_value_obj(field_name) # if not item.item_is_valid(): # return False # return True def check_invalid ( self ) -> typing . Optional [ typing . Dict [ str , str ]]: \"\"\"Check whether the value set is invalid, if it is, return a description of what's wrong.\"\"\" invalid = {} for field_name , item in self . items (): if not item . item_is_valid (): if item . value_schema . is_required (): if not item . is_set : msg = \"not set\" elif item . is_none : msg = \"no value\" else : msg = \"n/a\" else : msg = \"n/a\" invalid [ field_name ] = msg return invalid def get_all_value_objects ( self ) -> typing . Mapping [ str , typing . Any ]: return { fn : self . get_value_obj ( fn ) for fn in self . get_all_field_names ()} def get_value_data_for_fields ( self , * field_names : str , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). \"\"\" result : typing . Dict [ str , typing . Any ] = {} unset : typing . List [ str ] = [] for k in field_names : v = self . get_value_obj ( k ) if not v . is_set : if raise_exception_when_unset : unset . append ( k ) else : result [ k ] = None else : data = v . get_value_data () result [ k ] = data if unset : raise Exception ( f \"Can't get data for fields, one or several of the requested fields are not set yet: { ', ' . join ( unset ) } .\" ) return result def get_value_data ( self , field_name : str , raise_exception_when_unset : bool = False ) -> typing . Any : return self . get_value_data_for_fields ( field_name , raise_exception_when_unset = raise_exception_when_unset )[ field_name ] def get_all_value_data ( self , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: return self . get_value_data_for_fields ( * self . get_all_field_names (), raise_exception_when_unset = raise_exception_when_unset , ) def save_all ( self , aliases : typing . Optional [ typing . Iterable [ str ]] = None ): if aliases : aliases = set ( aliases ) for k , v in self . items (): field_aliases = None if aliases : field_aliases = [ f \" { a } __ { k } \" for a in aliases ] v . save ( aliases = field_aliases ) def __getitem__ ( self , item : str ) -> \"Value\" : return self . get_value_obj ( item ) def __setitem__ ( self , key : str , value ): self . set_value ( key , value ) def __delitem__ ( self , key : str ): raise Exception ( f \"Removing items not supported: { key } \" ) def __iter__ ( self ) -> typing . Iterator [ str ]: return iter ( self . get_all_field_names ()) def __len__ ( self ): return len ( list ( self . get_all_field_names ())) def to_details ( self , ensure_metadata : bool = False ) -> \"PipelineValuesInfo\" : from kiara.pipeline import PipelineValueInfo , PipelineValuesInfo result = {} for name in self . get_all_field_names (): item = self . get_value_obj ( name ) result [ name ] = PipelineValueInfo . from_value_obj ( item , ensure_metadata = ensure_metadata ) return PipelineValuesInfo ( values = result ) def _create_rich_table ( self , show_headers : bool = True , ensure_metadata : bool = False ) -> Table : table = Table ( box = box . SIMPLE , show_header = show_headers ) table . add_column ( \"Field name\" , style = \"i\" ) table . add_column ( \"Type\" ) table . add_column ( \"Description\" ) table . add_column ( \"Required\" ) table . add_column ( \"Is set\" ) for k in self . get_all_field_names (): v = self . get_value_obj ( k , ensure_metadata = ensure_metadata ) t = v . value_schema . type desc = v . value_schema . doc if not v . value_schema . is_required (): req = \"no\" else : if ( v . value_schema . default and v . value_schema . default != SpecialValue . NO_VALUE and v . value_schema . default != SpecialValue . NOT_SET ): req = \"no\" else : req = \"[red]yes[/red]\" is_set = \"yes\" if v . item_is_valid () else \"no\" table . add_row ( k , t , desc , req , is_set ) return table def __repr__ ( self ): title_str = \"\" if self . _title : title_str = f \" title=' { self . _title } '\" return f \" { self . __class__ . __name__ } (field_names= { list ( self . keys ()) }{ title_str } )\" def __str__ ( self ): return self . __repr__ () def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : title = self . _title if title : postfix = f \" [b] { title } [/b]\" else : postfix = \"\" yield Panel ( self . _create_rich_table ( show_headers = True ), box = box . ROUNDED , title_align = \"left\" , title = f \"Value-Set: { postfix } \" , )","title":"ValueSet"},{"location":"reference/kiara/data/values/value_set/#kiara.data.values.value_set.ValueSet.check_invalid","text":"Check whether the value set is invalid, if it is, return a description of what's wrong. Source code in kiara/data/values/value_set.py def check_invalid ( self ) -> typing . Optional [ typing . Dict [ str , str ]]: \"\"\"Check whether the value set is invalid, if it is, return a description of what's wrong.\"\"\" invalid = {} for field_name , item in self . items (): if not item . item_is_valid (): if item . value_schema . is_required (): if not item . is_set : msg = \"not set\" elif item . is_none : msg = \"no value\" else : msg = \"n/a\" else : msg = \"n/a\" invalid [ field_name ] = msg return invalid","title":"check_invalid()"},{"location":"reference/kiara/data/values/value_set/#kiara.data.values.value_set.ValueSet.get_value_data_for_fields","text":"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). Source code in kiara/data/values/value_set.py def get_value_data_for_fields ( self , * field_names : str , raise_exception_when_unset : bool = False ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the data for a one or several fields of this ValueSet. If a value is unset, by default 'None' is returned for it. Unless 'raise_exception_when_unset' is set to 'True', in which case an Exception will be raised (obviously). \"\"\" result : typing . Dict [ str , typing . Any ] = {} unset : typing . List [ str ] = [] for k in field_names : v = self . get_value_obj ( k ) if not v . is_set : if raise_exception_when_unset : unset . append ( k ) else : result [ k ] = None else : data = v . get_value_data () result [ k ] = data if unset : raise Exception ( f \"Can't get data for fields, one or several of the requested fields are not set yet: { ', ' . join ( unset ) } .\" ) return result","title":"get_value_data_for_fields()"},{"location":"reference/kiara/data/values/value_set/#kiara.data.values.value_set.ValueSet.set_values","text":"Batch set several values. If metadata is provided, it is added to all values. Source code in kiara/data/values/value_set.py def set_values ( self , metadata : typing . Optional [ typing . Mapping [ str , MetadataModel ]] = None , lineage : typing . Optional [ ValueLineage ] = None , ** values : typing . Any , ) -> typing . Mapping [ str , typing . Union [ Value , Exception ]]: \"\"\"Batch set several values. If metadata is provided, it is added to all values. \"\"\" if self . is_read_only (): raise Exception ( \"Can't set values: this value set is read-only.\" ) if not values : return {} invalid : typing . List [ str ] = [] for k in values . keys (): if k not in self . get_all_field_names (): invalid . append ( k ) if invalid : raise ValueError ( f \"No field(s) with name(s) { ', ' . join ( invalid ) } available, valid names: { ', ' . join ( self . get_all_field_names ()) } \" ) resolved_values = {} for field_name , data in values . items (): if isinstance ( data , str ) and data . startswith ( \"value:\" ): v = self . _kiara . get_value ( data ) resolved_values [ field_name ] = v else : resolved_values [ field_name ] = data try : value_set_result = self . _set_values ( metadata = metadata , lineage = lineage , ** resolved_values ) except Exception as e : log_message ( str ( e )) if is_debug (): import traceback traceback . print_exc () raise e result : typing . Dict [ str , typing . Union [ Value , Exception ]] = {} for field in values . keys (): if isinstance ( value_set_result [ field ], Exception ): result [ field ] = value_set_result [ field ] # type: ignore else : result [ field ] = self . get_value_obj ( field ) return result","title":"set_values()"},{"location":"reference/kiara/doc/__init__/","text":"Main module for code that helps with documentation auto-generation in supported projects. FrklDocumentationPlugin ( BasePlugin ) \u00b6 mkdocs plugin to render API documentation for a project. To add to a project, add this to the 'plugins' section of a mkdocs config file: - frkl-docgen : main_module : \"module_name\" This will add an API reference navigation item to your page navigation, with auto-generated entries for every Python module in your package. Source code in kiara/doc/__init__.py class FrklDocumentationPlugin ( BasePlugin ): \"\"\"[mkdocs](https://www.mkdocs.org/) plugin to render API documentation for a project. To add to a project, add this to the 'plugins' section of a mkdocs config file: ```yaml - frkl-docgen: main_module: \"module_name\" ``` This will add an ``API reference`` navigation item to your page navigation, with auto-generated entries for every Python module in your package. \"\"\" config_scheme = (( \"main_module\" , mkdocs . config . config_options . Type ( str )),) def __init__ ( self ): self . _doc_paths = None self . _dir = tempfile . TemporaryDirectory ( prefix = \"frkl_doc_gen_\" ) self . _doc_files = None super () . __init__ () def on_files ( self , files : Files , config : Config ) -> Files : self . _doc_paths = gen_pages_for_module ( self . config [ \"main_module\" ]) self . _doc_files = {} for k in sorted ( self . _doc_paths , key = lambda x : os . path . splitext ( x )[ 0 ]): content = self . _doc_paths [ k ][ \"content\" ] _file = File ( k , src_dir = self . _dir . name , dest_dir = config [ \"site_dir\" ], use_directory_urls = config [ \"use_directory_urls\" ], ) os . makedirs ( os . path . dirname ( _file . abs_src_path ), exist_ok = True ) with open ( _file . abs_src_path , \"w\" ) as f : f . write ( content ) self . _doc_files [ k ] = _file files . append ( _file ) return files def on_page_content ( self , html , page : Page , config : Config , files : Files ): repo_url = config . get ( \"repo_url\" , None ) python_src = config . get ( \"edit_uri\" , None ) if page . file . src_path in self . _doc_paths . keys (): src_path = self . _doc_paths . get ( page . file . src_path )[ \"python_src\" ][ \"rel_path\" ] rel_base = urllib . parse . urljoin ( repo_url , f \" { python_src } /../src/ { src_path } \" ) page . edit_url = rel_base return html def on_nav ( self , nav : Navigation , config : Config , files : Files ): for item in nav . items : if item . title and \"Api reference\" in item . title : return nav pages = [] for _file in self . _doc_files . values (): pages . append ( _file . page ) section = Section ( title = \"API reference\" , children = pages ) nav . items . append ( section ) nav . pages . extend ( pages ) _add_previous_and_next_links ( nav . pages ) _add_parent_links ( nav . items ) return nav def on_post_build ( self , config : Config ): self . _dir . cleanup () gen_info_pages \u00b6 render_item_listing ( kiara , item_type , limit_to_package = None ) \u00b6 Render an item listing summary page, for: https://oprypin.github.io/mkdocs-literate-nav/ This code is a terrible, terrible mess, but I just don't care enough. If the output it produces is wrong, it'll be obvious in the documentation (hopefully). I'd have to spend considerable time cleaning this up, and at the moment it does not seem worth it. Source code in kiara/doc/gen_info_pages.py def render_item_listing ( kiara : Kiara , item_type : str , limit_to_package : typing . Optional [ str ] = None ) -> typing . Optional [ str ]: \"\"\"Render an item listing summary page, for: https://oprypin.github.io/mkdocs-literate-nav/ This code is a terrible, terrible mess, but I just don't care enough. If the output it produces is wrong, it'll be obvious in the documentation (hopefully). I'd have to spend considerable time cleaning this up, and at the moment it does not seem worth it. \"\"\" info : KiaraInfoModel = KiaraContext . get_info ( kiara = kiara ) tree = info . get_subcomponent_tree () if tree is None : raise Exception ( \"Can't render item listing, no subcomponent tree available.\" ) def extract_cls_from_kiara_module_type_metadata ( obj ): return obj . python_class . get_class () def extract_cls_from_operation ( obj ): return obj . module . __class__ def extract_cls_from_op_type ( obj ): return obj . python_class . get_class () def extract_cls_from_value_type ( obj ): return obj . python_class . get_class () item_type_map : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { \"module\" : { \"cls\" : KiaraModuleTypeMetadata , \"extract\" : extract_cls_from_kiara_module_type_metadata , }, \"value_type\" : { \"cls\" : ValueTypeMetadata , \"extract\" : extract_cls_from_value_type , }, \"operation\" : { \"cls\" : Operation , \"extract\" : extract_cls_from_operation }, \"operation_type\" : { \"cls\" : OperationsMetadata , \"extract\" : extract_cls_from_op_type , }, } item_cls = item_type_map [ item_type ][ \"cls\" ] plural = f \" { item_type } s\" item_summaries : typing . Dict [ typing . Tuple , typing . List ] = {} list_template = get_jina_env () . get_template ( f \" { item_type } _list.md.j2\" ) for node in tree . nodes (): tokens = node . split ( \".\" ) if len ( tokens ) < 3 : continue category = tokens [ 1 ] obj = tree . nodes [ node ][ \"obj\" ] path_tokens = tokens [ 2 :] if category == plural : if isinstance ( obj , item_cls ): # type: ignore if limit_to_package : cls = item_type_map [ item_type ][ \"extract\" ]( obj ) md = ContextMetadataModel . from_class ( cls ) if md . labels . get ( \"package\" , None ) != limit_to_package : continue item_summaries . setdefault ( tuple ( path_tokens ), []) . append ( obj ) elif isinstance ( obj , KiaraDynamicInfoModel ): item_summaries . setdefault ( tuple ( path_tokens ), []) new_summary : typing . Dict [ typing . Tuple , typing . List ] = {} no_childs = [] for path_tokens , items in item_summaries . items (): # if len(path_tokens) == 1: # new_summary[path_tokens] = item_summaries[path_tokens] # full_path = [path_tokens[0]] full_path : typing . List [ str ] = [] collect : typing . Optional [ typing . List [ str ]] = None for token in path_tokens : if collect is not None : collect . append ( token ) t = tuple ( full_path + collect ) if not item_summaries [ t ]: continue else : new_summary . setdefault ( tuple ( full_path ), []) . extend ( item_summaries [ t ] ) else : full_path . append ( token ) t = tuple ( full_path ) if item_summaries [ t ]: new_summary [ t ] = item_summaries [ t ] else : match = False any_childs = False for k in item_summaries . keys (): if len ( full_path ) == 1 and full_path [ 0 ] == k [ 0 ] and len ( k ) > 2 : match = True if ( len ( k ) > 1 and len ( t ) <= len ( k ) and k [ 0 : len ( t )] == t # noqa and item_summaries [ k ] ): any_childs = True if not any_childs : no_childs . append ( t ) if not match : new_summary . setdefault ( t , []) collect = [] else : new_summary . setdefault ( t , []) . extend ( item_summaries [ t ]) main_summary = [] for summary_path , items in new_summary . items (): if not summary_path : continue match = False for nc in no_childs : if len ( summary_path ) >= len ( nc ): if summary_path [ 0 : len ( nc )] == nc : # noqa match = True break if match : continue padding = \" \" * len ( summary_path ) path = os . path . join ( * summary_path ) rendered = list_template . render ( ** { \"path\" : path , plural : items }) p_write = os . path . join ( plural , path , \"index.md\" ) p_index = os . path . join ( path , \"index.md\" ) with mkdocs_gen_files . open ( p_write , \"w\" ) as f : f . write ( rendered ) main_summary . append ( f \" { padding } * [ { summary_path [ - 1 ] } ]( { p_index } )\" ) modules_content = \"xxxxxxx\" with mkdocs_gen_files . open ( f \" { plural } /index.md\" , \"w\" ) as f : f . write ( modules_content ) summary_content = \" \\n \" . join ( main_summary ) summary_page = f \" { plural } /SUMMARY.md\" with mkdocs_gen_files . open ( summary_page , \"w\" ) as f : f . write ( summary_content ) return f \" { plural } /\" generate_api_doc \u00b6 gen_pages_for_module ( module , prefix = 'api_reference' ) \u00b6 Generate modules for a set of modules (using the mkdocstring package. Source code in kiara/doc/generate_api_doc.py def gen_pages_for_module ( module : typing . Union [ str , ModuleType ], prefix : str = \"api_reference\" ): \"\"\"Generate modules for a set of modules (using the [mkdocstring](https://github.com/mkdocstrings/mkdocstrings) package.\"\"\" result = {} modules_info = get_source_tree ( module ) for module_name , path in modules_info . items (): page_name = module_name if page_name . endswith ( \"__init__\" ): page_name = page_name [ 0 : - 9 ] if page_name . endswith ( \"._frkl\" ): continue doc_path = f \" { prefix }{ os . path . sep }{ page_name } .md\" p = Path ( \"..\" , path [ \"abs_path\" ]) if not p . read_text () . strip (): continue main_module = path [ \"main_module\" ] if page_name == main_module : title = page_name else : title = page_name . replace ( f \" { main_module } .\" , \"\u279c\u2007\" ) result [ doc_path ] = { \"python_src\" : path , \"content\" : f \"--- \\n title: { title } \\n --- \\n # { page_name } \\n\\n ::: { module_name } \" , } return result get_source_tree ( module ) \u00b6 Find all python source files for a module. Source code in kiara/doc/generate_api_doc.py def get_source_tree ( module : typing . Union [ str , ModuleType ]): \"\"\"Find all python source files for a module.\"\"\" if isinstance ( module , str ): module = importlib . import_module ( module ) if not isinstance ( module , ModuleType ): raise TypeError ( f \"Invalid type ' { type ( module ) } ', input needs to be a string or module.\" ) module_file = module . __file__ assert module_file is not None module_root = os . path . dirname ( module_file ) module_name = module . __name__ src = {} for path in Path ( module_root ) . glob ( \"**/*.py\" ): rel = os . path . relpath ( path , module_root ) mod_name = f \" { module_name } . { rel [ 0 : - 3 ] . replace ( os . path . sep , '.' ) } \" rel_path = f \" { module_name }{ os . path . sep }{ rel } \" src [ mod_name ] = { \"rel_path\" : rel_path , \"abs_path\" : path , \"main_module\" : module_name , } return src mkdocs_macros_cli \u00b6 define_env ( env ) \u00b6 Helper macros for Python project documentation. Currently, those macros are available (check the source code for more details): cli \u00b6 Execute a command on the command-line, capture the output and return it to be used in a documentation page. inline_file_as_codeblock \u00b6 Read an external file, and return its content as a markdown code block. Source code in kiara/doc/mkdocs_macros_cli.py def define_env ( env ): \"\"\" Helper macros for Python project documentation. Currently, those macros are available (check the source code for more details): ## ``cli`` Execute a command on the command-line, capture the output and return it to be used in a documentation page. ## ``inline_file_as_codeblock`` Read an external file, and return its content as a markdown code block. \"\"\" # env.variables[\"baz\"] = \"John Doe\" @env . macro def cli ( * command , print_command : bool = True , code_block : bool = True , split_command_and_output : bool = True , max_height : Optional [ int ] = None , cache_key : Optional [ str ] = None , extra_env : Optional [ Dict [ str , str ]] = None , fake_command : Optional [ str ] = None , ): \"\"\"Execute the provided command, save the output and return it to be used in documentation modules.\"\"\" hashes = DeepHash ( command ) hash_str = hashes [ command ] hashes_env = DeepHash ( extra_env ) hashes_env_str = hashes_env [ extra_env ] hash_str = hash_str + \"_\" + hashes_env_str if cache_key : hash_str = hash_str + \"_\" + cache_key cache_file : Path = Path ( os . path . join ( CACHE_DIR , str ( hash_str ))) _run_env = dict ( os_env_vars ) if extra_env : _run_env . update ( extra_env ) if cache_file . is_file (): stdout = cache_file . read_text () else : try : print ( f \"RUNNING: { ' ' . join ( command ) } \" ) result = subprocess . check_output ( command , env = _run_env ) stdout = result . decode () cache_file . write_text ( stdout ) except subprocess . CalledProcessError as e : stdout = f \"Error: { e } \\n\\n Stdout: { e . stdout } \\n\\n Stderr: { e . stderr } \" print ( \"stdout:\" ) print ( e . stdout ) print ( \"stderr:\" ) print ( e . stderr ) if os . getenv ( \"FAIL_DOC_BUILD_ON_ERROR\" ) == \"true\" : sys . exit ( 1 ) if fake_command : command_str = fake_command else : command_str = \" \" . join ( command ) if split_command_and_output and print_command : _c = f \" \\n ``` console \\n { command_str } \\n ``` \\n \" _output = \"``` console \\n \" + stdout + \" \\n ``` \\n \" if max_height is not None and max_height > 0 : _output = f \"<div style='max-height: { max_height } px;overflow:auto'> \\n { _output } \\n </div>\" _stdout = _c + _output else : if print_command : _stdout = f \"> { command_str } \\n { stdout } \" if code_block : _stdout = \"``` console \\n \" + _stdout + \" \\n ``` \\n \" if max_height is not None and max_height > 0 : _stdout = f \"<div style='max-height: { max_height } px;overflow:auto'> \\n { _stdout } \\n </div>\" return _stdout @env . macro def inline_file_as_codeblock ( path , format : str = \"\" ): \"\"\"Import external file and return its content as a markdown code block.\"\"\" f = Path ( path ) return f \"``` { format } \\n { f . read_text () } \\n ```\" mkdocs_macros_kiara \u00b6 define_env ( env ) \u00b6 This is the hook for defining variables, macros and filters variables: the dictionary that contains the environment variables macro: a decorator function, to declare a macro. Source code in kiara/doc/mkdocs_macros_kiara.py def define_env ( env ): \"\"\" This is the hook for defining variables, macros and filters - variables: the dictionary that contains the environment variables - macro: a decorator function, to declare a macro. \"\"\" # env.variables[\"baz\"] = \"John Doe\" @env . macro def get_schema_for_model ( model_class : typing . Union [ str , typing . Type [ BaseModel ]]): if isinstance ( model_class , str ): _class : typing . Type [ BaseModel ] = locate ( model_class ) # type: ignore else : _class = model_class schema_json = _class . schema_json ( indent = 2 ) return schema_json @env . macro def get_src_of_object ( obj : typing . Union [ str , typing . Any ]): try : if isinstance ( obj , str ): _obj : typing . Type [ BaseModel ] = locate ( obj ) # type: ignore else : _obj = obj src = inspect . getsource ( _obj ) return src except Exception as e : return f \"Can't render object source: { str ( e ) } \" @env . macro def get_pipeline_config ( pipeline_name : str ): pmm = PipelineModuleManager () desc = pmm . pipeline_descs [ pipeline_name ][ \"data\" ] desc_str = yaml . dump ( desc ) return desc_str @env . macro def get_module_info ( module_type : str ): try : m_cls = Kiara . instance () . get_module_class ( module_type ) info = KiaraModuleTypeMetadata . from_module_class ( m_cls ) from rich.console import Console console = Console ( record = True ) console . print ( info ) html = console . export_text () return html except Exception as e : return f \"Can't render module info: { str ( e ) } \" @env . macro def get_module_list_for_package ( package_name : str , include_core_modules : bool = True , include_pipelines : bool = True , ): modules = kiara_obj . module_mgmt . find_modules_for_package ( package_name , include_core_modules = include_core_modules , include_pipelines = include_pipelines , ) result = [] for name , info in modules . items (): type_md = info . get_type_metadata () result . append ( f \"- [`` { name } ``][kiara_info.modules. { name } ]\" ) result . append ( f \" - [`` { name } ``]( { type_md . context . get_url_for_reference ( 'module_doc' ) } ): { type_md . documentation . description } \" ) print ( result ) return \" \\n \" . join ( result ) @env . macro def get_value_types_for_package ( package_name : str ): value_types = kiara_obj . type_mgmt . find_value_types_for_package ( package_name ) result = [] for name , info in value_types . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result ) @env . macro def get_metadata_models_for_package ( package_name : str ): metadata_schemas = kiara_obj . metadata_mgmt . find_all_models_for_package ( package_name ) result = [] for name , info in metadata_schemas . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result ) @env . macro def get_kiara_context () -> KiaraContext : return kiara_context on_post_build ( env ) \u00b6 Post-build actions Source code in kiara/doc/mkdocs_macros_kiara.py def on_post_build ( env ): \"Post-build actions\" site_dir = env . conf [ \"site_dir\" ] for category , classes in KIARA_MODEL_CLASSES . items (): for cls in classes : schema_json = cls . schema_json ( indent = 2 ) file_path = os . path . join ( site_dir , \"development\" , \"entities\" , category , f \" { cls . __name__ } .json\" ) os . makedirs ( os . path . dirname ( file_path ), exist_ok = True ) with open ( file_path , \"w\" ) as f : f . write ( schema_json ) mkdocstrings special \u00b6 collector \u00b6 KiaraCollector ( BaseCollector ) \u00b6 The class responsible for loading Jinja templates and rendering them. It defines some configuration options, implements the render method, and overrides the update_env method of the BaseRenderer class . Source code in kiara/doc/mkdocstrings/collector.py class KiaraCollector ( BaseCollector ): \"\"\"The class responsible for loading Jinja templates and rendering them. It defines some configuration options, implements the `render` method, and overrides the `update_env` method of the [`BaseRenderer` class][mkdocstrings.handlers.base.BaseRenderer]. \"\"\" default_config : dict = { \"docstring_style\" : \"google\" , \"docstring_options\" : {}} \"\"\"The default selection options. Option | Type | Description | Default ------ | ---- | ----------- | ------- **`docstring_style`** | `\"google\" | \"numpy\" | \"sphinx\" | None` | The docstring style to use. | `\"google\"` **`docstring_options`** | `dict[str, Any]` | The options for the docstring parser. | `{}` \"\"\" fallback_config : dict = { \"fallback\" : True } def __init__ ( self ) -> None : \"\"\"Initialize the collector.\"\"\" self . _kiara : Kiara = Kiara . instance () self . _context : KiaraContext = KiaraContext . get_info ( kiara = self . _kiara , ignore_errors = False ) self . _component_tree : nx . DiGraph = self . _context . get_subcomponent_tree () # type: ignore def collect ( self , identifier : str , config : dict ) -> CollectorItem : # noqa: WPS231 \"\"\"Collect the documentation tree given an identifier and selection options. Arguments: identifier: The dotted-path of a Python object available in the Python path. config: Selection options, used to alter the data collection done by `pytkdocs`. Raises: CollectionError: When there was a problem collecting the object documentation. Returns: The collected object-tree. \"\"\" tokens = identifier . split ( \".\" ) kiara_id = \".\" . join ( tokens [ 1 :]) print ( f \"REQUESTED: { identifier } \" ) if tokens [ 0 ] != \"kiara_info\" : return None # raise Exception( # f\"Handler 'kiara' can only be used with identifiers that start with 'kiara_info.', the provided id is invalid: {identifier}\" # ) try : info = self . _component_tree . nodes [ f \"__self__. { kiara_id } \" ][ \"obj\" ] except Exception as e : import traceback traceback . print_exc () raise e print ( f \"DONE: { identifier } \" ) return { \"obj\" : info , \"identifier\" : identifier , \"kiara_id\" : kiara_id } default_config : dict \u00b6 The default selection options. Option | Type | Description | Default ------ | ---- | ----------- | ------- docstring_style | \"google\" | \"numpy\" | \"sphinx\" | None | The docstring style to use. | \"google\" docstring_options | dict[str, Any] | The options for the docstring parser. | {} __init__ ( self ) special \u00b6 Initialize the collector. Source code in kiara/doc/mkdocstrings/collector.py def __init__ ( self ) -> None : \"\"\"Initialize the collector.\"\"\" self . _kiara : Kiara = Kiara . instance () self . _context : KiaraContext = KiaraContext . get_info ( kiara = self . _kiara , ignore_errors = False ) self . _component_tree : nx . DiGraph = self . _context . get_subcomponent_tree () # type: ignore collect ( self , identifier , config ) \u00b6 Collect the documentation tree given an identifier and selection options. Parameters: Name Type Description Default identifier str The dotted-path of a Python object available in the Python path. required config dict Selection options, used to alter the data collection done by pytkdocs . required Exceptions: Type Description CollectionError When there was a problem collecting the object documentation. Returns: Type Description CollectorItem The collected object-tree. Source code in kiara/doc/mkdocstrings/collector.py def collect ( self , identifier : str , config : dict ) -> CollectorItem : # noqa: WPS231 \"\"\"Collect the documentation tree given an identifier and selection options. Arguments: identifier: The dotted-path of a Python object available in the Python path. config: Selection options, used to alter the data collection done by `pytkdocs`. Raises: CollectionError: When there was a problem collecting the object documentation. Returns: The collected object-tree. \"\"\" tokens = identifier . split ( \".\" ) kiara_id = \".\" . join ( tokens [ 1 :]) print ( f \"REQUESTED: { identifier } \" ) if tokens [ 0 ] != \"kiara_info\" : return None # raise Exception( # f\"Handler 'kiara' can only be used with identifiers that start with 'kiara_info.', the provided id is invalid: {identifier}\" # ) try : info = self . _component_tree . nodes [ f \"__self__. { kiara_id } \" ][ \"obj\" ] except Exception as e : import traceback traceback . print_exc () raise e print ( f \"DONE: { identifier } \" ) return { \"obj\" : info , \"identifier\" : identifier , \"kiara_id\" : kiara_id } handler \u00b6 KiaraHandler ( BaseHandler ) \u00b6 The kiara handler class. Attributes: Name Type Description domain str The cross-documentation domain/language for this handler. enable_inventory bool Whether this handler is interested in enabling the creation of the objects.inv Sphinx inventory file. Source code in kiara/doc/mkdocstrings/handler.py class KiaraHandler ( BaseHandler ): \"\"\"The kiara handler class. Attributes: domain: The cross-documentation domain/language for this handler. enable_inventory: Whether this handler is interested in enabling the creation of the `objects.inv` Sphinx inventory file. \"\"\" domain : str = \"kiara\" enable_inventory : bool = True # load_inventory = staticmethod(inventory.list_object_urls) # # @classmethod # def load_inventory( # cls, # in_file: typing.BinaryIO, # url: str, # base_url: typing.Optional[str] = None, # **kwargs: typing.Any, # ) -> typing.Iterator[typing.Tuple[str, str]]: # \"\"\"Yield items and their URLs from an inventory file streamed from `in_file`. # This implements mkdocstrings' `load_inventory` \"protocol\" (see plugin.py). # Arguments: # in_file: The binary file-like object to read the inventory from. # url: The URL that this file is being streamed from (used to guess `base_url`). # base_url: The URL that this inventory's sub-paths are relative to. # **kwargs: Ignore additional arguments passed from the config. # Yields: # Tuples of (item identifier, item URL). # \"\"\" # # print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\") # # if base_url is None: # base_url = posixpath.dirname(url) # # for item in Inventory.parse_sphinx( # in_file, domain_filter=(\"py\",) # ).values(): # noqa: WPS526 # yield item.name, posixpath.join(base_url, item.uri) get_handler ( theme , custom_templates = None , ** config ) \u00b6 Simply return an instance of PythonHandler . Parameters: Name Type Description Default theme str The theme to use when rendering contents. required custom_templates Optional[str] Directory containing custom templates. None **config Any Configuration passed to the handler. {} Returns: Type Description KiaraHandler An instance of PythonHandler . Source code in kiara/doc/mkdocstrings/handler.py def get_handler ( theme : str , # noqa: W0613 (unused argument config) custom_templates : typing . Optional [ str ] = None , ** config : typing . Any , ) -> KiaraHandler : \"\"\"Simply return an instance of `PythonHandler`. Arguments: theme: The theme to use when rendering contents. custom_templates: Directory containing custom templates. **config: Configuration passed to the handler. Returns: An instance of `PythonHandler`. \"\"\" if custom_templates is not None : raise Exception ( \"Custom templates are not supported for the kiara renderer.\" ) custom_templates = os . path . join ( KIARA_RESOURCES_FOLDER , \"templates\" , \"info_templates\" ) return KiaraHandler ( collector = KiaraCollector (), renderer = KiaraInfoRenderer ( \"kiara\" , theme , custom_templates ), ) renderer \u00b6 KiaraInfoRenderer ( BaseRenderer ) \u00b6 Source code in kiara/doc/mkdocstrings/renderer.py class KiaraInfoRenderer ( BaseRenderer ): default_config : dict = {} def render ( self , data : typing . Dict [ str , typing . Any ], config : dict ) -> str : final_config = ChainMap ( config , self . default_config ) obj = data [ \"obj\" ] data_type = obj . __class__ . __name__ . lower () func_name = f \"render_ { data_type } \" if hasattr ( self , func_name ): func = getattr ( self , func_name ) return func ( payload = data , config = final_config ) else : html = obj . create_html () print ( f \"Rendered: { data [ 'identifier' ] } \" ) return html render ( self , data , config ) \u00b6 Render a template using provided data and configuration options. Parameters: Name Type Description Default data Dict[str, Any] The collected data to render. required config dict The rendering options. required Returns: Type Description str The rendered template as HTML. Source code in kiara/doc/mkdocstrings/renderer.py def render ( self , data : typing . Dict [ str , typing . Any ], config : dict ) -> str : final_config = ChainMap ( config , self . default_config ) obj = data [ \"obj\" ] data_type = obj . __class__ . __name__ . lower () func_name = f \"render_ { data_type } \" if hasattr ( self , func_name ): func = getattr ( self , func_name ) return func ( payload = data , config = final_config ) else : html = obj . create_html () print ( f \"Rendered: { data [ 'identifier' ] } \" ) return html","title":"doc"},{"location":"reference/kiara/doc/__init__/#kiara.doc.FrklDocumentationPlugin","text":"mkdocs plugin to render API documentation for a project. To add to a project, add this to the 'plugins' section of a mkdocs config file: - frkl-docgen : main_module : \"module_name\" This will add an API reference navigation item to your page navigation, with auto-generated entries for every Python module in your package. Source code in kiara/doc/__init__.py class FrklDocumentationPlugin ( BasePlugin ): \"\"\"[mkdocs](https://www.mkdocs.org/) plugin to render API documentation for a project. To add to a project, add this to the 'plugins' section of a mkdocs config file: ```yaml - frkl-docgen: main_module: \"module_name\" ``` This will add an ``API reference`` navigation item to your page navigation, with auto-generated entries for every Python module in your package. \"\"\" config_scheme = (( \"main_module\" , mkdocs . config . config_options . Type ( str )),) def __init__ ( self ): self . _doc_paths = None self . _dir = tempfile . TemporaryDirectory ( prefix = \"frkl_doc_gen_\" ) self . _doc_files = None super () . __init__ () def on_files ( self , files : Files , config : Config ) -> Files : self . _doc_paths = gen_pages_for_module ( self . config [ \"main_module\" ]) self . _doc_files = {} for k in sorted ( self . _doc_paths , key = lambda x : os . path . splitext ( x )[ 0 ]): content = self . _doc_paths [ k ][ \"content\" ] _file = File ( k , src_dir = self . _dir . name , dest_dir = config [ \"site_dir\" ], use_directory_urls = config [ \"use_directory_urls\" ], ) os . makedirs ( os . path . dirname ( _file . abs_src_path ), exist_ok = True ) with open ( _file . abs_src_path , \"w\" ) as f : f . write ( content ) self . _doc_files [ k ] = _file files . append ( _file ) return files def on_page_content ( self , html , page : Page , config : Config , files : Files ): repo_url = config . get ( \"repo_url\" , None ) python_src = config . get ( \"edit_uri\" , None ) if page . file . src_path in self . _doc_paths . keys (): src_path = self . _doc_paths . get ( page . file . src_path )[ \"python_src\" ][ \"rel_path\" ] rel_base = urllib . parse . urljoin ( repo_url , f \" { python_src } /../src/ { src_path } \" ) page . edit_url = rel_base return html def on_nav ( self , nav : Navigation , config : Config , files : Files ): for item in nav . items : if item . title and \"Api reference\" in item . title : return nav pages = [] for _file in self . _doc_files . values (): pages . append ( _file . page ) section = Section ( title = \"API reference\" , children = pages ) nav . items . append ( section ) nav . pages . extend ( pages ) _add_previous_and_next_links ( nav . pages ) _add_parent_links ( nav . items ) return nav def on_post_build ( self , config : Config ): self . _dir . cleanup ()","title":"FrklDocumentationPlugin"},{"location":"reference/kiara/doc/__init__/#kiara.doc.gen_info_pages","text":"","title":"gen_info_pages"},{"location":"reference/kiara/doc/__init__/#kiara.doc.gen_info_pages.render_item_listing","text":"Render an item listing summary page, for: https://oprypin.github.io/mkdocs-literate-nav/ This code is a terrible, terrible mess, but I just don't care enough. If the output it produces is wrong, it'll be obvious in the documentation (hopefully). I'd have to spend considerable time cleaning this up, and at the moment it does not seem worth it. Source code in kiara/doc/gen_info_pages.py def render_item_listing ( kiara : Kiara , item_type : str , limit_to_package : typing . Optional [ str ] = None ) -> typing . Optional [ str ]: \"\"\"Render an item listing summary page, for: https://oprypin.github.io/mkdocs-literate-nav/ This code is a terrible, terrible mess, but I just don't care enough. If the output it produces is wrong, it'll be obvious in the documentation (hopefully). I'd have to spend considerable time cleaning this up, and at the moment it does not seem worth it. \"\"\" info : KiaraInfoModel = KiaraContext . get_info ( kiara = kiara ) tree = info . get_subcomponent_tree () if tree is None : raise Exception ( \"Can't render item listing, no subcomponent tree available.\" ) def extract_cls_from_kiara_module_type_metadata ( obj ): return obj . python_class . get_class () def extract_cls_from_operation ( obj ): return obj . module . __class__ def extract_cls_from_op_type ( obj ): return obj . python_class . get_class () def extract_cls_from_value_type ( obj ): return obj . python_class . get_class () item_type_map : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { \"module\" : { \"cls\" : KiaraModuleTypeMetadata , \"extract\" : extract_cls_from_kiara_module_type_metadata , }, \"value_type\" : { \"cls\" : ValueTypeMetadata , \"extract\" : extract_cls_from_value_type , }, \"operation\" : { \"cls\" : Operation , \"extract\" : extract_cls_from_operation }, \"operation_type\" : { \"cls\" : OperationsMetadata , \"extract\" : extract_cls_from_op_type , }, } item_cls = item_type_map [ item_type ][ \"cls\" ] plural = f \" { item_type } s\" item_summaries : typing . Dict [ typing . Tuple , typing . List ] = {} list_template = get_jina_env () . get_template ( f \" { item_type } _list.md.j2\" ) for node in tree . nodes (): tokens = node . split ( \".\" ) if len ( tokens ) < 3 : continue category = tokens [ 1 ] obj = tree . nodes [ node ][ \"obj\" ] path_tokens = tokens [ 2 :] if category == plural : if isinstance ( obj , item_cls ): # type: ignore if limit_to_package : cls = item_type_map [ item_type ][ \"extract\" ]( obj ) md = ContextMetadataModel . from_class ( cls ) if md . labels . get ( \"package\" , None ) != limit_to_package : continue item_summaries . setdefault ( tuple ( path_tokens ), []) . append ( obj ) elif isinstance ( obj , KiaraDynamicInfoModel ): item_summaries . setdefault ( tuple ( path_tokens ), []) new_summary : typing . Dict [ typing . Tuple , typing . List ] = {} no_childs = [] for path_tokens , items in item_summaries . items (): # if len(path_tokens) == 1: # new_summary[path_tokens] = item_summaries[path_tokens] # full_path = [path_tokens[0]] full_path : typing . List [ str ] = [] collect : typing . Optional [ typing . List [ str ]] = None for token in path_tokens : if collect is not None : collect . append ( token ) t = tuple ( full_path + collect ) if not item_summaries [ t ]: continue else : new_summary . setdefault ( tuple ( full_path ), []) . extend ( item_summaries [ t ] ) else : full_path . append ( token ) t = tuple ( full_path ) if item_summaries [ t ]: new_summary [ t ] = item_summaries [ t ] else : match = False any_childs = False for k in item_summaries . keys (): if len ( full_path ) == 1 and full_path [ 0 ] == k [ 0 ] and len ( k ) > 2 : match = True if ( len ( k ) > 1 and len ( t ) <= len ( k ) and k [ 0 : len ( t )] == t # noqa and item_summaries [ k ] ): any_childs = True if not any_childs : no_childs . append ( t ) if not match : new_summary . setdefault ( t , []) collect = [] else : new_summary . setdefault ( t , []) . extend ( item_summaries [ t ]) main_summary = [] for summary_path , items in new_summary . items (): if not summary_path : continue match = False for nc in no_childs : if len ( summary_path ) >= len ( nc ): if summary_path [ 0 : len ( nc )] == nc : # noqa match = True break if match : continue padding = \" \" * len ( summary_path ) path = os . path . join ( * summary_path ) rendered = list_template . render ( ** { \"path\" : path , plural : items }) p_write = os . path . join ( plural , path , \"index.md\" ) p_index = os . path . join ( path , \"index.md\" ) with mkdocs_gen_files . open ( p_write , \"w\" ) as f : f . write ( rendered ) main_summary . append ( f \" { padding } * [ { summary_path [ - 1 ] } ]( { p_index } )\" ) modules_content = \"xxxxxxx\" with mkdocs_gen_files . open ( f \" { plural } /index.md\" , \"w\" ) as f : f . write ( modules_content ) summary_content = \" \\n \" . join ( main_summary ) summary_page = f \" { plural } /SUMMARY.md\" with mkdocs_gen_files . open ( summary_page , \"w\" ) as f : f . write ( summary_content ) return f \" { plural } /\"","title":"render_item_listing()"},{"location":"reference/kiara/doc/__init__/#kiara.doc.generate_api_doc","text":"","title":"generate_api_doc"},{"location":"reference/kiara/doc/__init__/#kiara.doc.generate_api_doc.gen_pages_for_module","text":"Generate modules for a set of modules (using the mkdocstring package. Source code in kiara/doc/generate_api_doc.py def gen_pages_for_module ( module : typing . Union [ str , ModuleType ], prefix : str = \"api_reference\" ): \"\"\"Generate modules for a set of modules (using the [mkdocstring](https://github.com/mkdocstrings/mkdocstrings) package.\"\"\" result = {} modules_info = get_source_tree ( module ) for module_name , path in modules_info . items (): page_name = module_name if page_name . endswith ( \"__init__\" ): page_name = page_name [ 0 : - 9 ] if page_name . endswith ( \"._frkl\" ): continue doc_path = f \" { prefix }{ os . path . sep }{ page_name } .md\" p = Path ( \"..\" , path [ \"abs_path\" ]) if not p . read_text () . strip (): continue main_module = path [ \"main_module\" ] if page_name == main_module : title = page_name else : title = page_name . replace ( f \" { main_module } .\" , \"\u279c\u2007\" ) result [ doc_path ] = { \"python_src\" : path , \"content\" : f \"--- \\n title: { title } \\n --- \\n # { page_name } \\n\\n ::: { module_name } \" , } return result","title":"gen_pages_for_module()"},{"location":"reference/kiara/doc/__init__/#kiara.doc.generate_api_doc.get_source_tree","text":"Find all python source files for a module. Source code in kiara/doc/generate_api_doc.py def get_source_tree ( module : typing . Union [ str , ModuleType ]): \"\"\"Find all python source files for a module.\"\"\" if isinstance ( module , str ): module = importlib . import_module ( module ) if not isinstance ( module , ModuleType ): raise TypeError ( f \"Invalid type ' { type ( module ) } ', input needs to be a string or module.\" ) module_file = module . __file__ assert module_file is not None module_root = os . path . dirname ( module_file ) module_name = module . __name__ src = {} for path in Path ( module_root ) . glob ( \"**/*.py\" ): rel = os . path . relpath ( path , module_root ) mod_name = f \" { module_name } . { rel [ 0 : - 3 ] . replace ( os . path . sep , '.' ) } \" rel_path = f \" { module_name }{ os . path . sep }{ rel } \" src [ mod_name ] = { \"rel_path\" : rel_path , \"abs_path\" : path , \"main_module\" : module_name , } return src","title":"get_source_tree()"},{"location":"reference/kiara/doc/__init__/#kiara.doc.mkdocs_macros_cli","text":"","title":"mkdocs_macros_cli"},{"location":"reference/kiara/doc/__init__/#kiara.doc.mkdocs_macros_cli.define_env","text":"Helper macros for Python project documentation. Currently, those macros are available (check the source code for more details):","title":"define_env()"},{"location":"reference/kiara/doc/__init__/#kiara.doc.mkdocs_macros_kiara","text":"","title":"mkdocs_macros_kiara"},{"location":"reference/kiara/doc/__init__/#kiara.doc.mkdocs_macros_kiara.define_env","text":"This is the hook for defining variables, macros and filters variables: the dictionary that contains the environment variables macro: a decorator function, to declare a macro. Source code in kiara/doc/mkdocs_macros_kiara.py def define_env ( env ): \"\"\" This is the hook for defining variables, macros and filters - variables: the dictionary that contains the environment variables - macro: a decorator function, to declare a macro. \"\"\" # env.variables[\"baz\"] = \"John Doe\" @env . macro def get_schema_for_model ( model_class : typing . Union [ str , typing . Type [ BaseModel ]]): if isinstance ( model_class , str ): _class : typing . Type [ BaseModel ] = locate ( model_class ) # type: ignore else : _class = model_class schema_json = _class . schema_json ( indent = 2 ) return schema_json @env . macro def get_src_of_object ( obj : typing . Union [ str , typing . Any ]): try : if isinstance ( obj , str ): _obj : typing . Type [ BaseModel ] = locate ( obj ) # type: ignore else : _obj = obj src = inspect . getsource ( _obj ) return src except Exception as e : return f \"Can't render object source: { str ( e ) } \" @env . macro def get_pipeline_config ( pipeline_name : str ): pmm = PipelineModuleManager () desc = pmm . pipeline_descs [ pipeline_name ][ \"data\" ] desc_str = yaml . dump ( desc ) return desc_str @env . macro def get_module_info ( module_type : str ): try : m_cls = Kiara . instance () . get_module_class ( module_type ) info = KiaraModuleTypeMetadata . from_module_class ( m_cls ) from rich.console import Console console = Console ( record = True ) console . print ( info ) html = console . export_text () return html except Exception as e : return f \"Can't render module info: { str ( e ) } \" @env . macro def get_module_list_for_package ( package_name : str , include_core_modules : bool = True , include_pipelines : bool = True , ): modules = kiara_obj . module_mgmt . find_modules_for_package ( package_name , include_core_modules = include_core_modules , include_pipelines = include_pipelines , ) result = [] for name , info in modules . items (): type_md = info . get_type_metadata () result . append ( f \"- [`` { name } ``][kiara_info.modules. { name } ]\" ) result . append ( f \" - [`` { name } ``]( { type_md . context . get_url_for_reference ( 'module_doc' ) } ): { type_md . documentation . description } \" ) print ( result ) return \" \\n \" . join ( result ) @env . macro def get_value_types_for_package ( package_name : str ): value_types = kiara_obj . type_mgmt . find_value_types_for_package ( package_name ) result = [] for name , info in value_types . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result ) @env . macro def get_metadata_models_for_package ( package_name : str ): metadata_schemas = kiara_obj . metadata_mgmt . find_all_models_for_package ( package_name ) result = [] for name , info in metadata_schemas . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result ) @env . macro def get_kiara_context () -> KiaraContext : return kiara_context","title":"define_env()"},{"location":"reference/kiara/doc/__init__/#kiara.doc.mkdocs_macros_kiara.on_post_build","text":"Post-build actions Source code in kiara/doc/mkdocs_macros_kiara.py def on_post_build ( env ): \"Post-build actions\" site_dir = env . conf [ \"site_dir\" ] for category , classes in KIARA_MODEL_CLASSES . items (): for cls in classes : schema_json = cls . schema_json ( indent = 2 ) file_path = os . path . join ( site_dir , \"development\" , \"entities\" , category , f \" { cls . __name__ } .json\" ) os . makedirs ( os . path . dirname ( file_path ), exist_ok = True ) with open ( file_path , \"w\" ) as f : f . write ( schema_json )","title":"on_post_build()"},{"location":"reference/kiara/doc/__init__/#kiara.doc.mkdocstrings","text":"","title":"mkdocstrings"},{"location":"reference/kiara/doc/__init__/#kiara.doc.mkdocstrings.collector","text":"","title":"collector"},{"location":"reference/kiara/doc/__init__/#kiara.doc.mkdocstrings.handler","text":"","title":"handler"},{"location":"reference/kiara/doc/__init__/#kiara.doc.mkdocstrings.renderer","text":"","title":"renderer"},{"location":"reference/kiara/doc/gen_info_pages/","text":"render_item_listing ( kiara , item_type , limit_to_package = None ) \u00b6 Render an item listing summary page, for: https://oprypin.github.io/mkdocs-literate-nav/ This code is a terrible, terrible mess, but I just don't care enough. If the output it produces is wrong, it'll be obvious in the documentation (hopefully). I'd have to spend considerable time cleaning this up, and at the moment it does not seem worth it. Source code in kiara/doc/gen_info_pages.py def render_item_listing ( kiara : Kiara , item_type : str , limit_to_package : typing . Optional [ str ] = None ) -> typing . Optional [ str ]: \"\"\"Render an item listing summary page, for: https://oprypin.github.io/mkdocs-literate-nav/ This code is a terrible, terrible mess, but I just don't care enough. If the output it produces is wrong, it'll be obvious in the documentation (hopefully). I'd have to spend considerable time cleaning this up, and at the moment it does not seem worth it. \"\"\" info : KiaraInfoModel = KiaraContext . get_info ( kiara = kiara ) tree = info . get_subcomponent_tree () if tree is None : raise Exception ( \"Can't render item listing, no subcomponent tree available.\" ) def extract_cls_from_kiara_module_type_metadata ( obj ): return obj . python_class . get_class () def extract_cls_from_operation ( obj ): return obj . module . __class__ def extract_cls_from_op_type ( obj ): return obj . python_class . get_class () def extract_cls_from_value_type ( obj ): return obj . python_class . get_class () item_type_map : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { \"module\" : { \"cls\" : KiaraModuleTypeMetadata , \"extract\" : extract_cls_from_kiara_module_type_metadata , }, \"value_type\" : { \"cls\" : ValueTypeMetadata , \"extract\" : extract_cls_from_value_type , }, \"operation\" : { \"cls\" : Operation , \"extract\" : extract_cls_from_operation }, \"operation_type\" : { \"cls\" : OperationsMetadata , \"extract\" : extract_cls_from_op_type , }, } item_cls = item_type_map [ item_type ][ \"cls\" ] plural = f \" { item_type } s\" item_summaries : typing . Dict [ typing . Tuple , typing . List ] = {} list_template = get_jina_env () . get_template ( f \" { item_type } _list.md.j2\" ) for node in tree . nodes (): tokens = node . split ( \".\" ) if len ( tokens ) < 3 : continue category = tokens [ 1 ] obj = tree . nodes [ node ][ \"obj\" ] path_tokens = tokens [ 2 :] if category == plural : if isinstance ( obj , item_cls ): # type: ignore if limit_to_package : cls = item_type_map [ item_type ][ \"extract\" ]( obj ) md = ContextMetadataModel . from_class ( cls ) if md . labels . get ( \"package\" , None ) != limit_to_package : continue item_summaries . setdefault ( tuple ( path_tokens ), []) . append ( obj ) elif isinstance ( obj , KiaraDynamicInfoModel ): item_summaries . setdefault ( tuple ( path_tokens ), []) new_summary : typing . Dict [ typing . Tuple , typing . List ] = {} no_childs = [] for path_tokens , items in item_summaries . items (): # if len(path_tokens) == 1: # new_summary[path_tokens] = item_summaries[path_tokens] # full_path = [path_tokens[0]] full_path : typing . List [ str ] = [] collect : typing . Optional [ typing . List [ str ]] = None for token in path_tokens : if collect is not None : collect . append ( token ) t = tuple ( full_path + collect ) if not item_summaries [ t ]: continue else : new_summary . setdefault ( tuple ( full_path ), []) . extend ( item_summaries [ t ] ) else : full_path . append ( token ) t = tuple ( full_path ) if item_summaries [ t ]: new_summary [ t ] = item_summaries [ t ] else : match = False any_childs = False for k in item_summaries . keys (): if len ( full_path ) == 1 and full_path [ 0 ] == k [ 0 ] and len ( k ) > 2 : match = True if ( len ( k ) > 1 and len ( t ) <= len ( k ) and k [ 0 : len ( t )] == t # noqa and item_summaries [ k ] ): any_childs = True if not any_childs : no_childs . append ( t ) if not match : new_summary . setdefault ( t , []) collect = [] else : new_summary . setdefault ( t , []) . extend ( item_summaries [ t ]) main_summary = [] for summary_path , items in new_summary . items (): if not summary_path : continue match = False for nc in no_childs : if len ( summary_path ) >= len ( nc ): if summary_path [ 0 : len ( nc )] == nc : # noqa match = True break if match : continue padding = \" \" * len ( summary_path ) path = os . path . join ( * summary_path ) rendered = list_template . render ( ** { \"path\" : path , plural : items }) p_write = os . path . join ( plural , path , \"index.md\" ) p_index = os . path . join ( path , \"index.md\" ) with mkdocs_gen_files . open ( p_write , \"w\" ) as f : f . write ( rendered ) main_summary . append ( f \" { padding } * [ { summary_path [ - 1 ] } ]( { p_index } )\" ) modules_content = \"xxxxxxx\" with mkdocs_gen_files . open ( f \" { plural } /index.md\" , \"w\" ) as f : f . write ( modules_content ) summary_content = \" \\n \" . join ( main_summary ) summary_page = f \" { plural } /SUMMARY.md\" with mkdocs_gen_files . open ( summary_page , \"w\" ) as f : f . write ( summary_content ) return f \" { plural } /\"","title":"gen_info_pages"},{"location":"reference/kiara/doc/gen_info_pages/#kiara.doc.gen_info_pages.render_item_listing","text":"Render an item listing summary page, for: https://oprypin.github.io/mkdocs-literate-nav/ This code is a terrible, terrible mess, but I just don't care enough. If the output it produces is wrong, it'll be obvious in the documentation (hopefully). I'd have to spend considerable time cleaning this up, and at the moment it does not seem worth it. Source code in kiara/doc/gen_info_pages.py def render_item_listing ( kiara : Kiara , item_type : str , limit_to_package : typing . Optional [ str ] = None ) -> typing . Optional [ str ]: \"\"\"Render an item listing summary page, for: https://oprypin.github.io/mkdocs-literate-nav/ This code is a terrible, terrible mess, but I just don't care enough. If the output it produces is wrong, it'll be obvious in the documentation (hopefully). I'd have to spend considerable time cleaning this up, and at the moment it does not seem worth it. \"\"\" info : KiaraInfoModel = KiaraContext . get_info ( kiara = kiara ) tree = info . get_subcomponent_tree () if tree is None : raise Exception ( \"Can't render item listing, no subcomponent tree available.\" ) def extract_cls_from_kiara_module_type_metadata ( obj ): return obj . python_class . get_class () def extract_cls_from_operation ( obj ): return obj . module . __class__ def extract_cls_from_op_type ( obj ): return obj . python_class . get_class () def extract_cls_from_value_type ( obj ): return obj . python_class . get_class () item_type_map : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { \"module\" : { \"cls\" : KiaraModuleTypeMetadata , \"extract\" : extract_cls_from_kiara_module_type_metadata , }, \"value_type\" : { \"cls\" : ValueTypeMetadata , \"extract\" : extract_cls_from_value_type , }, \"operation\" : { \"cls\" : Operation , \"extract\" : extract_cls_from_operation }, \"operation_type\" : { \"cls\" : OperationsMetadata , \"extract\" : extract_cls_from_op_type , }, } item_cls = item_type_map [ item_type ][ \"cls\" ] plural = f \" { item_type } s\" item_summaries : typing . Dict [ typing . Tuple , typing . List ] = {} list_template = get_jina_env () . get_template ( f \" { item_type } _list.md.j2\" ) for node in tree . nodes (): tokens = node . split ( \".\" ) if len ( tokens ) < 3 : continue category = tokens [ 1 ] obj = tree . nodes [ node ][ \"obj\" ] path_tokens = tokens [ 2 :] if category == plural : if isinstance ( obj , item_cls ): # type: ignore if limit_to_package : cls = item_type_map [ item_type ][ \"extract\" ]( obj ) md = ContextMetadataModel . from_class ( cls ) if md . labels . get ( \"package\" , None ) != limit_to_package : continue item_summaries . setdefault ( tuple ( path_tokens ), []) . append ( obj ) elif isinstance ( obj , KiaraDynamicInfoModel ): item_summaries . setdefault ( tuple ( path_tokens ), []) new_summary : typing . Dict [ typing . Tuple , typing . List ] = {} no_childs = [] for path_tokens , items in item_summaries . items (): # if len(path_tokens) == 1: # new_summary[path_tokens] = item_summaries[path_tokens] # full_path = [path_tokens[0]] full_path : typing . List [ str ] = [] collect : typing . Optional [ typing . List [ str ]] = None for token in path_tokens : if collect is not None : collect . append ( token ) t = tuple ( full_path + collect ) if not item_summaries [ t ]: continue else : new_summary . setdefault ( tuple ( full_path ), []) . extend ( item_summaries [ t ] ) else : full_path . append ( token ) t = tuple ( full_path ) if item_summaries [ t ]: new_summary [ t ] = item_summaries [ t ] else : match = False any_childs = False for k in item_summaries . keys (): if len ( full_path ) == 1 and full_path [ 0 ] == k [ 0 ] and len ( k ) > 2 : match = True if ( len ( k ) > 1 and len ( t ) <= len ( k ) and k [ 0 : len ( t )] == t # noqa and item_summaries [ k ] ): any_childs = True if not any_childs : no_childs . append ( t ) if not match : new_summary . setdefault ( t , []) collect = [] else : new_summary . setdefault ( t , []) . extend ( item_summaries [ t ]) main_summary = [] for summary_path , items in new_summary . items (): if not summary_path : continue match = False for nc in no_childs : if len ( summary_path ) >= len ( nc ): if summary_path [ 0 : len ( nc )] == nc : # noqa match = True break if match : continue padding = \" \" * len ( summary_path ) path = os . path . join ( * summary_path ) rendered = list_template . render ( ** { \"path\" : path , plural : items }) p_write = os . path . join ( plural , path , \"index.md\" ) p_index = os . path . join ( path , \"index.md\" ) with mkdocs_gen_files . open ( p_write , \"w\" ) as f : f . write ( rendered ) main_summary . append ( f \" { padding } * [ { summary_path [ - 1 ] } ]( { p_index } )\" ) modules_content = \"xxxxxxx\" with mkdocs_gen_files . open ( f \" { plural } /index.md\" , \"w\" ) as f : f . write ( modules_content ) summary_content = \" \\n \" . join ( main_summary ) summary_page = f \" { plural } /SUMMARY.md\" with mkdocs_gen_files . open ( summary_page , \"w\" ) as f : f . write ( summary_content ) return f \" { plural } /\"","title":"render_item_listing()"},{"location":"reference/kiara/doc/generate_api_doc/","text":"gen_pages_for_module ( module , prefix = 'api_reference' ) \u00b6 Generate modules for a set of modules (using the mkdocstring package. Source code in kiara/doc/generate_api_doc.py def gen_pages_for_module ( module : typing . Union [ str , ModuleType ], prefix : str = \"api_reference\" ): \"\"\"Generate modules for a set of modules (using the [mkdocstring](https://github.com/mkdocstrings/mkdocstrings) package.\"\"\" result = {} modules_info = get_source_tree ( module ) for module_name , path in modules_info . items (): page_name = module_name if page_name . endswith ( \"__init__\" ): page_name = page_name [ 0 : - 9 ] if page_name . endswith ( \"._frkl\" ): continue doc_path = f \" { prefix }{ os . path . sep }{ page_name } .md\" p = Path ( \"..\" , path [ \"abs_path\" ]) if not p . read_text () . strip (): continue main_module = path [ \"main_module\" ] if page_name == main_module : title = page_name else : title = page_name . replace ( f \" { main_module } .\" , \"\u279c\u2007\" ) result [ doc_path ] = { \"python_src\" : path , \"content\" : f \"--- \\n title: { title } \\n --- \\n # { page_name } \\n\\n ::: { module_name } \" , } return result get_source_tree ( module ) \u00b6 Find all python source files for a module. Source code in kiara/doc/generate_api_doc.py def get_source_tree ( module : typing . Union [ str , ModuleType ]): \"\"\"Find all python source files for a module.\"\"\" if isinstance ( module , str ): module = importlib . import_module ( module ) if not isinstance ( module , ModuleType ): raise TypeError ( f \"Invalid type ' { type ( module ) } ', input needs to be a string or module.\" ) module_file = module . __file__ assert module_file is not None module_root = os . path . dirname ( module_file ) module_name = module . __name__ src = {} for path in Path ( module_root ) . glob ( \"**/*.py\" ): rel = os . path . relpath ( path , module_root ) mod_name = f \" { module_name } . { rel [ 0 : - 3 ] . replace ( os . path . sep , '.' ) } \" rel_path = f \" { module_name }{ os . path . sep }{ rel } \" src [ mod_name ] = { \"rel_path\" : rel_path , \"abs_path\" : path , \"main_module\" : module_name , } return src","title":"generate_api_doc"},{"location":"reference/kiara/doc/generate_api_doc/#kiara.doc.generate_api_doc.gen_pages_for_module","text":"Generate modules for a set of modules (using the mkdocstring package. Source code in kiara/doc/generate_api_doc.py def gen_pages_for_module ( module : typing . Union [ str , ModuleType ], prefix : str = \"api_reference\" ): \"\"\"Generate modules for a set of modules (using the [mkdocstring](https://github.com/mkdocstrings/mkdocstrings) package.\"\"\" result = {} modules_info = get_source_tree ( module ) for module_name , path in modules_info . items (): page_name = module_name if page_name . endswith ( \"__init__\" ): page_name = page_name [ 0 : - 9 ] if page_name . endswith ( \"._frkl\" ): continue doc_path = f \" { prefix }{ os . path . sep }{ page_name } .md\" p = Path ( \"..\" , path [ \"abs_path\" ]) if not p . read_text () . strip (): continue main_module = path [ \"main_module\" ] if page_name == main_module : title = page_name else : title = page_name . replace ( f \" { main_module } .\" , \"\u279c\u2007\" ) result [ doc_path ] = { \"python_src\" : path , \"content\" : f \"--- \\n title: { title } \\n --- \\n # { page_name } \\n\\n ::: { module_name } \" , } return result","title":"gen_pages_for_module()"},{"location":"reference/kiara/doc/generate_api_doc/#kiara.doc.generate_api_doc.get_source_tree","text":"Find all python source files for a module. Source code in kiara/doc/generate_api_doc.py def get_source_tree ( module : typing . Union [ str , ModuleType ]): \"\"\"Find all python source files for a module.\"\"\" if isinstance ( module , str ): module = importlib . import_module ( module ) if not isinstance ( module , ModuleType ): raise TypeError ( f \"Invalid type ' { type ( module ) } ', input needs to be a string or module.\" ) module_file = module . __file__ assert module_file is not None module_root = os . path . dirname ( module_file ) module_name = module . __name__ src = {} for path in Path ( module_root ) . glob ( \"**/*.py\" ): rel = os . path . relpath ( path , module_root ) mod_name = f \" { module_name } . { rel [ 0 : - 3 ] . replace ( os . path . sep , '.' ) } \" rel_path = f \" { module_name }{ os . path . sep }{ rel } \" src [ mod_name ] = { \"rel_path\" : rel_path , \"abs_path\" : path , \"main_module\" : module_name , } return src","title":"get_source_tree()"},{"location":"reference/kiara/doc/mkdocs_macros_cli/","text":"define_env ( env ) \u00b6 Helper macros for Python project documentation. Currently, those macros are available (check the source code for more details): cli \u00b6 Execute a command on the command-line, capture the output and return it to be used in a documentation page. inline_file_as_codeblock \u00b6 Read an external file, and return its content as a markdown code block. Source code in kiara/doc/mkdocs_macros_cli.py def define_env ( env ): \"\"\" Helper macros for Python project documentation. Currently, those macros are available (check the source code for more details): ## ``cli`` Execute a command on the command-line, capture the output and return it to be used in a documentation page. ## ``inline_file_as_codeblock`` Read an external file, and return its content as a markdown code block. \"\"\" # env.variables[\"baz\"] = \"John Doe\" @env . macro def cli ( * command , print_command : bool = True , code_block : bool = True , split_command_and_output : bool = True , max_height : Optional [ int ] = None , cache_key : Optional [ str ] = None , extra_env : Optional [ Dict [ str , str ]] = None , fake_command : Optional [ str ] = None , ): \"\"\"Execute the provided command, save the output and return it to be used in documentation modules.\"\"\" hashes = DeepHash ( command ) hash_str = hashes [ command ] hashes_env = DeepHash ( extra_env ) hashes_env_str = hashes_env [ extra_env ] hash_str = hash_str + \"_\" + hashes_env_str if cache_key : hash_str = hash_str + \"_\" + cache_key cache_file : Path = Path ( os . path . join ( CACHE_DIR , str ( hash_str ))) _run_env = dict ( os_env_vars ) if extra_env : _run_env . update ( extra_env ) if cache_file . is_file (): stdout = cache_file . read_text () else : try : print ( f \"RUNNING: { ' ' . join ( command ) } \" ) result = subprocess . check_output ( command , env = _run_env ) stdout = result . decode () cache_file . write_text ( stdout ) except subprocess . CalledProcessError as e : stdout = f \"Error: { e } \\n\\n Stdout: { e . stdout } \\n\\n Stderr: { e . stderr } \" print ( \"stdout:\" ) print ( e . stdout ) print ( \"stderr:\" ) print ( e . stderr ) if os . getenv ( \"FAIL_DOC_BUILD_ON_ERROR\" ) == \"true\" : sys . exit ( 1 ) if fake_command : command_str = fake_command else : command_str = \" \" . join ( command ) if split_command_and_output and print_command : _c = f \" \\n ``` console \\n { command_str } \\n ``` \\n \" _output = \"``` console \\n \" + stdout + \" \\n ``` \\n \" if max_height is not None and max_height > 0 : _output = f \"<div style='max-height: { max_height } px;overflow:auto'> \\n { _output } \\n </div>\" _stdout = _c + _output else : if print_command : _stdout = f \"> { command_str } \\n { stdout } \" if code_block : _stdout = \"``` console \\n \" + _stdout + \" \\n ``` \\n \" if max_height is not None and max_height > 0 : _stdout = f \"<div style='max-height: { max_height } px;overflow:auto'> \\n { _stdout } \\n </div>\" return _stdout @env . macro def inline_file_as_codeblock ( path , format : str = \"\" ): \"\"\"Import external file and return its content as a markdown code block.\"\"\" f = Path ( path ) return f \"``` { format } \\n { f . read_text () } \\n ```\"","title":"mkdocs_macros_cli"},{"location":"reference/kiara/doc/mkdocs_macros_cli/#kiara.doc.mkdocs_macros_cli.define_env","text":"Helper macros for Python project documentation. Currently, those macros are available (check the source code for more details):","title":"define_env()"},{"location":"reference/kiara/doc/mkdocs_macros_kiara/","text":"define_env ( env ) \u00b6 This is the hook for defining variables, macros and filters variables: the dictionary that contains the environment variables macro: a decorator function, to declare a macro. Source code in kiara/doc/mkdocs_macros_kiara.py def define_env ( env ): \"\"\" This is the hook for defining variables, macros and filters - variables: the dictionary that contains the environment variables - macro: a decorator function, to declare a macro. \"\"\" # env.variables[\"baz\"] = \"John Doe\" @env . macro def get_schema_for_model ( model_class : typing . Union [ str , typing . Type [ BaseModel ]]): if isinstance ( model_class , str ): _class : typing . Type [ BaseModel ] = locate ( model_class ) # type: ignore else : _class = model_class schema_json = _class . schema_json ( indent = 2 ) return schema_json @env . macro def get_src_of_object ( obj : typing . Union [ str , typing . Any ]): try : if isinstance ( obj , str ): _obj : typing . Type [ BaseModel ] = locate ( obj ) # type: ignore else : _obj = obj src = inspect . getsource ( _obj ) return src except Exception as e : return f \"Can't render object source: { str ( e ) } \" @env . macro def get_pipeline_config ( pipeline_name : str ): pmm = PipelineModuleManager () desc = pmm . pipeline_descs [ pipeline_name ][ \"data\" ] desc_str = yaml . dump ( desc ) return desc_str @env . macro def get_module_info ( module_type : str ): try : m_cls = Kiara . instance () . get_module_class ( module_type ) info = KiaraModuleTypeMetadata . from_module_class ( m_cls ) from rich.console import Console console = Console ( record = True ) console . print ( info ) html = console . export_text () return html except Exception as e : return f \"Can't render module info: { str ( e ) } \" @env . macro def get_module_list_for_package ( package_name : str , include_core_modules : bool = True , include_pipelines : bool = True , ): modules = kiara_obj . module_mgmt . find_modules_for_package ( package_name , include_core_modules = include_core_modules , include_pipelines = include_pipelines , ) result = [] for name , info in modules . items (): type_md = info . get_type_metadata () result . append ( f \"- [`` { name } ``][kiara_info.modules. { name } ]\" ) result . append ( f \" - [`` { name } ``]( { type_md . context . get_url_for_reference ( 'module_doc' ) } ): { type_md . documentation . description } \" ) print ( result ) return \" \\n \" . join ( result ) @env . macro def get_value_types_for_package ( package_name : str ): value_types = kiara_obj . type_mgmt . find_value_types_for_package ( package_name ) result = [] for name , info in value_types . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result ) @env . macro def get_metadata_models_for_package ( package_name : str ): metadata_schemas = kiara_obj . metadata_mgmt . find_all_models_for_package ( package_name ) result = [] for name , info in metadata_schemas . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result ) @env . macro def get_kiara_context () -> KiaraContext : return kiara_context on_post_build ( env ) \u00b6 Post-build actions Source code in kiara/doc/mkdocs_macros_kiara.py def on_post_build ( env ): \"Post-build actions\" site_dir = env . conf [ \"site_dir\" ] for category , classes in KIARA_MODEL_CLASSES . items (): for cls in classes : schema_json = cls . schema_json ( indent = 2 ) file_path = os . path . join ( site_dir , \"development\" , \"entities\" , category , f \" { cls . __name__ } .json\" ) os . makedirs ( os . path . dirname ( file_path ), exist_ok = True ) with open ( file_path , \"w\" ) as f : f . write ( schema_json )","title":"mkdocs_macros_kiara"},{"location":"reference/kiara/doc/mkdocs_macros_kiara/#kiara.doc.mkdocs_macros_kiara.define_env","text":"This is the hook for defining variables, macros and filters variables: the dictionary that contains the environment variables macro: a decorator function, to declare a macro. Source code in kiara/doc/mkdocs_macros_kiara.py def define_env ( env ): \"\"\" This is the hook for defining variables, macros and filters - variables: the dictionary that contains the environment variables - macro: a decorator function, to declare a macro. \"\"\" # env.variables[\"baz\"] = \"John Doe\" @env . macro def get_schema_for_model ( model_class : typing . Union [ str , typing . Type [ BaseModel ]]): if isinstance ( model_class , str ): _class : typing . Type [ BaseModel ] = locate ( model_class ) # type: ignore else : _class = model_class schema_json = _class . schema_json ( indent = 2 ) return schema_json @env . macro def get_src_of_object ( obj : typing . Union [ str , typing . Any ]): try : if isinstance ( obj , str ): _obj : typing . Type [ BaseModel ] = locate ( obj ) # type: ignore else : _obj = obj src = inspect . getsource ( _obj ) return src except Exception as e : return f \"Can't render object source: { str ( e ) } \" @env . macro def get_pipeline_config ( pipeline_name : str ): pmm = PipelineModuleManager () desc = pmm . pipeline_descs [ pipeline_name ][ \"data\" ] desc_str = yaml . dump ( desc ) return desc_str @env . macro def get_module_info ( module_type : str ): try : m_cls = Kiara . instance () . get_module_class ( module_type ) info = KiaraModuleTypeMetadata . from_module_class ( m_cls ) from rich.console import Console console = Console ( record = True ) console . print ( info ) html = console . export_text () return html except Exception as e : return f \"Can't render module info: { str ( e ) } \" @env . macro def get_module_list_for_package ( package_name : str , include_core_modules : bool = True , include_pipelines : bool = True , ): modules = kiara_obj . module_mgmt . find_modules_for_package ( package_name , include_core_modules = include_core_modules , include_pipelines = include_pipelines , ) result = [] for name , info in modules . items (): type_md = info . get_type_metadata () result . append ( f \"- [`` { name } ``][kiara_info.modules. { name } ]\" ) result . append ( f \" - [`` { name } ``]( { type_md . context . get_url_for_reference ( 'module_doc' ) } ): { type_md . documentation . description } \" ) print ( result ) return \" \\n \" . join ( result ) @env . macro def get_value_types_for_package ( package_name : str ): value_types = kiara_obj . type_mgmt . find_value_types_for_package ( package_name ) result = [] for name , info in value_types . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result ) @env . macro def get_metadata_models_for_package ( package_name : str ): metadata_schemas = kiara_obj . metadata_mgmt . find_all_models_for_package ( package_name ) result = [] for name , info in metadata_schemas . items (): type_md = info . get_type_metadata () result . append ( f \" - `` { name } ``: { type_md . documentation . description } \" ) return \" \\n \" . join ( result ) @env . macro def get_kiara_context () -> KiaraContext : return kiara_context","title":"define_env()"},{"location":"reference/kiara/doc/mkdocs_macros_kiara/#kiara.doc.mkdocs_macros_kiara.on_post_build","text":"Post-build actions Source code in kiara/doc/mkdocs_macros_kiara.py def on_post_build ( env ): \"Post-build actions\" site_dir = env . conf [ \"site_dir\" ] for category , classes in KIARA_MODEL_CLASSES . items (): for cls in classes : schema_json = cls . schema_json ( indent = 2 ) file_path = os . path . join ( site_dir , \"development\" , \"entities\" , category , f \" { cls . __name__ } .json\" ) os . makedirs ( os . path . dirname ( file_path ), exist_ok = True ) with open ( file_path , \"w\" ) as f : f . write ( schema_json )","title":"on_post_build()"},{"location":"reference/kiara/doc/mkdocstrings/__init__/","text":"collector \u00b6 KiaraCollector ( BaseCollector ) \u00b6 The class responsible for loading Jinja templates and rendering them. It defines some configuration options, implements the render method, and overrides the update_env method of the BaseRenderer class . Source code in kiara/doc/mkdocstrings/collector.py class KiaraCollector ( BaseCollector ): \"\"\"The class responsible for loading Jinja templates and rendering them. It defines some configuration options, implements the `render` method, and overrides the `update_env` method of the [`BaseRenderer` class][mkdocstrings.handlers.base.BaseRenderer]. \"\"\" default_config : dict = { \"docstring_style\" : \"google\" , \"docstring_options\" : {}} \"\"\"The default selection options. Option | Type | Description | Default ------ | ---- | ----------- | ------- **`docstring_style`** | `\"google\" | \"numpy\" | \"sphinx\" | None` | The docstring style to use. | `\"google\"` **`docstring_options`** | `dict[str, Any]` | The options for the docstring parser. | `{}` \"\"\" fallback_config : dict = { \"fallback\" : True } def __init__ ( self ) -> None : \"\"\"Initialize the collector.\"\"\" self . _kiara : Kiara = Kiara . instance () self . _context : KiaraContext = KiaraContext . get_info ( kiara = self . _kiara , ignore_errors = False ) self . _component_tree : nx . DiGraph = self . _context . get_subcomponent_tree () # type: ignore def collect ( self , identifier : str , config : dict ) -> CollectorItem : # noqa: WPS231 \"\"\"Collect the documentation tree given an identifier and selection options. Arguments: identifier: The dotted-path of a Python object available in the Python path. config: Selection options, used to alter the data collection done by `pytkdocs`. Raises: CollectionError: When there was a problem collecting the object documentation. Returns: The collected object-tree. \"\"\" tokens = identifier . split ( \".\" ) kiara_id = \".\" . join ( tokens [ 1 :]) print ( f \"REQUESTED: { identifier } \" ) if tokens [ 0 ] != \"kiara_info\" : return None # raise Exception( # f\"Handler 'kiara' can only be used with identifiers that start with 'kiara_info.', the provided id is invalid: {identifier}\" # ) try : info = self . _component_tree . nodes [ f \"__self__. { kiara_id } \" ][ \"obj\" ] except Exception as e : import traceback traceback . print_exc () raise e print ( f \"DONE: { identifier } \" ) return { \"obj\" : info , \"identifier\" : identifier , \"kiara_id\" : kiara_id } default_config : dict \u00b6 The default selection options. Option | Type | Description | Default ------ | ---- | ----------- | ------- docstring_style | \"google\" | \"numpy\" | \"sphinx\" | None | The docstring style to use. | \"google\" docstring_options | dict[str, Any] | The options for the docstring parser. | {} __init__ ( self ) special \u00b6 Initialize the collector. Source code in kiara/doc/mkdocstrings/collector.py def __init__ ( self ) -> None : \"\"\"Initialize the collector.\"\"\" self . _kiara : Kiara = Kiara . instance () self . _context : KiaraContext = KiaraContext . get_info ( kiara = self . _kiara , ignore_errors = False ) self . _component_tree : nx . DiGraph = self . _context . get_subcomponent_tree () # type: ignore collect ( self , identifier , config ) \u00b6 Collect the documentation tree given an identifier and selection options. Parameters: Name Type Description Default identifier str The dotted-path of a Python object available in the Python path. required config dict Selection options, used to alter the data collection done by pytkdocs . required Exceptions: Type Description CollectionError When there was a problem collecting the object documentation. Returns: Type Description CollectorItem The collected object-tree. Source code in kiara/doc/mkdocstrings/collector.py def collect ( self , identifier : str , config : dict ) -> CollectorItem : # noqa: WPS231 \"\"\"Collect the documentation tree given an identifier and selection options. Arguments: identifier: The dotted-path of a Python object available in the Python path. config: Selection options, used to alter the data collection done by `pytkdocs`. Raises: CollectionError: When there was a problem collecting the object documentation. Returns: The collected object-tree. \"\"\" tokens = identifier . split ( \".\" ) kiara_id = \".\" . join ( tokens [ 1 :]) print ( f \"REQUESTED: { identifier } \" ) if tokens [ 0 ] != \"kiara_info\" : return None # raise Exception( # f\"Handler 'kiara' can only be used with identifiers that start with 'kiara_info.', the provided id is invalid: {identifier}\" # ) try : info = self . _component_tree . nodes [ f \"__self__. { kiara_id } \" ][ \"obj\" ] except Exception as e : import traceback traceback . print_exc () raise e print ( f \"DONE: { identifier } \" ) return { \"obj\" : info , \"identifier\" : identifier , \"kiara_id\" : kiara_id } handler \u00b6 KiaraHandler ( BaseHandler ) \u00b6 The kiara handler class. Attributes: Name Type Description domain str The cross-documentation domain/language for this handler. enable_inventory bool Whether this handler is interested in enabling the creation of the objects.inv Sphinx inventory file. Source code in kiara/doc/mkdocstrings/handler.py class KiaraHandler ( BaseHandler ): \"\"\"The kiara handler class. Attributes: domain: The cross-documentation domain/language for this handler. enable_inventory: Whether this handler is interested in enabling the creation of the `objects.inv` Sphinx inventory file. \"\"\" domain : str = \"kiara\" enable_inventory : bool = True # load_inventory = staticmethod(inventory.list_object_urls) # # @classmethod # def load_inventory( # cls, # in_file: typing.BinaryIO, # url: str, # base_url: typing.Optional[str] = None, # **kwargs: typing.Any, # ) -> typing.Iterator[typing.Tuple[str, str]]: # \"\"\"Yield items and their URLs from an inventory file streamed from `in_file`. # This implements mkdocstrings' `load_inventory` \"protocol\" (see plugin.py). # Arguments: # in_file: The binary file-like object to read the inventory from. # url: The URL that this file is being streamed from (used to guess `base_url`). # base_url: The URL that this inventory's sub-paths are relative to. # **kwargs: Ignore additional arguments passed from the config. # Yields: # Tuples of (item identifier, item URL). # \"\"\" # # print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\") # # if base_url is None: # base_url = posixpath.dirname(url) # # for item in Inventory.parse_sphinx( # in_file, domain_filter=(\"py\",) # ).values(): # noqa: WPS526 # yield item.name, posixpath.join(base_url, item.uri) get_handler ( theme , custom_templates = None , ** config ) \u00b6 Simply return an instance of PythonHandler . Parameters: Name Type Description Default theme str The theme to use when rendering contents. required custom_templates Optional[str] Directory containing custom templates. None **config Any Configuration passed to the handler. {} Returns: Type Description KiaraHandler An instance of PythonHandler . Source code in kiara/doc/mkdocstrings/handler.py def get_handler ( theme : str , # noqa: W0613 (unused argument config) custom_templates : typing . Optional [ str ] = None , ** config : typing . Any , ) -> KiaraHandler : \"\"\"Simply return an instance of `PythonHandler`. Arguments: theme: The theme to use when rendering contents. custom_templates: Directory containing custom templates. **config: Configuration passed to the handler. Returns: An instance of `PythonHandler`. \"\"\" if custom_templates is not None : raise Exception ( \"Custom templates are not supported for the kiara renderer.\" ) custom_templates = os . path . join ( KIARA_RESOURCES_FOLDER , \"templates\" , \"info_templates\" ) return KiaraHandler ( collector = KiaraCollector (), renderer = KiaraInfoRenderer ( \"kiara\" , theme , custom_templates ), ) renderer \u00b6 KiaraInfoRenderer ( BaseRenderer ) \u00b6 Source code in kiara/doc/mkdocstrings/renderer.py class KiaraInfoRenderer ( BaseRenderer ): default_config : dict = {} def render ( self , data : typing . Dict [ str , typing . Any ], config : dict ) -> str : final_config = ChainMap ( config , self . default_config ) obj = data [ \"obj\" ] data_type = obj . __class__ . __name__ . lower () func_name = f \"render_ { data_type } \" if hasattr ( self , func_name ): func = getattr ( self , func_name ) return func ( payload = data , config = final_config ) else : html = obj . create_html () print ( f \"Rendered: { data [ 'identifier' ] } \" ) return html render ( self , data , config ) \u00b6 Render a template using provided data and configuration options. Parameters: Name Type Description Default data Dict[str, Any] The collected data to render. required config dict The rendering options. required Returns: Type Description str The rendered template as HTML. Source code in kiara/doc/mkdocstrings/renderer.py def render ( self , data : typing . Dict [ str , typing . Any ], config : dict ) -> str : final_config = ChainMap ( config , self . default_config ) obj = data [ \"obj\" ] data_type = obj . __class__ . __name__ . lower () func_name = f \"render_ { data_type } \" if hasattr ( self , func_name ): func = getattr ( self , func_name ) return func ( payload = data , config = final_config ) else : html = obj . create_html () print ( f \"Rendered: { data [ 'identifier' ] } \" ) return html","title":"mkdocstrings"},{"location":"reference/kiara/doc/mkdocstrings/__init__/#kiara.doc.mkdocstrings.collector","text":"","title":"collector"},{"location":"reference/kiara/doc/mkdocstrings/__init__/#kiara.doc.mkdocstrings.collector.KiaraCollector","text":"The class responsible for loading Jinja templates and rendering them. It defines some configuration options, implements the render method, and overrides the update_env method of the BaseRenderer class . Source code in kiara/doc/mkdocstrings/collector.py class KiaraCollector ( BaseCollector ): \"\"\"The class responsible for loading Jinja templates and rendering them. It defines some configuration options, implements the `render` method, and overrides the `update_env` method of the [`BaseRenderer` class][mkdocstrings.handlers.base.BaseRenderer]. \"\"\" default_config : dict = { \"docstring_style\" : \"google\" , \"docstring_options\" : {}} \"\"\"The default selection options. Option | Type | Description | Default ------ | ---- | ----------- | ------- **`docstring_style`** | `\"google\" | \"numpy\" | \"sphinx\" | None` | The docstring style to use. | `\"google\"` **`docstring_options`** | `dict[str, Any]` | The options for the docstring parser. | `{}` \"\"\" fallback_config : dict = { \"fallback\" : True } def __init__ ( self ) -> None : \"\"\"Initialize the collector.\"\"\" self . _kiara : Kiara = Kiara . instance () self . _context : KiaraContext = KiaraContext . get_info ( kiara = self . _kiara , ignore_errors = False ) self . _component_tree : nx . DiGraph = self . _context . get_subcomponent_tree () # type: ignore def collect ( self , identifier : str , config : dict ) -> CollectorItem : # noqa: WPS231 \"\"\"Collect the documentation tree given an identifier and selection options. Arguments: identifier: The dotted-path of a Python object available in the Python path. config: Selection options, used to alter the data collection done by `pytkdocs`. Raises: CollectionError: When there was a problem collecting the object documentation. Returns: The collected object-tree. \"\"\" tokens = identifier . split ( \".\" ) kiara_id = \".\" . join ( tokens [ 1 :]) print ( f \"REQUESTED: { identifier } \" ) if tokens [ 0 ] != \"kiara_info\" : return None # raise Exception( # f\"Handler 'kiara' can only be used with identifiers that start with 'kiara_info.', the provided id is invalid: {identifier}\" # ) try : info = self . _component_tree . nodes [ f \"__self__. { kiara_id } \" ][ \"obj\" ] except Exception as e : import traceback traceback . print_exc () raise e print ( f \"DONE: { identifier } \" ) return { \"obj\" : info , \"identifier\" : identifier , \"kiara_id\" : kiara_id }","title":"KiaraCollector"},{"location":"reference/kiara/doc/mkdocstrings/__init__/#kiara.doc.mkdocstrings.handler","text":"","title":"handler"},{"location":"reference/kiara/doc/mkdocstrings/__init__/#kiara.doc.mkdocstrings.handler.KiaraHandler","text":"The kiara handler class. Attributes: Name Type Description domain str The cross-documentation domain/language for this handler. enable_inventory bool Whether this handler is interested in enabling the creation of the objects.inv Sphinx inventory file. Source code in kiara/doc/mkdocstrings/handler.py class KiaraHandler ( BaseHandler ): \"\"\"The kiara handler class. Attributes: domain: The cross-documentation domain/language for this handler. enable_inventory: Whether this handler is interested in enabling the creation of the `objects.inv` Sphinx inventory file. \"\"\" domain : str = \"kiara\" enable_inventory : bool = True # load_inventory = staticmethod(inventory.list_object_urls) # # @classmethod # def load_inventory( # cls, # in_file: typing.BinaryIO, # url: str, # base_url: typing.Optional[str] = None, # **kwargs: typing.Any, # ) -> typing.Iterator[typing.Tuple[str, str]]: # \"\"\"Yield items and their URLs from an inventory file streamed from `in_file`. # This implements mkdocstrings' `load_inventory` \"protocol\" (see plugin.py). # Arguments: # in_file: The binary file-like object to read the inventory from. # url: The URL that this file is being streamed from (used to guess `base_url`). # base_url: The URL that this inventory's sub-paths are relative to. # **kwargs: Ignore additional arguments passed from the config. # Yields: # Tuples of (item identifier, item URL). # \"\"\" # # print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\") # # if base_url is None: # base_url = posixpath.dirname(url) # # for item in Inventory.parse_sphinx( # in_file, domain_filter=(\"py\",) # ).values(): # noqa: WPS526 # yield item.name, posixpath.join(base_url, item.uri)","title":"KiaraHandler"},{"location":"reference/kiara/doc/mkdocstrings/__init__/#kiara.doc.mkdocstrings.handler.get_handler","text":"Simply return an instance of PythonHandler . Parameters: Name Type Description Default theme str The theme to use when rendering contents. required custom_templates Optional[str] Directory containing custom templates. None **config Any Configuration passed to the handler. {} Returns: Type Description KiaraHandler An instance of PythonHandler . Source code in kiara/doc/mkdocstrings/handler.py def get_handler ( theme : str , # noqa: W0613 (unused argument config) custom_templates : typing . Optional [ str ] = None , ** config : typing . Any , ) -> KiaraHandler : \"\"\"Simply return an instance of `PythonHandler`. Arguments: theme: The theme to use when rendering contents. custom_templates: Directory containing custom templates. **config: Configuration passed to the handler. Returns: An instance of `PythonHandler`. \"\"\" if custom_templates is not None : raise Exception ( \"Custom templates are not supported for the kiara renderer.\" ) custom_templates = os . path . join ( KIARA_RESOURCES_FOLDER , \"templates\" , \"info_templates\" ) return KiaraHandler ( collector = KiaraCollector (), renderer = KiaraInfoRenderer ( \"kiara\" , theme , custom_templates ), )","title":"get_handler()"},{"location":"reference/kiara/doc/mkdocstrings/__init__/#kiara.doc.mkdocstrings.renderer","text":"","title":"renderer"},{"location":"reference/kiara/doc/mkdocstrings/__init__/#kiara.doc.mkdocstrings.renderer.KiaraInfoRenderer","text":"Source code in kiara/doc/mkdocstrings/renderer.py class KiaraInfoRenderer ( BaseRenderer ): default_config : dict = {} def render ( self , data : typing . Dict [ str , typing . Any ], config : dict ) -> str : final_config = ChainMap ( config , self . default_config ) obj = data [ \"obj\" ] data_type = obj . __class__ . __name__ . lower () func_name = f \"render_ { data_type } \" if hasattr ( self , func_name ): func = getattr ( self , func_name ) return func ( payload = data , config = final_config ) else : html = obj . create_html () print ( f \"Rendered: { data [ 'identifier' ] } \" ) return html","title":"KiaraInfoRenderer"},{"location":"reference/kiara/doc/mkdocstrings/collector/","text":"KiaraCollector ( BaseCollector ) \u00b6 The class responsible for loading Jinja templates and rendering them. It defines some configuration options, implements the render method, and overrides the update_env method of the BaseRenderer class . Source code in kiara/doc/mkdocstrings/collector.py class KiaraCollector ( BaseCollector ): \"\"\"The class responsible for loading Jinja templates and rendering them. It defines some configuration options, implements the `render` method, and overrides the `update_env` method of the [`BaseRenderer` class][mkdocstrings.handlers.base.BaseRenderer]. \"\"\" default_config : dict = { \"docstring_style\" : \"google\" , \"docstring_options\" : {}} \"\"\"The default selection options. Option | Type | Description | Default ------ | ---- | ----------- | ------- **`docstring_style`** | `\"google\" | \"numpy\" | \"sphinx\" | None` | The docstring style to use. | `\"google\"` **`docstring_options`** | `dict[str, Any]` | The options for the docstring parser. | `{}` \"\"\" fallback_config : dict = { \"fallback\" : True } def __init__ ( self ) -> None : \"\"\"Initialize the collector.\"\"\" self . _kiara : Kiara = Kiara . instance () self . _context : KiaraContext = KiaraContext . get_info ( kiara = self . _kiara , ignore_errors = False ) self . _component_tree : nx . DiGraph = self . _context . get_subcomponent_tree () # type: ignore def collect ( self , identifier : str , config : dict ) -> CollectorItem : # noqa: WPS231 \"\"\"Collect the documentation tree given an identifier and selection options. Arguments: identifier: The dotted-path of a Python object available in the Python path. config: Selection options, used to alter the data collection done by `pytkdocs`. Raises: CollectionError: When there was a problem collecting the object documentation. Returns: The collected object-tree. \"\"\" tokens = identifier . split ( \".\" ) kiara_id = \".\" . join ( tokens [ 1 :]) print ( f \"REQUESTED: { identifier } \" ) if tokens [ 0 ] != \"kiara_info\" : return None # raise Exception( # f\"Handler 'kiara' can only be used with identifiers that start with 'kiara_info.', the provided id is invalid: {identifier}\" # ) try : info = self . _component_tree . nodes [ f \"__self__. { kiara_id } \" ][ \"obj\" ] except Exception as e : import traceback traceback . print_exc () raise e print ( f \"DONE: { identifier } \" ) return { \"obj\" : info , \"identifier\" : identifier , \"kiara_id\" : kiara_id } default_config : dict \u00b6 The default selection options. Option | Type | Description | Default ------ | ---- | ----------- | ------- docstring_style | \"google\" | \"numpy\" | \"sphinx\" | None | The docstring style to use. | \"google\" docstring_options | dict[str, Any] | The options for the docstring parser. | {} __init__ ( self ) special \u00b6 Initialize the collector. Source code in kiara/doc/mkdocstrings/collector.py def __init__ ( self ) -> None : \"\"\"Initialize the collector.\"\"\" self . _kiara : Kiara = Kiara . instance () self . _context : KiaraContext = KiaraContext . get_info ( kiara = self . _kiara , ignore_errors = False ) self . _component_tree : nx . DiGraph = self . _context . get_subcomponent_tree () # type: ignore collect ( self , identifier , config ) \u00b6 Collect the documentation tree given an identifier and selection options. Parameters: Name Type Description Default identifier str The dotted-path of a Python object available in the Python path. required config dict Selection options, used to alter the data collection done by pytkdocs . required Exceptions: Type Description CollectionError When there was a problem collecting the object documentation. Returns: Type Description CollectorItem The collected object-tree. Source code in kiara/doc/mkdocstrings/collector.py def collect ( self , identifier : str , config : dict ) -> CollectorItem : # noqa: WPS231 \"\"\"Collect the documentation tree given an identifier and selection options. Arguments: identifier: The dotted-path of a Python object available in the Python path. config: Selection options, used to alter the data collection done by `pytkdocs`. Raises: CollectionError: When there was a problem collecting the object documentation. Returns: The collected object-tree. \"\"\" tokens = identifier . split ( \".\" ) kiara_id = \".\" . join ( tokens [ 1 :]) print ( f \"REQUESTED: { identifier } \" ) if tokens [ 0 ] != \"kiara_info\" : return None # raise Exception( # f\"Handler 'kiara' can only be used with identifiers that start with 'kiara_info.', the provided id is invalid: {identifier}\" # ) try : info = self . _component_tree . nodes [ f \"__self__. { kiara_id } \" ][ \"obj\" ] except Exception as e : import traceback traceback . print_exc () raise e print ( f \"DONE: { identifier } \" ) return { \"obj\" : info , \"identifier\" : identifier , \"kiara_id\" : kiara_id }","title":"collector"},{"location":"reference/kiara/doc/mkdocstrings/collector/#kiara.doc.mkdocstrings.collector.KiaraCollector","text":"The class responsible for loading Jinja templates and rendering them. It defines some configuration options, implements the render method, and overrides the update_env method of the BaseRenderer class . Source code in kiara/doc/mkdocstrings/collector.py class KiaraCollector ( BaseCollector ): \"\"\"The class responsible for loading Jinja templates and rendering them. It defines some configuration options, implements the `render` method, and overrides the `update_env` method of the [`BaseRenderer` class][mkdocstrings.handlers.base.BaseRenderer]. \"\"\" default_config : dict = { \"docstring_style\" : \"google\" , \"docstring_options\" : {}} \"\"\"The default selection options. Option | Type | Description | Default ------ | ---- | ----------- | ------- **`docstring_style`** | `\"google\" | \"numpy\" | \"sphinx\" | None` | The docstring style to use. | `\"google\"` **`docstring_options`** | `dict[str, Any]` | The options for the docstring parser. | `{}` \"\"\" fallback_config : dict = { \"fallback\" : True } def __init__ ( self ) -> None : \"\"\"Initialize the collector.\"\"\" self . _kiara : Kiara = Kiara . instance () self . _context : KiaraContext = KiaraContext . get_info ( kiara = self . _kiara , ignore_errors = False ) self . _component_tree : nx . DiGraph = self . _context . get_subcomponent_tree () # type: ignore def collect ( self , identifier : str , config : dict ) -> CollectorItem : # noqa: WPS231 \"\"\"Collect the documentation tree given an identifier and selection options. Arguments: identifier: The dotted-path of a Python object available in the Python path. config: Selection options, used to alter the data collection done by `pytkdocs`. Raises: CollectionError: When there was a problem collecting the object documentation. Returns: The collected object-tree. \"\"\" tokens = identifier . split ( \".\" ) kiara_id = \".\" . join ( tokens [ 1 :]) print ( f \"REQUESTED: { identifier } \" ) if tokens [ 0 ] != \"kiara_info\" : return None # raise Exception( # f\"Handler 'kiara' can only be used with identifiers that start with 'kiara_info.', the provided id is invalid: {identifier}\" # ) try : info = self . _component_tree . nodes [ f \"__self__. { kiara_id } \" ][ \"obj\" ] except Exception as e : import traceback traceback . print_exc () raise e print ( f \"DONE: { identifier } \" ) return { \"obj\" : info , \"identifier\" : identifier , \"kiara_id\" : kiara_id }","title":"KiaraCollector"},{"location":"reference/kiara/doc/mkdocstrings/collector/#kiara.doc.mkdocstrings.collector.KiaraCollector.default_config","text":"The default selection options. Option | Type | Description | Default ------ | ---- | ----------- | ------- docstring_style | \"google\" | \"numpy\" | \"sphinx\" | None | The docstring style to use. | \"google\" docstring_options | dict[str, Any] | The options for the docstring parser. | {}","title":"default_config"},{"location":"reference/kiara/doc/mkdocstrings/collector/#kiara.doc.mkdocstrings.collector.KiaraCollector.__init__","text":"Initialize the collector. Source code in kiara/doc/mkdocstrings/collector.py def __init__ ( self ) -> None : \"\"\"Initialize the collector.\"\"\" self . _kiara : Kiara = Kiara . instance () self . _context : KiaraContext = KiaraContext . get_info ( kiara = self . _kiara , ignore_errors = False ) self . _component_tree : nx . DiGraph = self . _context . get_subcomponent_tree () # type: ignore","title":"__init__()"},{"location":"reference/kiara/doc/mkdocstrings/collector/#kiara.doc.mkdocstrings.collector.KiaraCollector.collect","text":"Collect the documentation tree given an identifier and selection options. Parameters: Name Type Description Default identifier str The dotted-path of a Python object available in the Python path. required config dict Selection options, used to alter the data collection done by pytkdocs . required Exceptions: Type Description CollectionError When there was a problem collecting the object documentation. Returns: Type Description CollectorItem The collected object-tree. Source code in kiara/doc/mkdocstrings/collector.py def collect ( self , identifier : str , config : dict ) -> CollectorItem : # noqa: WPS231 \"\"\"Collect the documentation tree given an identifier and selection options. Arguments: identifier: The dotted-path of a Python object available in the Python path. config: Selection options, used to alter the data collection done by `pytkdocs`. Raises: CollectionError: When there was a problem collecting the object documentation. Returns: The collected object-tree. \"\"\" tokens = identifier . split ( \".\" ) kiara_id = \".\" . join ( tokens [ 1 :]) print ( f \"REQUESTED: { identifier } \" ) if tokens [ 0 ] != \"kiara_info\" : return None # raise Exception( # f\"Handler 'kiara' can only be used with identifiers that start with 'kiara_info.', the provided id is invalid: {identifier}\" # ) try : info = self . _component_tree . nodes [ f \"__self__. { kiara_id } \" ][ \"obj\" ] except Exception as e : import traceback traceback . print_exc () raise e print ( f \"DONE: { identifier } \" ) return { \"obj\" : info , \"identifier\" : identifier , \"kiara_id\" : kiara_id }","title":"collect()"},{"location":"reference/kiara/doc/mkdocstrings/handler/","text":"KiaraHandler ( BaseHandler ) \u00b6 The kiara handler class. Attributes: Name Type Description domain str The cross-documentation domain/language for this handler. enable_inventory bool Whether this handler is interested in enabling the creation of the objects.inv Sphinx inventory file. Source code in kiara/doc/mkdocstrings/handler.py class KiaraHandler ( BaseHandler ): \"\"\"The kiara handler class. Attributes: domain: The cross-documentation domain/language for this handler. enable_inventory: Whether this handler is interested in enabling the creation of the `objects.inv` Sphinx inventory file. \"\"\" domain : str = \"kiara\" enable_inventory : bool = True # load_inventory = staticmethod(inventory.list_object_urls) # # @classmethod # def load_inventory( # cls, # in_file: typing.BinaryIO, # url: str, # base_url: typing.Optional[str] = None, # **kwargs: typing.Any, # ) -> typing.Iterator[typing.Tuple[str, str]]: # \"\"\"Yield items and their URLs from an inventory file streamed from `in_file`. # This implements mkdocstrings' `load_inventory` \"protocol\" (see plugin.py). # Arguments: # in_file: The binary file-like object to read the inventory from. # url: The URL that this file is being streamed from (used to guess `base_url`). # base_url: The URL that this inventory's sub-paths are relative to. # **kwargs: Ignore additional arguments passed from the config. # Yields: # Tuples of (item identifier, item URL). # \"\"\" # # print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\") # # if base_url is None: # base_url = posixpath.dirname(url) # # for item in Inventory.parse_sphinx( # in_file, domain_filter=(\"py\",) # ).values(): # noqa: WPS526 # yield item.name, posixpath.join(base_url, item.uri) get_handler ( theme , custom_templates = None , ** config ) \u00b6 Simply return an instance of PythonHandler . Parameters: Name Type Description Default theme str The theme to use when rendering contents. required custom_templates Optional[str] Directory containing custom templates. None **config Any Configuration passed to the handler. {} Returns: Type Description KiaraHandler An instance of PythonHandler . Source code in kiara/doc/mkdocstrings/handler.py def get_handler ( theme : str , # noqa: W0613 (unused argument config) custom_templates : typing . Optional [ str ] = None , ** config : typing . Any , ) -> KiaraHandler : \"\"\"Simply return an instance of `PythonHandler`. Arguments: theme: The theme to use when rendering contents. custom_templates: Directory containing custom templates. **config: Configuration passed to the handler. Returns: An instance of `PythonHandler`. \"\"\" if custom_templates is not None : raise Exception ( \"Custom templates are not supported for the kiara renderer.\" ) custom_templates = os . path . join ( KIARA_RESOURCES_FOLDER , \"templates\" , \"info_templates\" ) return KiaraHandler ( collector = KiaraCollector (), renderer = KiaraInfoRenderer ( \"kiara\" , theme , custom_templates ), )","title":"handler"},{"location":"reference/kiara/doc/mkdocstrings/handler/#kiara.doc.mkdocstrings.handler.KiaraHandler","text":"The kiara handler class. Attributes: Name Type Description domain str The cross-documentation domain/language for this handler. enable_inventory bool Whether this handler is interested in enabling the creation of the objects.inv Sphinx inventory file. Source code in kiara/doc/mkdocstrings/handler.py class KiaraHandler ( BaseHandler ): \"\"\"The kiara handler class. Attributes: domain: The cross-documentation domain/language for this handler. enable_inventory: Whether this handler is interested in enabling the creation of the `objects.inv` Sphinx inventory file. \"\"\" domain : str = \"kiara\" enable_inventory : bool = True # load_inventory = staticmethod(inventory.list_object_urls) # # @classmethod # def load_inventory( # cls, # in_file: typing.BinaryIO, # url: str, # base_url: typing.Optional[str] = None, # **kwargs: typing.Any, # ) -> typing.Iterator[typing.Tuple[str, str]]: # \"\"\"Yield items and their URLs from an inventory file streamed from `in_file`. # This implements mkdocstrings' `load_inventory` \"protocol\" (see plugin.py). # Arguments: # in_file: The binary file-like object to read the inventory from. # url: The URL that this file is being streamed from (used to guess `base_url`). # base_url: The URL that this inventory's sub-paths are relative to. # **kwargs: Ignore additional arguments passed from the config. # Yields: # Tuples of (item identifier, item URL). # \"\"\" # # print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\") # # if base_url is None: # base_url = posixpath.dirname(url) # # for item in Inventory.parse_sphinx( # in_file, domain_filter=(\"py\",) # ).values(): # noqa: WPS526 # yield item.name, posixpath.join(base_url, item.uri)","title":"KiaraHandler"},{"location":"reference/kiara/doc/mkdocstrings/handler/#kiara.doc.mkdocstrings.handler.get_handler","text":"Simply return an instance of PythonHandler . Parameters: Name Type Description Default theme str The theme to use when rendering contents. required custom_templates Optional[str] Directory containing custom templates. None **config Any Configuration passed to the handler. {} Returns: Type Description KiaraHandler An instance of PythonHandler . Source code in kiara/doc/mkdocstrings/handler.py def get_handler ( theme : str , # noqa: W0613 (unused argument config) custom_templates : typing . Optional [ str ] = None , ** config : typing . Any , ) -> KiaraHandler : \"\"\"Simply return an instance of `PythonHandler`. Arguments: theme: The theme to use when rendering contents. custom_templates: Directory containing custom templates. **config: Configuration passed to the handler. Returns: An instance of `PythonHandler`. \"\"\" if custom_templates is not None : raise Exception ( \"Custom templates are not supported for the kiara renderer.\" ) custom_templates = os . path . join ( KIARA_RESOURCES_FOLDER , \"templates\" , \"info_templates\" ) return KiaraHandler ( collector = KiaraCollector (), renderer = KiaraInfoRenderer ( \"kiara\" , theme , custom_templates ), )","title":"get_handler()"},{"location":"reference/kiara/doc/mkdocstrings/renderer/","text":"KiaraInfoRenderer ( BaseRenderer ) \u00b6 Source code in kiara/doc/mkdocstrings/renderer.py class KiaraInfoRenderer ( BaseRenderer ): default_config : dict = {} def render ( self , data : typing . Dict [ str , typing . Any ], config : dict ) -> str : final_config = ChainMap ( config , self . default_config ) obj = data [ \"obj\" ] data_type = obj . __class__ . __name__ . lower () func_name = f \"render_ { data_type } \" if hasattr ( self , func_name ): func = getattr ( self , func_name ) return func ( payload = data , config = final_config ) else : html = obj . create_html () print ( f \"Rendered: { data [ 'identifier' ] } \" ) return html render ( self , data , config ) \u00b6 Render a template using provided data and configuration options. Parameters: Name Type Description Default data Dict[str, Any] The collected data to render. required config dict The rendering options. required Returns: Type Description str The rendered template as HTML. Source code in kiara/doc/mkdocstrings/renderer.py def render ( self , data : typing . Dict [ str , typing . Any ], config : dict ) -> str : final_config = ChainMap ( config , self . default_config ) obj = data [ \"obj\" ] data_type = obj . __class__ . __name__ . lower () func_name = f \"render_ { data_type } \" if hasattr ( self , func_name ): func = getattr ( self , func_name ) return func ( payload = data , config = final_config ) else : html = obj . create_html () print ( f \"Rendered: { data [ 'identifier' ] } \" ) return html","title":"renderer"},{"location":"reference/kiara/doc/mkdocstrings/renderer/#kiara.doc.mkdocstrings.renderer.KiaraInfoRenderer","text":"Source code in kiara/doc/mkdocstrings/renderer.py class KiaraInfoRenderer ( BaseRenderer ): default_config : dict = {} def render ( self , data : typing . Dict [ str , typing . Any ], config : dict ) -> str : final_config = ChainMap ( config , self . default_config ) obj = data [ \"obj\" ] data_type = obj . __class__ . __name__ . lower () func_name = f \"render_ { data_type } \" if hasattr ( self , func_name ): func = getattr ( self , func_name ) return func ( payload = data , config = final_config ) else : html = obj . create_html () print ( f \"Rendered: { data [ 'identifier' ] } \" ) return html","title":"KiaraInfoRenderer"},{"location":"reference/kiara/doc/mkdocstrings/renderer/#kiara.doc.mkdocstrings.renderer.KiaraInfoRenderer.render","text":"Render a template using provided data and configuration options. Parameters: Name Type Description Default data Dict[str, Any] The collected data to render. required config dict The rendering options. required Returns: Type Description str The rendered template as HTML. Source code in kiara/doc/mkdocstrings/renderer.py def render ( self , data : typing . Dict [ str , typing . Any ], config : dict ) -> str : final_config = ChainMap ( config , self . default_config ) obj = data [ \"obj\" ] data_type = obj . __class__ . __name__ . lower () func_name = f \"render_ { data_type } \" if hasattr ( self , func_name ): func = getattr ( self , func_name ) return func ( payload = data , config = final_config ) else : html = obj . create_html () print ( f \"Rendered: { data [ 'identifier' ] } \" ) return html","title":"render()"},{"location":"reference/kiara/environment/__init__/","text":"RuntimeEnvironmentConfig ( BaseModel ) pydantic-model \u00b6 Source code in kiara/environment/__init__.py class RuntimeEnvironmentConfig ( BaseModel ): class Config : allow_mutation = False include_all_info : bool = Field ( default = False , description = \"Whether to include all properties, even if it might include potentially private/sensitive information. This is mainly used for debug purposes.\" , ) include_all_info : bool pydantic-field \u00b6 Whether to include all properties, even if it might include potentially private/sensitive information. This is mainly used for debug purposes. operating_system \u00b6 OSRuntimeEnvironment ( BaseRuntimeEnvironment ) pydantic-model \u00b6 Manages information about the OS this kiara instance is running in. TODO: details for other OS's (mainly BSDs) \u00b6 Source code in kiara/environment/operating_system.py class OSRuntimeEnvironment ( BaseRuntimeEnvironment ): \"\"\"Manages information about the OS this kiara instance is running in. # TODO: details for other OS's (mainly BSDs) \"\"\" environment_type : typing . Literal [ \"operating_system\" ] operation_system : str = Field ( description = \"The operation system name.\" ) platform : str = Field ( description = \"The platform name.\" ) release : str = Field ( description = \"The platform release name.\" ) version : str = Field ( description = \"The platform version name.\" ) machine : str = Field ( description = \"The architecture.\" ) os_specific : typing . Dict [ str , typing . Any ] = Field ( description = \"OS specific platform metadata.\" , default_factory = dict ) uname : typing . Optional [ typing . Dict [ str , str ]] = Field ( description = \"Platform uname information.\" ) @classmethod def retrieve_environment_data ( self , config : RuntimeEnvironmentConfig ) -> typing . Dict [ str , typing . Any ]: os_specific : typing . Dict [ str , typing . Any ] = {} platform_system = platform . system () if platform_system == \"Linux\" : import distro data = distro . linux_distribution () os_specific [ \"distribution\" ] = { \"name\" : data [ 0 ], \"version\" : data [ 1 ], \"codename\" : data [ 2 ], } elif platform_system == \"Darwin\" : mac_version = platform . mac_ver () os_specific [ \"mac_ver_release\" ] = mac_version [ 0 ] os_specific [ \"mac_ver_machine\" ] = mac_version [ 2 ] result = { \"operation_system\" : os . name , \"platform\" : platform_system , \"release\" : platform . release (), \"version\" : platform . version (), \"machine\" : platform . machine (), \"os_specific\" : os_specific , } if config . include_all_info : result [ \"uname\" ] = platform . uname () . _asdict () return result machine : str pydantic-field required \u00b6 The architecture. operation_system : str pydantic-field required \u00b6 The operation system name. os_specific : Dict [ str , Any ] pydantic-field \u00b6 OS specific platform metadata. platform : str pydantic-field required \u00b6 The platform name. release : str pydantic-field required \u00b6 The platform release name. uname : Dict [ str , str ] pydantic-field \u00b6 Platform uname information. version : str pydantic-field required \u00b6 The platform version name. python \u00b6 PythonPackage ( BaseModel ) pydantic-model \u00b6 Source code in kiara/environment/python.py class PythonPackage ( BaseModel ): name : str = Field ( description = \"The name of the Python package.\" ) version : str = Field ( description = \"The version of the package.\" ) name : str pydantic-field required \u00b6 The name of the Python package. version : str pydantic-field required \u00b6 The version of the package. PythonRuntimeEnvironment ( BaseRuntimeEnvironment ) pydantic-model \u00b6 Source code in kiara/environment/python.py class PythonRuntimeEnvironment ( BaseRuntimeEnvironment ): environment_type : typing . Literal [ \"python\" ] python_version : str = Field ( description = \"The version of Python.\" ) packages : typing . List [ PythonPackage ] = Field ( description = \"The packages installed in the Python (virtual) environment.\" ) python_config : typing . Dict [ str , str ] = Field ( description = \"Configuration details about the Python installation.\" ) @classmethod def retrieve_environment_data ( cls , config : RuntimeEnvironmentConfig ) -> typing . Dict [ str , typing . Any ]: packages = [] all_packages = packages_distributions () for name , pkgs in all_packages . items (): for pkg in pkgs : dist = distribution ( pkg ) packages . append ({ \"name\" : name , \"version\" : dist . version }) result : typing . Dict [ str , typing . Any ] = { \"python_version\" : sys . version , \"packages\" : sorted ( packages , key = lambda x : x [ \"name\" ]), } if config . include_all_info : import sysconfig result [ \"python_config\" ] = sysconfig . get_config_vars () return result packages : List [ kiara . environment . python . PythonPackage ] pydantic-field required \u00b6 The packages installed in the Python (virtual) environment. python_config : Dict [ str , str ] pydantic-field required \u00b6 Configuration details about the Python installation. python_version : str pydantic-field required \u00b6 The version of Python.","title":"environment"},{"location":"reference/kiara/environment/__init__/#kiara.environment.RuntimeEnvironmentConfig","text":"Source code in kiara/environment/__init__.py class RuntimeEnvironmentConfig ( BaseModel ): class Config : allow_mutation = False include_all_info : bool = Field ( default = False , description = \"Whether to include all properties, even if it might include potentially private/sensitive information. This is mainly used for debug purposes.\" , )","title":"RuntimeEnvironmentConfig"},{"location":"reference/kiara/environment/__init__/#kiara.environment.RuntimeEnvironmentConfig.include_all_info","text":"Whether to include all properties, even if it might include potentially private/sensitive information. This is mainly used for debug purposes.","title":"include_all_info"},{"location":"reference/kiara/environment/__init__/#kiara.environment.operating_system","text":"","title":"operating_system"},{"location":"reference/kiara/environment/__init__/#kiara.environment.operating_system.OSRuntimeEnvironment","text":"Manages information about the OS this kiara instance is running in.","title":"OSRuntimeEnvironment"},{"location":"reference/kiara/environment/__init__/#kiara.environment.python","text":"","title":"python"},{"location":"reference/kiara/environment/__init__/#kiara.environment.python.PythonPackage","text":"Source code in kiara/environment/python.py class PythonPackage ( BaseModel ): name : str = Field ( description = \"The name of the Python package.\" ) version : str = Field ( description = \"The version of the package.\" )","title":"PythonPackage"},{"location":"reference/kiara/environment/__init__/#kiara.environment.python.PythonRuntimeEnvironment","text":"Source code in kiara/environment/python.py class PythonRuntimeEnvironment ( BaseRuntimeEnvironment ): environment_type : typing . Literal [ \"python\" ] python_version : str = Field ( description = \"The version of Python.\" ) packages : typing . List [ PythonPackage ] = Field ( description = \"The packages installed in the Python (virtual) environment.\" ) python_config : typing . Dict [ str , str ] = Field ( description = \"Configuration details about the Python installation.\" ) @classmethod def retrieve_environment_data ( cls , config : RuntimeEnvironmentConfig ) -> typing . Dict [ str , typing . Any ]: packages = [] all_packages = packages_distributions () for name , pkgs in all_packages . items (): for pkg in pkgs : dist = distribution ( pkg ) packages . append ({ \"name\" : name , \"version\" : dist . version }) result : typing . Dict [ str , typing . Any ] = { \"python_version\" : sys . version , \"packages\" : sorted ( packages , key = lambda x : x [ \"name\" ]), } if config . include_all_info : import sysconfig result [ \"python_config\" ] = sysconfig . get_config_vars () return result","title":"PythonRuntimeEnvironment"},{"location":"reference/kiara/environment/operating_system/","text":"OSRuntimeEnvironment ( BaseRuntimeEnvironment ) pydantic-model \u00b6 Manages information about the OS this kiara instance is running in. TODO: details for other OS's (mainly BSDs) \u00b6 Source code in kiara/environment/operating_system.py class OSRuntimeEnvironment ( BaseRuntimeEnvironment ): \"\"\"Manages information about the OS this kiara instance is running in. # TODO: details for other OS's (mainly BSDs) \"\"\" environment_type : typing . Literal [ \"operating_system\" ] operation_system : str = Field ( description = \"The operation system name.\" ) platform : str = Field ( description = \"The platform name.\" ) release : str = Field ( description = \"The platform release name.\" ) version : str = Field ( description = \"The platform version name.\" ) machine : str = Field ( description = \"The architecture.\" ) os_specific : typing . Dict [ str , typing . Any ] = Field ( description = \"OS specific platform metadata.\" , default_factory = dict ) uname : typing . Optional [ typing . Dict [ str , str ]] = Field ( description = \"Platform uname information.\" ) @classmethod def retrieve_environment_data ( self , config : RuntimeEnvironmentConfig ) -> typing . Dict [ str , typing . Any ]: os_specific : typing . Dict [ str , typing . Any ] = {} platform_system = platform . system () if platform_system == \"Linux\" : import distro data = distro . linux_distribution () os_specific [ \"distribution\" ] = { \"name\" : data [ 0 ], \"version\" : data [ 1 ], \"codename\" : data [ 2 ], } elif platform_system == \"Darwin\" : mac_version = platform . mac_ver () os_specific [ \"mac_ver_release\" ] = mac_version [ 0 ] os_specific [ \"mac_ver_machine\" ] = mac_version [ 2 ] result = { \"operation_system\" : os . name , \"platform\" : platform_system , \"release\" : platform . release (), \"version\" : platform . version (), \"machine\" : platform . machine (), \"os_specific\" : os_specific , } if config . include_all_info : result [ \"uname\" ] = platform . uname () . _asdict () return result machine : str pydantic-field required \u00b6 The architecture. operation_system : str pydantic-field required \u00b6 The operation system name. os_specific : Dict [ str , Any ] pydantic-field \u00b6 OS specific platform metadata. platform : str pydantic-field required \u00b6 The platform name. release : str pydantic-field required \u00b6 The platform release name. uname : Dict [ str , str ] pydantic-field \u00b6 Platform uname information. version : str pydantic-field required \u00b6 The platform version name.","title":"operating_system"},{"location":"reference/kiara/environment/operating_system/#kiara.environment.operating_system.OSRuntimeEnvironment","text":"Manages information about the OS this kiara instance is running in.","title":"OSRuntimeEnvironment"},{"location":"reference/kiara/environment/operating_system/#kiara.environment.operating_system.OSRuntimeEnvironment--todo-details-for-other-oss-mainly-bsds","text":"Source code in kiara/environment/operating_system.py class OSRuntimeEnvironment ( BaseRuntimeEnvironment ): \"\"\"Manages information about the OS this kiara instance is running in. # TODO: details for other OS's (mainly BSDs) \"\"\" environment_type : typing . Literal [ \"operating_system\" ] operation_system : str = Field ( description = \"The operation system name.\" ) platform : str = Field ( description = \"The platform name.\" ) release : str = Field ( description = \"The platform release name.\" ) version : str = Field ( description = \"The platform version name.\" ) machine : str = Field ( description = \"The architecture.\" ) os_specific : typing . Dict [ str , typing . Any ] = Field ( description = \"OS specific platform metadata.\" , default_factory = dict ) uname : typing . Optional [ typing . Dict [ str , str ]] = Field ( description = \"Platform uname information.\" ) @classmethod def retrieve_environment_data ( self , config : RuntimeEnvironmentConfig ) -> typing . Dict [ str , typing . Any ]: os_specific : typing . Dict [ str , typing . Any ] = {} platform_system = platform . system () if platform_system == \"Linux\" : import distro data = distro . linux_distribution () os_specific [ \"distribution\" ] = { \"name\" : data [ 0 ], \"version\" : data [ 1 ], \"codename\" : data [ 2 ], } elif platform_system == \"Darwin\" : mac_version = platform . mac_ver () os_specific [ \"mac_ver_release\" ] = mac_version [ 0 ] os_specific [ \"mac_ver_machine\" ] = mac_version [ 2 ] result = { \"operation_system\" : os . name , \"platform\" : platform_system , \"release\" : platform . release (), \"version\" : platform . version (), \"machine\" : platform . machine (), \"os_specific\" : os_specific , } if config . include_all_info : result [ \"uname\" ] = platform . uname () . _asdict () return result","title":"TODO: details for other OS's (mainly BSDs)"},{"location":"reference/kiara/environment/operating_system/#kiara.environment.operating_system.OSRuntimeEnvironment.machine","text":"The architecture.","title":"machine"},{"location":"reference/kiara/environment/operating_system/#kiara.environment.operating_system.OSRuntimeEnvironment.operation_system","text":"The operation system name.","title":"operation_system"},{"location":"reference/kiara/environment/operating_system/#kiara.environment.operating_system.OSRuntimeEnvironment.os_specific","text":"OS specific platform metadata.","title":"os_specific"},{"location":"reference/kiara/environment/operating_system/#kiara.environment.operating_system.OSRuntimeEnvironment.platform","text":"The platform name.","title":"platform"},{"location":"reference/kiara/environment/operating_system/#kiara.environment.operating_system.OSRuntimeEnvironment.release","text":"The platform release name.","title":"release"},{"location":"reference/kiara/environment/operating_system/#kiara.environment.operating_system.OSRuntimeEnvironment.uname","text":"Platform uname information.","title":"uname"},{"location":"reference/kiara/environment/operating_system/#kiara.environment.operating_system.OSRuntimeEnvironment.version","text":"The platform version name.","title":"version"},{"location":"reference/kiara/environment/python/","text":"PythonPackage ( BaseModel ) pydantic-model \u00b6 Source code in kiara/environment/python.py class PythonPackage ( BaseModel ): name : str = Field ( description = \"The name of the Python package.\" ) version : str = Field ( description = \"The version of the package.\" ) name : str pydantic-field required \u00b6 The name of the Python package. version : str pydantic-field required \u00b6 The version of the package. PythonRuntimeEnvironment ( BaseRuntimeEnvironment ) pydantic-model \u00b6 Source code in kiara/environment/python.py class PythonRuntimeEnvironment ( BaseRuntimeEnvironment ): environment_type : typing . Literal [ \"python\" ] python_version : str = Field ( description = \"The version of Python.\" ) packages : typing . List [ PythonPackage ] = Field ( description = \"The packages installed in the Python (virtual) environment.\" ) python_config : typing . Dict [ str , str ] = Field ( description = \"Configuration details about the Python installation.\" ) @classmethod def retrieve_environment_data ( cls , config : RuntimeEnvironmentConfig ) -> typing . Dict [ str , typing . Any ]: packages = [] all_packages = packages_distributions () for name , pkgs in all_packages . items (): for pkg in pkgs : dist = distribution ( pkg ) packages . append ({ \"name\" : name , \"version\" : dist . version }) result : typing . Dict [ str , typing . Any ] = { \"python_version\" : sys . version , \"packages\" : sorted ( packages , key = lambda x : x [ \"name\" ]), } if config . include_all_info : import sysconfig result [ \"python_config\" ] = sysconfig . get_config_vars () return result packages : List [ kiara . environment . python . PythonPackage ] pydantic-field required \u00b6 The packages installed in the Python (virtual) environment. python_config : Dict [ str , str ] pydantic-field required \u00b6 Configuration details about the Python installation. python_version : str pydantic-field required \u00b6 The version of Python.","title":"python"},{"location":"reference/kiara/environment/python/#kiara.environment.python.PythonPackage","text":"Source code in kiara/environment/python.py class PythonPackage ( BaseModel ): name : str = Field ( description = \"The name of the Python package.\" ) version : str = Field ( description = \"The version of the package.\" )","title":"PythonPackage"},{"location":"reference/kiara/environment/python/#kiara.environment.python.PythonPackage.name","text":"The name of the Python package.","title":"name"},{"location":"reference/kiara/environment/python/#kiara.environment.python.PythonPackage.version","text":"The version of the package.","title":"version"},{"location":"reference/kiara/environment/python/#kiara.environment.python.PythonRuntimeEnvironment","text":"Source code in kiara/environment/python.py class PythonRuntimeEnvironment ( BaseRuntimeEnvironment ): environment_type : typing . Literal [ \"python\" ] python_version : str = Field ( description = \"The version of Python.\" ) packages : typing . List [ PythonPackage ] = Field ( description = \"The packages installed in the Python (virtual) environment.\" ) python_config : typing . Dict [ str , str ] = Field ( description = \"Configuration details about the Python installation.\" ) @classmethod def retrieve_environment_data ( cls , config : RuntimeEnvironmentConfig ) -> typing . Dict [ str , typing . Any ]: packages = [] all_packages = packages_distributions () for name , pkgs in all_packages . items (): for pkg in pkgs : dist = distribution ( pkg ) packages . append ({ \"name\" : name , \"version\" : dist . version }) result : typing . Dict [ str , typing . Any ] = { \"python_version\" : sys . version , \"packages\" : sorted ( packages , key = lambda x : x [ \"name\" ]), } if config . include_all_info : import sysconfig result [ \"python_config\" ] = sysconfig . get_config_vars () return result","title":"PythonRuntimeEnvironment"},{"location":"reference/kiara/environment/python/#kiara.environment.python.PythonRuntimeEnvironment.packages","text":"The packages installed in the Python (virtual) environment.","title":"packages"},{"location":"reference/kiara/environment/python/#kiara.environment.python.PythonRuntimeEnvironment.python_config","text":"Configuration details about the Python installation.","title":"python_config"},{"location":"reference/kiara/environment/python/#kiara.environment.python.PythonRuntimeEnvironment.python_version","text":"The version of Python.","title":"python_version"},{"location":"reference/kiara/examples/__init__/","text":"example_controller \u00b6 ExampleController ( PipelineController ) \u00b6 Source code in kiara/examples/example_controller.py class ExampleController ( PipelineController ): def pipeline_inputs_changed ( self , event : PipelineInputEvent ): print ( f \"Pipeline inputs changed: { event . updated_pipeline_inputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): print ( f \"Pipeline outputs changed: { event . updated_pipeline_outputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) def step_inputs_changed ( self , event : StepInputEvent ): print ( \"Step inputs changed, new values:\" ) for step_id , input_names in event . updated_step_inputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in input_names : new_value = self . get_step_inputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) def step_outputs_changed ( self , event : StepOutputEvent ): print ( \"Step outputs changed, new values:\" ) for step_id , output_names in event . updated_step_outputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in output_names : new_value = self . get_step_outputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) def execute ( self ): print ( \"Executing steps: 'and', 'not'...\" ) and_job_id = self . process_step ( \"and\" ) self . wait_for_jobs ( and_job_id ) not_job_id = self . process_step ( \"not\" ) self . wait_for_jobs ( not_job_id ) pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/examples/example_controller.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): print ( f \"Pipeline inputs changed: { event . updated_pipeline_inputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/examples/example_controller.py def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): print ( f \"Pipeline outputs changed: { event . updated_pipeline_outputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/examples/example_controller.py def step_inputs_changed ( self , event : StepInputEvent ): print ( \"Step inputs changed, new values:\" ) for step_id , input_names in event . updated_step_inputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in input_names : new_value = self . get_step_inputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) step_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Parameters: Name Type Description Default event StepOutputEvent the step output event required Source code in kiara/examples/example_controller.py def step_outputs_changed ( self , event : StepOutputEvent ): print ( \"Step outputs changed, new values:\" ) for step_id , output_names in event . updated_step_outputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in output_names : new_value = self . get_step_outputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" )","title":"examples"},{"location":"reference/kiara/examples/__init__/#kiara.examples.example_controller","text":"","title":"example_controller"},{"location":"reference/kiara/examples/__init__/#kiara.examples.example_controller.ExampleController","text":"Source code in kiara/examples/example_controller.py class ExampleController ( PipelineController ): def pipeline_inputs_changed ( self , event : PipelineInputEvent ): print ( f \"Pipeline inputs changed: { event . updated_pipeline_inputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): print ( f \"Pipeline outputs changed: { event . updated_pipeline_outputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) def step_inputs_changed ( self , event : StepInputEvent ): print ( \"Step inputs changed, new values:\" ) for step_id , input_names in event . updated_step_inputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in input_names : new_value = self . get_step_inputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) def step_outputs_changed ( self , event : StepOutputEvent ): print ( \"Step outputs changed, new values:\" ) for step_id , output_names in event . updated_step_outputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in output_names : new_value = self . get_step_outputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) def execute ( self ): print ( \"Executing steps: 'and', 'not'...\" ) and_job_id = self . process_step ( \"and\" ) self . wait_for_jobs ( and_job_id ) not_job_id = self . process_step ( \"not\" ) self . wait_for_jobs ( not_job_id )","title":"ExampleController"},{"location":"reference/kiara/examples/example_controller/","text":"ExampleController ( PipelineController ) \u00b6 Source code in kiara/examples/example_controller.py class ExampleController ( PipelineController ): def pipeline_inputs_changed ( self , event : PipelineInputEvent ): print ( f \"Pipeline inputs changed: { event . updated_pipeline_inputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): print ( f \"Pipeline outputs changed: { event . updated_pipeline_outputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) def step_inputs_changed ( self , event : StepInputEvent ): print ( \"Step inputs changed, new values:\" ) for step_id , input_names in event . updated_step_inputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in input_names : new_value = self . get_step_inputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) def step_outputs_changed ( self , event : StepOutputEvent ): print ( \"Step outputs changed, new values:\" ) for step_id , output_names in event . updated_step_outputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in output_names : new_value = self . get_step_outputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) def execute ( self ): print ( \"Executing steps: 'and', 'not'...\" ) and_job_id = self . process_step ( \"and\" ) self . wait_for_jobs ( and_job_id ) not_job_id = self . process_step ( \"not\" ) self . wait_for_jobs ( not_job_id ) pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/examples/example_controller.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): print ( f \"Pipeline inputs changed: { event . updated_pipeline_inputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/examples/example_controller.py def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): print ( f \"Pipeline outputs changed: { event . updated_pipeline_outputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/examples/example_controller.py def step_inputs_changed ( self , event : StepInputEvent ): print ( \"Step inputs changed, new values:\" ) for step_id , input_names in event . updated_step_inputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in input_names : new_value = self . get_step_inputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) step_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Parameters: Name Type Description Default event StepOutputEvent the step output event required Source code in kiara/examples/example_controller.py def step_outputs_changed ( self , event : StepOutputEvent ): print ( \"Step outputs changed, new values:\" ) for step_id , output_names in event . updated_step_outputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in output_names : new_value = self . get_step_outputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" )","title":"example_controller"},{"location":"reference/kiara/examples/example_controller/#kiara.examples.example_controller.ExampleController","text":"Source code in kiara/examples/example_controller.py class ExampleController ( PipelineController ): def pipeline_inputs_changed ( self , event : PipelineInputEvent ): print ( f \"Pipeline inputs changed: { event . updated_pipeline_inputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): print ( f \"Pipeline outputs changed: { event . updated_pipeline_outputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" ) def step_inputs_changed ( self , event : StepInputEvent ): print ( \"Step inputs changed, new values:\" ) for step_id , input_names in event . updated_step_inputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in input_names : new_value = self . get_step_inputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) def step_outputs_changed ( self , event : StepOutputEvent ): print ( \"Step outputs changed, new values:\" ) for step_id , output_names in event . updated_step_outputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in output_names : new_value = self . get_step_outputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" ) def execute ( self ): print ( \"Executing steps: 'and', 'not'...\" ) and_job_id = self . process_step ( \"and\" ) self . wait_for_jobs ( and_job_id ) not_job_id = self . process_step ( \"not\" ) self . wait_for_jobs ( not_job_id )","title":"ExampleController"},{"location":"reference/kiara/examples/example_controller/#kiara.examples.example_controller.ExampleController.pipeline_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/examples/example_controller.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): print ( f \"Pipeline inputs changed: { event . updated_pipeline_inputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" )","title":"pipeline_inputs_changed()"},{"location":"reference/kiara/examples/example_controller/#kiara.examples.example_controller.ExampleController.pipeline_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/examples/example_controller.py def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): print ( f \"Pipeline outputs changed: { event . updated_pipeline_outputs } \" ) print ( f \" -> pipeline status: { self . pipeline_status . name } \" )","title":"pipeline_outputs_changed()"},{"location":"reference/kiara/examples/example_controller/#kiara.examples.example_controller.ExampleController.step_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/examples/example_controller.py def step_inputs_changed ( self , event : StepInputEvent ): print ( \"Step inputs changed, new values:\" ) for step_id , input_names in event . updated_step_inputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in input_names : new_value = self . get_step_inputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" )","title":"step_inputs_changed()"},{"location":"reference/kiara/examples/example_controller/#kiara.examples.example_controller.ExampleController.step_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Parameters: Name Type Description Default event StepOutputEvent the step output event required Source code in kiara/examples/example_controller.py def step_outputs_changed ( self , event : StepOutputEvent ): print ( \"Step outputs changed, new values:\" ) for step_id , output_names in event . updated_step_outputs . items (): print ( f \" - step ' { step_id } ':\" ) for name in output_names : new_value = self . get_step_outputs ( step_id ) . get ( name ) . get_value_data () print ( f \" { name } : { new_value } \" )","title":"step_outputs_changed()"},{"location":"reference/kiara/info/__init__/","text":"kiara \u00b6 KiaraContext ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/kiara.py class KiaraContext ( KiaraInfoModel ): available_categories : typing . ClassVar = [ \"value_types\" , \"modules\" , \"pipelines\" , \"operations\" , \"operation_types\" , ] _info_cache : typing . ClassVar = {} @classmethod def get_info_for_category ( cls , kiara : \"Kiara\" , category_name : str , ignore_errors : bool = False ) -> KiaraInfoModel : if category_name not in cls . available_categories : raise Exception ( f \"Can't provide information for category ' { category_name } ': invalid category name. Valid names: { ', ' . join ( cls . available_categories ) } \" ) cache = ( cls . _info_cache . get ( kiara . _id , {}) . get ( category_name , {}) . get ( ignore_errors , None ) ) if cache is not None : return cache if category_name == \"value_types\" : all_types = ValueTypeMetadata . create_all ( kiara = kiara ) info = KiaraDynamicInfoModel . create_from_child_models ( ** all_types ) elif category_name == \"modules\" : info = ModuleTypesGroupInfo . from_type_names ( kiara = kiara , ignore_pipeline_modules = True ) elif category_name == \"pipelines\" : info = PipelineTypesGroupInfo . create ( kiara = kiara , ignore_errors = ignore_errors ) elif category_name == \"operation_types\" : all_op_types = OperationsMetadata . create_all ( kiara = kiara ) info = KiaraDynamicInfoModel . create_from_child_models ( ** all_op_types ) elif category_name == \"operations\" : ops_infos = KiaraDynamicInfoModel . create_from_child_models ( ** OperationsInfo . create_all ( kiara = kiara ) ) operation_configs = {} for v in ops_infos . __root__ . values (): configs = v . operation_configs operation_configs . update ( configs ) info = KiaraDynamicInfoModel . create_from_child_models ( ** operation_configs ) else : raise NotImplementedError ( f \"Category not available: { category_name } \" ) cls . _info_cache . setdefault ( kiara . _id , {}) . setdefault ( ignore_errors , {})[ category_name ] = info return info @classmethod def get_info ( cls , kiara : \"Kiara\" , sub_type : typing . Optional [ str ] = None , ignore_errors : bool = False , ): if sub_type is None : module_types = cls . get_info_for_category ( kiara = kiara , category_name = \"modules\" , ignore_errors = ignore_errors ) value_types = cls . get_info_for_category ( kiara = kiara , category_name = \"value_types\" , ignore_errors = ignore_errors ) pipeline_types = cls . get_info_for_category ( kiara = kiara , category_name = \"pipelines\" , ignore_errors = ignore_errors ) op_group = cls . get_info_for_category ( kiara = kiara , category_name = \"operations\" , ignore_errors = ignore_errors ) op_types = cls . get_info_for_category ( kiara = kiara , category_name = \"operation_types\" , ignore_errors = ignore_errors , ) return KiaraContext ( value_types = value_types , modules = module_types , pipelines = pipeline_types , operations = op_group , operation_types = op_types , ) else : tokens = sub_type . split ( \".\" ) current = cls . get_info_for_category ( kiara = kiara , category_name = tokens [ 0 ], ignore_errors = ignore_errors ) if len ( tokens ) == 1 : return current path = \".\" . join ( tokens [ 1 :]) try : current = current . get_subcomponent ( path ) except Exception as e : raise Exception ( f \"Can't get ' { path } ' information for ' { tokens [ 0 ] } ': { e } \" ) return current value_types : KiaraDynamicInfoModel = Field ( description = \"Information about available value types.\" ) modules : ModuleTypesGroupInfo = Field ( description = \"Information about the available modules.\" ) pipelines : PipelineTypesGroupInfo = Field ( description = \"Information about available pipelines.\" ) operation_types : KiaraDynamicInfoModel = Field ( description = \"Information about operation types contained in the current kiara context.\" ) operations : KiaraDynamicInfoModel = Field ( description = \"Available operations.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) value_type_table = self . value_types . create_renderable () table . add_row ( \"value_types\" , value_type_table ) module_table = self . modules . create_renderable () table . add_row ( \"modules\" , module_table ) pipelines_table = self . pipelines . create_renderable () table . add_row ( \"pipelines\" , pipelines_table ) operation_types_table = self . operation_types . create_renderable () table . add_row ( \"operation_types\" , operation_types_table ) operations_table = self . operations . create_renderable () table . add_row ( \"operations\" , operations_table ) return table modules : ModuleTypesGroupInfo pydantic-field required \u00b6 Information about the available modules. operation_types : KiaraDynamicInfoModel pydantic-field required \u00b6 Information about operation types contained in the current kiara context. operations : KiaraDynamicInfoModel pydantic-field required \u00b6 Available operations. pipelines : PipelineTypesGroupInfo pydantic-field required \u00b6 Information about available pipelines. value_types : KiaraDynamicInfoModel pydantic-field required \u00b6 Information about available value types. modules \u00b6 ModuleTypesGroupInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/modules.py class ModuleTypesGroupInfo ( KiaraInfoModel ): __root__ : typing . Dict [ str , KiaraModuleTypeMetadata ] @classmethod def from_type_names ( cls , kiara : \"Kiara\" , type_names : typing . Optional [ typing . Iterable [ str ]] = None , ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if type_names is None : type_names = kiara . available_module_types classes = {} for tn in type_names : _cls = kiara . get_module_class ( tn ) if ignore_pipeline_modules and _cls . is_pipeline (): continue if ignore_non_pipeline_modules and not _cls . is_pipeline (): continue classes [ tn ] = KiaraModuleTypeMetadata . from_module_class ( _cls ) return ModuleTypesGroupInfo ( __root__ = classes ) @classmethod def create_renderable_from_type_names ( cls , kiara : \"Kiara\" , type_names : typing . Iterable [ str ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): classes = {} for tn in type_names : _cls = kiara . get_module_class ( tn ) classes [ tn ] = _cls return cls . create_renderable_from_module_type_map ( module_types = classes , ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config ) @classmethod def create_renderable_from_module_type_map ( cls , module_types : typing . Mapping [ str , typing . Type [ \"KiaraModule\" ]], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" return cls . create_renderable_from_module_info_map ( { k : KiaraModuleTypeMetadata . from_module_class ( v ) for k , v in module_types . items () }, ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config ) @classmethod def create_renderable_from_module_info_map ( cls , module_types : typing . Mapping [ str , KiaraModuleTypeMetadata ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if ignore_pipeline_modules : module_types = { k : v for k , v in module_types . items () if not v . is_pipeline } elif ignore_non_pipeline_modules : module_types = { k : v for k , v in module_types . items () if v . is_pipeline } show_lines = False table = Table ( show_header = False , box = box . SIMPLE , show_lines = show_lines ) table . add_column ( \"name\" , style = \"b\" ) table . add_column ( \"desc\" , style = \"i\" ) for name , details in module_types . items (): if config . get ( \"include_full_doc\" , False ): table . add_row ( name , details . documentation . full_doc ) else : table . add_row ( name , details . documentation . description ) return table def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return ModuleTypesGroupInfo . create_renderable_from_module_info_map ( self . __root__ ) create_renderable_from_module_info_map ( module_types , ignore_pipeline_modules = False , ignore_non_pipeline_modules = False , ** config ) classmethod \u00b6 Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description Source code in kiara/info/modules.py @classmethod def create_renderable_from_module_info_map ( cls , module_types : typing . Mapping [ str , KiaraModuleTypeMetadata ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if ignore_pipeline_modules : module_types = { k : v for k , v in module_types . items () if not v . is_pipeline } elif ignore_non_pipeline_modules : module_types = { k : v for k , v in module_types . items () if v . is_pipeline } show_lines = False table = Table ( show_header = False , box = box . SIMPLE , show_lines = show_lines ) table . add_column ( \"name\" , style = \"b\" ) table . add_column ( \"desc\" , style = \"i\" ) for name , details in module_types . items (): if config . get ( \"include_full_doc\" , False ): table . add_row ( name , details . documentation . full_doc ) else : table . add_row ( name , details . documentation . description ) return table create_renderable_from_module_type_map ( module_types , ignore_pipeline_modules = False , ignore_non_pipeline_modules = False , ** config ) classmethod \u00b6 Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description Source code in kiara/info/modules.py @classmethod def create_renderable_from_module_type_map ( cls , module_types : typing . Mapping [ str , typing . Type [ \"KiaraModule\" ]], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" return cls . create_renderable_from_module_info_map ( { k : KiaraModuleTypeMetadata . from_module_class ( v ) for k , v in module_types . items () }, ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config ) operations \u00b6 OperationsGroupInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/operations.py class OperationsGroupInfo ( KiaraInfoModel ): @classmethod def create ( cls , kiara : \"Kiara\" , ignore_errors : bool = False ): operation_types = OperationsInfo . create_all ( kiara = kiara ) operation_configs = operation_types . pop ( \"all\" ) return OperationsGroupInfo ( operation_types = operation_types , operation_configs = operation_configs . operation_configs , ) operation_types : typing . Dict [ str , OperationsInfo ] = Field ( description = \"The available operation types and their details.\" ) operation_configs : typing . Dict [ str , Operation ] = Field ( description = \"The available operation ids and module_configs.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) op_map_table = OperationsInfo . create_renderable_from_operations_map ( self . operation_types ) table . add_row ( \"operation types\" , op_map_table ) configs = ModuleConfig . create_renderable_from_module_instance_configs ( self . operation_configs ) table . add_row ( \"operations\" , configs ) return table operation_configs : Dict [ str , kiara . operations . Operation ] pydantic-field required \u00b6 The available operation ids and module_configs. operation_types : Dict [ str , kiara . info . operations . OperationsInfo ] pydantic-field required \u00b6 The available operation types and their details. OperationsInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/operations.py class OperationsInfo ( KiaraInfoModel ): @classmethod def create_all ( self , kiara : Kiara ) -> typing . Dict [ str , \"OperationsInfo\" ]: op_types = kiara . operation_mgmt . operation_types return { op_name : OperationsInfo . create ( operations = op_types [ op_name ]) for op_name in sorted ( op_types . keys ()) } @classmethod def create ( cls , operations : OperationType ): info = OperationsMetadata . from_operations_class ( operations . __class__ ) return OperationsInfo ( info = info , operation_configs = dict ( operations . operations )) @classmethod def create_renderable_from_operations_map ( cls , op_map : typing . Mapping [ str , \"OperationsInfo\" ] ): table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) for op_name , ops in op_map . items (): table . add_row ( op_name , ops ) return table info : OperationsMetadata = Field ( description = \"Details about the type of operations contained in this collection.\" ) operation_configs : typing . Dict [ str , Operation ] = Field ( description = \"All available operation ids and their configurations.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return self . info . create_renderable ( operations = self . operation_configs , omit_type_name = True ) info : OperationsMetadata pydantic-field required \u00b6 Details about the type of operations contained in this collection. operation_configs : Dict [ str , kiara . operations . Operation ] pydantic-field required \u00b6 All available operation ids and their configurations. pipelines \u00b6 PipelineModuleInfo ( KiaraModuleTypeMetadata ) pydantic-model \u00b6 Source code in kiara/info/pipelines.py class PipelineModuleInfo ( KiaraModuleTypeMetadata ): class Config : extra = Extra . forbid @classmethod def from_type_name ( cls , module_type_name : str , kiara : \"Kiara\" ): m = kiara . get_module_class ( module_type = module_type_name ) base_conf : \"PipelineConfig\" = m . _base_pipeline_config # type: ignore structure = base_conf . create_pipeline_structure ( kiara = kiara ) struc_desc = PipelineStructureDesc . create_pipeline_structure_desc ( pipeline = structure ) attrs = PipelineModuleInfo . extract_module_attributes ( module_cls = m ) attrs [ \"structure\" ] = struc_desc pmi = PipelineModuleInfo ( ** attrs ) pmi . _kiara = kiara pmi . _structure = structure return pmi _kiara : \"Kiara\" = PrivateAttr () _structure : \"PipelineStructure\" = PrivateAttr () structure : PipelineStructureDesc = Field ( description = \"The pipeline structure.\" ) def print_data_flow_graph ( self , simplified : bool = True ) -> None : structure = self . _structure if simplified : graph = structure . data_flow_graph_simple else : graph = structure . data_flow_graph print_ascii_graph ( graph ) def print_execution_graph ( self ) -> None : structure = self . _structure print_ascii_graph ( structure . execution_graph ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : my_table = Table ( box = box . SIMPLE , show_lines = True , show_header = False ) my_table . add_column ( \"Property\" , style = \"i\" ) my_table . add_column ( \"Value\" ) my_table . add_row ( \"class\" , self . python_class . full_name ) my_table . add_row ( \"is pipeline\" , \"yes\" ) my_table . add_row ( \"doc\" , self . documentation . full_doc ) my_table . add_row ( \"config class\" , self . config . python_class . full_name ) my_table . add_row ( \"config\" , create_table_from_config_class ( self . config . python_class . get_class (), # type: ignore remove_pipeline_config = True , ), ) structure = self . _structure p_inputs = {} for input_name , schema in structure . pipeline_input_schema . items (): p_inputs [ input_name ] = { \"type\" : schema . type , \"doc\" : schema . doc } inputs_str = yaml . dump ( p_inputs ) _inputs_txt = Syntax ( inputs_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"pipeline inputs\" , _inputs_txt ) outputs = {} for output_name , schema in structure . pipeline_output_schema . items (): outputs [ output_name ] = { \"type\" : schema . type , \"doc\" : schema . doc } outputs_str = yaml . dump ( outputs ) _outputs_txt = Syntax ( outputs_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"pipeline outputs\" , _outputs_txt ) stages : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} for nr , stage in enumerate ( structure . processing_stages ): for s_id in stage : step = structure . get_step ( s_id ) mc = self . _kiara . get_module_class ( step . module_type ) desc = mc . get_type_metadata () . documentation . full_doc inputs : typing . Dict [ \"ValueRef\" , typing . List [ str ]] = {} for inp in structure . steps_inputs . values (): if inp . step_id != s_id : continue if inp . connected_outputs : for co in inp . connected_outputs : inputs . setdefault ( inp , []) . append ( co . alias ) else : inputs . setdefault ( inp , []) . append ( f \"__pipeline__. { inp . connected_pipeline_input } \" ) inp_str = [] for k , v in inputs . items (): s = f \" { k . value_name } \u2190 { ', ' . join ( v ) } \" inp_str . append ( s ) outp_str = [] for outp in structure . steps_outputs . values (): if outp . step_id != s_id : continue if outp . pipeline_output : outp_str . append ( f \" { outp . value_name } \u2192 __pipeline__. { outp . pipeline_output } \" ) else : outp_str . append ( outp . value_name ) stages . setdefault ( f \"stage { nr } \" , {})[ s_id ] = { \"module\" : step . module_type , \"desc\" : desc , \"inputs\" : inp_str , \"outputs\" : outp_str , } stages_str = yaml . dump ( stages ) _stages_txt = Syntax ( stages_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"processing stages\" , _stages_txt ) return my_table structure : PipelineStructureDesc pydantic-field required \u00b6 The pipeline structure. PipelineState ( KiaraInfoModel ) pydantic-model \u00b6 Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the dict or json methods to convert this object into a generic data structure. Source code in kiara/info/pipelines.py class PipelineState ( KiaraInfoModel ): \"\"\"Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the ``dict`` or ``json`` methods to convert this object into a generic data structure. \"\"\" structure : PipelineStructureDesc = Field ( description = \"The structure (interconnections of modules/steps) of the pipeline.\" ) pipeline_inputs : PipelineValuesInfo = Field ( description = \"The current (externally facing) input values of this pipeline.\" ) pipeline_outputs : PipelineValuesInfo = Field ( description = \"The current (externally facing) output values of this pipeline.\" ) step_states : typing . Dict [ str , StepStatus ] = Field ( description = \"The status of each step.\" ) step_inputs : typing . Dict [ str , PipelineValuesInfo ] = Field ( description = \"The current (internal) input values of each step of this pipeline.\" ) step_outputs : typing . Dict [ str , PipelineValuesInfo ] = Field ( description = \"The current (internal) output values of each step of this pipeline.\" ) status : StepStatus = Field ( description = \"The current overal status of the pipeline.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : from kiara.pipeline.pipeline import StepStatus all : typing . List [ RenderableType ] = [] all . append ( \"[b]Pipeline state[/b]\" ) all . append ( \"\" ) if self . status == StepStatus . RESULTS_READY : c = \"green\" elif self . status == StepStatus . INPUTS_READY : c = \"yellow\" else : c = \"red\" all . append ( f \"[b]Status[/b]: [b i { c } ] { self . status . name } [/b i { c } ]\" ) all . append ( \"\" ) all . append ( \"[b]Inputs / Outputs[/b]\" ) r_gro = [] inp_table = Table ( show_header = True , box = box . SIMPLE ) inp_table . add_column ( \"Field name\" , style = \"i\" ) inp_table . add_column ( \"Type\" ) inp_table . add_column ( \"Description\" ) inp_table . add_column ( \"Required\" ) inp_table . add_column ( \"Status\" , justify = \"center\" ) inp_table . add_column ( \"Ready\" , justify = \"center\" ) for field_name , value in self . pipeline_inputs . values . items (): req = value . value_schema . is_required () if not req : req_string = \"no\" else : req_string = \"[bold]yes[/bold]\" if value . is_set : status = \"-- set --\" valid = \"[green]yes[/green]\" else : valid = \"[green]yes[/green]\" if value . is_valid else \"[red]no[/red]\" if value . is_valid : status = \"-- not set --\" else : status = \"-- not set --\" inp_table . add_row ( field_name , value . value_schema . type , value . value_schema . doc , req_string , status , valid , ) r_gro . append ( Panel ( inp_table , box = box . ROUNDED , title_align = \"left\" , title = \"Inputs\" ) ) out_table = Table ( show_header = True , box = box . SIMPLE ) out_table . add_column ( \"Field name\" , style = \"i\" ) out_table . add_column ( \"Type\" ) out_table . add_column ( \"Description\" ) out_table . add_column ( \"Required\" ) out_table . add_column ( \"Status\" , justify = \"center\" ) out_table . add_column ( \"Ready\" , justify = \"center\" ) for field_name , value in self . pipeline_outputs . values . items (): req = value . value_schema . is_required () if not req : req_string = \"no\" else : req_string = \"[bold]yes[/bold]\" if value . is_set : status = \"-- set --\" valid = \"[green]yes[/green]\" else : valid = \"[green]yes[/green]\" if value . is_valid else \"[red]no[/red]\" status = \"-- not set --\" out_table . add_row ( field_name , value . value_schema . type , value . value_schema . doc , req_string , status , valid , ) r_gro . append ( Panel ( out_table , box = box . ROUNDED , title_align = \"left\" , title = \"Outputs\" ) ) all . append ( Panel ( RenderGroup ( * r_gro ), box = box . SIMPLE )) rg = [] for nr , stage in enumerate ( self . structure . processing_stages ): render_group = [] for s in self . structure . steps . values (): if s . step . step_id not in stage : continue step_table = create_pipeline_step_table ( self , s ) render_group . append ( step_table ) panel = Panel ( RenderGroup ( * render_group ), box = box . ROUNDED , title = f \"Processing stage: { nr + 1 } \" , title_align = \"left\" , ) rg . append ( panel ) all . append ( \"[b]Steps[/b]\" ) r_panel = Panel ( RenderGroup ( * rg ), box = box . SIMPLE ) all . append ( r_panel ) return RenderGroup ( * all ) pipeline_inputs : PipelineValuesInfo pydantic-field required \u00b6 The current (externally facing) input values of this pipeline. pipeline_outputs : PipelineValuesInfo pydantic-field required \u00b6 The current (externally facing) output values of this pipeline. status : StepStatus pydantic-field required \u00b6 The current overal status of the pipeline. step_inputs : Dict [ str , kiara . pipeline . PipelineValuesInfo ] pydantic-field required \u00b6 The current (internal) input values of each step of this pipeline. step_outputs : Dict [ str , kiara . pipeline . PipelineValuesInfo ] pydantic-field required \u00b6 The current (internal) output values of each step of this pipeline. step_states : Dict [ str , kiara . pipeline . StepStatus ] pydantic-field required \u00b6 The status of each step. structure : PipelineStructureDesc pydantic-field required \u00b6 The structure (interconnections of modules/steps) of the pipeline. PipelineStructureDesc ( BaseModel ) pydantic-model \u00b6 Outlines the internal structure of a Pipeline . Source code in kiara/info/pipelines.py class PipelineStructureDesc ( BaseModel ): \"\"\"Outlines the internal structure of a [Pipeline][kiara.pipeline.pipeline.Pipeline].\"\"\" @classmethod def create_pipeline_structure_desc ( cls , pipeline : typing . Union [ \"Pipeline\" , \"PipelineStructure\" ] ) -> \"PipelineStructureDesc\" : from kiara.pipeline.pipeline import Pipeline from kiara.pipeline.structure import PipelineStructure if isinstance ( pipeline , Pipeline ): structure : PipelineStructure = pipeline . structure elif isinstance ( pipeline , PipelineStructure ): structure = pipeline else : raise TypeError ( f \"Invalid type ' { type ( pipeline ) } ' for pipeline.\" ) steps = {} workflow_inputs : typing . Dict [ str , typing . List [ str ]] = {} workflow_outputs : typing . Dict [ str , str ] = {} for m_id , details in structure . steps_details . items (): step = details [ \"step\" ] input_connections : typing . Dict [ str , typing . List [ str ]] = {} for k , v in details [ \"inputs\" ] . items (): if v . connected_pipeline_input is not None : connected_item = v . connected_pipeline_input input_connections [ k ] = [ generate_step_alias ( PIPELINE_PARENT_MARKER , connected_item ) ] workflow_inputs . setdefault ( f \" { connected_item } \" , []) . append ( v . alias ) elif v . connected_outputs is not None : assert len ( v . connected_outputs ) > 0 for co in v . connected_outputs : input_connections . setdefault ( k , []) . append ( co . alias ) else : raise TypeError ( f \"Invalid connection type: { v } \" ) output_connections : typing . Dict [ str , typing . Any ] = {} for k , v in details [ \"outputs\" ] . items (): for connected_item in v . connected_inputs : output_connections . setdefault ( k , []) . append ( generate_step_alias ( connected_item . step_id , connected_item . value_name ) ) if v . pipeline_output : output_connections . setdefault ( k , []) . append ( generate_step_alias ( PIPELINE_PARENT_MARKER , v . pipeline_output ) ) workflow_outputs [ v . pipeline_output ] = v . alias steps [ step . step_id ] = StepDesc ( step = step , processing_stage = details [ \"processing_stage\" ], input_connections = input_connections , output_connections = output_connections , required = step . required , ) return PipelineStructureDesc ( steps = steps , processing_stages = structure . processing_stages , pipeline_input_connections = workflow_inputs , pipeline_output_connections = workflow_outputs , pipeline_inputs = structure . pipeline_inputs , pipeline_outputs = structure . pipeline_outputs , ) class Config : allow_mutation = False extra = Extra . forbid steps : typing . Dict [ str , StepDesc ] = Field ( description = \"The steps contained in this pipeline, with the 'step_id' as key.\" ) processing_stages : typing . List [ typing . List [ str ]] = Field ( description = \"The order in which this pipeline has to be processed (basically the dependencies of each step on other steps, if any).\" ) pipeline_input_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"The connections of this pipelines input fields. One input field can be connected to one or several step input fields.\" ) pipeline_output_connections : typing . Dict [ str , str ] = Field ( description = \"The connections of this pipelines output fields. Each pipeline output is connected to exactly one step output field.\" ) pipeline_inputs : typing . Dict [ str , PipelineInputRef ] = Field ( description = \"The pipeline inputs.\" ) pipeline_outputs : typing . Dict [ str , PipelineOutputRef ] = Field ( description = \"The pipeline outputs.\" ) @property def steps_info ( self ) -> StepsInfo : return StepsInfo ( processing_stages = self . processing_stages , steps = self . steps , ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield \"[b]Pipeline structure[/b] \\n \" yield \"[b]Inputs / Outputs[/b]\" data_panel : typing . List [ typing . Any ] = [] inp_table = Table ( show_header = True , box = box . SIMPLE , show_lines = True ) inp_table . add_column ( \"Name\" , style = \"i\" ) inp_table . add_column ( \"Type\" ) inp_table . add_column ( \"Description\" ) inp_table . add_column ( \"Required\" , justify = \"center\" ) inp_table . add_column ( \"Default\" , justify = \"center\" ) for inp , details in self . pipeline_inputs . items (): req = details . value_schema . is_required () if not req : req_str = \"no\" else : d = details . value_schema . default if d in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ]: req_str = \"[b]yes[/b]\" else : req_str = \"no\" default = details . value_schema . default if default in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ]: default = \"-- no default --\" else : default = str ( default ) inp_table . add_row ( inp , details . value_schema . type , details . value_schema . doc , req_str , default , ) p_inp = Panel ( inp_table , box = box . ROUNDED , title = \"Input fields\" , title_align = \"left\" ) data_panel . append ( p_inp ) # yield \"[b]Pipeline outputs[/b]\" out_table = Table ( show_header = True , box = box . SIMPLE , show_lines = True ) out_table . add_column ( \"Name\" , style = \"i\" ) out_table . add_column ( \"Type\" ) out_table . add_column ( \"Description\" ) for inp , details_o in self . pipeline_outputs . items (): out_table . add_row ( inp , details_o . value_schema . type , details_o . value_schema . doc , ) outp = Panel ( out_table , box = box . ROUNDED , title = \"Output fields\" , title_align = \"left\" ) data_panel . append ( outp ) yield Panel ( RenderGroup ( * data_panel ), box = box . SIMPLE ) step_color_map = {} for i , s in enumerate ( self . steps . values ()): step_color_map [ s . step . step_id ] = COLOR_LIST [ i % len ( COLOR_LIST )] rg = [] for nr , stage in enumerate ( self . processing_stages ): render_group = [] for s in self . steps . values (): if s . step . step_id not in stage : continue step_table = create_step_table ( s , step_color_map ) render_group . append ( step_table ) panel = Panel ( RenderGroup ( * render_group ), box = box . ROUNDED , title = f \"Processing stage: { nr + 1 } \" , title_align = \"left\" , ) rg . append ( panel ) yield \"[b]Steps[/b]\" r_panel = Panel ( RenderGroup ( * rg ), box = box . SIMPLE ) yield r_panel pipeline_input_connections : Dict [ str , List [ str ]] pydantic-field required \u00b6 The connections of this pipelines input fields. One input field can be connected to one or several step input fields. pipeline_inputs : Dict [ str , kiara . pipeline . values . PipelineInputRef ] pydantic-field required \u00b6 The pipeline inputs. pipeline_output_connections : Dict [ str , str ] pydantic-field required \u00b6 The connections of this pipelines output fields. Each pipeline output is connected to exactly one step output field. pipeline_outputs : Dict [ str , kiara . pipeline . values . PipelineOutputRef ] pydantic-field required \u00b6 The pipeline outputs. processing_stages : List [ List [ str ]] pydantic-field required \u00b6 The order in which this pipeline has to be processed (basically the dependencies of each step on other steps, if any). steps : Dict [ str , kiara . pipeline . config . StepDesc ] pydantic-field required \u00b6 The steps contained in this pipeline, with the 'step_id' as key. StepsInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/pipelines.py class StepsInfo ( KiaraInfoModel ): steps : typing . Dict [ str , StepDesc ] = Field ( description = \"A list of step details.\" ) processing_stages : typing . List [ typing . List [ str ]] = Field ( description = \"The stages in which the steps are processed.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( box = box . SIMPLE , show_lines = True ) table . add_column ( \"Stage\" , justify = \"center\" ) table . add_column ( \"Step Id\" ) table . add_column ( \"Module type\" , style = \"i\" ) table . add_column ( \"Description\" ) for nr , stage in enumerate ( self . processing_stages ): for i , step_id in enumerate ( stage ): step : StepDesc = self . steps [ step_id ] if step . required : title = f \"[b] { step_id } [/b]\" else : title = f \"[b] { step_id } [/b] [i](optional)[/i]\" if hasattr ( step . step . module , \"instance_doc\" ): doc = step . step . module . module_instance_doc else : doc = step . step . module . get_type_metadata () . documentation . full_doc row : typing . List [ typing . Any ] = [] if i == 0 : row . append ( str ( nr )) else : row . append ( \"\" ) row . append ( title ) # TODO; generate the right link here module_link = ( step . step . module . get_type_metadata () . context . references . get ( \"sources\" , None ) ) if module_link : module_str = f \"[link= { module_link } ] { step . step . module_type } [/link]\" else : module_str = step . step . module_type row . append ( module_str ) if doc and doc != \"-- n/a --\" : m = Markdown ( doc + \" \\n\\n --- \\n \" ) row . append ( m ) else : row . append ( \"\" ) table . add_row ( * row ) return table # yield Panel(table, title_align=\"left\", title=\"Processing stages\") # def __rich_console_old__( # self, console: Console, options: ConsoleOptions # ) -> RenderResult: # # explanation = {} # # for nr, stage in enumerate(self.processing_stages): # # stage_details = {} # for step_id in stage: # step: StepDesc = self.steps[step_id] # if step.required: # title = step_id # else: # title = f\"{step_id} (optional)\" # stage_details[title] = step.step.module.get_type_metadata().model_doc() # # explanation[nr + 1] = stage_details # # lines = [] # for stage_nr, stage_steps in explanation.items(): # lines.append(f\"[bold]Processing stage {stage_nr}[/bold]:\") # lines.append(\"\") # for step_id, desc in stage_steps.items(): # if desc == DEFAULT_NO_DESC_VALUE: # lines.append(f\" - {step_id}\") # else: # lines.append(f\" - {step_id}: [i]{desc}[/i]\") # lines.append(\"\") # # padding = (1, 2, 0, 2) # yield Panel( # \"\\n\".join(lines), # box=box.ROUNDED, # title_align=\"left\", # title=f\"Stages for pipeline: [b]{self.pipeline_id}[/b]\", # padding=padding, # ) processing_stages : List [ List [ str ]] pydantic-field required \u00b6 The stages in which the steps are processed. steps : Dict [ str , kiara . pipeline . config . StepDesc ] pydantic-field required \u00b6 A list of step details. types \u00b6 ValueTypeInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/types.py class ValueTypeInfo ( KiaraInfoModel ): @classmethod def from_type_class ( cls , type_cls : typing . Type [ \"ValueType\" ], kiara : typing . Optional [ Kiara ] = None ): type_attrs = cls . extract_type_attributes ( type_cls = type_cls , kiara = kiara ) return cls ( ** type_attrs ) @classmethod def extract_type_attributes ( self , type_cls : typing . Type [ \"ValueType\" ], kiara : typing . Optional [ Kiara ] = None ) -> typing . Dict [ str , typing . Any ]: if kiara is None : kiara = Kiara . instance () origin_md = OriginMetadataModel . from_class ( type_cls ) doc = DocumentationMetadataModel . from_class_doc ( type_cls ) python_class = PythonClassMetadata . from_class ( type_cls ) properties_md = ContextMetadataModel . from_class ( type_cls ) value_type : str = type_cls . _value_type_name # type: ignore config = ValueTypeConfigMetadata . from_config_class ( type_cls . type_config_cls ()) metadata_keys = kiara . metadata_mgmt . get_metadata_keys_for_type ( value_type = value_type ) metadata_models : typing . Dict [ str , typing . Type [ MetadataModel ]] = {} for metadata_key in metadata_keys : schema = kiara . metadata_mgmt . all_schemas . get ( metadata_key ) if schema is not None : metadata_models [ metadata_key ] = MetadataModelMetadata . from_model_class ( schema ) return { \"type_name\" : type_cls . _value_type_name , # type: ignore \"documentation\" : doc , \"origin\" : origin_md , \"context\" : properties_md , \"python_class\" : python_class , \"config\" : config , \"metadata_types\" : metadata_models , } type_name : str = Field ( description = \"The name under which the type is registered.\" ) documentation : DocumentationMetadataModel = Field ( description = \"The documentation for this value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about authorship for the value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type (description, tags, labels, references, ...).\" ) python_class : PythonClassMetadata = Field ( description = \"Information about the Python class for this value type.\" ) config : ValueTypeConfigMetadata = Field ( description = \"Details on how this value type can be configured.\" ) metadata_types : typing . Dict [ str , MetadataModelMetadata ] = Field ( description = \"The available metadata types for this value type.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_config_schema = config . get ( \"include_config_schema\" , True ) include_doc = config . get ( \"include_doc\" , True ) include_full_metadata = config . get ( \"include_full_metadata\" , True ) table = Table ( box = box . SIMPLE , show_header = False , padding = ( 0 , 0 , 0 , 0 )) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) if include_config_schema : if config : config_cls = self . config . python_class . get_class () table . add_row ( \"Type config\" , create_table_from_base_model ( config_cls )) else : table . add_row ( \"Type config\" , \"-- no config --\" ) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) if include_full_metadata and self . metadata_types : md_table = Table ( show_header = False , box = box . SIMPLE ) for key , md in self . metadata_types . items (): fields_table = md . create_fields_table ( show_header = False , show_required = False ) md_table . add_row ( f \"[i] { key } [/i]\" , fields_table ) table . add_row ( \"Type metadata\" , md_table ) return table config : ValueTypeConfigMetadata pydantic-field required \u00b6 Details on how this value type can be configured. context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type (description, tags, labels, references, ...). documentation : DocumentationMetadataModel pydantic-field required \u00b6 The documentation for this value type. metadata_types : Dict [ str , kiara . metadata . core_models . MetadataModelMetadata ] pydantic-field required \u00b6 The available metadata types for this value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about authorship for the value type. python_class : PythonClassMetadata pydantic-field required \u00b6 Information about the Python class for this value type. type_name : str pydantic-field required \u00b6 The name under which the type is registered.","title":"info"},{"location":"reference/kiara/info/__init__/#kiara.info.kiara","text":"","title":"kiara"},{"location":"reference/kiara/info/__init__/#kiara.info.kiara.KiaraContext","text":"Source code in kiara/info/kiara.py class KiaraContext ( KiaraInfoModel ): available_categories : typing . ClassVar = [ \"value_types\" , \"modules\" , \"pipelines\" , \"operations\" , \"operation_types\" , ] _info_cache : typing . ClassVar = {} @classmethod def get_info_for_category ( cls , kiara : \"Kiara\" , category_name : str , ignore_errors : bool = False ) -> KiaraInfoModel : if category_name not in cls . available_categories : raise Exception ( f \"Can't provide information for category ' { category_name } ': invalid category name. Valid names: { ', ' . join ( cls . available_categories ) } \" ) cache = ( cls . _info_cache . get ( kiara . _id , {}) . get ( category_name , {}) . get ( ignore_errors , None ) ) if cache is not None : return cache if category_name == \"value_types\" : all_types = ValueTypeMetadata . create_all ( kiara = kiara ) info = KiaraDynamicInfoModel . create_from_child_models ( ** all_types ) elif category_name == \"modules\" : info = ModuleTypesGroupInfo . from_type_names ( kiara = kiara , ignore_pipeline_modules = True ) elif category_name == \"pipelines\" : info = PipelineTypesGroupInfo . create ( kiara = kiara , ignore_errors = ignore_errors ) elif category_name == \"operation_types\" : all_op_types = OperationsMetadata . create_all ( kiara = kiara ) info = KiaraDynamicInfoModel . create_from_child_models ( ** all_op_types ) elif category_name == \"operations\" : ops_infos = KiaraDynamicInfoModel . create_from_child_models ( ** OperationsInfo . create_all ( kiara = kiara ) ) operation_configs = {} for v in ops_infos . __root__ . values (): configs = v . operation_configs operation_configs . update ( configs ) info = KiaraDynamicInfoModel . create_from_child_models ( ** operation_configs ) else : raise NotImplementedError ( f \"Category not available: { category_name } \" ) cls . _info_cache . setdefault ( kiara . _id , {}) . setdefault ( ignore_errors , {})[ category_name ] = info return info @classmethod def get_info ( cls , kiara : \"Kiara\" , sub_type : typing . Optional [ str ] = None , ignore_errors : bool = False , ): if sub_type is None : module_types = cls . get_info_for_category ( kiara = kiara , category_name = \"modules\" , ignore_errors = ignore_errors ) value_types = cls . get_info_for_category ( kiara = kiara , category_name = \"value_types\" , ignore_errors = ignore_errors ) pipeline_types = cls . get_info_for_category ( kiara = kiara , category_name = \"pipelines\" , ignore_errors = ignore_errors ) op_group = cls . get_info_for_category ( kiara = kiara , category_name = \"operations\" , ignore_errors = ignore_errors ) op_types = cls . get_info_for_category ( kiara = kiara , category_name = \"operation_types\" , ignore_errors = ignore_errors , ) return KiaraContext ( value_types = value_types , modules = module_types , pipelines = pipeline_types , operations = op_group , operation_types = op_types , ) else : tokens = sub_type . split ( \".\" ) current = cls . get_info_for_category ( kiara = kiara , category_name = tokens [ 0 ], ignore_errors = ignore_errors ) if len ( tokens ) == 1 : return current path = \".\" . join ( tokens [ 1 :]) try : current = current . get_subcomponent ( path ) except Exception as e : raise Exception ( f \"Can't get ' { path } ' information for ' { tokens [ 0 ] } ': { e } \" ) return current value_types : KiaraDynamicInfoModel = Field ( description = \"Information about available value types.\" ) modules : ModuleTypesGroupInfo = Field ( description = \"Information about the available modules.\" ) pipelines : PipelineTypesGroupInfo = Field ( description = \"Information about available pipelines.\" ) operation_types : KiaraDynamicInfoModel = Field ( description = \"Information about operation types contained in the current kiara context.\" ) operations : KiaraDynamicInfoModel = Field ( description = \"Available operations.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) value_type_table = self . value_types . create_renderable () table . add_row ( \"value_types\" , value_type_table ) module_table = self . modules . create_renderable () table . add_row ( \"modules\" , module_table ) pipelines_table = self . pipelines . create_renderable () table . add_row ( \"pipelines\" , pipelines_table ) operation_types_table = self . operation_types . create_renderable () table . add_row ( \"operation_types\" , operation_types_table ) operations_table = self . operations . create_renderable () table . add_row ( \"operations\" , operations_table ) return table","title":"KiaraContext"},{"location":"reference/kiara/info/__init__/#kiara.info.modules","text":"","title":"modules"},{"location":"reference/kiara/info/__init__/#kiara.info.modules.ModuleTypesGroupInfo","text":"Source code in kiara/info/modules.py class ModuleTypesGroupInfo ( KiaraInfoModel ): __root__ : typing . Dict [ str , KiaraModuleTypeMetadata ] @classmethod def from_type_names ( cls , kiara : \"Kiara\" , type_names : typing . Optional [ typing . Iterable [ str ]] = None , ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if type_names is None : type_names = kiara . available_module_types classes = {} for tn in type_names : _cls = kiara . get_module_class ( tn ) if ignore_pipeline_modules and _cls . is_pipeline (): continue if ignore_non_pipeline_modules and not _cls . is_pipeline (): continue classes [ tn ] = KiaraModuleTypeMetadata . from_module_class ( _cls ) return ModuleTypesGroupInfo ( __root__ = classes ) @classmethod def create_renderable_from_type_names ( cls , kiara : \"Kiara\" , type_names : typing . Iterable [ str ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): classes = {} for tn in type_names : _cls = kiara . get_module_class ( tn ) classes [ tn ] = _cls return cls . create_renderable_from_module_type_map ( module_types = classes , ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config ) @classmethod def create_renderable_from_module_type_map ( cls , module_types : typing . Mapping [ str , typing . Type [ \"KiaraModule\" ]], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" return cls . create_renderable_from_module_info_map ( { k : KiaraModuleTypeMetadata . from_module_class ( v ) for k , v in module_types . items () }, ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config ) @classmethod def create_renderable_from_module_info_map ( cls , module_types : typing . Mapping [ str , KiaraModuleTypeMetadata ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if ignore_pipeline_modules : module_types = { k : v for k , v in module_types . items () if not v . is_pipeline } elif ignore_non_pipeline_modules : module_types = { k : v for k , v in module_types . items () if v . is_pipeline } show_lines = False table = Table ( show_header = False , box = box . SIMPLE , show_lines = show_lines ) table . add_column ( \"name\" , style = \"b\" ) table . add_column ( \"desc\" , style = \"i\" ) for name , details in module_types . items (): if config . get ( \"include_full_doc\" , False ): table . add_row ( name , details . documentation . full_doc ) else : table . add_row ( name , details . documentation . description ) return table def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return ModuleTypesGroupInfo . create_renderable_from_module_info_map ( self . __root__ )","title":"ModuleTypesGroupInfo"},{"location":"reference/kiara/info/__init__/#kiara.info.operations","text":"","title":"operations"},{"location":"reference/kiara/info/__init__/#kiara.info.operations.OperationsGroupInfo","text":"Source code in kiara/info/operations.py class OperationsGroupInfo ( KiaraInfoModel ): @classmethod def create ( cls , kiara : \"Kiara\" , ignore_errors : bool = False ): operation_types = OperationsInfo . create_all ( kiara = kiara ) operation_configs = operation_types . pop ( \"all\" ) return OperationsGroupInfo ( operation_types = operation_types , operation_configs = operation_configs . operation_configs , ) operation_types : typing . Dict [ str , OperationsInfo ] = Field ( description = \"The available operation types and their details.\" ) operation_configs : typing . Dict [ str , Operation ] = Field ( description = \"The available operation ids and module_configs.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) op_map_table = OperationsInfo . create_renderable_from_operations_map ( self . operation_types ) table . add_row ( \"operation types\" , op_map_table ) configs = ModuleConfig . create_renderable_from_module_instance_configs ( self . operation_configs ) table . add_row ( \"operations\" , configs ) return table","title":"OperationsGroupInfo"},{"location":"reference/kiara/info/__init__/#kiara.info.operations.OperationsInfo","text":"Source code in kiara/info/operations.py class OperationsInfo ( KiaraInfoModel ): @classmethod def create_all ( self , kiara : Kiara ) -> typing . Dict [ str , \"OperationsInfo\" ]: op_types = kiara . operation_mgmt . operation_types return { op_name : OperationsInfo . create ( operations = op_types [ op_name ]) for op_name in sorted ( op_types . keys ()) } @classmethod def create ( cls , operations : OperationType ): info = OperationsMetadata . from_operations_class ( operations . __class__ ) return OperationsInfo ( info = info , operation_configs = dict ( operations . operations )) @classmethod def create_renderable_from_operations_map ( cls , op_map : typing . Mapping [ str , \"OperationsInfo\" ] ): table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) for op_name , ops in op_map . items (): table . add_row ( op_name , ops ) return table info : OperationsMetadata = Field ( description = \"Details about the type of operations contained in this collection.\" ) operation_configs : typing . Dict [ str , Operation ] = Field ( description = \"All available operation ids and their configurations.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return self . info . create_renderable ( operations = self . operation_configs , omit_type_name = True )","title":"OperationsInfo"},{"location":"reference/kiara/info/__init__/#kiara.info.pipelines","text":"","title":"pipelines"},{"location":"reference/kiara/info/__init__/#kiara.info.pipelines.PipelineModuleInfo","text":"Source code in kiara/info/pipelines.py class PipelineModuleInfo ( KiaraModuleTypeMetadata ): class Config : extra = Extra . forbid @classmethod def from_type_name ( cls , module_type_name : str , kiara : \"Kiara\" ): m = kiara . get_module_class ( module_type = module_type_name ) base_conf : \"PipelineConfig\" = m . _base_pipeline_config # type: ignore structure = base_conf . create_pipeline_structure ( kiara = kiara ) struc_desc = PipelineStructureDesc . create_pipeline_structure_desc ( pipeline = structure ) attrs = PipelineModuleInfo . extract_module_attributes ( module_cls = m ) attrs [ \"structure\" ] = struc_desc pmi = PipelineModuleInfo ( ** attrs ) pmi . _kiara = kiara pmi . _structure = structure return pmi _kiara : \"Kiara\" = PrivateAttr () _structure : \"PipelineStructure\" = PrivateAttr () structure : PipelineStructureDesc = Field ( description = \"The pipeline structure.\" ) def print_data_flow_graph ( self , simplified : bool = True ) -> None : structure = self . _structure if simplified : graph = structure . data_flow_graph_simple else : graph = structure . data_flow_graph print_ascii_graph ( graph ) def print_execution_graph ( self ) -> None : structure = self . _structure print_ascii_graph ( structure . execution_graph ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : my_table = Table ( box = box . SIMPLE , show_lines = True , show_header = False ) my_table . add_column ( \"Property\" , style = \"i\" ) my_table . add_column ( \"Value\" ) my_table . add_row ( \"class\" , self . python_class . full_name ) my_table . add_row ( \"is pipeline\" , \"yes\" ) my_table . add_row ( \"doc\" , self . documentation . full_doc ) my_table . add_row ( \"config class\" , self . config . python_class . full_name ) my_table . add_row ( \"config\" , create_table_from_config_class ( self . config . python_class . get_class (), # type: ignore remove_pipeline_config = True , ), ) structure = self . _structure p_inputs = {} for input_name , schema in structure . pipeline_input_schema . items (): p_inputs [ input_name ] = { \"type\" : schema . type , \"doc\" : schema . doc } inputs_str = yaml . dump ( p_inputs ) _inputs_txt = Syntax ( inputs_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"pipeline inputs\" , _inputs_txt ) outputs = {} for output_name , schema in structure . pipeline_output_schema . items (): outputs [ output_name ] = { \"type\" : schema . type , \"doc\" : schema . doc } outputs_str = yaml . dump ( outputs ) _outputs_txt = Syntax ( outputs_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"pipeline outputs\" , _outputs_txt ) stages : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} for nr , stage in enumerate ( structure . processing_stages ): for s_id in stage : step = structure . get_step ( s_id ) mc = self . _kiara . get_module_class ( step . module_type ) desc = mc . get_type_metadata () . documentation . full_doc inputs : typing . Dict [ \"ValueRef\" , typing . List [ str ]] = {} for inp in structure . steps_inputs . values (): if inp . step_id != s_id : continue if inp . connected_outputs : for co in inp . connected_outputs : inputs . setdefault ( inp , []) . append ( co . alias ) else : inputs . setdefault ( inp , []) . append ( f \"__pipeline__. { inp . connected_pipeline_input } \" ) inp_str = [] for k , v in inputs . items (): s = f \" { k . value_name } \u2190 { ', ' . join ( v ) } \" inp_str . append ( s ) outp_str = [] for outp in structure . steps_outputs . values (): if outp . step_id != s_id : continue if outp . pipeline_output : outp_str . append ( f \" { outp . value_name } \u2192 __pipeline__. { outp . pipeline_output } \" ) else : outp_str . append ( outp . value_name ) stages . setdefault ( f \"stage { nr } \" , {})[ s_id ] = { \"module\" : step . module_type , \"desc\" : desc , \"inputs\" : inp_str , \"outputs\" : outp_str , } stages_str = yaml . dump ( stages ) _stages_txt = Syntax ( stages_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"processing stages\" , _stages_txt ) return my_table","title":"PipelineModuleInfo"},{"location":"reference/kiara/info/__init__/#kiara.info.pipelines.PipelineState","text":"Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the dict or json methods to convert this object into a generic data structure. Source code in kiara/info/pipelines.py class PipelineState ( KiaraInfoModel ): \"\"\"Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the ``dict`` or ``json`` methods to convert this object into a generic data structure. \"\"\" structure : PipelineStructureDesc = Field ( description = \"The structure (interconnections of modules/steps) of the pipeline.\" ) pipeline_inputs : PipelineValuesInfo = Field ( description = \"The current (externally facing) input values of this pipeline.\" ) pipeline_outputs : PipelineValuesInfo = Field ( description = \"The current (externally facing) output values of this pipeline.\" ) step_states : typing . Dict [ str , StepStatus ] = Field ( description = \"The status of each step.\" ) step_inputs : typing . Dict [ str , PipelineValuesInfo ] = Field ( description = \"The current (internal) input values of each step of this pipeline.\" ) step_outputs : typing . Dict [ str , PipelineValuesInfo ] = Field ( description = \"The current (internal) output values of each step of this pipeline.\" ) status : StepStatus = Field ( description = \"The current overal status of the pipeline.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : from kiara.pipeline.pipeline import StepStatus all : typing . List [ RenderableType ] = [] all . append ( \"[b]Pipeline state[/b]\" ) all . append ( \"\" ) if self . status == StepStatus . RESULTS_READY : c = \"green\" elif self . status == StepStatus . INPUTS_READY : c = \"yellow\" else : c = \"red\" all . append ( f \"[b]Status[/b]: [b i { c } ] { self . status . name } [/b i { c } ]\" ) all . append ( \"\" ) all . append ( \"[b]Inputs / Outputs[/b]\" ) r_gro = [] inp_table = Table ( show_header = True , box = box . SIMPLE ) inp_table . add_column ( \"Field name\" , style = \"i\" ) inp_table . add_column ( \"Type\" ) inp_table . add_column ( \"Description\" ) inp_table . add_column ( \"Required\" ) inp_table . add_column ( \"Status\" , justify = \"center\" ) inp_table . add_column ( \"Ready\" , justify = \"center\" ) for field_name , value in self . pipeline_inputs . values . items (): req = value . value_schema . is_required () if not req : req_string = \"no\" else : req_string = \"[bold]yes[/bold]\" if value . is_set : status = \"-- set --\" valid = \"[green]yes[/green]\" else : valid = \"[green]yes[/green]\" if value . is_valid else \"[red]no[/red]\" if value . is_valid : status = \"-- not set --\" else : status = \"-- not set --\" inp_table . add_row ( field_name , value . value_schema . type , value . value_schema . doc , req_string , status , valid , ) r_gro . append ( Panel ( inp_table , box = box . ROUNDED , title_align = \"left\" , title = \"Inputs\" ) ) out_table = Table ( show_header = True , box = box . SIMPLE ) out_table . add_column ( \"Field name\" , style = \"i\" ) out_table . add_column ( \"Type\" ) out_table . add_column ( \"Description\" ) out_table . add_column ( \"Required\" ) out_table . add_column ( \"Status\" , justify = \"center\" ) out_table . add_column ( \"Ready\" , justify = \"center\" ) for field_name , value in self . pipeline_outputs . values . items (): req = value . value_schema . is_required () if not req : req_string = \"no\" else : req_string = \"[bold]yes[/bold]\" if value . is_set : status = \"-- set --\" valid = \"[green]yes[/green]\" else : valid = \"[green]yes[/green]\" if value . is_valid else \"[red]no[/red]\" status = \"-- not set --\" out_table . add_row ( field_name , value . value_schema . type , value . value_schema . doc , req_string , status , valid , ) r_gro . append ( Panel ( out_table , box = box . ROUNDED , title_align = \"left\" , title = \"Outputs\" ) ) all . append ( Panel ( RenderGroup ( * r_gro ), box = box . SIMPLE )) rg = [] for nr , stage in enumerate ( self . structure . processing_stages ): render_group = [] for s in self . structure . steps . values (): if s . step . step_id not in stage : continue step_table = create_pipeline_step_table ( self , s ) render_group . append ( step_table ) panel = Panel ( RenderGroup ( * render_group ), box = box . ROUNDED , title = f \"Processing stage: { nr + 1 } \" , title_align = \"left\" , ) rg . append ( panel ) all . append ( \"[b]Steps[/b]\" ) r_panel = Panel ( RenderGroup ( * rg ), box = box . SIMPLE ) all . append ( r_panel ) return RenderGroup ( * all )","title":"PipelineState"},{"location":"reference/kiara/info/__init__/#kiara.info.pipelines.PipelineStructureDesc","text":"Outlines the internal structure of a Pipeline . Source code in kiara/info/pipelines.py class PipelineStructureDesc ( BaseModel ): \"\"\"Outlines the internal structure of a [Pipeline][kiara.pipeline.pipeline.Pipeline].\"\"\" @classmethod def create_pipeline_structure_desc ( cls , pipeline : typing . Union [ \"Pipeline\" , \"PipelineStructure\" ] ) -> \"PipelineStructureDesc\" : from kiara.pipeline.pipeline import Pipeline from kiara.pipeline.structure import PipelineStructure if isinstance ( pipeline , Pipeline ): structure : PipelineStructure = pipeline . structure elif isinstance ( pipeline , PipelineStructure ): structure = pipeline else : raise TypeError ( f \"Invalid type ' { type ( pipeline ) } ' for pipeline.\" ) steps = {} workflow_inputs : typing . Dict [ str , typing . List [ str ]] = {} workflow_outputs : typing . Dict [ str , str ] = {} for m_id , details in structure . steps_details . items (): step = details [ \"step\" ] input_connections : typing . Dict [ str , typing . List [ str ]] = {} for k , v in details [ \"inputs\" ] . items (): if v . connected_pipeline_input is not None : connected_item = v . connected_pipeline_input input_connections [ k ] = [ generate_step_alias ( PIPELINE_PARENT_MARKER , connected_item ) ] workflow_inputs . setdefault ( f \" { connected_item } \" , []) . append ( v . alias ) elif v . connected_outputs is not None : assert len ( v . connected_outputs ) > 0 for co in v . connected_outputs : input_connections . setdefault ( k , []) . append ( co . alias ) else : raise TypeError ( f \"Invalid connection type: { v } \" ) output_connections : typing . Dict [ str , typing . Any ] = {} for k , v in details [ \"outputs\" ] . items (): for connected_item in v . connected_inputs : output_connections . setdefault ( k , []) . append ( generate_step_alias ( connected_item . step_id , connected_item . value_name ) ) if v . pipeline_output : output_connections . setdefault ( k , []) . append ( generate_step_alias ( PIPELINE_PARENT_MARKER , v . pipeline_output ) ) workflow_outputs [ v . pipeline_output ] = v . alias steps [ step . step_id ] = StepDesc ( step = step , processing_stage = details [ \"processing_stage\" ], input_connections = input_connections , output_connections = output_connections , required = step . required , ) return PipelineStructureDesc ( steps = steps , processing_stages = structure . processing_stages , pipeline_input_connections = workflow_inputs , pipeline_output_connections = workflow_outputs , pipeline_inputs = structure . pipeline_inputs , pipeline_outputs = structure . pipeline_outputs , ) class Config : allow_mutation = False extra = Extra . forbid steps : typing . Dict [ str , StepDesc ] = Field ( description = \"The steps contained in this pipeline, with the 'step_id' as key.\" ) processing_stages : typing . List [ typing . List [ str ]] = Field ( description = \"The order in which this pipeline has to be processed (basically the dependencies of each step on other steps, if any).\" ) pipeline_input_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"The connections of this pipelines input fields. One input field can be connected to one or several step input fields.\" ) pipeline_output_connections : typing . Dict [ str , str ] = Field ( description = \"The connections of this pipelines output fields. Each pipeline output is connected to exactly one step output field.\" ) pipeline_inputs : typing . Dict [ str , PipelineInputRef ] = Field ( description = \"The pipeline inputs.\" ) pipeline_outputs : typing . Dict [ str , PipelineOutputRef ] = Field ( description = \"The pipeline outputs.\" ) @property def steps_info ( self ) -> StepsInfo : return StepsInfo ( processing_stages = self . processing_stages , steps = self . steps , ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield \"[b]Pipeline structure[/b] \\n \" yield \"[b]Inputs / Outputs[/b]\" data_panel : typing . List [ typing . Any ] = [] inp_table = Table ( show_header = True , box = box . SIMPLE , show_lines = True ) inp_table . add_column ( \"Name\" , style = \"i\" ) inp_table . add_column ( \"Type\" ) inp_table . add_column ( \"Description\" ) inp_table . add_column ( \"Required\" , justify = \"center\" ) inp_table . add_column ( \"Default\" , justify = \"center\" ) for inp , details in self . pipeline_inputs . items (): req = details . value_schema . is_required () if not req : req_str = \"no\" else : d = details . value_schema . default if d in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ]: req_str = \"[b]yes[/b]\" else : req_str = \"no\" default = details . value_schema . default if default in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ]: default = \"-- no default --\" else : default = str ( default ) inp_table . add_row ( inp , details . value_schema . type , details . value_schema . doc , req_str , default , ) p_inp = Panel ( inp_table , box = box . ROUNDED , title = \"Input fields\" , title_align = \"left\" ) data_panel . append ( p_inp ) # yield \"[b]Pipeline outputs[/b]\" out_table = Table ( show_header = True , box = box . SIMPLE , show_lines = True ) out_table . add_column ( \"Name\" , style = \"i\" ) out_table . add_column ( \"Type\" ) out_table . add_column ( \"Description\" ) for inp , details_o in self . pipeline_outputs . items (): out_table . add_row ( inp , details_o . value_schema . type , details_o . value_schema . doc , ) outp = Panel ( out_table , box = box . ROUNDED , title = \"Output fields\" , title_align = \"left\" ) data_panel . append ( outp ) yield Panel ( RenderGroup ( * data_panel ), box = box . SIMPLE ) step_color_map = {} for i , s in enumerate ( self . steps . values ()): step_color_map [ s . step . step_id ] = COLOR_LIST [ i % len ( COLOR_LIST )] rg = [] for nr , stage in enumerate ( self . processing_stages ): render_group = [] for s in self . steps . values (): if s . step . step_id not in stage : continue step_table = create_step_table ( s , step_color_map ) render_group . append ( step_table ) panel = Panel ( RenderGroup ( * render_group ), box = box . ROUNDED , title = f \"Processing stage: { nr + 1 } \" , title_align = \"left\" , ) rg . append ( panel ) yield \"[b]Steps[/b]\" r_panel = Panel ( RenderGroup ( * rg ), box = box . SIMPLE ) yield r_panel","title":"PipelineStructureDesc"},{"location":"reference/kiara/info/__init__/#kiara.info.pipelines.StepsInfo","text":"Source code in kiara/info/pipelines.py class StepsInfo ( KiaraInfoModel ): steps : typing . Dict [ str , StepDesc ] = Field ( description = \"A list of step details.\" ) processing_stages : typing . List [ typing . List [ str ]] = Field ( description = \"The stages in which the steps are processed.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( box = box . SIMPLE , show_lines = True ) table . add_column ( \"Stage\" , justify = \"center\" ) table . add_column ( \"Step Id\" ) table . add_column ( \"Module type\" , style = \"i\" ) table . add_column ( \"Description\" ) for nr , stage in enumerate ( self . processing_stages ): for i , step_id in enumerate ( stage ): step : StepDesc = self . steps [ step_id ] if step . required : title = f \"[b] { step_id } [/b]\" else : title = f \"[b] { step_id } [/b] [i](optional)[/i]\" if hasattr ( step . step . module , \"instance_doc\" ): doc = step . step . module . module_instance_doc else : doc = step . step . module . get_type_metadata () . documentation . full_doc row : typing . List [ typing . Any ] = [] if i == 0 : row . append ( str ( nr )) else : row . append ( \"\" ) row . append ( title ) # TODO; generate the right link here module_link = ( step . step . module . get_type_metadata () . context . references . get ( \"sources\" , None ) ) if module_link : module_str = f \"[link= { module_link } ] { step . step . module_type } [/link]\" else : module_str = step . step . module_type row . append ( module_str ) if doc and doc != \"-- n/a --\" : m = Markdown ( doc + \" \\n\\n --- \\n \" ) row . append ( m ) else : row . append ( \"\" ) table . add_row ( * row ) return table # yield Panel(table, title_align=\"left\", title=\"Processing stages\") # def __rich_console_old__( # self, console: Console, options: ConsoleOptions # ) -> RenderResult: # # explanation = {} # # for nr, stage in enumerate(self.processing_stages): # # stage_details = {} # for step_id in stage: # step: StepDesc = self.steps[step_id] # if step.required: # title = step_id # else: # title = f\"{step_id} (optional)\" # stage_details[title] = step.step.module.get_type_metadata().model_doc() # # explanation[nr + 1] = stage_details # # lines = [] # for stage_nr, stage_steps in explanation.items(): # lines.append(f\"[bold]Processing stage {stage_nr}[/bold]:\") # lines.append(\"\") # for step_id, desc in stage_steps.items(): # if desc == DEFAULT_NO_DESC_VALUE: # lines.append(f\" - {step_id}\") # else: # lines.append(f\" - {step_id}: [i]{desc}[/i]\") # lines.append(\"\") # # padding = (1, 2, 0, 2) # yield Panel( # \"\\n\".join(lines), # box=box.ROUNDED, # title_align=\"left\", # title=f\"Stages for pipeline: [b]{self.pipeline_id}[/b]\", # padding=padding, # )","title":"StepsInfo"},{"location":"reference/kiara/info/__init__/#kiara.info.types","text":"","title":"types"},{"location":"reference/kiara/info/__init__/#kiara.info.types.ValueTypeInfo","text":"Source code in kiara/info/types.py class ValueTypeInfo ( KiaraInfoModel ): @classmethod def from_type_class ( cls , type_cls : typing . Type [ \"ValueType\" ], kiara : typing . Optional [ Kiara ] = None ): type_attrs = cls . extract_type_attributes ( type_cls = type_cls , kiara = kiara ) return cls ( ** type_attrs ) @classmethod def extract_type_attributes ( self , type_cls : typing . Type [ \"ValueType\" ], kiara : typing . Optional [ Kiara ] = None ) -> typing . Dict [ str , typing . Any ]: if kiara is None : kiara = Kiara . instance () origin_md = OriginMetadataModel . from_class ( type_cls ) doc = DocumentationMetadataModel . from_class_doc ( type_cls ) python_class = PythonClassMetadata . from_class ( type_cls ) properties_md = ContextMetadataModel . from_class ( type_cls ) value_type : str = type_cls . _value_type_name # type: ignore config = ValueTypeConfigMetadata . from_config_class ( type_cls . type_config_cls ()) metadata_keys = kiara . metadata_mgmt . get_metadata_keys_for_type ( value_type = value_type ) metadata_models : typing . Dict [ str , typing . Type [ MetadataModel ]] = {} for metadata_key in metadata_keys : schema = kiara . metadata_mgmt . all_schemas . get ( metadata_key ) if schema is not None : metadata_models [ metadata_key ] = MetadataModelMetadata . from_model_class ( schema ) return { \"type_name\" : type_cls . _value_type_name , # type: ignore \"documentation\" : doc , \"origin\" : origin_md , \"context\" : properties_md , \"python_class\" : python_class , \"config\" : config , \"metadata_types\" : metadata_models , } type_name : str = Field ( description = \"The name under which the type is registered.\" ) documentation : DocumentationMetadataModel = Field ( description = \"The documentation for this value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about authorship for the value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type (description, tags, labels, references, ...).\" ) python_class : PythonClassMetadata = Field ( description = \"Information about the Python class for this value type.\" ) config : ValueTypeConfigMetadata = Field ( description = \"Details on how this value type can be configured.\" ) metadata_types : typing . Dict [ str , MetadataModelMetadata ] = Field ( description = \"The available metadata types for this value type.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_config_schema = config . get ( \"include_config_schema\" , True ) include_doc = config . get ( \"include_doc\" , True ) include_full_metadata = config . get ( \"include_full_metadata\" , True ) table = Table ( box = box . SIMPLE , show_header = False , padding = ( 0 , 0 , 0 , 0 )) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) if include_config_schema : if config : config_cls = self . config . python_class . get_class () table . add_row ( \"Type config\" , create_table_from_base_model ( config_cls )) else : table . add_row ( \"Type config\" , \"-- no config --\" ) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) if include_full_metadata and self . metadata_types : md_table = Table ( show_header = False , box = box . SIMPLE ) for key , md in self . metadata_types . items (): fields_table = md . create_fields_table ( show_header = False , show_required = False ) md_table . add_row ( f \"[i] { key } [/i]\" , fields_table ) table . add_row ( \"Type metadata\" , md_table ) return table","title":"ValueTypeInfo"},{"location":"reference/kiara/info/kiara/","text":"KiaraContext ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/kiara.py class KiaraContext ( KiaraInfoModel ): available_categories : typing . ClassVar = [ \"value_types\" , \"modules\" , \"pipelines\" , \"operations\" , \"operation_types\" , ] _info_cache : typing . ClassVar = {} @classmethod def get_info_for_category ( cls , kiara : \"Kiara\" , category_name : str , ignore_errors : bool = False ) -> KiaraInfoModel : if category_name not in cls . available_categories : raise Exception ( f \"Can't provide information for category ' { category_name } ': invalid category name. Valid names: { ', ' . join ( cls . available_categories ) } \" ) cache = ( cls . _info_cache . get ( kiara . _id , {}) . get ( category_name , {}) . get ( ignore_errors , None ) ) if cache is not None : return cache if category_name == \"value_types\" : all_types = ValueTypeMetadata . create_all ( kiara = kiara ) info = KiaraDynamicInfoModel . create_from_child_models ( ** all_types ) elif category_name == \"modules\" : info = ModuleTypesGroupInfo . from_type_names ( kiara = kiara , ignore_pipeline_modules = True ) elif category_name == \"pipelines\" : info = PipelineTypesGroupInfo . create ( kiara = kiara , ignore_errors = ignore_errors ) elif category_name == \"operation_types\" : all_op_types = OperationsMetadata . create_all ( kiara = kiara ) info = KiaraDynamicInfoModel . create_from_child_models ( ** all_op_types ) elif category_name == \"operations\" : ops_infos = KiaraDynamicInfoModel . create_from_child_models ( ** OperationsInfo . create_all ( kiara = kiara ) ) operation_configs = {} for v in ops_infos . __root__ . values (): configs = v . operation_configs operation_configs . update ( configs ) info = KiaraDynamicInfoModel . create_from_child_models ( ** operation_configs ) else : raise NotImplementedError ( f \"Category not available: { category_name } \" ) cls . _info_cache . setdefault ( kiara . _id , {}) . setdefault ( ignore_errors , {})[ category_name ] = info return info @classmethod def get_info ( cls , kiara : \"Kiara\" , sub_type : typing . Optional [ str ] = None , ignore_errors : bool = False , ): if sub_type is None : module_types = cls . get_info_for_category ( kiara = kiara , category_name = \"modules\" , ignore_errors = ignore_errors ) value_types = cls . get_info_for_category ( kiara = kiara , category_name = \"value_types\" , ignore_errors = ignore_errors ) pipeline_types = cls . get_info_for_category ( kiara = kiara , category_name = \"pipelines\" , ignore_errors = ignore_errors ) op_group = cls . get_info_for_category ( kiara = kiara , category_name = \"operations\" , ignore_errors = ignore_errors ) op_types = cls . get_info_for_category ( kiara = kiara , category_name = \"operation_types\" , ignore_errors = ignore_errors , ) return KiaraContext ( value_types = value_types , modules = module_types , pipelines = pipeline_types , operations = op_group , operation_types = op_types , ) else : tokens = sub_type . split ( \".\" ) current = cls . get_info_for_category ( kiara = kiara , category_name = tokens [ 0 ], ignore_errors = ignore_errors ) if len ( tokens ) == 1 : return current path = \".\" . join ( tokens [ 1 :]) try : current = current . get_subcomponent ( path ) except Exception as e : raise Exception ( f \"Can't get ' { path } ' information for ' { tokens [ 0 ] } ': { e } \" ) return current value_types : KiaraDynamicInfoModel = Field ( description = \"Information about available value types.\" ) modules : ModuleTypesGroupInfo = Field ( description = \"Information about the available modules.\" ) pipelines : PipelineTypesGroupInfo = Field ( description = \"Information about available pipelines.\" ) operation_types : KiaraDynamicInfoModel = Field ( description = \"Information about operation types contained in the current kiara context.\" ) operations : KiaraDynamicInfoModel = Field ( description = \"Available operations.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) value_type_table = self . value_types . create_renderable () table . add_row ( \"value_types\" , value_type_table ) module_table = self . modules . create_renderable () table . add_row ( \"modules\" , module_table ) pipelines_table = self . pipelines . create_renderable () table . add_row ( \"pipelines\" , pipelines_table ) operation_types_table = self . operation_types . create_renderable () table . add_row ( \"operation_types\" , operation_types_table ) operations_table = self . operations . create_renderable () table . add_row ( \"operations\" , operations_table ) return table modules : ModuleTypesGroupInfo pydantic-field required \u00b6 Information about the available modules. operation_types : KiaraDynamicInfoModel pydantic-field required \u00b6 Information about operation types contained in the current kiara context. operations : KiaraDynamicInfoModel pydantic-field required \u00b6 Available operations. pipelines : PipelineTypesGroupInfo pydantic-field required \u00b6 Information about available pipelines. value_types : KiaraDynamicInfoModel pydantic-field required \u00b6 Information about available value types.","title":"kiara"},{"location":"reference/kiara/info/kiara/#kiara.info.kiara.KiaraContext","text":"Source code in kiara/info/kiara.py class KiaraContext ( KiaraInfoModel ): available_categories : typing . ClassVar = [ \"value_types\" , \"modules\" , \"pipelines\" , \"operations\" , \"operation_types\" , ] _info_cache : typing . ClassVar = {} @classmethod def get_info_for_category ( cls , kiara : \"Kiara\" , category_name : str , ignore_errors : bool = False ) -> KiaraInfoModel : if category_name not in cls . available_categories : raise Exception ( f \"Can't provide information for category ' { category_name } ': invalid category name. Valid names: { ', ' . join ( cls . available_categories ) } \" ) cache = ( cls . _info_cache . get ( kiara . _id , {}) . get ( category_name , {}) . get ( ignore_errors , None ) ) if cache is not None : return cache if category_name == \"value_types\" : all_types = ValueTypeMetadata . create_all ( kiara = kiara ) info = KiaraDynamicInfoModel . create_from_child_models ( ** all_types ) elif category_name == \"modules\" : info = ModuleTypesGroupInfo . from_type_names ( kiara = kiara , ignore_pipeline_modules = True ) elif category_name == \"pipelines\" : info = PipelineTypesGroupInfo . create ( kiara = kiara , ignore_errors = ignore_errors ) elif category_name == \"operation_types\" : all_op_types = OperationsMetadata . create_all ( kiara = kiara ) info = KiaraDynamicInfoModel . create_from_child_models ( ** all_op_types ) elif category_name == \"operations\" : ops_infos = KiaraDynamicInfoModel . create_from_child_models ( ** OperationsInfo . create_all ( kiara = kiara ) ) operation_configs = {} for v in ops_infos . __root__ . values (): configs = v . operation_configs operation_configs . update ( configs ) info = KiaraDynamicInfoModel . create_from_child_models ( ** operation_configs ) else : raise NotImplementedError ( f \"Category not available: { category_name } \" ) cls . _info_cache . setdefault ( kiara . _id , {}) . setdefault ( ignore_errors , {})[ category_name ] = info return info @classmethod def get_info ( cls , kiara : \"Kiara\" , sub_type : typing . Optional [ str ] = None , ignore_errors : bool = False , ): if sub_type is None : module_types = cls . get_info_for_category ( kiara = kiara , category_name = \"modules\" , ignore_errors = ignore_errors ) value_types = cls . get_info_for_category ( kiara = kiara , category_name = \"value_types\" , ignore_errors = ignore_errors ) pipeline_types = cls . get_info_for_category ( kiara = kiara , category_name = \"pipelines\" , ignore_errors = ignore_errors ) op_group = cls . get_info_for_category ( kiara = kiara , category_name = \"operations\" , ignore_errors = ignore_errors ) op_types = cls . get_info_for_category ( kiara = kiara , category_name = \"operation_types\" , ignore_errors = ignore_errors , ) return KiaraContext ( value_types = value_types , modules = module_types , pipelines = pipeline_types , operations = op_group , operation_types = op_types , ) else : tokens = sub_type . split ( \".\" ) current = cls . get_info_for_category ( kiara = kiara , category_name = tokens [ 0 ], ignore_errors = ignore_errors ) if len ( tokens ) == 1 : return current path = \".\" . join ( tokens [ 1 :]) try : current = current . get_subcomponent ( path ) except Exception as e : raise Exception ( f \"Can't get ' { path } ' information for ' { tokens [ 0 ] } ': { e } \" ) return current value_types : KiaraDynamicInfoModel = Field ( description = \"Information about available value types.\" ) modules : ModuleTypesGroupInfo = Field ( description = \"Information about the available modules.\" ) pipelines : PipelineTypesGroupInfo = Field ( description = \"Information about available pipelines.\" ) operation_types : KiaraDynamicInfoModel = Field ( description = \"Information about operation types contained in the current kiara context.\" ) operations : KiaraDynamicInfoModel = Field ( description = \"Available operations.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) value_type_table = self . value_types . create_renderable () table . add_row ( \"value_types\" , value_type_table ) module_table = self . modules . create_renderable () table . add_row ( \"modules\" , module_table ) pipelines_table = self . pipelines . create_renderable () table . add_row ( \"pipelines\" , pipelines_table ) operation_types_table = self . operation_types . create_renderable () table . add_row ( \"operation_types\" , operation_types_table ) operations_table = self . operations . create_renderable () table . add_row ( \"operations\" , operations_table ) return table","title":"KiaraContext"},{"location":"reference/kiara/info/kiara/#kiara.info.kiara.KiaraContext.modules","text":"Information about the available modules.","title":"modules"},{"location":"reference/kiara/info/kiara/#kiara.info.kiara.KiaraContext.operation_types","text":"Information about operation types contained in the current kiara context.","title":"operation_types"},{"location":"reference/kiara/info/kiara/#kiara.info.kiara.KiaraContext.operations","text":"Available operations.","title":"operations"},{"location":"reference/kiara/info/kiara/#kiara.info.kiara.KiaraContext.pipelines","text":"Information about available pipelines.","title":"pipelines"},{"location":"reference/kiara/info/kiara/#kiara.info.kiara.KiaraContext.value_types","text":"Information about available value types.","title":"value_types"},{"location":"reference/kiara/info/metadata/","text":"","title":"metadata"},{"location":"reference/kiara/info/modules/","text":"ModuleTypesGroupInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/modules.py class ModuleTypesGroupInfo ( KiaraInfoModel ): __root__ : typing . Dict [ str , KiaraModuleTypeMetadata ] @classmethod def from_type_names ( cls , kiara : \"Kiara\" , type_names : typing . Optional [ typing . Iterable [ str ]] = None , ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if type_names is None : type_names = kiara . available_module_types classes = {} for tn in type_names : _cls = kiara . get_module_class ( tn ) if ignore_pipeline_modules and _cls . is_pipeline (): continue if ignore_non_pipeline_modules and not _cls . is_pipeline (): continue classes [ tn ] = KiaraModuleTypeMetadata . from_module_class ( _cls ) return ModuleTypesGroupInfo ( __root__ = classes ) @classmethod def create_renderable_from_type_names ( cls , kiara : \"Kiara\" , type_names : typing . Iterable [ str ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): classes = {} for tn in type_names : _cls = kiara . get_module_class ( tn ) classes [ tn ] = _cls return cls . create_renderable_from_module_type_map ( module_types = classes , ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config ) @classmethod def create_renderable_from_module_type_map ( cls , module_types : typing . Mapping [ str , typing . Type [ \"KiaraModule\" ]], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" return cls . create_renderable_from_module_info_map ( { k : KiaraModuleTypeMetadata . from_module_class ( v ) for k , v in module_types . items () }, ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config ) @classmethod def create_renderable_from_module_info_map ( cls , module_types : typing . Mapping [ str , KiaraModuleTypeMetadata ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if ignore_pipeline_modules : module_types = { k : v for k , v in module_types . items () if not v . is_pipeline } elif ignore_non_pipeline_modules : module_types = { k : v for k , v in module_types . items () if v . is_pipeline } show_lines = False table = Table ( show_header = False , box = box . SIMPLE , show_lines = show_lines ) table . add_column ( \"name\" , style = \"b\" ) table . add_column ( \"desc\" , style = \"i\" ) for name , details in module_types . items (): if config . get ( \"include_full_doc\" , False ): table . add_row ( name , details . documentation . full_doc ) else : table . add_row ( name , details . documentation . description ) return table def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return ModuleTypesGroupInfo . create_renderable_from_module_info_map ( self . __root__ ) create_renderable_from_module_info_map ( module_types , ignore_pipeline_modules = False , ignore_non_pipeline_modules = False , ** config ) classmethod \u00b6 Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description Source code in kiara/info/modules.py @classmethod def create_renderable_from_module_info_map ( cls , module_types : typing . Mapping [ str , KiaraModuleTypeMetadata ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if ignore_pipeline_modules : module_types = { k : v for k , v in module_types . items () if not v . is_pipeline } elif ignore_non_pipeline_modules : module_types = { k : v for k , v in module_types . items () if v . is_pipeline } show_lines = False table = Table ( show_header = False , box = box . SIMPLE , show_lines = show_lines ) table . add_column ( \"name\" , style = \"b\" ) table . add_column ( \"desc\" , style = \"i\" ) for name , details in module_types . items (): if config . get ( \"include_full_doc\" , False ): table . add_row ( name , details . documentation . full_doc ) else : table . add_row ( name , details . documentation . description ) return table create_renderable_from_module_type_map ( module_types , ignore_pipeline_modules = False , ignore_non_pipeline_modules = False , ** config ) classmethod \u00b6 Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description Source code in kiara/info/modules.py @classmethod def create_renderable_from_module_type_map ( cls , module_types : typing . Mapping [ str , typing . Type [ \"KiaraModule\" ]], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" return cls . create_renderable_from_module_info_map ( { k : KiaraModuleTypeMetadata . from_module_class ( v ) for k , v in module_types . items () }, ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config )","title":"modules"},{"location":"reference/kiara/info/modules/#kiara.info.modules.ModuleTypesGroupInfo","text":"Source code in kiara/info/modules.py class ModuleTypesGroupInfo ( KiaraInfoModel ): __root__ : typing . Dict [ str , KiaraModuleTypeMetadata ] @classmethod def from_type_names ( cls , kiara : \"Kiara\" , type_names : typing . Optional [ typing . Iterable [ str ]] = None , ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if type_names is None : type_names = kiara . available_module_types classes = {} for tn in type_names : _cls = kiara . get_module_class ( tn ) if ignore_pipeline_modules and _cls . is_pipeline (): continue if ignore_non_pipeline_modules and not _cls . is_pipeline (): continue classes [ tn ] = KiaraModuleTypeMetadata . from_module_class ( _cls ) return ModuleTypesGroupInfo ( __root__ = classes ) @classmethod def create_renderable_from_type_names ( cls , kiara : \"Kiara\" , type_names : typing . Iterable [ str ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): classes = {} for tn in type_names : _cls = kiara . get_module_class ( tn ) classes [ tn ] = _cls return cls . create_renderable_from_module_type_map ( module_types = classes , ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config ) @classmethod def create_renderable_from_module_type_map ( cls , module_types : typing . Mapping [ str , typing . Type [ \"KiaraModule\" ]], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" return cls . create_renderable_from_module_info_map ( { k : KiaraModuleTypeMetadata . from_module_class ( v ) for k , v in module_types . items () }, ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config ) @classmethod def create_renderable_from_module_info_map ( cls , module_types : typing . Mapping [ str , KiaraModuleTypeMetadata ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if ignore_pipeline_modules : module_types = { k : v for k , v in module_types . items () if not v . is_pipeline } elif ignore_non_pipeline_modules : module_types = { k : v for k , v in module_types . items () if v . is_pipeline } show_lines = False table = Table ( show_header = False , box = box . SIMPLE , show_lines = show_lines ) table . add_column ( \"name\" , style = \"b\" ) table . add_column ( \"desc\" , style = \"i\" ) for name , details in module_types . items (): if config . get ( \"include_full_doc\" , False ): table . add_row ( name , details . documentation . full_doc ) else : table . add_row ( name , details . documentation . description ) return table def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return ModuleTypesGroupInfo . create_renderable_from_module_info_map ( self . __root__ )","title":"ModuleTypesGroupInfo"},{"location":"reference/kiara/info/modules/#kiara.info.modules.ModuleTypesGroupInfo.create_renderable_from_module_info_map","text":"Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description Source code in kiara/info/modules.py @classmethod def create_renderable_from_module_info_map ( cls , module_types : typing . Mapping [ str , KiaraModuleTypeMetadata ], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module info wrappers. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" if ignore_pipeline_modules and ignore_non_pipeline_modules : raise Exception ( \"Can't ignore both pipeline and non-pipeline modules.\" ) if ignore_pipeline_modules : module_types = { k : v for k , v in module_types . items () if not v . is_pipeline } elif ignore_non_pipeline_modules : module_types = { k : v for k , v in module_types . items () if v . is_pipeline } show_lines = False table = Table ( show_header = False , box = box . SIMPLE , show_lines = show_lines ) table . add_column ( \"name\" , style = \"b\" ) table . add_column ( \"desc\" , style = \"i\" ) for name , details in module_types . items (): if config . get ( \"include_full_doc\" , False ): table . add_row ( name , details . documentation . full_doc ) else : table . add_row ( name , details . documentation . description ) return table","title":"create_renderable_from_module_info_map()"},{"location":"reference/kiara/info/modules/#kiara.info.modules.ModuleTypesGroupInfo.create_renderable_from_module_type_map","text":"Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description Source code in kiara/info/modules.py @classmethod def create_renderable_from_module_type_map ( cls , module_types : typing . Mapping [ str , typing . Type [ \"KiaraModule\" ]], ignore_pipeline_modules : bool = False , ignore_non_pipeline_modules : bool = False , ** config : typing . Any ): \"\"\"Create a renderable from a map of module classes. Render-configuration options: - include_full_doc (default: False): include the full documentation, instead of just a one line description \"\"\" return cls . create_renderable_from_module_info_map ( { k : KiaraModuleTypeMetadata . from_module_class ( v ) for k , v in module_types . items () }, ignore_pipeline_modules = ignore_pipeline_modules , ignore_non_pipeline_modules = ignore_non_pipeline_modules , ** config )","title":"create_renderable_from_module_type_map()"},{"location":"reference/kiara/info/operations/","text":"OperationsGroupInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/operations.py class OperationsGroupInfo ( KiaraInfoModel ): @classmethod def create ( cls , kiara : \"Kiara\" , ignore_errors : bool = False ): operation_types = OperationsInfo . create_all ( kiara = kiara ) operation_configs = operation_types . pop ( \"all\" ) return OperationsGroupInfo ( operation_types = operation_types , operation_configs = operation_configs . operation_configs , ) operation_types : typing . Dict [ str , OperationsInfo ] = Field ( description = \"The available operation types and their details.\" ) operation_configs : typing . Dict [ str , Operation ] = Field ( description = \"The available operation ids and module_configs.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) op_map_table = OperationsInfo . create_renderable_from_operations_map ( self . operation_types ) table . add_row ( \"operation types\" , op_map_table ) configs = ModuleConfig . create_renderable_from_module_instance_configs ( self . operation_configs ) table . add_row ( \"operations\" , configs ) return table operation_configs : Dict [ str , kiara . operations . Operation ] pydantic-field required \u00b6 The available operation ids and module_configs. operation_types : Dict [ str , kiara . info . operations . OperationsInfo ] pydantic-field required \u00b6 The available operation types and their details. OperationsInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/operations.py class OperationsInfo ( KiaraInfoModel ): @classmethod def create_all ( self , kiara : Kiara ) -> typing . Dict [ str , \"OperationsInfo\" ]: op_types = kiara . operation_mgmt . operation_types return { op_name : OperationsInfo . create ( operations = op_types [ op_name ]) for op_name in sorted ( op_types . keys ()) } @classmethod def create ( cls , operations : OperationType ): info = OperationsMetadata . from_operations_class ( operations . __class__ ) return OperationsInfo ( info = info , operation_configs = dict ( operations . operations )) @classmethod def create_renderable_from_operations_map ( cls , op_map : typing . Mapping [ str , \"OperationsInfo\" ] ): table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) for op_name , ops in op_map . items (): table . add_row ( op_name , ops ) return table info : OperationsMetadata = Field ( description = \"Details about the type of operations contained in this collection.\" ) operation_configs : typing . Dict [ str , Operation ] = Field ( description = \"All available operation ids and their configurations.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return self . info . create_renderable ( operations = self . operation_configs , omit_type_name = True ) info : OperationsMetadata pydantic-field required \u00b6 Details about the type of operations contained in this collection. operation_configs : Dict [ str , kiara . operations . Operation ] pydantic-field required \u00b6 All available operation ids and their configurations.","title":"operations"},{"location":"reference/kiara/info/operations/#kiara.info.operations.OperationsGroupInfo","text":"Source code in kiara/info/operations.py class OperationsGroupInfo ( KiaraInfoModel ): @classmethod def create ( cls , kiara : \"Kiara\" , ignore_errors : bool = False ): operation_types = OperationsInfo . create_all ( kiara = kiara ) operation_configs = operation_types . pop ( \"all\" ) return OperationsGroupInfo ( operation_types = operation_types , operation_configs = operation_configs . operation_configs , ) operation_types : typing . Dict [ str , OperationsInfo ] = Field ( description = \"The available operation types and their details.\" ) operation_configs : typing . Dict [ str , Operation ] = Field ( description = \"The available operation ids and module_configs.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) op_map_table = OperationsInfo . create_renderable_from_operations_map ( self . operation_types ) table . add_row ( \"operation types\" , op_map_table ) configs = ModuleConfig . create_renderable_from_module_instance_configs ( self . operation_configs ) table . add_row ( \"operations\" , configs ) return table","title":"OperationsGroupInfo"},{"location":"reference/kiara/info/operations/#kiara.info.operations.OperationsGroupInfo.operation_configs","text":"The available operation ids and module_configs.","title":"operation_configs"},{"location":"reference/kiara/info/operations/#kiara.info.operations.OperationsGroupInfo.operation_types","text":"The available operation types and their details.","title":"operation_types"},{"location":"reference/kiara/info/operations/#kiara.info.operations.OperationsInfo","text":"Source code in kiara/info/operations.py class OperationsInfo ( KiaraInfoModel ): @classmethod def create_all ( self , kiara : Kiara ) -> typing . Dict [ str , \"OperationsInfo\" ]: op_types = kiara . operation_mgmt . operation_types return { op_name : OperationsInfo . create ( operations = op_types [ op_name ]) for op_name in sorted ( op_types . keys ()) } @classmethod def create ( cls , operations : OperationType ): info = OperationsMetadata . from_operations_class ( operations . __class__ ) return OperationsInfo ( info = info , operation_configs = dict ( operations . operations )) @classmethod def create_renderable_from_operations_map ( cls , op_map : typing . Mapping [ str , \"OperationsInfo\" ] ): table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) for op_name , ops in op_map . items (): table . add_row ( op_name , ops ) return table info : OperationsMetadata = Field ( description = \"Details about the type of operations contained in this collection.\" ) operation_configs : typing . Dict [ str , Operation ] = Field ( description = \"All available operation ids and their configurations.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return self . info . create_renderable ( operations = self . operation_configs , omit_type_name = True )","title":"OperationsInfo"},{"location":"reference/kiara/info/operations/#kiara.info.operations.OperationsInfo.info","text":"Details about the type of operations contained in this collection.","title":"info"},{"location":"reference/kiara/info/operations/#kiara.info.operations.OperationsInfo.operation_configs","text":"All available operation ids and their configurations.","title":"operation_configs"},{"location":"reference/kiara/info/pipelines/","text":"PipelineModuleInfo ( KiaraModuleTypeMetadata ) pydantic-model \u00b6 Source code in kiara/info/pipelines.py class PipelineModuleInfo ( KiaraModuleTypeMetadata ): class Config : extra = Extra . forbid @classmethod def from_type_name ( cls , module_type_name : str , kiara : \"Kiara\" ): m = kiara . get_module_class ( module_type = module_type_name ) base_conf : \"PipelineConfig\" = m . _base_pipeline_config # type: ignore structure = base_conf . create_pipeline_structure ( kiara = kiara ) struc_desc = PipelineStructureDesc . create_pipeline_structure_desc ( pipeline = structure ) attrs = PipelineModuleInfo . extract_module_attributes ( module_cls = m ) attrs [ \"structure\" ] = struc_desc pmi = PipelineModuleInfo ( ** attrs ) pmi . _kiara = kiara pmi . _structure = structure return pmi _kiara : \"Kiara\" = PrivateAttr () _structure : \"PipelineStructure\" = PrivateAttr () structure : PipelineStructureDesc = Field ( description = \"The pipeline structure.\" ) def print_data_flow_graph ( self , simplified : bool = True ) -> None : structure = self . _structure if simplified : graph = structure . data_flow_graph_simple else : graph = structure . data_flow_graph print_ascii_graph ( graph ) def print_execution_graph ( self ) -> None : structure = self . _structure print_ascii_graph ( structure . execution_graph ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : my_table = Table ( box = box . SIMPLE , show_lines = True , show_header = False ) my_table . add_column ( \"Property\" , style = \"i\" ) my_table . add_column ( \"Value\" ) my_table . add_row ( \"class\" , self . python_class . full_name ) my_table . add_row ( \"is pipeline\" , \"yes\" ) my_table . add_row ( \"doc\" , self . documentation . full_doc ) my_table . add_row ( \"config class\" , self . config . python_class . full_name ) my_table . add_row ( \"config\" , create_table_from_config_class ( self . config . python_class . get_class (), # type: ignore remove_pipeline_config = True , ), ) structure = self . _structure p_inputs = {} for input_name , schema in structure . pipeline_input_schema . items (): p_inputs [ input_name ] = { \"type\" : schema . type , \"doc\" : schema . doc } inputs_str = yaml . dump ( p_inputs ) _inputs_txt = Syntax ( inputs_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"pipeline inputs\" , _inputs_txt ) outputs = {} for output_name , schema in structure . pipeline_output_schema . items (): outputs [ output_name ] = { \"type\" : schema . type , \"doc\" : schema . doc } outputs_str = yaml . dump ( outputs ) _outputs_txt = Syntax ( outputs_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"pipeline outputs\" , _outputs_txt ) stages : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} for nr , stage in enumerate ( structure . processing_stages ): for s_id in stage : step = structure . get_step ( s_id ) mc = self . _kiara . get_module_class ( step . module_type ) desc = mc . get_type_metadata () . documentation . full_doc inputs : typing . Dict [ \"ValueRef\" , typing . List [ str ]] = {} for inp in structure . steps_inputs . values (): if inp . step_id != s_id : continue if inp . connected_outputs : for co in inp . connected_outputs : inputs . setdefault ( inp , []) . append ( co . alias ) else : inputs . setdefault ( inp , []) . append ( f \"__pipeline__. { inp . connected_pipeline_input } \" ) inp_str = [] for k , v in inputs . items (): s = f \" { k . value_name } \u2190 { ', ' . join ( v ) } \" inp_str . append ( s ) outp_str = [] for outp in structure . steps_outputs . values (): if outp . step_id != s_id : continue if outp . pipeline_output : outp_str . append ( f \" { outp . value_name } \u2192 __pipeline__. { outp . pipeline_output } \" ) else : outp_str . append ( outp . value_name ) stages . setdefault ( f \"stage { nr } \" , {})[ s_id ] = { \"module\" : step . module_type , \"desc\" : desc , \"inputs\" : inp_str , \"outputs\" : outp_str , } stages_str = yaml . dump ( stages ) _stages_txt = Syntax ( stages_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"processing stages\" , _stages_txt ) return my_table structure : PipelineStructureDesc pydantic-field required \u00b6 The pipeline structure. PipelineState ( KiaraInfoModel ) pydantic-model \u00b6 Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the dict or json methods to convert this object into a generic data structure. Source code in kiara/info/pipelines.py class PipelineState ( KiaraInfoModel ): \"\"\"Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the ``dict`` or ``json`` methods to convert this object into a generic data structure. \"\"\" structure : PipelineStructureDesc = Field ( description = \"The structure (interconnections of modules/steps) of the pipeline.\" ) pipeline_inputs : PipelineValuesInfo = Field ( description = \"The current (externally facing) input values of this pipeline.\" ) pipeline_outputs : PipelineValuesInfo = Field ( description = \"The current (externally facing) output values of this pipeline.\" ) step_states : typing . Dict [ str , StepStatus ] = Field ( description = \"The status of each step.\" ) step_inputs : typing . Dict [ str , PipelineValuesInfo ] = Field ( description = \"The current (internal) input values of each step of this pipeline.\" ) step_outputs : typing . Dict [ str , PipelineValuesInfo ] = Field ( description = \"The current (internal) output values of each step of this pipeline.\" ) status : StepStatus = Field ( description = \"The current overal status of the pipeline.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : from kiara.pipeline.pipeline import StepStatus all : typing . List [ RenderableType ] = [] all . append ( \"[b]Pipeline state[/b]\" ) all . append ( \"\" ) if self . status == StepStatus . RESULTS_READY : c = \"green\" elif self . status == StepStatus . INPUTS_READY : c = \"yellow\" else : c = \"red\" all . append ( f \"[b]Status[/b]: [b i { c } ] { self . status . name } [/b i { c } ]\" ) all . append ( \"\" ) all . append ( \"[b]Inputs / Outputs[/b]\" ) r_gro = [] inp_table = Table ( show_header = True , box = box . SIMPLE ) inp_table . add_column ( \"Field name\" , style = \"i\" ) inp_table . add_column ( \"Type\" ) inp_table . add_column ( \"Description\" ) inp_table . add_column ( \"Required\" ) inp_table . add_column ( \"Status\" , justify = \"center\" ) inp_table . add_column ( \"Ready\" , justify = \"center\" ) for field_name , value in self . pipeline_inputs . values . items (): req = value . value_schema . is_required () if not req : req_string = \"no\" else : req_string = \"[bold]yes[/bold]\" if value . is_set : status = \"-- set --\" valid = \"[green]yes[/green]\" else : valid = \"[green]yes[/green]\" if value . is_valid else \"[red]no[/red]\" if value . is_valid : status = \"-- not set --\" else : status = \"-- not set --\" inp_table . add_row ( field_name , value . value_schema . type , value . value_schema . doc , req_string , status , valid , ) r_gro . append ( Panel ( inp_table , box = box . ROUNDED , title_align = \"left\" , title = \"Inputs\" ) ) out_table = Table ( show_header = True , box = box . SIMPLE ) out_table . add_column ( \"Field name\" , style = \"i\" ) out_table . add_column ( \"Type\" ) out_table . add_column ( \"Description\" ) out_table . add_column ( \"Required\" ) out_table . add_column ( \"Status\" , justify = \"center\" ) out_table . add_column ( \"Ready\" , justify = \"center\" ) for field_name , value in self . pipeline_outputs . values . items (): req = value . value_schema . is_required () if not req : req_string = \"no\" else : req_string = \"[bold]yes[/bold]\" if value . is_set : status = \"-- set --\" valid = \"[green]yes[/green]\" else : valid = \"[green]yes[/green]\" if value . is_valid else \"[red]no[/red]\" status = \"-- not set --\" out_table . add_row ( field_name , value . value_schema . type , value . value_schema . doc , req_string , status , valid , ) r_gro . append ( Panel ( out_table , box = box . ROUNDED , title_align = \"left\" , title = \"Outputs\" ) ) all . append ( Panel ( RenderGroup ( * r_gro ), box = box . SIMPLE )) rg = [] for nr , stage in enumerate ( self . structure . processing_stages ): render_group = [] for s in self . structure . steps . values (): if s . step . step_id not in stage : continue step_table = create_pipeline_step_table ( self , s ) render_group . append ( step_table ) panel = Panel ( RenderGroup ( * render_group ), box = box . ROUNDED , title = f \"Processing stage: { nr + 1 } \" , title_align = \"left\" , ) rg . append ( panel ) all . append ( \"[b]Steps[/b]\" ) r_panel = Panel ( RenderGroup ( * rg ), box = box . SIMPLE ) all . append ( r_panel ) return RenderGroup ( * all ) pipeline_inputs : PipelineValuesInfo pydantic-field required \u00b6 The current (externally facing) input values of this pipeline. pipeline_outputs : PipelineValuesInfo pydantic-field required \u00b6 The current (externally facing) output values of this pipeline. status : StepStatus pydantic-field required \u00b6 The current overal status of the pipeline. step_inputs : Dict [ str , kiara . pipeline . PipelineValuesInfo ] pydantic-field required \u00b6 The current (internal) input values of each step of this pipeline. step_outputs : Dict [ str , kiara . pipeline . PipelineValuesInfo ] pydantic-field required \u00b6 The current (internal) output values of each step of this pipeline. step_states : Dict [ str , kiara . pipeline . StepStatus ] pydantic-field required \u00b6 The status of each step. structure : PipelineStructureDesc pydantic-field required \u00b6 The structure (interconnections of modules/steps) of the pipeline. PipelineStructureDesc ( BaseModel ) pydantic-model \u00b6 Outlines the internal structure of a Pipeline . Source code in kiara/info/pipelines.py class PipelineStructureDesc ( BaseModel ): \"\"\"Outlines the internal structure of a [Pipeline][kiara.pipeline.pipeline.Pipeline].\"\"\" @classmethod def create_pipeline_structure_desc ( cls , pipeline : typing . Union [ \"Pipeline\" , \"PipelineStructure\" ] ) -> \"PipelineStructureDesc\" : from kiara.pipeline.pipeline import Pipeline from kiara.pipeline.structure import PipelineStructure if isinstance ( pipeline , Pipeline ): structure : PipelineStructure = pipeline . structure elif isinstance ( pipeline , PipelineStructure ): structure = pipeline else : raise TypeError ( f \"Invalid type ' { type ( pipeline ) } ' for pipeline.\" ) steps = {} workflow_inputs : typing . Dict [ str , typing . List [ str ]] = {} workflow_outputs : typing . Dict [ str , str ] = {} for m_id , details in structure . steps_details . items (): step = details [ \"step\" ] input_connections : typing . Dict [ str , typing . List [ str ]] = {} for k , v in details [ \"inputs\" ] . items (): if v . connected_pipeline_input is not None : connected_item = v . connected_pipeline_input input_connections [ k ] = [ generate_step_alias ( PIPELINE_PARENT_MARKER , connected_item ) ] workflow_inputs . setdefault ( f \" { connected_item } \" , []) . append ( v . alias ) elif v . connected_outputs is not None : assert len ( v . connected_outputs ) > 0 for co in v . connected_outputs : input_connections . setdefault ( k , []) . append ( co . alias ) else : raise TypeError ( f \"Invalid connection type: { v } \" ) output_connections : typing . Dict [ str , typing . Any ] = {} for k , v in details [ \"outputs\" ] . items (): for connected_item in v . connected_inputs : output_connections . setdefault ( k , []) . append ( generate_step_alias ( connected_item . step_id , connected_item . value_name ) ) if v . pipeline_output : output_connections . setdefault ( k , []) . append ( generate_step_alias ( PIPELINE_PARENT_MARKER , v . pipeline_output ) ) workflow_outputs [ v . pipeline_output ] = v . alias steps [ step . step_id ] = StepDesc ( step = step , processing_stage = details [ \"processing_stage\" ], input_connections = input_connections , output_connections = output_connections , required = step . required , ) return PipelineStructureDesc ( steps = steps , processing_stages = structure . processing_stages , pipeline_input_connections = workflow_inputs , pipeline_output_connections = workflow_outputs , pipeline_inputs = structure . pipeline_inputs , pipeline_outputs = structure . pipeline_outputs , ) class Config : allow_mutation = False extra = Extra . forbid steps : typing . Dict [ str , StepDesc ] = Field ( description = \"The steps contained in this pipeline, with the 'step_id' as key.\" ) processing_stages : typing . List [ typing . List [ str ]] = Field ( description = \"The order in which this pipeline has to be processed (basically the dependencies of each step on other steps, if any).\" ) pipeline_input_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"The connections of this pipelines input fields. One input field can be connected to one or several step input fields.\" ) pipeline_output_connections : typing . Dict [ str , str ] = Field ( description = \"The connections of this pipelines output fields. Each pipeline output is connected to exactly one step output field.\" ) pipeline_inputs : typing . Dict [ str , PipelineInputRef ] = Field ( description = \"The pipeline inputs.\" ) pipeline_outputs : typing . Dict [ str , PipelineOutputRef ] = Field ( description = \"The pipeline outputs.\" ) @property def steps_info ( self ) -> StepsInfo : return StepsInfo ( processing_stages = self . processing_stages , steps = self . steps , ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield \"[b]Pipeline structure[/b] \\n \" yield \"[b]Inputs / Outputs[/b]\" data_panel : typing . List [ typing . Any ] = [] inp_table = Table ( show_header = True , box = box . SIMPLE , show_lines = True ) inp_table . add_column ( \"Name\" , style = \"i\" ) inp_table . add_column ( \"Type\" ) inp_table . add_column ( \"Description\" ) inp_table . add_column ( \"Required\" , justify = \"center\" ) inp_table . add_column ( \"Default\" , justify = \"center\" ) for inp , details in self . pipeline_inputs . items (): req = details . value_schema . is_required () if not req : req_str = \"no\" else : d = details . value_schema . default if d in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ]: req_str = \"[b]yes[/b]\" else : req_str = \"no\" default = details . value_schema . default if default in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ]: default = \"-- no default --\" else : default = str ( default ) inp_table . add_row ( inp , details . value_schema . type , details . value_schema . doc , req_str , default , ) p_inp = Panel ( inp_table , box = box . ROUNDED , title = \"Input fields\" , title_align = \"left\" ) data_panel . append ( p_inp ) # yield \"[b]Pipeline outputs[/b]\" out_table = Table ( show_header = True , box = box . SIMPLE , show_lines = True ) out_table . add_column ( \"Name\" , style = \"i\" ) out_table . add_column ( \"Type\" ) out_table . add_column ( \"Description\" ) for inp , details_o in self . pipeline_outputs . items (): out_table . add_row ( inp , details_o . value_schema . type , details_o . value_schema . doc , ) outp = Panel ( out_table , box = box . ROUNDED , title = \"Output fields\" , title_align = \"left\" ) data_panel . append ( outp ) yield Panel ( RenderGroup ( * data_panel ), box = box . SIMPLE ) step_color_map = {} for i , s in enumerate ( self . steps . values ()): step_color_map [ s . step . step_id ] = COLOR_LIST [ i % len ( COLOR_LIST )] rg = [] for nr , stage in enumerate ( self . processing_stages ): render_group = [] for s in self . steps . values (): if s . step . step_id not in stage : continue step_table = create_step_table ( s , step_color_map ) render_group . append ( step_table ) panel = Panel ( RenderGroup ( * render_group ), box = box . ROUNDED , title = f \"Processing stage: { nr + 1 } \" , title_align = \"left\" , ) rg . append ( panel ) yield \"[b]Steps[/b]\" r_panel = Panel ( RenderGroup ( * rg ), box = box . SIMPLE ) yield r_panel pipeline_input_connections : Dict [ str , List [ str ]] pydantic-field required \u00b6 The connections of this pipelines input fields. One input field can be connected to one or several step input fields. pipeline_inputs : Dict [ str , kiara . pipeline . values . PipelineInputRef ] pydantic-field required \u00b6 The pipeline inputs. pipeline_output_connections : Dict [ str , str ] pydantic-field required \u00b6 The connections of this pipelines output fields. Each pipeline output is connected to exactly one step output field. pipeline_outputs : Dict [ str , kiara . pipeline . values . PipelineOutputRef ] pydantic-field required \u00b6 The pipeline outputs. processing_stages : List [ List [ str ]] pydantic-field required \u00b6 The order in which this pipeline has to be processed (basically the dependencies of each step on other steps, if any). steps : Dict [ str , kiara . pipeline . config . StepDesc ] pydantic-field required \u00b6 The steps contained in this pipeline, with the 'step_id' as key. StepsInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/pipelines.py class StepsInfo ( KiaraInfoModel ): steps : typing . Dict [ str , StepDesc ] = Field ( description = \"A list of step details.\" ) processing_stages : typing . List [ typing . List [ str ]] = Field ( description = \"The stages in which the steps are processed.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( box = box . SIMPLE , show_lines = True ) table . add_column ( \"Stage\" , justify = \"center\" ) table . add_column ( \"Step Id\" ) table . add_column ( \"Module type\" , style = \"i\" ) table . add_column ( \"Description\" ) for nr , stage in enumerate ( self . processing_stages ): for i , step_id in enumerate ( stage ): step : StepDesc = self . steps [ step_id ] if step . required : title = f \"[b] { step_id } [/b]\" else : title = f \"[b] { step_id } [/b] [i](optional)[/i]\" if hasattr ( step . step . module , \"instance_doc\" ): doc = step . step . module . module_instance_doc else : doc = step . step . module . get_type_metadata () . documentation . full_doc row : typing . List [ typing . Any ] = [] if i == 0 : row . append ( str ( nr )) else : row . append ( \"\" ) row . append ( title ) # TODO; generate the right link here module_link = ( step . step . module . get_type_metadata () . context . references . get ( \"sources\" , None ) ) if module_link : module_str = f \"[link= { module_link } ] { step . step . module_type } [/link]\" else : module_str = step . step . module_type row . append ( module_str ) if doc and doc != \"-- n/a --\" : m = Markdown ( doc + \" \\n\\n --- \\n \" ) row . append ( m ) else : row . append ( \"\" ) table . add_row ( * row ) return table # yield Panel(table, title_align=\"left\", title=\"Processing stages\") # def __rich_console_old__( # self, console: Console, options: ConsoleOptions # ) -> RenderResult: # # explanation = {} # # for nr, stage in enumerate(self.processing_stages): # # stage_details = {} # for step_id in stage: # step: StepDesc = self.steps[step_id] # if step.required: # title = step_id # else: # title = f\"{step_id} (optional)\" # stage_details[title] = step.step.module.get_type_metadata().model_doc() # # explanation[nr + 1] = stage_details # # lines = [] # for stage_nr, stage_steps in explanation.items(): # lines.append(f\"[bold]Processing stage {stage_nr}[/bold]:\") # lines.append(\"\") # for step_id, desc in stage_steps.items(): # if desc == DEFAULT_NO_DESC_VALUE: # lines.append(f\" - {step_id}\") # else: # lines.append(f\" - {step_id}: [i]{desc}[/i]\") # lines.append(\"\") # # padding = (1, 2, 0, 2) # yield Panel( # \"\\n\".join(lines), # box=box.ROUNDED, # title_align=\"left\", # title=f\"Stages for pipeline: [b]{self.pipeline_id}[/b]\", # padding=padding, # ) processing_stages : List [ List [ str ]] pydantic-field required \u00b6 The stages in which the steps are processed. steps : Dict [ str , kiara . pipeline . config . StepDesc ] pydantic-field required \u00b6 A list of step details.","title":"pipelines"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineModuleInfo","text":"Source code in kiara/info/pipelines.py class PipelineModuleInfo ( KiaraModuleTypeMetadata ): class Config : extra = Extra . forbid @classmethod def from_type_name ( cls , module_type_name : str , kiara : \"Kiara\" ): m = kiara . get_module_class ( module_type = module_type_name ) base_conf : \"PipelineConfig\" = m . _base_pipeline_config # type: ignore structure = base_conf . create_pipeline_structure ( kiara = kiara ) struc_desc = PipelineStructureDesc . create_pipeline_structure_desc ( pipeline = structure ) attrs = PipelineModuleInfo . extract_module_attributes ( module_cls = m ) attrs [ \"structure\" ] = struc_desc pmi = PipelineModuleInfo ( ** attrs ) pmi . _kiara = kiara pmi . _structure = structure return pmi _kiara : \"Kiara\" = PrivateAttr () _structure : \"PipelineStructure\" = PrivateAttr () structure : PipelineStructureDesc = Field ( description = \"The pipeline structure.\" ) def print_data_flow_graph ( self , simplified : bool = True ) -> None : structure = self . _structure if simplified : graph = structure . data_flow_graph_simple else : graph = structure . data_flow_graph print_ascii_graph ( graph ) def print_execution_graph ( self ) -> None : structure = self . _structure print_ascii_graph ( structure . execution_graph ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : my_table = Table ( box = box . SIMPLE , show_lines = True , show_header = False ) my_table . add_column ( \"Property\" , style = \"i\" ) my_table . add_column ( \"Value\" ) my_table . add_row ( \"class\" , self . python_class . full_name ) my_table . add_row ( \"is pipeline\" , \"yes\" ) my_table . add_row ( \"doc\" , self . documentation . full_doc ) my_table . add_row ( \"config class\" , self . config . python_class . full_name ) my_table . add_row ( \"config\" , create_table_from_config_class ( self . config . python_class . get_class (), # type: ignore remove_pipeline_config = True , ), ) structure = self . _structure p_inputs = {} for input_name , schema in structure . pipeline_input_schema . items (): p_inputs [ input_name ] = { \"type\" : schema . type , \"doc\" : schema . doc } inputs_str = yaml . dump ( p_inputs ) _inputs_txt = Syntax ( inputs_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"pipeline inputs\" , _inputs_txt ) outputs = {} for output_name , schema in structure . pipeline_output_schema . items (): outputs [ output_name ] = { \"type\" : schema . type , \"doc\" : schema . doc } outputs_str = yaml . dump ( outputs ) _outputs_txt = Syntax ( outputs_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"pipeline outputs\" , _outputs_txt ) stages : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} for nr , stage in enumerate ( structure . processing_stages ): for s_id in stage : step = structure . get_step ( s_id ) mc = self . _kiara . get_module_class ( step . module_type ) desc = mc . get_type_metadata () . documentation . full_doc inputs : typing . Dict [ \"ValueRef\" , typing . List [ str ]] = {} for inp in structure . steps_inputs . values (): if inp . step_id != s_id : continue if inp . connected_outputs : for co in inp . connected_outputs : inputs . setdefault ( inp , []) . append ( co . alias ) else : inputs . setdefault ( inp , []) . append ( f \"__pipeline__. { inp . connected_pipeline_input } \" ) inp_str = [] for k , v in inputs . items (): s = f \" { k . value_name } \u2190 { ', ' . join ( v ) } \" inp_str . append ( s ) outp_str = [] for outp in structure . steps_outputs . values (): if outp . step_id != s_id : continue if outp . pipeline_output : outp_str . append ( f \" { outp . value_name } \u2192 __pipeline__. { outp . pipeline_output } \" ) else : outp_str . append ( outp . value_name ) stages . setdefault ( f \"stage { nr } \" , {})[ s_id ] = { \"module\" : step . module_type , \"desc\" : desc , \"inputs\" : inp_str , \"outputs\" : outp_str , } stages_str = yaml . dump ( stages ) _stages_txt = Syntax ( stages_str , \"yaml\" , background_color = \"default\" ) my_table . add_row ( \"processing stages\" , _stages_txt ) return my_table","title":"PipelineModuleInfo"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineModuleInfo.structure","text":"The pipeline structure.","title":"structure"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineState","text":"Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the dict or json methods to convert this object into a generic data structure. Source code in kiara/info/pipelines.py class PipelineState ( KiaraInfoModel ): \"\"\"Describes the current state of a pipeline. This includes the structure of the pipeline (how the internal modules/steps are connected to each other), as well as all current input/output values for the pipeline itself, as well as for all internal steps. Use the ``dict`` or ``json`` methods to convert this object into a generic data structure. \"\"\" structure : PipelineStructureDesc = Field ( description = \"The structure (interconnections of modules/steps) of the pipeline.\" ) pipeline_inputs : PipelineValuesInfo = Field ( description = \"The current (externally facing) input values of this pipeline.\" ) pipeline_outputs : PipelineValuesInfo = Field ( description = \"The current (externally facing) output values of this pipeline.\" ) step_states : typing . Dict [ str , StepStatus ] = Field ( description = \"The status of each step.\" ) step_inputs : typing . Dict [ str , PipelineValuesInfo ] = Field ( description = \"The current (internal) input values of each step of this pipeline.\" ) step_outputs : typing . Dict [ str , PipelineValuesInfo ] = Field ( description = \"The current (internal) output values of each step of this pipeline.\" ) status : StepStatus = Field ( description = \"The current overal status of the pipeline.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : from kiara.pipeline.pipeline import StepStatus all : typing . List [ RenderableType ] = [] all . append ( \"[b]Pipeline state[/b]\" ) all . append ( \"\" ) if self . status == StepStatus . RESULTS_READY : c = \"green\" elif self . status == StepStatus . INPUTS_READY : c = \"yellow\" else : c = \"red\" all . append ( f \"[b]Status[/b]: [b i { c } ] { self . status . name } [/b i { c } ]\" ) all . append ( \"\" ) all . append ( \"[b]Inputs / Outputs[/b]\" ) r_gro = [] inp_table = Table ( show_header = True , box = box . SIMPLE ) inp_table . add_column ( \"Field name\" , style = \"i\" ) inp_table . add_column ( \"Type\" ) inp_table . add_column ( \"Description\" ) inp_table . add_column ( \"Required\" ) inp_table . add_column ( \"Status\" , justify = \"center\" ) inp_table . add_column ( \"Ready\" , justify = \"center\" ) for field_name , value in self . pipeline_inputs . values . items (): req = value . value_schema . is_required () if not req : req_string = \"no\" else : req_string = \"[bold]yes[/bold]\" if value . is_set : status = \"-- set --\" valid = \"[green]yes[/green]\" else : valid = \"[green]yes[/green]\" if value . is_valid else \"[red]no[/red]\" if value . is_valid : status = \"-- not set --\" else : status = \"-- not set --\" inp_table . add_row ( field_name , value . value_schema . type , value . value_schema . doc , req_string , status , valid , ) r_gro . append ( Panel ( inp_table , box = box . ROUNDED , title_align = \"left\" , title = \"Inputs\" ) ) out_table = Table ( show_header = True , box = box . SIMPLE ) out_table . add_column ( \"Field name\" , style = \"i\" ) out_table . add_column ( \"Type\" ) out_table . add_column ( \"Description\" ) out_table . add_column ( \"Required\" ) out_table . add_column ( \"Status\" , justify = \"center\" ) out_table . add_column ( \"Ready\" , justify = \"center\" ) for field_name , value in self . pipeline_outputs . values . items (): req = value . value_schema . is_required () if not req : req_string = \"no\" else : req_string = \"[bold]yes[/bold]\" if value . is_set : status = \"-- set --\" valid = \"[green]yes[/green]\" else : valid = \"[green]yes[/green]\" if value . is_valid else \"[red]no[/red]\" status = \"-- not set --\" out_table . add_row ( field_name , value . value_schema . type , value . value_schema . doc , req_string , status , valid , ) r_gro . append ( Panel ( out_table , box = box . ROUNDED , title_align = \"left\" , title = \"Outputs\" ) ) all . append ( Panel ( RenderGroup ( * r_gro ), box = box . SIMPLE )) rg = [] for nr , stage in enumerate ( self . structure . processing_stages ): render_group = [] for s in self . structure . steps . values (): if s . step . step_id not in stage : continue step_table = create_pipeline_step_table ( self , s ) render_group . append ( step_table ) panel = Panel ( RenderGroup ( * render_group ), box = box . ROUNDED , title = f \"Processing stage: { nr + 1 } \" , title_align = \"left\" , ) rg . append ( panel ) all . append ( \"[b]Steps[/b]\" ) r_panel = Panel ( RenderGroup ( * rg ), box = box . SIMPLE ) all . append ( r_panel ) return RenderGroup ( * all )","title":"PipelineState"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineState.pipeline_inputs","text":"The current (externally facing) input values of this pipeline.","title":"pipeline_inputs"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineState.pipeline_outputs","text":"The current (externally facing) output values of this pipeline.","title":"pipeline_outputs"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineState.status","text":"The current overal status of the pipeline.","title":"status"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineState.step_inputs","text":"The current (internal) input values of each step of this pipeline.","title":"step_inputs"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineState.step_outputs","text":"The current (internal) output values of each step of this pipeline.","title":"step_outputs"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineState.step_states","text":"The status of each step.","title":"step_states"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineState.structure","text":"The structure (interconnections of modules/steps) of the pipeline.","title":"structure"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineStructureDesc","text":"Outlines the internal structure of a Pipeline . Source code in kiara/info/pipelines.py class PipelineStructureDesc ( BaseModel ): \"\"\"Outlines the internal structure of a [Pipeline][kiara.pipeline.pipeline.Pipeline].\"\"\" @classmethod def create_pipeline_structure_desc ( cls , pipeline : typing . Union [ \"Pipeline\" , \"PipelineStructure\" ] ) -> \"PipelineStructureDesc\" : from kiara.pipeline.pipeline import Pipeline from kiara.pipeline.structure import PipelineStructure if isinstance ( pipeline , Pipeline ): structure : PipelineStructure = pipeline . structure elif isinstance ( pipeline , PipelineStructure ): structure = pipeline else : raise TypeError ( f \"Invalid type ' { type ( pipeline ) } ' for pipeline.\" ) steps = {} workflow_inputs : typing . Dict [ str , typing . List [ str ]] = {} workflow_outputs : typing . Dict [ str , str ] = {} for m_id , details in structure . steps_details . items (): step = details [ \"step\" ] input_connections : typing . Dict [ str , typing . List [ str ]] = {} for k , v in details [ \"inputs\" ] . items (): if v . connected_pipeline_input is not None : connected_item = v . connected_pipeline_input input_connections [ k ] = [ generate_step_alias ( PIPELINE_PARENT_MARKER , connected_item ) ] workflow_inputs . setdefault ( f \" { connected_item } \" , []) . append ( v . alias ) elif v . connected_outputs is not None : assert len ( v . connected_outputs ) > 0 for co in v . connected_outputs : input_connections . setdefault ( k , []) . append ( co . alias ) else : raise TypeError ( f \"Invalid connection type: { v } \" ) output_connections : typing . Dict [ str , typing . Any ] = {} for k , v in details [ \"outputs\" ] . items (): for connected_item in v . connected_inputs : output_connections . setdefault ( k , []) . append ( generate_step_alias ( connected_item . step_id , connected_item . value_name ) ) if v . pipeline_output : output_connections . setdefault ( k , []) . append ( generate_step_alias ( PIPELINE_PARENT_MARKER , v . pipeline_output ) ) workflow_outputs [ v . pipeline_output ] = v . alias steps [ step . step_id ] = StepDesc ( step = step , processing_stage = details [ \"processing_stage\" ], input_connections = input_connections , output_connections = output_connections , required = step . required , ) return PipelineStructureDesc ( steps = steps , processing_stages = structure . processing_stages , pipeline_input_connections = workflow_inputs , pipeline_output_connections = workflow_outputs , pipeline_inputs = structure . pipeline_inputs , pipeline_outputs = structure . pipeline_outputs , ) class Config : allow_mutation = False extra = Extra . forbid steps : typing . Dict [ str , StepDesc ] = Field ( description = \"The steps contained in this pipeline, with the 'step_id' as key.\" ) processing_stages : typing . List [ typing . List [ str ]] = Field ( description = \"The order in which this pipeline has to be processed (basically the dependencies of each step on other steps, if any).\" ) pipeline_input_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"The connections of this pipelines input fields. One input field can be connected to one or several step input fields.\" ) pipeline_output_connections : typing . Dict [ str , str ] = Field ( description = \"The connections of this pipelines output fields. Each pipeline output is connected to exactly one step output field.\" ) pipeline_inputs : typing . Dict [ str , PipelineInputRef ] = Field ( description = \"The pipeline inputs.\" ) pipeline_outputs : typing . Dict [ str , PipelineOutputRef ] = Field ( description = \"The pipeline outputs.\" ) @property def steps_info ( self ) -> StepsInfo : return StepsInfo ( processing_stages = self . processing_stages , steps = self . steps , ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield \"[b]Pipeline structure[/b] \\n \" yield \"[b]Inputs / Outputs[/b]\" data_panel : typing . List [ typing . Any ] = [] inp_table = Table ( show_header = True , box = box . SIMPLE , show_lines = True ) inp_table . add_column ( \"Name\" , style = \"i\" ) inp_table . add_column ( \"Type\" ) inp_table . add_column ( \"Description\" ) inp_table . add_column ( \"Required\" , justify = \"center\" ) inp_table . add_column ( \"Default\" , justify = \"center\" ) for inp , details in self . pipeline_inputs . items (): req = details . value_schema . is_required () if not req : req_str = \"no\" else : d = details . value_schema . default if d in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ]: req_str = \"[b]yes[/b]\" else : req_str = \"no\" default = details . value_schema . default if default in [ None , SpecialValue . NO_VALUE , SpecialValue . NOT_SET ]: default = \"-- no default --\" else : default = str ( default ) inp_table . add_row ( inp , details . value_schema . type , details . value_schema . doc , req_str , default , ) p_inp = Panel ( inp_table , box = box . ROUNDED , title = \"Input fields\" , title_align = \"left\" ) data_panel . append ( p_inp ) # yield \"[b]Pipeline outputs[/b]\" out_table = Table ( show_header = True , box = box . SIMPLE , show_lines = True ) out_table . add_column ( \"Name\" , style = \"i\" ) out_table . add_column ( \"Type\" ) out_table . add_column ( \"Description\" ) for inp , details_o in self . pipeline_outputs . items (): out_table . add_row ( inp , details_o . value_schema . type , details_o . value_schema . doc , ) outp = Panel ( out_table , box = box . ROUNDED , title = \"Output fields\" , title_align = \"left\" ) data_panel . append ( outp ) yield Panel ( RenderGroup ( * data_panel ), box = box . SIMPLE ) step_color_map = {} for i , s in enumerate ( self . steps . values ()): step_color_map [ s . step . step_id ] = COLOR_LIST [ i % len ( COLOR_LIST )] rg = [] for nr , stage in enumerate ( self . processing_stages ): render_group = [] for s in self . steps . values (): if s . step . step_id not in stage : continue step_table = create_step_table ( s , step_color_map ) render_group . append ( step_table ) panel = Panel ( RenderGroup ( * render_group ), box = box . ROUNDED , title = f \"Processing stage: { nr + 1 } \" , title_align = \"left\" , ) rg . append ( panel ) yield \"[b]Steps[/b]\" r_panel = Panel ( RenderGroup ( * rg ), box = box . SIMPLE ) yield r_panel","title":"PipelineStructureDesc"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineStructureDesc.pipeline_input_connections","text":"The connections of this pipelines input fields. One input field can be connected to one or several step input fields.","title":"pipeline_input_connections"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineStructureDesc.pipeline_inputs","text":"The pipeline inputs.","title":"pipeline_inputs"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineStructureDesc.pipeline_output_connections","text":"The connections of this pipelines output fields. Each pipeline output is connected to exactly one step output field.","title":"pipeline_output_connections"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineStructureDesc.pipeline_outputs","text":"The pipeline outputs.","title":"pipeline_outputs"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineStructureDesc.processing_stages","text":"The order in which this pipeline has to be processed (basically the dependencies of each step on other steps, if any).","title":"processing_stages"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.PipelineStructureDesc.steps","text":"The steps contained in this pipeline, with the 'step_id' as key.","title":"steps"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.StepsInfo","text":"Source code in kiara/info/pipelines.py class StepsInfo ( KiaraInfoModel ): steps : typing . Dict [ str , StepDesc ] = Field ( description = \"A list of step details.\" ) processing_stages : typing . List [ typing . List [ str ]] = Field ( description = \"The stages in which the steps are processed.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( box = box . SIMPLE , show_lines = True ) table . add_column ( \"Stage\" , justify = \"center\" ) table . add_column ( \"Step Id\" ) table . add_column ( \"Module type\" , style = \"i\" ) table . add_column ( \"Description\" ) for nr , stage in enumerate ( self . processing_stages ): for i , step_id in enumerate ( stage ): step : StepDesc = self . steps [ step_id ] if step . required : title = f \"[b] { step_id } [/b]\" else : title = f \"[b] { step_id } [/b] [i](optional)[/i]\" if hasattr ( step . step . module , \"instance_doc\" ): doc = step . step . module . module_instance_doc else : doc = step . step . module . get_type_metadata () . documentation . full_doc row : typing . List [ typing . Any ] = [] if i == 0 : row . append ( str ( nr )) else : row . append ( \"\" ) row . append ( title ) # TODO; generate the right link here module_link = ( step . step . module . get_type_metadata () . context . references . get ( \"sources\" , None ) ) if module_link : module_str = f \"[link= { module_link } ] { step . step . module_type } [/link]\" else : module_str = step . step . module_type row . append ( module_str ) if doc and doc != \"-- n/a --\" : m = Markdown ( doc + \" \\n\\n --- \\n \" ) row . append ( m ) else : row . append ( \"\" ) table . add_row ( * row ) return table # yield Panel(table, title_align=\"left\", title=\"Processing stages\") # def __rich_console_old__( # self, console: Console, options: ConsoleOptions # ) -> RenderResult: # # explanation = {} # # for nr, stage in enumerate(self.processing_stages): # # stage_details = {} # for step_id in stage: # step: StepDesc = self.steps[step_id] # if step.required: # title = step_id # else: # title = f\"{step_id} (optional)\" # stage_details[title] = step.step.module.get_type_metadata().model_doc() # # explanation[nr + 1] = stage_details # # lines = [] # for stage_nr, stage_steps in explanation.items(): # lines.append(f\"[bold]Processing stage {stage_nr}[/bold]:\") # lines.append(\"\") # for step_id, desc in stage_steps.items(): # if desc == DEFAULT_NO_DESC_VALUE: # lines.append(f\" - {step_id}\") # else: # lines.append(f\" - {step_id}: [i]{desc}[/i]\") # lines.append(\"\") # # padding = (1, 2, 0, 2) # yield Panel( # \"\\n\".join(lines), # box=box.ROUNDED, # title_align=\"left\", # title=f\"Stages for pipeline: [b]{self.pipeline_id}[/b]\", # padding=padding, # )","title":"StepsInfo"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.StepsInfo.processing_stages","text":"The stages in which the steps are processed.","title":"processing_stages"},{"location":"reference/kiara/info/pipelines/#kiara.info.pipelines.StepsInfo.steps","text":"A list of step details.","title":"steps"},{"location":"reference/kiara/info/types/","text":"ValueTypeInfo ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/info/types.py class ValueTypeInfo ( KiaraInfoModel ): @classmethod def from_type_class ( cls , type_cls : typing . Type [ \"ValueType\" ], kiara : typing . Optional [ Kiara ] = None ): type_attrs = cls . extract_type_attributes ( type_cls = type_cls , kiara = kiara ) return cls ( ** type_attrs ) @classmethod def extract_type_attributes ( self , type_cls : typing . Type [ \"ValueType\" ], kiara : typing . Optional [ Kiara ] = None ) -> typing . Dict [ str , typing . Any ]: if kiara is None : kiara = Kiara . instance () origin_md = OriginMetadataModel . from_class ( type_cls ) doc = DocumentationMetadataModel . from_class_doc ( type_cls ) python_class = PythonClassMetadata . from_class ( type_cls ) properties_md = ContextMetadataModel . from_class ( type_cls ) value_type : str = type_cls . _value_type_name # type: ignore config = ValueTypeConfigMetadata . from_config_class ( type_cls . type_config_cls ()) metadata_keys = kiara . metadata_mgmt . get_metadata_keys_for_type ( value_type = value_type ) metadata_models : typing . Dict [ str , typing . Type [ MetadataModel ]] = {} for metadata_key in metadata_keys : schema = kiara . metadata_mgmt . all_schemas . get ( metadata_key ) if schema is not None : metadata_models [ metadata_key ] = MetadataModelMetadata . from_model_class ( schema ) return { \"type_name\" : type_cls . _value_type_name , # type: ignore \"documentation\" : doc , \"origin\" : origin_md , \"context\" : properties_md , \"python_class\" : python_class , \"config\" : config , \"metadata_types\" : metadata_models , } type_name : str = Field ( description = \"The name under which the type is registered.\" ) documentation : DocumentationMetadataModel = Field ( description = \"The documentation for this value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about authorship for the value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type (description, tags, labels, references, ...).\" ) python_class : PythonClassMetadata = Field ( description = \"Information about the Python class for this value type.\" ) config : ValueTypeConfigMetadata = Field ( description = \"Details on how this value type can be configured.\" ) metadata_types : typing . Dict [ str , MetadataModelMetadata ] = Field ( description = \"The available metadata types for this value type.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_config_schema = config . get ( \"include_config_schema\" , True ) include_doc = config . get ( \"include_doc\" , True ) include_full_metadata = config . get ( \"include_full_metadata\" , True ) table = Table ( box = box . SIMPLE , show_header = False , padding = ( 0 , 0 , 0 , 0 )) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) if include_config_schema : if config : config_cls = self . config . python_class . get_class () table . add_row ( \"Type config\" , create_table_from_base_model ( config_cls )) else : table . add_row ( \"Type config\" , \"-- no config --\" ) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) if include_full_metadata and self . metadata_types : md_table = Table ( show_header = False , box = box . SIMPLE ) for key , md in self . metadata_types . items (): fields_table = md . create_fields_table ( show_header = False , show_required = False ) md_table . add_row ( f \"[i] { key } [/i]\" , fields_table ) table . add_row ( \"Type metadata\" , md_table ) return table config : ValueTypeConfigMetadata pydantic-field required \u00b6 Details on how this value type can be configured. context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type (description, tags, labels, references, ...). documentation : DocumentationMetadataModel pydantic-field required \u00b6 The documentation for this value type. metadata_types : Dict [ str , kiara . metadata . core_models . MetadataModelMetadata ] pydantic-field required \u00b6 The available metadata types for this value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about authorship for the value type. python_class : PythonClassMetadata pydantic-field required \u00b6 Information about the Python class for this value type. type_name : str pydantic-field required \u00b6 The name under which the type is registered.","title":"types"},{"location":"reference/kiara/info/types/#kiara.info.types.ValueTypeInfo","text":"Source code in kiara/info/types.py class ValueTypeInfo ( KiaraInfoModel ): @classmethod def from_type_class ( cls , type_cls : typing . Type [ \"ValueType\" ], kiara : typing . Optional [ Kiara ] = None ): type_attrs = cls . extract_type_attributes ( type_cls = type_cls , kiara = kiara ) return cls ( ** type_attrs ) @classmethod def extract_type_attributes ( self , type_cls : typing . Type [ \"ValueType\" ], kiara : typing . Optional [ Kiara ] = None ) -> typing . Dict [ str , typing . Any ]: if kiara is None : kiara = Kiara . instance () origin_md = OriginMetadataModel . from_class ( type_cls ) doc = DocumentationMetadataModel . from_class_doc ( type_cls ) python_class = PythonClassMetadata . from_class ( type_cls ) properties_md = ContextMetadataModel . from_class ( type_cls ) value_type : str = type_cls . _value_type_name # type: ignore config = ValueTypeConfigMetadata . from_config_class ( type_cls . type_config_cls ()) metadata_keys = kiara . metadata_mgmt . get_metadata_keys_for_type ( value_type = value_type ) metadata_models : typing . Dict [ str , typing . Type [ MetadataModel ]] = {} for metadata_key in metadata_keys : schema = kiara . metadata_mgmt . all_schemas . get ( metadata_key ) if schema is not None : metadata_models [ metadata_key ] = MetadataModelMetadata . from_model_class ( schema ) return { \"type_name\" : type_cls . _value_type_name , # type: ignore \"documentation\" : doc , \"origin\" : origin_md , \"context\" : properties_md , \"python_class\" : python_class , \"config\" : config , \"metadata_types\" : metadata_models , } type_name : str = Field ( description = \"The name under which the type is registered.\" ) documentation : DocumentationMetadataModel = Field ( description = \"The documentation for this value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about authorship for the value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type (description, tags, labels, references, ...).\" ) python_class : PythonClassMetadata = Field ( description = \"Information about the Python class for this value type.\" ) config : ValueTypeConfigMetadata = Field ( description = \"Details on how this value type can be configured.\" ) metadata_types : typing . Dict [ str , MetadataModelMetadata ] = Field ( description = \"The available metadata types for this value type.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_config_schema = config . get ( \"include_config_schema\" , True ) include_doc = config . get ( \"include_doc\" , True ) include_full_metadata = config . get ( \"include_full_metadata\" , True ) table = Table ( box = box . SIMPLE , show_header = False , padding = ( 0 , 0 , 0 , 0 )) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) if include_config_schema : if config : config_cls = self . config . python_class . get_class () table . add_row ( \"Type config\" , create_table_from_base_model ( config_cls )) else : table . add_row ( \"Type config\" , \"-- no config --\" ) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) if include_full_metadata and self . metadata_types : md_table = Table ( show_header = False , box = box . SIMPLE ) for key , md in self . metadata_types . items (): fields_table = md . create_fields_table ( show_header = False , show_required = False ) md_table . add_row ( f \"[i] { key } [/i]\" , fields_table ) table . add_row ( \"Type metadata\" , md_table ) return table","title":"ValueTypeInfo"},{"location":"reference/kiara/info/types/#kiara.info.types.ValueTypeInfo.config","text":"Details on how this value type can be configured.","title":"config"},{"location":"reference/kiara/info/types/#kiara.info.types.ValueTypeInfo.context","text":"Generic properties of this value type (description, tags, labels, references, ...).","title":"context"},{"location":"reference/kiara/info/types/#kiara.info.types.ValueTypeInfo.documentation","text":"The documentation for this value type.","title":"documentation"},{"location":"reference/kiara/info/types/#kiara.info.types.ValueTypeInfo.metadata_types","text":"The available metadata types for this value type.","title":"metadata_types"},{"location":"reference/kiara/info/types/#kiara.info.types.ValueTypeInfo.origin","text":"Information about authorship for the value type.","title":"origin"},{"location":"reference/kiara/info/types/#kiara.info.types.ValueTypeInfo.python_class","text":"Information about the Python class for this value type.","title":"python_class"},{"location":"reference/kiara/info/types/#kiara.info.types.ValueTypeInfo.type_name","text":"The name under which the type is registered.","title":"type_name"},{"location":"reference/kiara/interfaces/__init__/","text":"Implementation of interfaces for Kiara . get_console () \u00b6 Get a global Console instance. Returns: Type Description Console A console instance. Source code in kiara/interfaces/__init__.py def get_console () -> Console : \"\"\"Get a global Console instance. Returns: Console: A console instance. \"\"\" global _console if _console is None or True : console_width = os . environ . get ( \"CONSOLE_WIDTH\" , None ) width = None if console_width : try : width = int ( console_width ) except Exception : pass _console = Console ( width = width ) return _console cli special \u00b6 A command-line interface for Kiara . data special \u00b6 commands \u00b6 Data-related sub-commands for the cli. explain \u00b6 The 'run' subcommand for the cli. KiaraEntityMatches ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/interfaces/cli/explain.py class KiaraEntityMatches ( KiaraInfoModel ): @classmethod def search ( self , search_term : str , search_ids : bool = True , search_aliases : bool = True , search_desc : bool = True , search_full_doc : bool = False , kiara : typing . Optional [ Kiara ] = None , ): if kiara is None : kiara = Kiara . instance () module_types = find_module_types ( item = search_term , search_desc = search_desc , search_full_doc = search_full_doc , kiara = kiara , ) operation_types = find_operation_types ( item = search_term , kiara = kiara ) operations = find_operations ( item = search_term , search_desc = search_desc , search_full_doc = search_full_doc , kiara = kiara , ) values = find_values ( item = search_term , search_ids = search_ids , search_aliases = search_aliases , kiara = kiara , ) return KiaraEntityMatches ( module_types = module_types , operation_types = operation_types , operations = operations , values = values , kiara = kiara , ) def __init__ ( self , module_types : typing . Optional [ typing . Dict [ str , typing . Type [ KiaraModule ]] ] = None , operation_types : typing . Optional [ typing . Dict [ str , OperationsMetadata ]] = None , operations : typing . Optional [ typing . Dict [ str , Operation ]] = None , values : typing . Optional [ typing . Dict [ str , Value ]] = None , kiara : Kiara = None , ): if kiara is None : kiara = Kiara . instance () init_dict : typing . Dict [ str , typing . Any ] = {} if module_types : init_dict [ \"module_types\" ] = module_types if operation_types : init_dict [ \"operation_types\" ] = operation_types if operations : init_dict [ \"operations\" ] = operations if values : init_dict [ \"values\" ] = values super () . __init__ ( ** init_dict ) self . _kiara = kiara _kiara : Kiara = PrivateAttr () module_types : typing . Dict [ str , KiaraModuleTypeMetadata ] = Field ( description = \"Matching module types.\" , default_factory = dict ) operation_types : typing . Dict [ str , OperationsMetadata ] = Field ( description = \"Matching operation types.\" , default_factory = dict ) operations : typing . Dict [ str , Operation ] = Field ( description = \"Matching operations.\" , default_factory = dict ) values : typing . Dict [ str , ValueInfo ] = Field ( description = \"Matching values.\" , default_factory = dict ) @property def no_module_types ( self ) -> int : return len ( self . module_types ) @property def no_operation_types ( self ) -> int : return len ( self . operation_types ) @property def no_operations ( self ) -> int : return len ( self . operations ) @property def no_values ( self ) -> int : return len ( self . values ) @property def no_results ( self ) -> int : return ( self . no_module_types + self . no_operation_types + self . no_operations + self . no_values ) def get_single_result ( self , ) -> typing . Optional [ typing . Tuple [ typing . Type , str , typing . Any ]]: if self . no_results != 1 : return None if self . module_types : match_key = next ( iter ( self . module_types . keys ())) match_value : typing . Any = self . module_types [ match_key ] return ( KiaraModuleTypeMetadata , match_key , match_value ) if self . operation_types : match_key = next ( iter ( self . operation_types . keys ())) match_value = self . operation_types [ match_key ] return ( OperationsMetadata , match_key , match_value ) if self . operations : match_key = next ( iter ( self . operations . keys ())) match_value = self . operations [ match_key ] return ( Operation , match_key , match_value ) if self . values : match_key = next ( iter ( self . values . keys ())) match_value = self . values [ match_key ] return ( ValueInfo , match_key , match_value ) raise Exception ( \"No match found. This is a bug.\" ) def merge ( self , other : \"KiaraEntityMatches\" ) -> \"KiaraEntityMatches\" : module_types = copy . copy ( self . module_types ) module_types . update ( other . module_types ) operation_types = copy . copy ( self . operation_types ) operation_types . update ( other . operation_types ) operations = copy . copy ( self . operations ) operations . update ( other . operations ) values = copy . copy ( self . values ) values . update ( other . values ) return KiaraEntityMatches ( module_types = module_types , operation_types = operation_types , operations = operations , values = values , kiara = self . _kiara , ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" ) table . add_column ( \"Description\" ) empty_line = False if self . module_types : table . add_row () table . add_row ( \"[b]Module types[/b]\" , \"\" ) table . add_row () for module_type , module_type_info in self . module_types . items (): table . add_row ( f \" [i] { module_type } [/i]\" , module_type_info . documentation . description , ) empty_line = True if self . operation_types : if empty_line : table . add_row () table . add_row ( \"[b]Operation types[/b]\" , \"\" ) table . add_row () for op_type , op_type_info in self . operation_types . items (): table . add_row ( f \" [i] { op_type } [/i]\" , op_type_info . documentation . description ) empty_line = True if self . operations : if empty_line : table . add_row () table . add_row ( \"[b]Operations[/b]\" , \"\" ) table . add_row () for op_id , op_info in self . operations . items (): table . add_row ( f \" [i] { op_id } [/i]\" , op_info . doc . description ) empty_line = True if self . values : if empty_line : table . add_row () table . add_row ( \"[b]Values[/b]\" , \"\" ) table . add_row () for value_id , value in self . values . items (): table . add_row ( f \" [i] { value_id } [/i]\" , value . value_schema . type ) return table module_types : Dict [ str , kiara . metadata . module_models . KiaraModuleTypeMetadata ] pydantic-field \u00b6 Matching module types. operation_types : Dict [ str , kiara . metadata . operation_models . OperationsMetadata ] pydantic-field \u00b6 Matching operation types. operations : Dict [ str , kiara . operations . Operation ] pydantic-field \u00b6 Matching operations. values : Dict [ str , kiara . data . values . ValueInfo ] pydantic-field \u00b6 Matching values. metadata special \u00b6 Metadata-related sub-commands for the cli. module special \u00b6 commands \u00b6 Module related subcommands for the cli. pipeline special \u00b6 commands \u00b6 Pipeline-related subcommands for the cli. run \u00b6 The 'run' subcommand for the cli. type special \u00b6 commands \u00b6 Type-related subcommands for the cli. utils \u00b6 Shared utilties to be used by cli sub-commands. python_api special \u00b6 A Python API for creating workflow sessions and dynamic pipelines in kiara . StepInfo ( JupyterMixin , BaseModel ) pydantic-model \u00b6 Source code in kiara/interfaces/python_api/__init__.py class StepInfo ( JupyterMixin , BaseModel ): step_id : str = Field ( description = \"The step id.\" ) module_metadata : KiaraModuleInstanceMetadata = Field ( description = \"The module metadata.\" ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : doc = self . module_metadata . type_metadata . documentation . create_renderable () metadata = self . module_metadata . create_renderable ( include_doc = False ) panel = Panel ( RenderGroup ( \"\" , Panel ( doc , box = box . SIMPLE ), \"\" , metadata ), title = f \"Step info: [b] { self . step_id } [/b] (type: [i] { self . module_metadata . type_metadata . type_id } [/i])\" , title_align = \"left\" , ) yield panel module_metadata : KiaraModuleInstanceMetadata pydantic-field required \u00b6 The module metadata. step_id : str pydantic-field required \u00b6 The step id. controller \u00b6 ApiController ( PipelineController ) \u00b6 A pipeline controller for pipelines generated by the kiara Python API. Source code in kiara/interfaces/python_api/controller.py class ApiController ( PipelineController ): \"\"\"A pipeline controller for pipelines generated by the *kiara* Python API.\"\"\" def __init__ ( self , pipeline : typing . Optional [ Pipeline ] = None , processor : typing . Optional [ ModuleProcessor ] = None , ): self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor ) def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : # from kiara.utils.output import rich_print # rich_print(self.pipeline.get_current_state()) raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False def step_inputs_changed ( self , event : StepInputEvent ): for step_id in event . updated_step_inputs . keys (): # outputs = self.get_step_outputs(step_id) raise NotImplementedError () # outputs.invalidate() def pipeline_inputs_changed ( self , event : PipelineInputEvent ): if self . _is_running : raise NotImplementedError () def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False def get_pipeline_input ( self , step_id : str , field_name : str ) -> Value : self . ensure_step ( step_id ) outputs = self . get_step_inputs ( step_id ) return outputs . get_value_obj ( field_name = field_name ) def get_pipeline_output ( self , step_id : str , field_name : str ) -> Value : self . ensure_step ( step_id ) outputs = self . get_step_outputs ( step_id ) return outputs . get_value_obj ( field_name = field_name ) def _process ( self , step_id : str ) -> typing . Union [ None , bool , str ]: \"\"\"Process the specified step. Returns: 'None', if no processing was needed, but the step output is valid, 'False' if it's not possible to process the step, or a string which represents the job id of the processing \"\"\" if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): return None else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) return job_id else : return None except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False def ensure_step ( self , step_id : str ) -> bool : \"\"\"Ensure a step has valid outputs. Returns: 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs \"\"\" if step_id not in self . pipeline . step_ids : raise Exception ( f \"No step with id: { step_id } \" ) outputs = self . get_step_outputs ( step_id = step_id ) if outputs . items_are_valid (): return True if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids : typing . List [ str ] = [] finished = False if step_id in stage : finished = True job_id = self . _process ( step_id = step_id ) if job_id is None : return True if not job_id : return False job_ids . append ( job_id ) # type: ignore else : for s_id in stage : job_id = self . _process ( step_id = s_id ) if job_id is None : continue elif job_id is False : return False job_ids . append ( job_id ) # type: ignore self . _processor . wait_for ( * job_ids ) if finished : break finally : self . _is_running = False return True ensure_step ( self , step_id ) \u00b6 Ensure a step has valid outputs. Returns: Type Description bool 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs Source code in kiara/interfaces/python_api/controller.py def ensure_step ( self , step_id : str ) -> bool : \"\"\"Ensure a step has valid outputs. Returns: 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs \"\"\" if step_id not in self . pipeline . step_ids : raise Exception ( f \"No step with id: { step_id } \" ) outputs = self . get_step_outputs ( step_id = step_id ) if outputs . items_are_valid (): return True if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids : typing . List [ str ] = [] finished = False if step_id in stage : finished = True job_id = self . _process ( step_id = step_id ) if job_id is None : return True if not job_id : return False job_ids . append ( job_id ) # type: ignore else : for s_id in stage : job_id = self . _process ( step_id = s_id ) if job_id is None : continue elif job_id is False : return False job_ids . append ( job_id ) # type: ignore self . _processor . wait_for ( * job_ids ) if finished : break finally : self . _is_running = False return True pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/interfaces/python_api/controller.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): if self . _is_running : raise NotImplementedError () pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/interfaces/python_api/controller.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/interfaces/python_api/controller.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : # from kiara.utils.output import rich_print # rich_print(self.pipeline.get_current_state()) raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/interfaces/python_api/controller.py def step_inputs_changed ( self , event : StepInputEvent ): for step_id in event . updated_step_inputs . keys (): # outputs = self.get_step_outputs(step_id) raise NotImplementedError () # outputs.invalidate()","title":"interfaces"},{"location":"reference/kiara/interfaces/__init__/#kiara.interfaces.get_console","text":"Get a global Console instance. Returns: Type Description Console A console instance. Source code in kiara/interfaces/__init__.py def get_console () -> Console : \"\"\"Get a global Console instance. Returns: Console: A console instance. \"\"\" global _console if _console is None or True : console_width = os . environ . get ( \"CONSOLE_WIDTH\" , None ) width = None if console_width : try : width = int ( console_width ) except Exception : pass _console = Console ( width = width ) return _console","title":"get_console()"},{"location":"reference/kiara/interfaces/__init__/#kiara.interfaces.cli","text":"A command-line interface for Kiara .","title":"cli"},{"location":"reference/kiara/interfaces/__init__/#kiara.interfaces.cli.data","text":"","title":"data"},{"location":"reference/kiara/interfaces/__init__/#kiara.interfaces.cli.explain","text":"The 'run' subcommand for the cli.","title":"explain"},{"location":"reference/kiara/interfaces/__init__/#kiara.interfaces.cli.metadata","text":"Metadata-related sub-commands for the cli.","title":"metadata"},{"location":"reference/kiara/interfaces/__init__/#kiara.interfaces.cli.module","text":"","title":"module"},{"location":"reference/kiara/interfaces/__init__/#kiara.interfaces.cli.pipeline","text":"","title":"pipeline"},{"location":"reference/kiara/interfaces/__init__/#kiara.interfaces.cli.run","text":"The 'run' subcommand for the cli.","title":"run"},{"location":"reference/kiara/interfaces/__init__/#kiara.interfaces.cli.type","text":"","title":"type"},{"location":"reference/kiara/interfaces/__init__/#kiara.interfaces.cli.utils","text":"Shared utilties to be used by cli sub-commands.","title":"utils"},{"location":"reference/kiara/interfaces/__init__/#kiara.interfaces.python_api","text":"A Python API for creating workflow sessions and dynamic pipelines in kiara .","title":"python_api"},{"location":"reference/kiara/interfaces/__init__/#kiara.interfaces.python_api.StepInfo","text":"Source code in kiara/interfaces/python_api/__init__.py class StepInfo ( JupyterMixin , BaseModel ): step_id : str = Field ( description = \"The step id.\" ) module_metadata : KiaraModuleInstanceMetadata = Field ( description = \"The module metadata.\" ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : doc = self . module_metadata . type_metadata . documentation . create_renderable () metadata = self . module_metadata . create_renderable ( include_doc = False ) panel = Panel ( RenderGroup ( \"\" , Panel ( doc , box = box . SIMPLE ), \"\" , metadata ), title = f \"Step info: [b] { self . step_id } [/b] (type: [i] { self . module_metadata . type_metadata . type_id } [/i])\" , title_align = \"left\" , ) yield panel","title":"StepInfo"},{"location":"reference/kiara/interfaces/__init__/#kiara.interfaces.python_api.controller","text":"","title":"controller"},{"location":"reference/kiara/interfaces/cli/__init__/","text":"A command-line interface for Kiara . data special \u00b6 commands \u00b6 Data-related sub-commands for the cli. explain \u00b6 The 'run' subcommand for the cli. KiaraEntityMatches ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/interfaces/cli/explain.py class KiaraEntityMatches ( KiaraInfoModel ): @classmethod def search ( self , search_term : str , search_ids : bool = True , search_aliases : bool = True , search_desc : bool = True , search_full_doc : bool = False , kiara : typing . Optional [ Kiara ] = None , ): if kiara is None : kiara = Kiara . instance () module_types = find_module_types ( item = search_term , search_desc = search_desc , search_full_doc = search_full_doc , kiara = kiara , ) operation_types = find_operation_types ( item = search_term , kiara = kiara ) operations = find_operations ( item = search_term , search_desc = search_desc , search_full_doc = search_full_doc , kiara = kiara , ) values = find_values ( item = search_term , search_ids = search_ids , search_aliases = search_aliases , kiara = kiara , ) return KiaraEntityMatches ( module_types = module_types , operation_types = operation_types , operations = operations , values = values , kiara = kiara , ) def __init__ ( self , module_types : typing . Optional [ typing . Dict [ str , typing . Type [ KiaraModule ]] ] = None , operation_types : typing . Optional [ typing . Dict [ str , OperationsMetadata ]] = None , operations : typing . Optional [ typing . Dict [ str , Operation ]] = None , values : typing . Optional [ typing . Dict [ str , Value ]] = None , kiara : Kiara = None , ): if kiara is None : kiara = Kiara . instance () init_dict : typing . Dict [ str , typing . Any ] = {} if module_types : init_dict [ \"module_types\" ] = module_types if operation_types : init_dict [ \"operation_types\" ] = operation_types if operations : init_dict [ \"operations\" ] = operations if values : init_dict [ \"values\" ] = values super () . __init__ ( ** init_dict ) self . _kiara = kiara _kiara : Kiara = PrivateAttr () module_types : typing . Dict [ str , KiaraModuleTypeMetadata ] = Field ( description = \"Matching module types.\" , default_factory = dict ) operation_types : typing . Dict [ str , OperationsMetadata ] = Field ( description = \"Matching operation types.\" , default_factory = dict ) operations : typing . Dict [ str , Operation ] = Field ( description = \"Matching operations.\" , default_factory = dict ) values : typing . Dict [ str , ValueInfo ] = Field ( description = \"Matching values.\" , default_factory = dict ) @property def no_module_types ( self ) -> int : return len ( self . module_types ) @property def no_operation_types ( self ) -> int : return len ( self . operation_types ) @property def no_operations ( self ) -> int : return len ( self . operations ) @property def no_values ( self ) -> int : return len ( self . values ) @property def no_results ( self ) -> int : return ( self . no_module_types + self . no_operation_types + self . no_operations + self . no_values ) def get_single_result ( self , ) -> typing . Optional [ typing . Tuple [ typing . Type , str , typing . Any ]]: if self . no_results != 1 : return None if self . module_types : match_key = next ( iter ( self . module_types . keys ())) match_value : typing . Any = self . module_types [ match_key ] return ( KiaraModuleTypeMetadata , match_key , match_value ) if self . operation_types : match_key = next ( iter ( self . operation_types . keys ())) match_value = self . operation_types [ match_key ] return ( OperationsMetadata , match_key , match_value ) if self . operations : match_key = next ( iter ( self . operations . keys ())) match_value = self . operations [ match_key ] return ( Operation , match_key , match_value ) if self . values : match_key = next ( iter ( self . values . keys ())) match_value = self . values [ match_key ] return ( ValueInfo , match_key , match_value ) raise Exception ( \"No match found. This is a bug.\" ) def merge ( self , other : \"KiaraEntityMatches\" ) -> \"KiaraEntityMatches\" : module_types = copy . copy ( self . module_types ) module_types . update ( other . module_types ) operation_types = copy . copy ( self . operation_types ) operation_types . update ( other . operation_types ) operations = copy . copy ( self . operations ) operations . update ( other . operations ) values = copy . copy ( self . values ) values . update ( other . values ) return KiaraEntityMatches ( module_types = module_types , operation_types = operation_types , operations = operations , values = values , kiara = self . _kiara , ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" ) table . add_column ( \"Description\" ) empty_line = False if self . module_types : table . add_row () table . add_row ( \"[b]Module types[/b]\" , \"\" ) table . add_row () for module_type , module_type_info in self . module_types . items (): table . add_row ( f \" [i] { module_type } [/i]\" , module_type_info . documentation . description , ) empty_line = True if self . operation_types : if empty_line : table . add_row () table . add_row ( \"[b]Operation types[/b]\" , \"\" ) table . add_row () for op_type , op_type_info in self . operation_types . items (): table . add_row ( f \" [i] { op_type } [/i]\" , op_type_info . documentation . description ) empty_line = True if self . operations : if empty_line : table . add_row () table . add_row ( \"[b]Operations[/b]\" , \"\" ) table . add_row () for op_id , op_info in self . operations . items (): table . add_row ( f \" [i] { op_id } [/i]\" , op_info . doc . description ) empty_line = True if self . values : if empty_line : table . add_row () table . add_row ( \"[b]Values[/b]\" , \"\" ) table . add_row () for value_id , value in self . values . items (): table . add_row ( f \" [i] { value_id } [/i]\" , value . value_schema . type ) return table module_types : Dict [ str , kiara . metadata . module_models . KiaraModuleTypeMetadata ] pydantic-field \u00b6 Matching module types. operation_types : Dict [ str , kiara . metadata . operation_models . OperationsMetadata ] pydantic-field \u00b6 Matching operation types. operations : Dict [ str , kiara . operations . Operation ] pydantic-field \u00b6 Matching operations. values : Dict [ str , kiara . data . values . ValueInfo ] pydantic-field \u00b6 Matching values. metadata special \u00b6 Metadata-related sub-commands for the cli. module special \u00b6 commands \u00b6 Module related subcommands for the cli. pipeline special \u00b6 commands \u00b6 Pipeline-related subcommands for the cli. run \u00b6 The 'run' subcommand for the cli. type special \u00b6 commands \u00b6 Type-related subcommands for the cli. utils \u00b6 Shared utilties to be used by cli sub-commands.","title":"cli"},{"location":"reference/kiara/interfaces/cli/__init__/#kiara.interfaces.cli.data","text":"","title":"data"},{"location":"reference/kiara/interfaces/cli/__init__/#kiara.interfaces.cli.data.commands","text":"Data-related sub-commands for the cli.","title":"commands"},{"location":"reference/kiara/interfaces/cli/__init__/#kiara.interfaces.cli.explain","text":"The 'run' subcommand for the cli.","title":"explain"},{"location":"reference/kiara/interfaces/cli/__init__/#kiara.interfaces.cli.explain.KiaraEntityMatches","text":"Source code in kiara/interfaces/cli/explain.py class KiaraEntityMatches ( KiaraInfoModel ): @classmethod def search ( self , search_term : str , search_ids : bool = True , search_aliases : bool = True , search_desc : bool = True , search_full_doc : bool = False , kiara : typing . Optional [ Kiara ] = None , ): if kiara is None : kiara = Kiara . instance () module_types = find_module_types ( item = search_term , search_desc = search_desc , search_full_doc = search_full_doc , kiara = kiara , ) operation_types = find_operation_types ( item = search_term , kiara = kiara ) operations = find_operations ( item = search_term , search_desc = search_desc , search_full_doc = search_full_doc , kiara = kiara , ) values = find_values ( item = search_term , search_ids = search_ids , search_aliases = search_aliases , kiara = kiara , ) return KiaraEntityMatches ( module_types = module_types , operation_types = operation_types , operations = operations , values = values , kiara = kiara , ) def __init__ ( self , module_types : typing . Optional [ typing . Dict [ str , typing . Type [ KiaraModule ]] ] = None , operation_types : typing . Optional [ typing . Dict [ str , OperationsMetadata ]] = None , operations : typing . Optional [ typing . Dict [ str , Operation ]] = None , values : typing . Optional [ typing . Dict [ str , Value ]] = None , kiara : Kiara = None , ): if kiara is None : kiara = Kiara . instance () init_dict : typing . Dict [ str , typing . Any ] = {} if module_types : init_dict [ \"module_types\" ] = module_types if operation_types : init_dict [ \"operation_types\" ] = operation_types if operations : init_dict [ \"operations\" ] = operations if values : init_dict [ \"values\" ] = values super () . __init__ ( ** init_dict ) self . _kiara = kiara _kiara : Kiara = PrivateAttr () module_types : typing . Dict [ str , KiaraModuleTypeMetadata ] = Field ( description = \"Matching module types.\" , default_factory = dict ) operation_types : typing . Dict [ str , OperationsMetadata ] = Field ( description = \"Matching operation types.\" , default_factory = dict ) operations : typing . Dict [ str , Operation ] = Field ( description = \"Matching operations.\" , default_factory = dict ) values : typing . Dict [ str , ValueInfo ] = Field ( description = \"Matching values.\" , default_factory = dict ) @property def no_module_types ( self ) -> int : return len ( self . module_types ) @property def no_operation_types ( self ) -> int : return len ( self . operation_types ) @property def no_operations ( self ) -> int : return len ( self . operations ) @property def no_values ( self ) -> int : return len ( self . values ) @property def no_results ( self ) -> int : return ( self . no_module_types + self . no_operation_types + self . no_operations + self . no_values ) def get_single_result ( self , ) -> typing . Optional [ typing . Tuple [ typing . Type , str , typing . Any ]]: if self . no_results != 1 : return None if self . module_types : match_key = next ( iter ( self . module_types . keys ())) match_value : typing . Any = self . module_types [ match_key ] return ( KiaraModuleTypeMetadata , match_key , match_value ) if self . operation_types : match_key = next ( iter ( self . operation_types . keys ())) match_value = self . operation_types [ match_key ] return ( OperationsMetadata , match_key , match_value ) if self . operations : match_key = next ( iter ( self . operations . keys ())) match_value = self . operations [ match_key ] return ( Operation , match_key , match_value ) if self . values : match_key = next ( iter ( self . values . keys ())) match_value = self . values [ match_key ] return ( ValueInfo , match_key , match_value ) raise Exception ( \"No match found. This is a bug.\" ) def merge ( self , other : \"KiaraEntityMatches\" ) -> \"KiaraEntityMatches\" : module_types = copy . copy ( self . module_types ) module_types . update ( other . module_types ) operation_types = copy . copy ( self . operation_types ) operation_types . update ( other . operation_types ) operations = copy . copy ( self . operations ) operations . update ( other . operations ) values = copy . copy ( self . values ) values . update ( other . values ) return KiaraEntityMatches ( module_types = module_types , operation_types = operation_types , operations = operations , values = values , kiara = self . _kiara , ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" ) table . add_column ( \"Description\" ) empty_line = False if self . module_types : table . add_row () table . add_row ( \"[b]Module types[/b]\" , \"\" ) table . add_row () for module_type , module_type_info in self . module_types . items (): table . add_row ( f \" [i] { module_type } [/i]\" , module_type_info . documentation . description , ) empty_line = True if self . operation_types : if empty_line : table . add_row () table . add_row ( \"[b]Operation types[/b]\" , \"\" ) table . add_row () for op_type , op_type_info in self . operation_types . items (): table . add_row ( f \" [i] { op_type } [/i]\" , op_type_info . documentation . description ) empty_line = True if self . operations : if empty_line : table . add_row () table . add_row ( \"[b]Operations[/b]\" , \"\" ) table . add_row () for op_id , op_info in self . operations . items (): table . add_row ( f \" [i] { op_id } [/i]\" , op_info . doc . description ) empty_line = True if self . values : if empty_line : table . add_row () table . add_row ( \"[b]Values[/b]\" , \"\" ) table . add_row () for value_id , value in self . values . items (): table . add_row ( f \" [i] { value_id } [/i]\" , value . value_schema . type ) return table","title":"KiaraEntityMatches"},{"location":"reference/kiara/interfaces/cli/__init__/#kiara.interfaces.cli.metadata","text":"Metadata-related sub-commands for the cli.","title":"metadata"},{"location":"reference/kiara/interfaces/cli/__init__/#kiara.interfaces.cli.module","text":"","title":"module"},{"location":"reference/kiara/interfaces/cli/__init__/#kiara.interfaces.cli.module.commands","text":"Module related subcommands for the cli.","title":"commands"},{"location":"reference/kiara/interfaces/cli/__init__/#kiara.interfaces.cli.pipeline","text":"","title":"pipeline"},{"location":"reference/kiara/interfaces/cli/__init__/#kiara.interfaces.cli.pipeline.commands","text":"Pipeline-related subcommands for the cli.","title":"commands"},{"location":"reference/kiara/interfaces/cli/__init__/#kiara.interfaces.cli.run","text":"The 'run' subcommand for the cli.","title":"run"},{"location":"reference/kiara/interfaces/cli/__init__/#kiara.interfaces.cli.type","text":"","title":"type"},{"location":"reference/kiara/interfaces/cli/__init__/#kiara.interfaces.cli.type.commands","text":"Type-related subcommands for the cli.","title":"commands"},{"location":"reference/kiara/interfaces/cli/__init__/#kiara.interfaces.cli.utils","text":"Shared utilties to be used by cli sub-commands.","title":"utils"},{"location":"reference/kiara/interfaces/cli/explain/","text":"The 'run' subcommand for the cli. KiaraEntityMatches ( KiaraInfoModel ) pydantic-model \u00b6 Source code in kiara/interfaces/cli/explain.py class KiaraEntityMatches ( KiaraInfoModel ): @classmethod def search ( self , search_term : str , search_ids : bool = True , search_aliases : bool = True , search_desc : bool = True , search_full_doc : bool = False , kiara : typing . Optional [ Kiara ] = None , ): if kiara is None : kiara = Kiara . instance () module_types = find_module_types ( item = search_term , search_desc = search_desc , search_full_doc = search_full_doc , kiara = kiara , ) operation_types = find_operation_types ( item = search_term , kiara = kiara ) operations = find_operations ( item = search_term , search_desc = search_desc , search_full_doc = search_full_doc , kiara = kiara , ) values = find_values ( item = search_term , search_ids = search_ids , search_aliases = search_aliases , kiara = kiara , ) return KiaraEntityMatches ( module_types = module_types , operation_types = operation_types , operations = operations , values = values , kiara = kiara , ) def __init__ ( self , module_types : typing . Optional [ typing . Dict [ str , typing . Type [ KiaraModule ]] ] = None , operation_types : typing . Optional [ typing . Dict [ str , OperationsMetadata ]] = None , operations : typing . Optional [ typing . Dict [ str , Operation ]] = None , values : typing . Optional [ typing . Dict [ str , Value ]] = None , kiara : Kiara = None , ): if kiara is None : kiara = Kiara . instance () init_dict : typing . Dict [ str , typing . Any ] = {} if module_types : init_dict [ \"module_types\" ] = module_types if operation_types : init_dict [ \"operation_types\" ] = operation_types if operations : init_dict [ \"operations\" ] = operations if values : init_dict [ \"values\" ] = values super () . __init__ ( ** init_dict ) self . _kiara = kiara _kiara : Kiara = PrivateAttr () module_types : typing . Dict [ str , KiaraModuleTypeMetadata ] = Field ( description = \"Matching module types.\" , default_factory = dict ) operation_types : typing . Dict [ str , OperationsMetadata ] = Field ( description = \"Matching operation types.\" , default_factory = dict ) operations : typing . Dict [ str , Operation ] = Field ( description = \"Matching operations.\" , default_factory = dict ) values : typing . Dict [ str , ValueInfo ] = Field ( description = \"Matching values.\" , default_factory = dict ) @property def no_module_types ( self ) -> int : return len ( self . module_types ) @property def no_operation_types ( self ) -> int : return len ( self . operation_types ) @property def no_operations ( self ) -> int : return len ( self . operations ) @property def no_values ( self ) -> int : return len ( self . values ) @property def no_results ( self ) -> int : return ( self . no_module_types + self . no_operation_types + self . no_operations + self . no_values ) def get_single_result ( self , ) -> typing . Optional [ typing . Tuple [ typing . Type , str , typing . Any ]]: if self . no_results != 1 : return None if self . module_types : match_key = next ( iter ( self . module_types . keys ())) match_value : typing . Any = self . module_types [ match_key ] return ( KiaraModuleTypeMetadata , match_key , match_value ) if self . operation_types : match_key = next ( iter ( self . operation_types . keys ())) match_value = self . operation_types [ match_key ] return ( OperationsMetadata , match_key , match_value ) if self . operations : match_key = next ( iter ( self . operations . keys ())) match_value = self . operations [ match_key ] return ( Operation , match_key , match_value ) if self . values : match_key = next ( iter ( self . values . keys ())) match_value = self . values [ match_key ] return ( ValueInfo , match_key , match_value ) raise Exception ( \"No match found. This is a bug.\" ) def merge ( self , other : \"KiaraEntityMatches\" ) -> \"KiaraEntityMatches\" : module_types = copy . copy ( self . module_types ) module_types . update ( other . module_types ) operation_types = copy . copy ( self . operation_types ) operation_types . update ( other . operation_types ) operations = copy . copy ( self . operations ) operations . update ( other . operations ) values = copy . copy ( self . values ) values . update ( other . values ) return KiaraEntityMatches ( module_types = module_types , operation_types = operation_types , operations = operations , values = values , kiara = self . _kiara , ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" ) table . add_column ( \"Description\" ) empty_line = False if self . module_types : table . add_row () table . add_row ( \"[b]Module types[/b]\" , \"\" ) table . add_row () for module_type , module_type_info in self . module_types . items (): table . add_row ( f \" [i] { module_type } [/i]\" , module_type_info . documentation . description , ) empty_line = True if self . operation_types : if empty_line : table . add_row () table . add_row ( \"[b]Operation types[/b]\" , \"\" ) table . add_row () for op_type , op_type_info in self . operation_types . items (): table . add_row ( f \" [i] { op_type } [/i]\" , op_type_info . documentation . description ) empty_line = True if self . operations : if empty_line : table . add_row () table . add_row ( \"[b]Operations[/b]\" , \"\" ) table . add_row () for op_id , op_info in self . operations . items (): table . add_row ( f \" [i] { op_id } [/i]\" , op_info . doc . description ) empty_line = True if self . values : if empty_line : table . add_row () table . add_row ( \"[b]Values[/b]\" , \"\" ) table . add_row () for value_id , value in self . values . items (): table . add_row ( f \" [i] { value_id } [/i]\" , value . value_schema . type ) return table module_types : Dict [ str , kiara . metadata . module_models . KiaraModuleTypeMetadata ] pydantic-field \u00b6 Matching module types. operation_types : Dict [ str , kiara . metadata . operation_models . OperationsMetadata ] pydantic-field \u00b6 Matching operation types. operations : Dict [ str , kiara . operations . Operation ] pydantic-field \u00b6 Matching operations. values : Dict [ str , kiara . data . values . ValueInfo ] pydantic-field \u00b6 Matching values.","title":"explain"},{"location":"reference/kiara/interfaces/cli/explain/#kiara.interfaces.cli.explain.KiaraEntityMatches","text":"Source code in kiara/interfaces/cli/explain.py class KiaraEntityMatches ( KiaraInfoModel ): @classmethod def search ( self , search_term : str , search_ids : bool = True , search_aliases : bool = True , search_desc : bool = True , search_full_doc : bool = False , kiara : typing . Optional [ Kiara ] = None , ): if kiara is None : kiara = Kiara . instance () module_types = find_module_types ( item = search_term , search_desc = search_desc , search_full_doc = search_full_doc , kiara = kiara , ) operation_types = find_operation_types ( item = search_term , kiara = kiara ) operations = find_operations ( item = search_term , search_desc = search_desc , search_full_doc = search_full_doc , kiara = kiara , ) values = find_values ( item = search_term , search_ids = search_ids , search_aliases = search_aliases , kiara = kiara , ) return KiaraEntityMatches ( module_types = module_types , operation_types = operation_types , operations = operations , values = values , kiara = kiara , ) def __init__ ( self , module_types : typing . Optional [ typing . Dict [ str , typing . Type [ KiaraModule ]] ] = None , operation_types : typing . Optional [ typing . Dict [ str , OperationsMetadata ]] = None , operations : typing . Optional [ typing . Dict [ str , Operation ]] = None , values : typing . Optional [ typing . Dict [ str , Value ]] = None , kiara : Kiara = None , ): if kiara is None : kiara = Kiara . instance () init_dict : typing . Dict [ str , typing . Any ] = {} if module_types : init_dict [ \"module_types\" ] = module_types if operation_types : init_dict [ \"operation_types\" ] = operation_types if operations : init_dict [ \"operations\" ] = operations if values : init_dict [ \"values\" ] = values super () . __init__ ( ** init_dict ) self . _kiara = kiara _kiara : Kiara = PrivateAttr () module_types : typing . Dict [ str , KiaraModuleTypeMetadata ] = Field ( description = \"Matching module types.\" , default_factory = dict ) operation_types : typing . Dict [ str , OperationsMetadata ] = Field ( description = \"Matching operation types.\" , default_factory = dict ) operations : typing . Dict [ str , Operation ] = Field ( description = \"Matching operations.\" , default_factory = dict ) values : typing . Dict [ str , ValueInfo ] = Field ( description = \"Matching values.\" , default_factory = dict ) @property def no_module_types ( self ) -> int : return len ( self . module_types ) @property def no_operation_types ( self ) -> int : return len ( self . operation_types ) @property def no_operations ( self ) -> int : return len ( self . operations ) @property def no_values ( self ) -> int : return len ( self . values ) @property def no_results ( self ) -> int : return ( self . no_module_types + self . no_operation_types + self . no_operations + self . no_values ) def get_single_result ( self , ) -> typing . Optional [ typing . Tuple [ typing . Type , str , typing . Any ]]: if self . no_results != 1 : return None if self . module_types : match_key = next ( iter ( self . module_types . keys ())) match_value : typing . Any = self . module_types [ match_key ] return ( KiaraModuleTypeMetadata , match_key , match_value ) if self . operation_types : match_key = next ( iter ( self . operation_types . keys ())) match_value = self . operation_types [ match_key ] return ( OperationsMetadata , match_key , match_value ) if self . operations : match_key = next ( iter ( self . operations . keys ())) match_value = self . operations [ match_key ] return ( Operation , match_key , match_value ) if self . values : match_key = next ( iter ( self . values . keys ())) match_value = self . values [ match_key ] return ( ValueInfo , match_key , match_value ) raise Exception ( \"No match found. This is a bug.\" ) def merge ( self , other : \"KiaraEntityMatches\" ) -> \"KiaraEntityMatches\" : module_types = copy . copy ( self . module_types ) module_types . update ( other . module_types ) operation_types = copy . copy ( self . operation_types ) operation_types . update ( other . operation_types ) operations = copy . copy ( self . operations ) operations . update ( other . operations ) values = copy . copy ( self . values ) values . update ( other . values ) return KiaraEntityMatches ( module_types = module_types , operation_types = operation_types , operations = operations , values = values , kiara = self . _kiara , ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Id\" ) table . add_column ( \"Description\" ) empty_line = False if self . module_types : table . add_row () table . add_row ( \"[b]Module types[/b]\" , \"\" ) table . add_row () for module_type , module_type_info in self . module_types . items (): table . add_row ( f \" [i] { module_type } [/i]\" , module_type_info . documentation . description , ) empty_line = True if self . operation_types : if empty_line : table . add_row () table . add_row ( \"[b]Operation types[/b]\" , \"\" ) table . add_row () for op_type , op_type_info in self . operation_types . items (): table . add_row ( f \" [i] { op_type } [/i]\" , op_type_info . documentation . description ) empty_line = True if self . operations : if empty_line : table . add_row () table . add_row ( \"[b]Operations[/b]\" , \"\" ) table . add_row () for op_id , op_info in self . operations . items (): table . add_row ( f \" [i] { op_id } [/i]\" , op_info . doc . description ) empty_line = True if self . values : if empty_line : table . add_row () table . add_row ( \"[b]Values[/b]\" , \"\" ) table . add_row () for value_id , value in self . values . items (): table . add_row ( f \" [i] { value_id } [/i]\" , value . value_schema . type ) return table","title":"KiaraEntityMatches"},{"location":"reference/kiara/interfaces/cli/explain/#kiara.interfaces.cli.explain.KiaraEntityMatches.module_types","text":"Matching module types.","title":"module_types"},{"location":"reference/kiara/interfaces/cli/explain/#kiara.interfaces.cli.explain.KiaraEntityMatches.operation_types","text":"Matching operation types.","title":"operation_types"},{"location":"reference/kiara/interfaces/cli/explain/#kiara.interfaces.cli.explain.KiaraEntityMatches.operations","text":"Matching operations.","title":"operations"},{"location":"reference/kiara/interfaces/cli/explain/#kiara.interfaces.cli.explain.KiaraEntityMatches.values","text":"Matching values.","title":"values"},{"location":"reference/kiara/interfaces/cli/run/","text":"The 'run' subcommand for the cli.","title":"run"},{"location":"reference/kiara/interfaces/cli/utils/","text":"Shared utilties to be used by cli sub-commands.","title":"utils"},{"location":"reference/kiara/interfaces/cli/data/__init__/","text":"commands \u00b6 Data-related sub-commands for the cli.","title":"data"},{"location":"reference/kiara/interfaces/cli/data/__init__/#kiara.interfaces.cli.data.commands","text":"Data-related sub-commands for the cli.","title":"commands"},{"location":"reference/kiara/interfaces/cli/data/commands/","text":"Data-related sub-commands for the cli.","title":"commands"},{"location":"reference/kiara/interfaces/cli/dev/__init__/","text":"","title":"dev"},{"location":"reference/kiara/interfaces/cli/dev/commands/","text":"","title":"commands"},{"location":"reference/kiara/interfaces/cli/environment/__init__/","text":"","title":"environment"},{"location":"reference/kiara/interfaces/cli/environment/commands/","text":"","title":"commands"},{"location":"reference/kiara/interfaces/cli/info/__init__/","text":"","title":"info"},{"location":"reference/kiara/interfaces/cli/info/commands/","text":"","title":"commands"},{"location":"reference/kiara/interfaces/cli/metadata/__init__/","text":"Metadata-related sub-commands for the cli.","title":"metadata"},{"location":"reference/kiara/interfaces/cli/metadata/commands/","text":"","title":"commands"},{"location":"reference/kiara/interfaces/cli/module/__init__/","text":"commands \u00b6 Module related subcommands for the cli.","title":"module"},{"location":"reference/kiara/interfaces/cli/module/__init__/#kiara.interfaces.cli.module.commands","text":"Module related subcommands for the cli.","title":"commands"},{"location":"reference/kiara/interfaces/cli/module/commands/","text":"Module related subcommands for the cli.","title":"commands"},{"location":"reference/kiara/interfaces/cli/operation/__init__/","text":"","title":"operation"},{"location":"reference/kiara/interfaces/cli/operation/commands/","text":"","title":"commands"},{"location":"reference/kiara/interfaces/cli/pipeline/__init__/","text":"commands \u00b6 Pipeline-related subcommands for the cli.","title":"pipeline"},{"location":"reference/kiara/interfaces/cli/pipeline/__init__/#kiara.interfaces.cli.pipeline.commands","text":"Pipeline-related subcommands for the cli.","title":"commands"},{"location":"reference/kiara/interfaces/cli/pipeline/commands/","text":"Pipeline-related subcommands for the cli.","title":"commands"},{"location":"reference/kiara/interfaces/cli/type/__init__/","text":"commands \u00b6 Type-related subcommands for the cli.","title":"type"},{"location":"reference/kiara/interfaces/cli/type/__init__/#kiara.interfaces.cli.type.commands","text":"Type-related subcommands for the cli.","title":"commands"},{"location":"reference/kiara/interfaces/cli/type/commands/","text":"Type-related subcommands for the cli.","title":"commands"},{"location":"reference/kiara/interfaces/python_api/__init__/","text":"A Python API for creating workflow sessions and dynamic pipelines in kiara . StepInfo ( JupyterMixin , BaseModel ) pydantic-model \u00b6 Source code in kiara/interfaces/python_api/__init__.py class StepInfo ( JupyterMixin , BaseModel ): step_id : str = Field ( description = \"The step id.\" ) module_metadata : KiaraModuleInstanceMetadata = Field ( description = \"The module metadata.\" ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : doc = self . module_metadata . type_metadata . documentation . create_renderable () metadata = self . module_metadata . create_renderable ( include_doc = False ) panel = Panel ( RenderGroup ( \"\" , Panel ( doc , box = box . SIMPLE ), \"\" , metadata ), title = f \"Step info: [b] { self . step_id } [/b] (type: [i] { self . module_metadata . type_metadata . type_id } [/i])\" , title_align = \"left\" , ) yield panel module_metadata : KiaraModuleInstanceMetadata pydantic-field required \u00b6 The module metadata. step_id : str pydantic-field required \u00b6 The step id. controller \u00b6 ApiController ( PipelineController ) \u00b6 A pipeline controller for pipelines generated by the kiara Python API. Source code in kiara/interfaces/python_api/controller.py class ApiController ( PipelineController ): \"\"\"A pipeline controller for pipelines generated by the *kiara* Python API.\"\"\" def __init__ ( self , pipeline : typing . Optional [ Pipeline ] = None , processor : typing . Optional [ ModuleProcessor ] = None , ): self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor ) def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : # from kiara.utils.output import rich_print # rich_print(self.pipeline.get_current_state()) raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False def step_inputs_changed ( self , event : StepInputEvent ): for step_id in event . updated_step_inputs . keys (): # outputs = self.get_step_outputs(step_id) raise NotImplementedError () # outputs.invalidate() def pipeline_inputs_changed ( self , event : PipelineInputEvent ): if self . _is_running : raise NotImplementedError () def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False def get_pipeline_input ( self , step_id : str , field_name : str ) -> Value : self . ensure_step ( step_id ) outputs = self . get_step_inputs ( step_id ) return outputs . get_value_obj ( field_name = field_name ) def get_pipeline_output ( self , step_id : str , field_name : str ) -> Value : self . ensure_step ( step_id ) outputs = self . get_step_outputs ( step_id ) return outputs . get_value_obj ( field_name = field_name ) def _process ( self , step_id : str ) -> typing . Union [ None , bool , str ]: \"\"\"Process the specified step. Returns: 'None', if no processing was needed, but the step output is valid, 'False' if it's not possible to process the step, or a string which represents the job id of the processing \"\"\" if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): return None else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) return job_id else : return None except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False def ensure_step ( self , step_id : str ) -> bool : \"\"\"Ensure a step has valid outputs. Returns: 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs \"\"\" if step_id not in self . pipeline . step_ids : raise Exception ( f \"No step with id: { step_id } \" ) outputs = self . get_step_outputs ( step_id = step_id ) if outputs . items_are_valid (): return True if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids : typing . List [ str ] = [] finished = False if step_id in stage : finished = True job_id = self . _process ( step_id = step_id ) if job_id is None : return True if not job_id : return False job_ids . append ( job_id ) # type: ignore else : for s_id in stage : job_id = self . _process ( step_id = s_id ) if job_id is None : continue elif job_id is False : return False job_ids . append ( job_id ) # type: ignore self . _processor . wait_for ( * job_ids ) if finished : break finally : self . _is_running = False return True ensure_step ( self , step_id ) \u00b6 Ensure a step has valid outputs. Returns: Type Description bool 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs Source code in kiara/interfaces/python_api/controller.py def ensure_step ( self , step_id : str ) -> bool : \"\"\"Ensure a step has valid outputs. Returns: 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs \"\"\" if step_id not in self . pipeline . step_ids : raise Exception ( f \"No step with id: { step_id } \" ) outputs = self . get_step_outputs ( step_id = step_id ) if outputs . items_are_valid (): return True if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids : typing . List [ str ] = [] finished = False if step_id in stage : finished = True job_id = self . _process ( step_id = step_id ) if job_id is None : return True if not job_id : return False job_ids . append ( job_id ) # type: ignore else : for s_id in stage : job_id = self . _process ( step_id = s_id ) if job_id is None : continue elif job_id is False : return False job_ids . append ( job_id ) # type: ignore self . _processor . wait_for ( * job_ids ) if finished : break finally : self . _is_running = False return True pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/interfaces/python_api/controller.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): if self . _is_running : raise NotImplementedError () pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/interfaces/python_api/controller.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/interfaces/python_api/controller.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : # from kiara.utils.output import rich_print # rich_print(self.pipeline.get_current_state()) raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/interfaces/python_api/controller.py def step_inputs_changed ( self , event : StepInputEvent ): for step_id in event . updated_step_inputs . keys (): # outputs = self.get_step_outputs(step_id) raise NotImplementedError () # outputs.invalidate()","title":"python_api"},{"location":"reference/kiara/interfaces/python_api/__init__/#kiara.interfaces.python_api.StepInfo","text":"Source code in kiara/interfaces/python_api/__init__.py class StepInfo ( JupyterMixin , BaseModel ): step_id : str = Field ( description = \"The step id.\" ) module_metadata : KiaraModuleInstanceMetadata = Field ( description = \"The module metadata.\" ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : doc = self . module_metadata . type_metadata . documentation . create_renderable () metadata = self . module_metadata . create_renderable ( include_doc = False ) panel = Panel ( RenderGroup ( \"\" , Panel ( doc , box = box . SIMPLE ), \"\" , metadata ), title = f \"Step info: [b] { self . step_id } [/b] (type: [i] { self . module_metadata . type_metadata . type_id } [/i])\" , title_align = \"left\" , ) yield panel","title":"StepInfo"},{"location":"reference/kiara/interfaces/python_api/__init__/#kiara.interfaces.python_api.StepInfo.module_metadata","text":"The module metadata.","title":"module_metadata"},{"location":"reference/kiara/interfaces/python_api/__init__/#kiara.interfaces.python_api.StepInfo.step_id","text":"The step id.","title":"step_id"},{"location":"reference/kiara/interfaces/python_api/__init__/#kiara.interfaces.python_api.controller","text":"","title":"controller"},{"location":"reference/kiara/interfaces/python_api/__init__/#kiara.interfaces.python_api.controller.ApiController","text":"A pipeline controller for pipelines generated by the kiara Python API. Source code in kiara/interfaces/python_api/controller.py class ApiController ( PipelineController ): \"\"\"A pipeline controller for pipelines generated by the *kiara* Python API.\"\"\" def __init__ ( self , pipeline : typing . Optional [ Pipeline ] = None , processor : typing . Optional [ ModuleProcessor ] = None , ): self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor ) def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : # from kiara.utils.output import rich_print # rich_print(self.pipeline.get_current_state()) raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False def step_inputs_changed ( self , event : StepInputEvent ): for step_id in event . updated_step_inputs . keys (): # outputs = self.get_step_outputs(step_id) raise NotImplementedError () # outputs.invalidate() def pipeline_inputs_changed ( self , event : PipelineInputEvent ): if self . _is_running : raise NotImplementedError () def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False def get_pipeline_input ( self , step_id : str , field_name : str ) -> Value : self . ensure_step ( step_id ) outputs = self . get_step_inputs ( step_id ) return outputs . get_value_obj ( field_name = field_name ) def get_pipeline_output ( self , step_id : str , field_name : str ) -> Value : self . ensure_step ( step_id ) outputs = self . get_step_outputs ( step_id ) return outputs . get_value_obj ( field_name = field_name ) def _process ( self , step_id : str ) -> typing . Union [ None , bool , str ]: \"\"\"Process the specified step. Returns: 'None', if no processing was needed, but the step output is valid, 'False' if it's not possible to process the step, or a string which represents the job id of the processing \"\"\" if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): return None else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) return job_id else : return None except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False def ensure_step ( self , step_id : str ) -> bool : \"\"\"Ensure a step has valid outputs. Returns: 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs \"\"\" if step_id not in self . pipeline . step_ids : raise Exception ( f \"No step with id: { step_id } \" ) outputs = self . get_step_outputs ( step_id = step_id ) if outputs . items_are_valid (): return True if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids : typing . List [ str ] = [] finished = False if step_id in stage : finished = True job_id = self . _process ( step_id = step_id ) if job_id is None : return True if not job_id : return False job_ids . append ( job_id ) # type: ignore else : for s_id in stage : job_id = self . _process ( step_id = s_id ) if job_id is None : continue elif job_id is False : return False job_ids . append ( job_id ) # type: ignore self . _processor . wait_for ( * job_ids ) if finished : break finally : self . _is_running = False return True","title":"ApiController"},{"location":"reference/kiara/interfaces/python_api/controller/","text":"ApiController ( PipelineController ) \u00b6 A pipeline controller for pipelines generated by the kiara Python API. Source code in kiara/interfaces/python_api/controller.py class ApiController ( PipelineController ): \"\"\"A pipeline controller for pipelines generated by the *kiara* Python API.\"\"\" def __init__ ( self , pipeline : typing . Optional [ Pipeline ] = None , processor : typing . Optional [ ModuleProcessor ] = None , ): self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor ) def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : # from kiara.utils.output import rich_print # rich_print(self.pipeline.get_current_state()) raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False def step_inputs_changed ( self , event : StepInputEvent ): for step_id in event . updated_step_inputs . keys (): # outputs = self.get_step_outputs(step_id) raise NotImplementedError () # outputs.invalidate() def pipeline_inputs_changed ( self , event : PipelineInputEvent ): if self . _is_running : raise NotImplementedError () def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False def get_pipeline_input ( self , step_id : str , field_name : str ) -> Value : self . ensure_step ( step_id ) outputs = self . get_step_inputs ( step_id ) return outputs . get_value_obj ( field_name = field_name ) def get_pipeline_output ( self , step_id : str , field_name : str ) -> Value : self . ensure_step ( step_id ) outputs = self . get_step_outputs ( step_id ) return outputs . get_value_obj ( field_name = field_name ) def _process ( self , step_id : str ) -> typing . Union [ None , bool , str ]: \"\"\"Process the specified step. Returns: 'None', if no processing was needed, but the step output is valid, 'False' if it's not possible to process the step, or a string which represents the job id of the processing \"\"\" if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): return None else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) return job_id else : return None except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False def ensure_step ( self , step_id : str ) -> bool : \"\"\"Ensure a step has valid outputs. Returns: 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs \"\"\" if step_id not in self . pipeline . step_ids : raise Exception ( f \"No step with id: { step_id } \" ) outputs = self . get_step_outputs ( step_id = step_id ) if outputs . items_are_valid (): return True if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids : typing . List [ str ] = [] finished = False if step_id in stage : finished = True job_id = self . _process ( step_id = step_id ) if job_id is None : return True if not job_id : return False job_ids . append ( job_id ) # type: ignore else : for s_id in stage : job_id = self . _process ( step_id = s_id ) if job_id is None : continue elif job_id is False : return False job_ids . append ( job_id ) # type: ignore self . _processor . wait_for ( * job_ids ) if finished : break finally : self . _is_running = False return True ensure_step ( self , step_id ) \u00b6 Ensure a step has valid outputs. Returns: Type Description bool 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs Source code in kiara/interfaces/python_api/controller.py def ensure_step ( self , step_id : str ) -> bool : \"\"\"Ensure a step has valid outputs. Returns: 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs \"\"\" if step_id not in self . pipeline . step_ids : raise Exception ( f \"No step with id: { step_id } \" ) outputs = self . get_step_outputs ( step_id = step_id ) if outputs . items_are_valid (): return True if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids : typing . List [ str ] = [] finished = False if step_id in stage : finished = True job_id = self . _process ( step_id = step_id ) if job_id is None : return True if not job_id : return False job_ids . append ( job_id ) # type: ignore else : for s_id in stage : job_id = self . _process ( step_id = s_id ) if job_id is None : continue elif job_id is False : return False job_ids . append ( job_id ) # type: ignore self . _processor . wait_for ( * job_ids ) if finished : break finally : self . _is_running = False return True pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/interfaces/python_api/controller.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): if self . _is_running : raise NotImplementedError () pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/interfaces/python_api/controller.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/interfaces/python_api/controller.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : # from kiara.utils.output import rich_print # rich_print(self.pipeline.get_current_state()) raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/interfaces/python_api/controller.py def step_inputs_changed ( self , event : StepInputEvent ): for step_id in event . updated_step_inputs . keys (): # outputs = self.get_step_outputs(step_id) raise NotImplementedError () # outputs.invalidate()","title":"controller"},{"location":"reference/kiara/interfaces/python_api/controller/#kiara.interfaces.python_api.controller.ApiController","text":"A pipeline controller for pipelines generated by the kiara Python API. Source code in kiara/interfaces/python_api/controller.py class ApiController ( PipelineController ): \"\"\"A pipeline controller for pipelines generated by the *kiara* Python API.\"\"\" def __init__ ( self , pipeline : typing . Optional [ Pipeline ] = None , processor : typing . Optional [ ModuleProcessor ] = None , ): self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor ) def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : # from kiara.utils.output import rich_print # rich_print(self.pipeline.get_current_state()) raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False def step_inputs_changed ( self , event : StepInputEvent ): for step_id in event . updated_step_inputs . keys (): # outputs = self.get_step_outputs(step_id) raise NotImplementedError () # outputs.invalidate() def pipeline_inputs_changed ( self , event : PipelineInputEvent ): if self . _is_running : raise NotImplementedError () def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False def get_pipeline_input ( self , step_id : str , field_name : str ) -> Value : self . ensure_step ( step_id ) outputs = self . get_step_inputs ( step_id ) return outputs . get_value_obj ( field_name = field_name ) def get_pipeline_output ( self , step_id : str , field_name : str ) -> Value : self . ensure_step ( step_id ) outputs = self . get_step_outputs ( step_id ) return outputs . get_value_obj ( field_name = field_name ) def _process ( self , step_id : str ) -> typing . Union [ None , bool , str ]: \"\"\"Process the specified step. Returns: 'None', if no processing was needed, but the step output is valid, 'False' if it's not possible to process the step, or a string which represents the job id of the processing \"\"\" if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): return None else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) return job_id else : return None except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False def ensure_step ( self , step_id : str ) -> bool : \"\"\"Ensure a step has valid outputs. Returns: 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs \"\"\" if step_id not in self . pipeline . step_ids : raise Exception ( f \"No step with id: { step_id } \" ) outputs = self . get_step_outputs ( step_id = step_id ) if outputs . items_are_valid (): return True if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids : typing . List [ str ] = [] finished = False if step_id in stage : finished = True job_id = self . _process ( step_id = step_id ) if job_id is None : return True if not job_id : return False job_ids . append ( job_id ) # type: ignore else : for s_id in stage : job_id = self . _process ( step_id = s_id ) if job_id is None : continue elif job_id is False : return False job_ids . append ( job_id ) # type: ignore self . _processor . wait_for ( * job_ids ) if finished : break finally : self . _is_running = False return True","title":"ApiController"},{"location":"reference/kiara/interfaces/python_api/controller/#kiara.interfaces.python_api.controller.ApiController.ensure_step","text":"Ensure a step has valid outputs. Returns: Type Description bool 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs Source code in kiara/interfaces/python_api/controller.py def ensure_step ( self , step_id : str ) -> bool : \"\"\"Ensure a step has valid outputs. Returns: 'True' if the step now has valid outputs, 'False` if that's not possible because of invalid/missing inputs \"\"\" if step_id not in self . pipeline . step_ids : raise Exception ( f \"No step with id: { step_id } \" ) outputs = self . get_step_outputs ( step_id = step_id ) if outputs . items_are_valid (): return True if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids : typing . List [ str ] = [] finished = False if step_id in stage : finished = True job_id = self . _process ( step_id = step_id ) if job_id is None : return True if not job_id : return False job_ids . append ( job_id ) # type: ignore else : for s_id in stage : job_id = self . _process ( step_id = s_id ) if job_id is None : continue elif job_id is False : return False job_ids . append ( job_id ) # type: ignore self . _processor . wait_for ( * job_ids ) if finished : break finally : self . _is_running = False return True","title":"ensure_step()"},{"location":"reference/kiara/interfaces/python_api/controller/#kiara.interfaces.python_api.controller.ApiController.pipeline_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/interfaces/python_api/controller.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): if self . _is_running : raise NotImplementedError ()","title":"pipeline_inputs_changed()"},{"location":"reference/kiara/interfaces/python_api/controller/#kiara.interfaces.python_api.controller.ApiController.pipeline_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/interfaces/python_api/controller.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False","title":"pipeline_outputs_changed()"},{"location":"reference/kiara/interfaces/python_api/controller/#kiara.interfaces.python_api.controller.ApiController.process_pipeline","text":"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/interfaces/python_api/controller.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : # from kiara.utils.output import rich_print # rich_print(self.pipeline.get_current_state()) raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : if not self . get_step_outputs ( step_id ) . items_are_valid (): job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False","title":"process_pipeline()"},{"location":"reference/kiara/interfaces/python_api/controller/#kiara.interfaces.python_api.controller.ApiController.step_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/interfaces/python_api/controller.py def step_inputs_changed ( self , event : StepInputEvent ): for step_id in event . updated_step_inputs . keys (): # outputs = self.get_step_outputs(step_id) raise NotImplementedError () # outputs.invalidate()","title":"step_inputs_changed()"},{"location":"reference/kiara/metadata/__init__/","text":"MetadataModel ( KiaraInfoModel ) pydantic-model \u00b6 Base class for classes that represent value metadata in kiara. Source code in kiara/metadata/__init__.py class MetadataModel ( KiaraInfoModel ): \"\"\"Base class for classes that represent value metadata in kiara.\"\"\" @classmethod def get_type_metadata ( cls ) -> \"MetadataModelMetadata\" : \"\"\"Return all metadata associated with this module type.\"\"\" from kiara.metadata.core_models import MetadataModelMetadata return MetadataModelMetadata . from_model_class ( model_cls = cls ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) for k in self . __fields__ . keys (): if k == \"type_name\" and config . get ( \"omit_type_name\" , False ): continue attr = getattr ( self , k ) v = extract_renderable ( attr ) table . add_row ( k , v ) if \"operations\" in config . keys (): ids = list ( config [ \"operations\" ] . keys ()) table . add_row ( \"operations\" , \" \\n \" . join ( ids )) return table get_type_metadata () classmethod \u00b6 Return all metadata associated with this module type. Source code in kiara/metadata/__init__.py @classmethod def get_type_metadata ( cls ) -> \"MetadataModelMetadata\" : \"\"\"Return all metadata associated with this module type.\"\"\" from kiara.metadata.core_models import MetadataModelMetadata return MetadataModelMetadata . from_model_class ( model_cls = cls ) ValueTypeAndDescription ( BaseModel ) pydantic-model \u00b6 Source code in kiara/metadata/__init__.py class ValueTypeAndDescription ( BaseModel ): description : str = Field ( description = \"The description for the value.\" ) type : str = Field ( description = \"The value type.\" ) value_default : typing . Any = Field ( description = \"Default for the value.\" , default = None ) required : bool = Field ( description = \"Whether this value is required\" ) description : str pydantic-field required \u00b6 The description for the value. required : bool pydantic-field required \u00b6 Whether this value is required type : str pydantic-field required \u00b6 The value type. value_default : Any pydantic-field \u00b6 Default for the value. core_models \u00b6 AuthorModel ( BaseModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class AuthorModel ( BaseModel ): name : str = Field ( description = \"The full name of the author.\" ) email : typing . Optional [ EmailStr ] = Field ( description = \"The email address of the author\" , default = None ) email : EmailStr pydantic-field \u00b6 The email address of the author name : str pydantic-field required \u00b6 The full name of the author. ContextMetadataModel ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class ContextMetadataModel ( MetadataModel ): @classmethod def from_class ( cls , item_cls : typing . Type ): data = get_metadata_for_python_module_or_class ( item_cls ) # type: ignore merged = merge_dicts ( * data ) return cls . parse_obj ( merged ) _metadata_key = \"properties\" references : typing . Dict [ str , LinkModel ] = Field ( description = \"References for the item.\" , default_factory = dict ) tags : typing . Set [ str ] = Field ( description = \"A list of tags for the item.\" , default_factory = set ) labels : typing . Dict [ str , str ] = Field ( description = \"A list of labels for the item.\" , default_factory = list ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . tags : table . add_row ( \"Tags\" , \", \" . join ( self . tags )) if self . labels : labels = [] for k , v in self . labels . items (): labels . append ( f \"[i] { k } [/i]: { v } \" ) table . add_row ( \"Labels\" , \" \\n \" . join ( labels )) if self . references : references = [] for _k , _v in self . references . items (): link = f \"[link= { _v . url } ] { _v . url } [/link]\" references . append ( f \"[i] { _k } [/i]: { link } \" ) table . add_row ( \"References\" , \" \\n \" . join ( references )) return table def add_reference ( self , ref_type : str , url : str , desc : typing . Optional [ str ] = None , force : bool = False , ): if ref_type in self . references . keys () and not force : raise Exception ( f \"Reference of type ' { ref_type } ' already present.\" ) link = LinkModel ( url = url , desc = desc ) self . references [ ref_type ] = link def get_url_for_reference ( self , ref : str ) -> typing . Optional [ str ]: link = self . references . get ( ref , None ) if not link : return None return link . url labels : Dict [ str , str ] pydantic-field \u00b6 A list of labels for the item. references : Dict [ str , kiara . metadata . core_models . LinkModel ] pydantic-field \u00b6 References for the item. tags : Set [ str ] pydantic-field \u00b6 A list of tags for the item. DocumentationMetadataModel ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class DocumentationMetadataModel ( MetadataModel ): _metadata_key = \"documentation\" @classmethod def from_class_doc ( cls , item_cls : typing . Type ): doc = item_cls . __doc__ if not doc : doc = DEFAULT_NO_DESC_VALUE doc = inspect . cleandoc ( doc ) return cls . from_string ( doc ) @classmethod def from_string ( cls , doc : typing . Optional [ str ]): if not doc : doc = DEFAULT_NO_DESC_VALUE if \" \\n \" in doc : desc , doc = doc . split ( \" \\n \" , maxsplit = 1 ) else : desc = doc doc = None if doc : doc = doc . strip () return cls ( description = desc . strip (), doc = doc ) @classmethod def from_dict ( cls , data : typing . Mapping ): doc = data . get ( \"doc\" , None ) desc = data . get ( \"description\" , None ) if desc is None : desc = data . get ( \"desc\" , None ) if not doc and not desc : return cls . from_string ( DEFAULT_NO_DESC_VALUE ) elif doc and not desc : return cls . from_string ( doc ) elif desc and not doc : return cls . from_string ( desc ) else : return DocumentationMetadataModel ( description = desc , doc = doc ) @classmethod def create ( cls , item : typing . Any ): if not item : return cls . from_string ( DEFAULT_NO_DESC_VALUE ) elif isinstance ( item , DocumentationMetadataModel ): return item elif isinstance ( item , typing . Mapping ): return cls . from_dict ( item ) if isinstance ( item , type ): return cls . from_class_doc ( item ) elif isinstance ( item , str ): return cls . from_string ( item ) else : raise TypeError ( f \"Can't create documentation from type ' { type ( item ) } '.\" ) description : str = Field ( description = \"Short description of the item.\" , default = DEFAULT_NO_DESC_VALUE ) doc : typing . Optional [ str ] = Field ( description = \"Detailed documentation of the item (in markdown).\" , default = None ) @property def full_doc ( self ): if self . doc : return f \" { self . description } \\n\\n { self . doc } \" else : return self . description def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return Markdown ( self . full_doc ) description : str pydantic-field \u00b6 Short description of the item. doc : str pydantic-field \u00b6 Detailed documentation of the item (in markdown). LinkModel ( BaseModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class LinkModel ( BaseModel ): url : AnyUrl = Field ( description = \"The url.\" ) desc : typing . Optional [ str ] = Field ( description = \"A short description of the link content.\" , default = DEFAULT_NO_DESC_VALUE , ) desc : str pydantic-field \u00b6 A short description of the link content. url : AnyUrl pydantic-field required \u00b6 The url. MetadataModelMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class MetadataModelMetadata ( MetadataModel ): @classmethod def from_model_class ( cls , model_cls : typing . Type [ MetadataModel ]): origin_md = OriginMetadataModel . from_class ( model_cls ) doc = DocumentationMetadataModel . from_class_doc ( model_cls ) python_class = PythonClassMetadata . from_class ( model_cls ) properties_md = ContextMetadataModel . from_class ( model_cls ) return MetadataModelMetadata ( type_name = model_cls . _metadata_key , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" ) def create_fields_table ( self , show_header : bool = True , show_required : bool = True ) -> Table : type_cls = self . python_class . get_class () fields_table = Table ( show_header = show_header , box = box . SIMPLE ) fields_table . add_column ( \"Field name\" , style = \"i\" ) fields_table . add_column ( \"Type\" ) if show_required : fields_table . add_column ( \"Required\" ) fields_table . add_column ( \"Description\" ) for field_name , details in type_cls . __fields__ . items (): field_type = type_cls . schema ()[ \"properties\" ][ field_name ][ \"type\" ] info = details . field_info . description if show_required : req = \"yes\" if details . required else \"no\" fields_table . add_row ( field_name , field_type , req , info ) else : fields_table . add_row ( field_name , field_type , info ) return fields_table def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_schema = config . get ( \"display_schema\" , False ) include_doc = config . get ( \"include_doc\" , True ) table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) fields_table = self . create_fields_table () table . add_row ( \"Fields\" , fields_table ) if include_schema : json_str = Syntax ( self . python_class . get_class () . schema_json ( indent = 2 ), \"json\" , background_color = \"default\" , ) table . add_row ( \"Json Schema\" , json_str ) return table context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type. documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about the creator of this value type. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this value type. type_name : str pydantic-field required \u00b6 The registered name for this value type. OriginMetadataModel ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class OriginMetadataModel ( MetadataModel ): _metadata_key = \"origin\" @classmethod def from_class ( cls , item_cls : typing . Type ): data = get_metadata_for_python_module_or_class ( item_cls ) # type: ignore merged = merge_dicts ( * data ) return cls . parse_obj ( merged ) authors : typing . List [ AuthorModel ] = Field ( description = \"The authors/creators of this item.\" , default_factory = list ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) authors = [] for author in reversed ( self . authors ): if author . email : authors . append ( f \" { author . name } ( { author . email } )\" ) else : authors . append ( author . name ) table . add_row ( \"Authors\" , \" \\n \" . join ( authors )) return table authors : List [ kiara . metadata . core_models . AuthorModel ] pydantic-field \u00b6 The authors/creators of this item. PythonClassMetadata ( MetadataModel ) pydantic-model \u00b6 Python class and module information. Source code in kiara/metadata/core_models.py class PythonClassMetadata ( MetadataModel ): \"\"\"Python class and module information.\"\"\" _metadata_key : typing . ClassVar [ str ] = \"python_class\" @classmethod def from_class ( cls , item_cls : typing . Type ): conf : typing . Dict [ str , typing . Any ] = { \"class_name\" : item_cls . __name__ , \"module_name\" : item_cls . __module__ , \"full_name\" : f \" { item_cls . __module__ } . { item_cls . __name__ } \" , } return PythonClassMetadata ( ** conf ) class_name : str = Field ( description = \"The name of the Python class.\" ) module_name : str = Field ( description = \"The name of the Python module this class lives in.\" ) full_name : str = Field ( description = \"The full class namespace.\" ) def get_class ( self ) -> typing . Type : m = self . get_module () return getattr ( m , self . class_name ) def get_module ( self ) -> ModuleType : m = importlib . import_module ( self . module_name ) return m class_name : str pydantic-field required \u00b6 The name of the Python class. full_name : str pydantic-field required \u00b6 The full class namespace. module_name : str pydantic-field required \u00b6 The name of the Python module this class lives in. data \u00b6 DeserializeConfig ( ModuleConfig ) pydantic-model \u00b6 Source code in kiara/metadata/data.py class DeserializeConfig ( ModuleConfig ): # value_id: str = Field(description=\"The id of the value.\") serialization_type : str = Field ( description = \"The serialization type.\" ) input : typing . Any = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) output_name : str = Field ( description = \"The name of the output field for the value.\" ) input : Any pydantic-field \u00b6 The inputs to use when running this module. output_name : str pydantic-field required \u00b6 The name of the output field for the value. serialization_type : str pydantic-field required \u00b6 The serialization type. LoadConfig ( ModuleConfig ) pydantic-model \u00b6 Source code in kiara/metadata/data.py class LoadConfig ( ModuleConfig ): value_id : str = Field ( description = \"The id of the value.\" ) base_path_input_name : typing . Optional [ str ] = Field ( description = \"The name of the input that stores the base_path where the value is saved.\" , default = None , ) inputs : typing . Dict [ str , typing . Any ] = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) output_name : str = Field ( description = \"The name of the output field for the value.\" ) base_path_input_name : str pydantic-field \u00b6 The name of the input that stores the base_path where the value is saved. inputs : Dict [ str , Any ] pydantic-field \u00b6 The inputs to use when running this module. output_name : str pydantic-field required \u00b6 The name of the output field for the value. value_id : str pydantic-field required \u00b6 The id of the value. SaveConfig ( ModuleConfig ) pydantic-model \u00b6 Source code in kiara/metadata/data.py class SaveConfig ( ModuleConfig ): inputs : typing . Dict [ str , typing . Any ] = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) load_config_output : str = Field ( description = \"The output name that will contain the load config output value.\" ) inputs : Dict [ str , Any ] pydantic-field \u00b6 The inputs to use when running this module. load_config_output : str pydantic-field required \u00b6 The output name that will contain the load config output value. module_models \u00b6 KiaraModuleConfigMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/module_models.py class KiaraModuleConfigMetadata ( MetadataModel ): @classmethod def from_config_class ( cls , config_cls : typing . Type [ ModuleTypeConfigSchema ], remove_pipeline_config : bool = False , ): flat_models = get_flat_models_from_model ( config_cls ) model_name_map = get_model_name_map ( flat_models ) m_schema , _ , _ = model_process_schema ( config_cls , model_name_map = model_name_map ) fields = m_schema [ \"properties\" ] config_values = {} for field_name , details in fields . items (): if remove_pipeline_config and field_name in [ \"steps\" , \"input_aliases\" , \"output_aliases\" , \"doc\" , ]: continue type_str = \"-- n/a --\" if \"type\" in details . keys (): type_str = details [ \"type\" ] desc = details . get ( \"description\" , DEFAULT_NO_DESC_VALUE ) default = config_cls . __fields__ [ field_name ] . default default = config_cls . __fields__ [ field_name ] . default if default is None : if callable ( config_cls . __fields__ [ field_name ] . default_factory ): default = config_cls . __fields__ [ field_name ] . default_factory () # type: ignore req = config_cls . __fields__ [ field_name ] . required config_values [ field_name ] = ValueTypeAndDescription ( description = desc , type = type_str , value_default = default , required = req ) python_cls = PythonClassMetadata . from_class ( config_cls ) return KiaraModuleConfigMetadata ( python_class = python_cls , config_values = config_values ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this configuration.\" ) config_values : typing . Dict [ str , ValueTypeAndDescription ] = Field ( description = \"The available configuration values.\" ) config_values : Dict [ str , kiara . metadata . ValueTypeAndDescription ] pydantic-field required \u00b6 The available configuration values. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this configuration. KiaraModuleInstanceMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/module_models.py class KiaraModuleInstanceMetadata ( MetadataModel ): @classmethod def from_module_obj ( cls , obj : \"KiaraModule\" ): config = obj . config . dict () for x in [ \"steps\" , \"input_aliases\" , \"output_aliases\" , \"module_type_name\" , \"doc\" , \"metadata\" , ]: config . pop ( x , None ) type_metadata = KiaraModuleTypeMetadata . from_module_class ( obj . __class__ ) result = KiaraModuleInstanceMetadata ( type_metadata = type_metadata , config = config , inputs_schema = obj . input_schemas , outputs_schema = obj . output_schemas , ) return result type_metadata : KiaraModuleTypeMetadata = Field ( description = \"Metadata for the module type of this instance.\" ) config : typing . Dict [ str , typing . Any ] = Field ( description = \"Configuration that was used to create this module instance.\" ) inputs_schema : typing . Dict [ str , ValueSchema ] = Field ( description = \"The schema for the module inputs.\" ) outputs_schema : typing . Dict [ str , ValueSchema ] = Field ( description = \"The schema for the module outputs.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_desc = config . get ( \"include_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if include_desc : table . add_row ( \"Description\" , self . type_metadata . documentation . description ) table . add_row ( \"Origin\" , self . type_metadata . origin . create_renderable ()) table . add_row ( \"Type context\" , self . type_metadata . context . create_renderable ()) table . add_row ( \"Python class\" , self . type_metadata . python_class . create_renderable () ) conf = Syntax ( json . dumps ( self . config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Configuration\" , conf ) constants = self . config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . inputs_schema , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . outputs_schema , ) table . add_row ( \"Outputs\" , outputs_table ) # table.add_row(\"Source code\", self.type_metadata.process_src) return table config : Dict [ str , Any ] pydantic-field required \u00b6 Configuration that was used to create this module instance. inputs_schema : Dict [ str , kiara . data . values . ValueSchema ] pydantic-field required \u00b6 The schema for the module inputs. outputs_schema : Dict [ str , kiara . data . values . ValueSchema ] pydantic-field required \u00b6 The schema for the module outputs. type_metadata : KiaraModuleTypeMetadata pydantic-field required \u00b6 Metadata for the module type of this instance. KiaraModuleTypeMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/module_models.py class KiaraModuleTypeMetadata ( MetadataModel ): @classmethod def from_module_class ( cls , module_cls : typing . Type [ \"KiaraModule\" ]): module_attrs = cls . extract_module_attributes ( module_cls = module_cls ) return cls ( ** module_attrs ) @classmethod def extract_module_attributes ( self , module_cls : typing . Type [ \"KiaraModule\" ] ) -> typing . Dict [ str , typing . Any ]: if not hasattr ( module_cls , \"process\" ): raise Exception ( f \"Module class ' { module_cls } ' misses 'process' method.\" ) proc_src = textwrap . dedent ( inspect . getsource ( module_cls . process )) # type: ignore origin_md = OriginMetadataModel . from_class ( module_cls ) doc = DocumentationMetadataModel . from_class_doc ( module_cls ) python_class = PythonClassMetadata . from_class ( module_cls ) properties_md = ContextMetadataModel . from_class ( module_cls ) is_pipeline = module_cls . is_pipeline () doc_url = properties_md . get_url_for_reference ( \"documentation\" ) if doc_url : class_doc = calculate_class_doc_url ( doc_url , module_cls . _module_type_id , pipeline = is_pipeline ) # type: ignore properties_md . add_reference ( \"module_doc\" , class_doc , \"A link to the published, auto-generated module documentation.\" , ) if not is_pipeline : repo_url = properties_md . get_url_for_reference ( \"source_repo\" ) if repo_url is not None : src_url = calculate_class_source_url ( repo_url , python_class ) properties_md . add_reference ( \"source_url\" , src_url , \"A link to the published source file that contains this module.\" , ) config = KiaraModuleConfigMetadata . from_config_class ( module_cls . _config_cls ) pipeline_config = None if module_cls . _module_type_id != \"pipeline\" and is_pipeline : # type: ignore pipeline_config = module_cls . _base_pipeline_config # type: ignore return { \"type_name\" : module_cls . _module_type_name , # type: ignore \"type_id\" : module_cls . _module_type_id , # type: ignore \"documentation\" : doc , \"origin\" : origin_md , \"context\" : properties_md , \"python_class\" : python_class , \"config\" : config , \"is_pipeline\" : is_pipeline , \"pipeline_config\" : pipeline_config , \"process_src\" : proc_src , } type_name : str = Field ( description = \"The registered name for this module type.\" ) type_id : str = Field ( description = \"The full type id.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the module.\" ) origin : OriginMetadataModel = Field ( description = \"Information about authorship for the module type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this module (description, tags, labels, references, ...).\" ) python_class : PythonClassMetadata = Field ( description = \"Information about the Python class for this module type.\" ) config : KiaraModuleConfigMetadata = Field ( description = \"Details on how this module type can be configured.\" ) is_pipeline : bool = Field ( description = \"Whether the module type is a pipeline, or a core module.\" ) pipeline_config : typing . Optional [ PipelineConfig ] = Field ( description = \"If this module is a pipeline, this field contains the pipeline configuration.\" , default_factory = None , ) process_src : str = Field ( description = \"The source code of the process method of the module.\" ) @validator ( \"documentation\" , pre = True ) def validate_doc ( cls , value ): return DocumentationMetadataModel . create ( value ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_config_schema = config . get ( \"include_config_schema\" , True ) include_src = config . get ( \"include_src\" , True ) include_doc = config . get ( \"include_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , padding = ( 0 , 0 , 0 , 0 )) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) if include_config_schema : if self . config : config_cls = self . config . python_class . get_class () table . add_row ( \"Module config\" , create_table_from_base_model ( config_cls )) table . add_row ( \"Module config\" , \"-- no config --\" ) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) if include_src : if self . is_pipeline : json_str = self . pipeline_config . json ( indent = 2 ) # type: ignore _config : Syntax = Syntax ( json_str , \"json\" , background_color = \"default\" ) table . add_row ( \"Pipeline config\" , Panel ( _config , box = box . HORIZONTALS )) else : _config = Syntax ( self . process_src , \"python\" , background_color = \"default\" ) table . add_row ( \"Processing source code\" , Panel ( _config , box = box . HORIZONTALS ) ) return table config : KiaraModuleConfigMetadata pydantic-field required \u00b6 Details on how this module type can be configured. context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this module (description, tags, labels, references, ...). documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the module. is_pipeline : bool pydantic-field required \u00b6 Whether the module type is a pipeline, or a core module. origin : OriginMetadataModel pydantic-field required \u00b6 Information about authorship for the module type. pipeline_config : PipelineConfig pydantic-field \u00b6 If this module is a pipeline, this field contains the pipeline configuration. process_src : str pydantic-field required \u00b6 The source code of the process method of the module. python_class : PythonClassMetadata pydantic-field required \u00b6 Information about the Python class for this module type. type_id : str pydantic-field required \u00b6 The full type id. type_name : str pydantic-field required \u00b6 The registered name for this module type. operation_models \u00b6 OperationsMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/operation_models.py class OperationsMetadata ( MetadataModel ): @classmethod def create_all ( cls , kiara : \"Kiara\" ) -> typing . Dict [ str , \"OperationsMetadata\" ]: op_types = kiara . operation_mgmt . operation_types result = {} for op_type in op_types : op_type_cls = kiara . operation_mgmt . get_operations ( op_type ) result [ op_type ] = cls . from_operations_class ( op_type_cls . __class__ ) return result @classmethod def from_operations_class ( cls , operation_type_cls : typing . Type [ \"OperationType\" ] ) -> \"OperationsMetadata\" : origin_md = OriginMetadataModel . from_class ( operation_type_cls ) doc = DocumentationMetadataModel . from_class_doc ( operation_type_cls ) python_class = PythonClassMetadata . from_class ( operation_type_cls ) properties_md = ContextMetadataModel . from_class ( operation_type_cls ) return OperationsMetadata . construct ( type_name = operation_type_cls . _operation_type_name , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" ) context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type. documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about the creator of this value type. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this value type. type_name : str pydantic-field required \u00b6 The registered name for this value type. type_models \u00b6 ValueTypeMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/type_models.py class ValueTypeMetadata ( MetadataModel ): @classmethod def create_all ( cls , kiara : \"Kiara\" ) -> typing . Dict [ str , \"ValueTypeMetadata\" ]: result = {} for vt in kiara . value_types : t_cls = kiara . get_value_type_cls ( vt ) result [ vt ] = cls . from_value_type_class ( t_cls ) return result @classmethod def from_value_type_class ( cls , value_type_cls : typing . Type [ \"ValueType\" ]): origin_md = OriginMetadataModel . from_class ( value_type_cls ) doc = DocumentationMetadataModel . from_class_doc ( value_type_cls ) python_class = PythonClassMetadata . from_class ( value_type_cls ) properties_md = ContextMetadataModel . from_class ( value_type_cls ) return ValueTypeMetadata ( type_name = value_type_cls . _value_type_name , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" ) context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type. documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about the creator of this value type. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this value type. type_name : str pydantic-field required \u00b6 The registered name for this value type.","title":"metadata"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.MetadataModel","text":"Base class for classes that represent value metadata in kiara. Source code in kiara/metadata/__init__.py class MetadataModel ( KiaraInfoModel ): \"\"\"Base class for classes that represent value metadata in kiara.\"\"\" @classmethod def get_type_metadata ( cls ) -> \"MetadataModelMetadata\" : \"\"\"Return all metadata associated with this module type.\"\"\" from kiara.metadata.core_models import MetadataModelMetadata return MetadataModelMetadata . from_model_class ( model_cls = cls ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) for k in self . __fields__ . keys (): if k == \"type_name\" and config . get ( \"omit_type_name\" , False ): continue attr = getattr ( self , k ) v = extract_renderable ( attr ) table . add_row ( k , v ) if \"operations\" in config . keys (): ids = list ( config [ \"operations\" ] . keys ()) table . add_row ( \"operations\" , \" \\n \" . join ( ids )) return table","title":"MetadataModel"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.MetadataModel.get_type_metadata","text":"Return all metadata associated with this module type. Source code in kiara/metadata/__init__.py @classmethod def get_type_metadata ( cls ) -> \"MetadataModelMetadata\" : \"\"\"Return all metadata associated with this module type.\"\"\" from kiara.metadata.core_models import MetadataModelMetadata return MetadataModelMetadata . from_model_class ( model_cls = cls )","title":"get_type_metadata()"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.ValueTypeAndDescription","text":"Source code in kiara/metadata/__init__.py class ValueTypeAndDescription ( BaseModel ): description : str = Field ( description = \"The description for the value.\" ) type : str = Field ( description = \"The value type.\" ) value_default : typing . Any = Field ( description = \"Default for the value.\" , default = None ) required : bool = Field ( description = \"Whether this value is required\" )","title":"ValueTypeAndDescription"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.ValueTypeAndDescription.description","text":"The description for the value.","title":"description"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.ValueTypeAndDescription.required","text":"Whether this value is required","title":"required"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.ValueTypeAndDescription.type","text":"The value type.","title":"type"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.ValueTypeAndDescription.value_default","text":"Default for the value.","title":"value_default"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.core_models","text":"","title":"core_models"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.core_models.AuthorModel","text":"Source code in kiara/metadata/core_models.py class AuthorModel ( BaseModel ): name : str = Field ( description = \"The full name of the author.\" ) email : typing . Optional [ EmailStr ] = Field ( description = \"The email address of the author\" , default = None )","title":"AuthorModel"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.core_models.ContextMetadataModel","text":"Source code in kiara/metadata/core_models.py class ContextMetadataModel ( MetadataModel ): @classmethod def from_class ( cls , item_cls : typing . Type ): data = get_metadata_for_python_module_or_class ( item_cls ) # type: ignore merged = merge_dicts ( * data ) return cls . parse_obj ( merged ) _metadata_key = \"properties\" references : typing . Dict [ str , LinkModel ] = Field ( description = \"References for the item.\" , default_factory = dict ) tags : typing . Set [ str ] = Field ( description = \"A list of tags for the item.\" , default_factory = set ) labels : typing . Dict [ str , str ] = Field ( description = \"A list of labels for the item.\" , default_factory = list ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . tags : table . add_row ( \"Tags\" , \", \" . join ( self . tags )) if self . labels : labels = [] for k , v in self . labels . items (): labels . append ( f \"[i] { k } [/i]: { v } \" ) table . add_row ( \"Labels\" , \" \\n \" . join ( labels )) if self . references : references = [] for _k , _v in self . references . items (): link = f \"[link= { _v . url } ] { _v . url } [/link]\" references . append ( f \"[i] { _k } [/i]: { link } \" ) table . add_row ( \"References\" , \" \\n \" . join ( references )) return table def add_reference ( self , ref_type : str , url : str , desc : typing . Optional [ str ] = None , force : bool = False , ): if ref_type in self . references . keys () and not force : raise Exception ( f \"Reference of type ' { ref_type } ' already present.\" ) link = LinkModel ( url = url , desc = desc ) self . references [ ref_type ] = link def get_url_for_reference ( self , ref : str ) -> typing . Optional [ str ]: link = self . references . get ( ref , None ) if not link : return None return link . url","title":"ContextMetadataModel"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.core_models.DocumentationMetadataModel","text":"Source code in kiara/metadata/core_models.py class DocumentationMetadataModel ( MetadataModel ): _metadata_key = \"documentation\" @classmethod def from_class_doc ( cls , item_cls : typing . Type ): doc = item_cls . __doc__ if not doc : doc = DEFAULT_NO_DESC_VALUE doc = inspect . cleandoc ( doc ) return cls . from_string ( doc ) @classmethod def from_string ( cls , doc : typing . Optional [ str ]): if not doc : doc = DEFAULT_NO_DESC_VALUE if \" \\n \" in doc : desc , doc = doc . split ( \" \\n \" , maxsplit = 1 ) else : desc = doc doc = None if doc : doc = doc . strip () return cls ( description = desc . strip (), doc = doc ) @classmethod def from_dict ( cls , data : typing . Mapping ): doc = data . get ( \"doc\" , None ) desc = data . get ( \"description\" , None ) if desc is None : desc = data . get ( \"desc\" , None ) if not doc and not desc : return cls . from_string ( DEFAULT_NO_DESC_VALUE ) elif doc and not desc : return cls . from_string ( doc ) elif desc and not doc : return cls . from_string ( desc ) else : return DocumentationMetadataModel ( description = desc , doc = doc ) @classmethod def create ( cls , item : typing . Any ): if not item : return cls . from_string ( DEFAULT_NO_DESC_VALUE ) elif isinstance ( item , DocumentationMetadataModel ): return item elif isinstance ( item , typing . Mapping ): return cls . from_dict ( item ) if isinstance ( item , type ): return cls . from_class_doc ( item ) elif isinstance ( item , str ): return cls . from_string ( item ) else : raise TypeError ( f \"Can't create documentation from type ' { type ( item ) } '.\" ) description : str = Field ( description = \"Short description of the item.\" , default = DEFAULT_NO_DESC_VALUE ) doc : typing . Optional [ str ] = Field ( description = \"Detailed documentation of the item (in markdown).\" , default = None ) @property def full_doc ( self ): if self . doc : return f \" { self . description } \\n\\n { self . doc } \" else : return self . description def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return Markdown ( self . full_doc )","title":"DocumentationMetadataModel"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.core_models.LinkModel","text":"Source code in kiara/metadata/core_models.py class LinkModel ( BaseModel ): url : AnyUrl = Field ( description = \"The url.\" ) desc : typing . Optional [ str ] = Field ( description = \"A short description of the link content.\" , default = DEFAULT_NO_DESC_VALUE , )","title":"LinkModel"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.core_models.MetadataModelMetadata","text":"Source code in kiara/metadata/core_models.py class MetadataModelMetadata ( MetadataModel ): @classmethod def from_model_class ( cls , model_cls : typing . Type [ MetadataModel ]): origin_md = OriginMetadataModel . from_class ( model_cls ) doc = DocumentationMetadataModel . from_class_doc ( model_cls ) python_class = PythonClassMetadata . from_class ( model_cls ) properties_md = ContextMetadataModel . from_class ( model_cls ) return MetadataModelMetadata ( type_name = model_cls . _metadata_key , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" ) def create_fields_table ( self , show_header : bool = True , show_required : bool = True ) -> Table : type_cls = self . python_class . get_class () fields_table = Table ( show_header = show_header , box = box . SIMPLE ) fields_table . add_column ( \"Field name\" , style = \"i\" ) fields_table . add_column ( \"Type\" ) if show_required : fields_table . add_column ( \"Required\" ) fields_table . add_column ( \"Description\" ) for field_name , details in type_cls . __fields__ . items (): field_type = type_cls . schema ()[ \"properties\" ][ field_name ][ \"type\" ] info = details . field_info . description if show_required : req = \"yes\" if details . required else \"no\" fields_table . add_row ( field_name , field_type , req , info ) else : fields_table . add_row ( field_name , field_type , info ) return fields_table def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_schema = config . get ( \"display_schema\" , False ) include_doc = config . get ( \"include_doc\" , True ) table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) fields_table = self . create_fields_table () table . add_row ( \"Fields\" , fields_table ) if include_schema : json_str = Syntax ( self . python_class . get_class () . schema_json ( indent = 2 ), \"json\" , background_color = \"default\" , ) table . add_row ( \"Json Schema\" , json_str ) return table","title":"MetadataModelMetadata"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.core_models.OriginMetadataModel","text":"Source code in kiara/metadata/core_models.py class OriginMetadataModel ( MetadataModel ): _metadata_key = \"origin\" @classmethod def from_class ( cls , item_cls : typing . Type ): data = get_metadata_for_python_module_or_class ( item_cls ) # type: ignore merged = merge_dicts ( * data ) return cls . parse_obj ( merged ) authors : typing . List [ AuthorModel ] = Field ( description = \"The authors/creators of this item.\" , default_factory = list ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) authors = [] for author in reversed ( self . authors ): if author . email : authors . append ( f \" { author . name } ( { author . email } )\" ) else : authors . append ( author . name ) table . add_row ( \"Authors\" , \" \\n \" . join ( authors )) return table","title":"OriginMetadataModel"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.core_models.PythonClassMetadata","text":"Python class and module information. Source code in kiara/metadata/core_models.py class PythonClassMetadata ( MetadataModel ): \"\"\"Python class and module information.\"\"\" _metadata_key : typing . ClassVar [ str ] = \"python_class\" @classmethod def from_class ( cls , item_cls : typing . Type ): conf : typing . Dict [ str , typing . Any ] = { \"class_name\" : item_cls . __name__ , \"module_name\" : item_cls . __module__ , \"full_name\" : f \" { item_cls . __module__ } . { item_cls . __name__ } \" , } return PythonClassMetadata ( ** conf ) class_name : str = Field ( description = \"The name of the Python class.\" ) module_name : str = Field ( description = \"The name of the Python module this class lives in.\" ) full_name : str = Field ( description = \"The full class namespace.\" ) def get_class ( self ) -> typing . Type : m = self . get_module () return getattr ( m , self . class_name ) def get_module ( self ) -> ModuleType : m = importlib . import_module ( self . module_name ) return m","title":"PythonClassMetadata"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.data","text":"","title":"data"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.data.DeserializeConfig","text":"Source code in kiara/metadata/data.py class DeserializeConfig ( ModuleConfig ): # value_id: str = Field(description=\"The id of the value.\") serialization_type : str = Field ( description = \"The serialization type.\" ) input : typing . Any = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) output_name : str = Field ( description = \"The name of the output field for the value.\" )","title":"DeserializeConfig"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.data.LoadConfig","text":"Source code in kiara/metadata/data.py class LoadConfig ( ModuleConfig ): value_id : str = Field ( description = \"The id of the value.\" ) base_path_input_name : typing . Optional [ str ] = Field ( description = \"The name of the input that stores the base_path where the value is saved.\" , default = None , ) inputs : typing . Dict [ str , typing . Any ] = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) output_name : str = Field ( description = \"The name of the output field for the value.\" )","title":"LoadConfig"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.data.SaveConfig","text":"Source code in kiara/metadata/data.py class SaveConfig ( ModuleConfig ): inputs : typing . Dict [ str , typing . Any ] = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) load_config_output : str = Field ( description = \"The output name that will contain the load config output value.\" )","title":"SaveConfig"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.module_models","text":"","title":"module_models"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.module_models.KiaraModuleConfigMetadata","text":"Source code in kiara/metadata/module_models.py class KiaraModuleConfigMetadata ( MetadataModel ): @classmethod def from_config_class ( cls , config_cls : typing . Type [ ModuleTypeConfigSchema ], remove_pipeline_config : bool = False , ): flat_models = get_flat_models_from_model ( config_cls ) model_name_map = get_model_name_map ( flat_models ) m_schema , _ , _ = model_process_schema ( config_cls , model_name_map = model_name_map ) fields = m_schema [ \"properties\" ] config_values = {} for field_name , details in fields . items (): if remove_pipeline_config and field_name in [ \"steps\" , \"input_aliases\" , \"output_aliases\" , \"doc\" , ]: continue type_str = \"-- n/a --\" if \"type\" in details . keys (): type_str = details [ \"type\" ] desc = details . get ( \"description\" , DEFAULT_NO_DESC_VALUE ) default = config_cls . __fields__ [ field_name ] . default default = config_cls . __fields__ [ field_name ] . default if default is None : if callable ( config_cls . __fields__ [ field_name ] . default_factory ): default = config_cls . __fields__ [ field_name ] . default_factory () # type: ignore req = config_cls . __fields__ [ field_name ] . required config_values [ field_name ] = ValueTypeAndDescription ( description = desc , type = type_str , value_default = default , required = req ) python_cls = PythonClassMetadata . from_class ( config_cls ) return KiaraModuleConfigMetadata ( python_class = python_cls , config_values = config_values ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this configuration.\" ) config_values : typing . Dict [ str , ValueTypeAndDescription ] = Field ( description = \"The available configuration values.\" )","title":"KiaraModuleConfigMetadata"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.module_models.KiaraModuleInstanceMetadata","text":"Source code in kiara/metadata/module_models.py class KiaraModuleInstanceMetadata ( MetadataModel ): @classmethod def from_module_obj ( cls , obj : \"KiaraModule\" ): config = obj . config . dict () for x in [ \"steps\" , \"input_aliases\" , \"output_aliases\" , \"module_type_name\" , \"doc\" , \"metadata\" , ]: config . pop ( x , None ) type_metadata = KiaraModuleTypeMetadata . from_module_class ( obj . __class__ ) result = KiaraModuleInstanceMetadata ( type_metadata = type_metadata , config = config , inputs_schema = obj . input_schemas , outputs_schema = obj . output_schemas , ) return result type_metadata : KiaraModuleTypeMetadata = Field ( description = \"Metadata for the module type of this instance.\" ) config : typing . Dict [ str , typing . Any ] = Field ( description = \"Configuration that was used to create this module instance.\" ) inputs_schema : typing . Dict [ str , ValueSchema ] = Field ( description = \"The schema for the module inputs.\" ) outputs_schema : typing . Dict [ str , ValueSchema ] = Field ( description = \"The schema for the module outputs.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_desc = config . get ( \"include_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if include_desc : table . add_row ( \"Description\" , self . type_metadata . documentation . description ) table . add_row ( \"Origin\" , self . type_metadata . origin . create_renderable ()) table . add_row ( \"Type context\" , self . type_metadata . context . create_renderable ()) table . add_row ( \"Python class\" , self . type_metadata . python_class . create_renderable () ) conf = Syntax ( json . dumps ( self . config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Configuration\" , conf ) constants = self . config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . inputs_schema , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . outputs_schema , ) table . add_row ( \"Outputs\" , outputs_table ) # table.add_row(\"Source code\", self.type_metadata.process_src) return table","title":"KiaraModuleInstanceMetadata"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.module_models.KiaraModuleTypeMetadata","text":"Source code in kiara/metadata/module_models.py class KiaraModuleTypeMetadata ( MetadataModel ): @classmethod def from_module_class ( cls , module_cls : typing . Type [ \"KiaraModule\" ]): module_attrs = cls . extract_module_attributes ( module_cls = module_cls ) return cls ( ** module_attrs ) @classmethod def extract_module_attributes ( self , module_cls : typing . Type [ \"KiaraModule\" ] ) -> typing . Dict [ str , typing . Any ]: if not hasattr ( module_cls , \"process\" ): raise Exception ( f \"Module class ' { module_cls } ' misses 'process' method.\" ) proc_src = textwrap . dedent ( inspect . getsource ( module_cls . process )) # type: ignore origin_md = OriginMetadataModel . from_class ( module_cls ) doc = DocumentationMetadataModel . from_class_doc ( module_cls ) python_class = PythonClassMetadata . from_class ( module_cls ) properties_md = ContextMetadataModel . from_class ( module_cls ) is_pipeline = module_cls . is_pipeline () doc_url = properties_md . get_url_for_reference ( \"documentation\" ) if doc_url : class_doc = calculate_class_doc_url ( doc_url , module_cls . _module_type_id , pipeline = is_pipeline ) # type: ignore properties_md . add_reference ( \"module_doc\" , class_doc , \"A link to the published, auto-generated module documentation.\" , ) if not is_pipeline : repo_url = properties_md . get_url_for_reference ( \"source_repo\" ) if repo_url is not None : src_url = calculate_class_source_url ( repo_url , python_class ) properties_md . add_reference ( \"source_url\" , src_url , \"A link to the published source file that contains this module.\" , ) config = KiaraModuleConfigMetadata . from_config_class ( module_cls . _config_cls ) pipeline_config = None if module_cls . _module_type_id != \"pipeline\" and is_pipeline : # type: ignore pipeline_config = module_cls . _base_pipeline_config # type: ignore return { \"type_name\" : module_cls . _module_type_name , # type: ignore \"type_id\" : module_cls . _module_type_id , # type: ignore \"documentation\" : doc , \"origin\" : origin_md , \"context\" : properties_md , \"python_class\" : python_class , \"config\" : config , \"is_pipeline\" : is_pipeline , \"pipeline_config\" : pipeline_config , \"process_src\" : proc_src , } type_name : str = Field ( description = \"The registered name for this module type.\" ) type_id : str = Field ( description = \"The full type id.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the module.\" ) origin : OriginMetadataModel = Field ( description = \"Information about authorship for the module type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this module (description, tags, labels, references, ...).\" ) python_class : PythonClassMetadata = Field ( description = \"Information about the Python class for this module type.\" ) config : KiaraModuleConfigMetadata = Field ( description = \"Details on how this module type can be configured.\" ) is_pipeline : bool = Field ( description = \"Whether the module type is a pipeline, or a core module.\" ) pipeline_config : typing . Optional [ PipelineConfig ] = Field ( description = \"If this module is a pipeline, this field contains the pipeline configuration.\" , default_factory = None , ) process_src : str = Field ( description = \"The source code of the process method of the module.\" ) @validator ( \"documentation\" , pre = True ) def validate_doc ( cls , value ): return DocumentationMetadataModel . create ( value ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_config_schema = config . get ( \"include_config_schema\" , True ) include_src = config . get ( \"include_src\" , True ) include_doc = config . get ( \"include_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , padding = ( 0 , 0 , 0 , 0 )) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) if include_config_schema : if self . config : config_cls = self . config . python_class . get_class () table . add_row ( \"Module config\" , create_table_from_base_model ( config_cls )) table . add_row ( \"Module config\" , \"-- no config --\" ) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) if include_src : if self . is_pipeline : json_str = self . pipeline_config . json ( indent = 2 ) # type: ignore _config : Syntax = Syntax ( json_str , \"json\" , background_color = \"default\" ) table . add_row ( \"Pipeline config\" , Panel ( _config , box = box . HORIZONTALS )) else : _config = Syntax ( self . process_src , \"python\" , background_color = \"default\" ) table . add_row ( \"Processing source code\" , Panel ( _config , box = box . HORIZONTALS ) ) return table","title":"KiaraModuleTypeMetadata"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.operation_models","text":"","title":"operation_models"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.operation_models.OperationsMetadata","text":"Source code in kiara/metadata/operation_models.py class OperationsMetadata ( MetadataModel ): @classmethod def create_all ( cls , kiara : \"Kiara\" ) -> typing . Dict [ str , \"OperationsMetadata\" ]: op_types = kiara . operation_mgmt . operation_types result = {} for op_type in op_types : op_type_cls = kiara . operation_mgmt . get_operations ( op_type ) result [ op_type ] = cls . from_operations_class ( op_type_cls . __class__ ) return result @classmethod def from_operations_class ( cls , operation_type_cls : typing . Type [ \"OperationType\" ] ) -> \"OperationsMetadata\" : origin_md = OriginMetadataModel . from_class ( operation_type_cls ) doc = DocumentationMetadataModel . from_class_doc ( operation_type_cls ) python_class = PythonClassMetadata . from_class ( operation_type_cls ) properties_md = ContextMetadataModel . from_class ( operation_type_cls ) return OperationsMetadata . construct ( type_name = operation_type_cls . _operation_type_name , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" )","title":"OperationsMetadata"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.type_models","text":"","title":"type_models"},{"location":"reference/kiara/metadata/__init__/#kiara.metadata.type_models.ValueTypeMetadata","text":"Source code in kiara/metadata/type_models.py class ValueTypeMetadata ( MetadataModel ): @classmethod def create_all ( cls , kiara : \"Kiara\" ) -> typing . Dict [ str , \"ValueTypeMetadata\" ]: result = {} for vt in kiara . value_types : t_cls = kiara . get_value_type_cls ( vt ) result [ vt ] = cls . from_value_type_class ( t_cls ) return result @classmethod def from_value_type_class ( cls , value_type_cls : typing . Type [ \"ValueType\" ]): origin_md = OriginMetadataModel . from_class ( value_type_cls ) doc = DocumentationMetadataModel . from_class_doc ( value_type_cls ) python_class = PythonClassMetadata . from_class ( value_type_cls ) properties_md = ContextMetadataModel . from_class ( value_type_cls ) return ValueTypeMetadata ( type_name = value_type_cls . _value_type_name , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" )","title":"ValueTypeMetadata"},{"location":"reference/kiara/metadata/core_models/","text":"AuthorModel ( BaseModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class AuthorModel ( BaseModel ): name : str = Field ( description = \"The full name of the author.\" ) email : typing . Optional [ EmailStr ] = Field ( description = \"The email address of the author\" , default = None ) email : EmailStr pydantic-field \u00b6 The email address of the author name : str pydantic-field required \u00b6 The full name of the author. ContextMetadataModel ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class ContextMetadataModel ( MetadataModel ): @classmethod def from_class ( cls , item_cls : typing . Type ): data = get_metadata_for_python_module_or_class ( item_cls ) # type: ignore merged = merge_dicts ( * data ) return cls . parse_obj ( merged ) _metadata_key = \"properties\" references : typing . Dict [ str , LinkModel ] = Field ( description = \"References for the item.\" , default_factory = dict ) tags : typing . Set [ str ] = Field ( description = \"A list of tags for the item.\" , default_factory = set ) labels : typing . Dict [ str , str ] = Field ( description = \"A list of labels for the item.\" , default_factory = list ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . tags : table . add_row ( \"Tags\" , \", \" . join ( self . tags )) if self . labels : labels = [] for k , v in self . labels . items (): labels . append ( f \"[i] { k } [/i]: { v } \" ) table . add_row ( \"Labels\" , \" \\n \" . join ( labels )) if self . references : references = [] for _k , _v in self . references . items (): link = f \"[link= { _v . url } ] { _v . url } [/link]\" references . append ( f \"[i] { _k } [/i]: { link } \" ) table . add_row ( \"References\" , \" \\n \" . join ( references )) return table def add_reference ( self , ref_type : str , url : str , desc : typing . Optional [ str ] = None , force : bool = False , ): if ref_type in self . references . keys () and not force : raise Exception ( f \"Reference of type ' { ref_type } ' already present.\" ) link = LinkModel ( url = url , desc = desc ) self . references [ ref_type ] = link def get_url_for_reference ( self , ref : str ) -> typing . Optional [ str ]: link = self . references . get ( ref , None ) if not link : return None return link . url labels : Dict [ str , str ] pydantic-field \u00b6 A list of labels for the item. references : Dict [ str , kiara . metadata . core_models . LinkModel ] pydantic-field \u00b6 References for the item. tags : Set [ str ] pydantic-field \u00b6 A list of tags for the item. DocumentationMetadataModel ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class DocumentationMetadataModel ( MetadataModel ): _metadata_key = \"documentation\" @classmethod def from_class_doc ( cls , item_cls : typing . Type ): doc = item_cls . __doc__ if not doc : doc = DEFAULT_NO_DESC_VALUE doc = inspect . cleandoc ( doc ) return cls . from_string ( doc ) @classmethod def from_string ( cls , doc : typing . Optional [ str ]): if not doc : doc = DEFAULT_NO_DESC_VALUE if \" \\n \" in doc : desc , doc = doc . split ( \" \\n \" , maxsplit = 1 ) else : desc = doc doc = None if doc : doc = doc . strip () return cls ( description = desc . strip (), doc = doc ) @classmethod def from_dict ( cls , data : typing . Mapping ): doc = data . get ( \"doc\" , None ) desc = data . get ( \"description\" , None ) if desc is None : desc = data . get ( \"desc\" , None ) if not doc and not desc : return cls . from_string ( DEFAULT_NO_DESC_VALUE ) elif doc and not desc : return cls . from_string ( doc ) elif desc and not doc : return cls . from_string ( desc ) else : return DocumentationMetadataModel ( description = desc , doc = doc ) @classmethod def create ( cls , item : typing . Any ): if not item : return cls . from_string ( DEFAULT_NO_DESC_VALUE ) elif isinstance ( item , DocumentationMetadataModel ): return item elif isinstance ( item , typing . Mapping ): return cls . from_dict ( item ) if isinstance ( item , type ): return cls . from_class_doc ( item ) elif isinstance ( item , str ): return cls . from_string ( item ) else : raise TypeError ( f \"Can't create documentation from type ' { type ( item ) } '.\" ) description : str = Field ( description = \"Short description of the item.\" , default = DEFAULT_NO_DESC_VALUE ) doc : typing . Optional [ str ] = Field ( description = \"Detailed documentation of the item (in markdown).\" , default = None ) @property def full_doc ( self ): if self . doc : return f \" { self . description } \\n\\n { self . doc } \" else : return self . description def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return Markdown ( self . full_doc ) description : str pydantic-field \u00b6 Short description of the item. doc : str pydantic-field \u00b6 Detailed documentation of the item (in markdown). LinkModel ( BaseModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class LinkModel ( BaseModel ): url : AnyUrl = Field ( description = \"The url.\" ) desc : typing . Optional [ str ] = Field ( description = \"A short description of the link content.\" , default = DEFAULT_NO_DESC_VALUE , ) desc : str pydantic-field \u00b6 A short description of the link content. url : AnyUrl pydantic-field required \u00b6 The url. MetadataModelMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class MetadataModelMetadata ( MetadataModel ): @classmethod def from_model_class ( cls , model_cls : typing . Type [ MetadataModel ]): origin_md = OriginMetadataModel . from_class ( model_cls ) doc = DocumentationMetadataModel . from_class_doc ( model_cls ) python_class = PythonClassMetadata . from_class ( model_cls ) properties_md = ContextMetadataModel . from_class ( model_cls ) return MetadataModelMetadata ( type_name = model_cls . _metadata_key , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" ) def create_fields_table ( self , show_header : bool = True , show_required : bool = True ) -> Table : type_cls = self . python_class . get_class () fields_table = Table ( show_header = show_header , box = box . SIMPLE ) fields_table . add_column ( \"Field name\" , style = \"i\" ) fields_table . add_column ( \"Type\" ) if show_required : fields_table . add_column ( \"Required\" ) fields_table . add_column ( \"Description\" ) for field_name , details in type_cls . __fields__ . items (): field_type = type_cls . schema ()[ \"properties\" ][ field_name ][ \"type\" ] info = details . field_info . description if show_required : req = \"yes\" if details . required else \"no\" fields_table . add_row ( field_name , field_type , req , info ) else : fields_table . add_row ( field_name , field_type , info ) return fields_table def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_schema = config . get ( \"display_schema\" , False ) include_doc = config . get ( \"include_doc\" , True ) table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) fields_table = self . create_fields_table () table . add_row ( \"Fields\" , fields_table ) if include_schema : json_str = Syntax ( self . python_class . get_class () . schema_json ( indent = 2 ), \"json\" , background_color = \"default\" , ) table . add_row ( \"Json Schema\" , json_str ) return table context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type. documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about the creator of this value type. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this value type. type_name : str pydantic-field required \u00b6 The registered name for this value type. OriginMetadataModel ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/core_models.py class OriginMetadataModel ( MetadataModel ): _metadata_key = \"origin\" @classmethod def from_class ( cls , item_cls : typing . Type ): data = get_metadata_for_python_module_or_class ( item_cls ) # type: ignore merged = merge_dicts ( * data ) return cls . parse_obj ( merged ) authors : typing . List [ AuthorModel ] = Field ( description = \"The authors/creators of this item.\" , default_factory = list ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) authors = [] for author in reversed ( self . authors ): if author . email : authors . append ( f \" { author . name } ( { author . email } )\" ) else : authors . append ( author . name ) table . add_row ( \"Authors\" , \" \\n \" . join ( authors )) return table authors : List [ kiara . metadata . core_models . AuthorModel ] pydantic-field \u00b6 The authors/creators of this item. PythonClassMetadata ( MetadataModel ) pydantic-model \u00b6 Python class and module information. Source code in kiara/metadata/core_models.py class PythonClassMetadata ( MetadataModel ): \"\"\"Python class and module information.\"\"\" _metadata_key : typing . ClassVar [ str ] = \"python_class\" @classmethod def from_class ( cls , item_cls : typing . Type ): conf : typing . Dict [ str , typing . Any ] = { \"class_name\" : item_cls . __name__ , \"module_name\" : item_cls . __module__ , \"full_name\" : f \" { item_cls . __module__ } . { item_cls . __name__ } \" , } return PythonClassMetadata ( ** conf ) class_name : str = Field ( description = \"The name of the Python class.\" ) module_name : str = Field ( description = \"The name of the Python module this class lives in.\" ) full_name : str = Field ( description = \"The full class namespace.\" ) def get_class ( self ) -> typing . Type : m = self . get_module () return getattr ( m , self . class_name ) def get_module ( self ) -> ModuleType : m = importlib . import_module ( self . module_name ) return m class_name : str pydantic-field required \u00b6 The name of the Python class. full_name : str pydantic-field required \u00b6 The full class namespace. module_name : str pydantic-field required \u00b6 The name of the Python module this class lives in.","title":"core_models"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.AuthorModel","text":"Source code in kiara/metadata/core_models.py class AuthorModel ( BaseModel ): name : str = Field ( description = \"The full name of the author.\" ) email : typing . Optional [ EmailStr ] = Field ( description = \"The email address of the author\" , default = None )","title":"AuthorModel"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.AuthorModel.email","text":"The email address of the author","title":"email"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.AuthorModel.name","text":"The full name of the author.","title":"name"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.ContextMetadataModel","text":"Source code in kiara/metadata/core_models.py class ContextMetadataModel ( MetadataModel ): @classmethod def from_class ( cls , item_cls : typing . Type ): data = get_metadata_for_python_module_or_class ( item_cls ) # type: ignore merged = merge_dicts ( * data ) return cls . parse_obj ( merged ) _metadata_key = \"properties\" references : typing . Dict [ str , LinkModel ] = Field ( description = \"References for the item.\" , default_factory = dict ) tags : typing . Set [ str ] = Field ( description = \"A list of tags for the item.\" , default_factory = set ) labels : typing . Dict [ str , str ] = Field ( description = \"A list of labels for the item.\" , default_factory = list ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . tags : table . add_row ( \"Tags\" , \", \" . join ( self . tags )) if self . labels : labels = [] for k , v in self . labels . items (): labels . append ( f \"[i] { k } [/i]: { v } \" ) table . add_row ( \"Labels\" , \" \\n \" . join ( labels )) if self . references : references = [] for _k , _v in self . references . items (): link = f \"[link= { _v . url } ] { _v . url } [/link]\" references . append ( f \"[i] { _k } [/i]: { link } \" ) table . add_row ( \"References\" , \" \\n \" . join ( references )) return table def add_reference ( self , ref_type : str , url : str , desc : typing . Optional [ str ] = None , force : bool = False , ): if ref_type in self . references . keys () and not force : raise Exception ( f \"Reference of type ' { ref_type } ' already present.\" ) link = LinkModel ( url = url , desc = desc ) self . references [ ref_type ] = link def get_url_for_reference ( self , ref : str ) -> typing . Optional [ str ]: link = self . references . get ( ref , None ) if not link : return None return link . url","title":"ContextMetadataModel"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.ContextMetadataModel.labels","text":"A list of labels for the item.","title":"labels"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.ContextMetadataModel.references","text":"References for the item.","title":"references"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.ContextMetadataModel.tags","text":"A list of tags for the item.","title":"tags"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.DocumentationMetadataModel","text":"Source code in kiara/metadata/core_models.py class DocumentationMetadataModel ( MetadataModel ): _metadata_key = \"documentation\" @classmethod def from_class_doc ( cls , item_cls : typing . Type ): doc = item_cls . __doc__ if not doc : doc = DEFAULT_NO_DESC_VALUE doc = inspect . cleandoc ( doc ) return cls . from_string ( doc ) @classmethod def from_string ( cls , doc : typing . Optional [ str ]): if not doc : doc = DEFAULT_NO_DESC_VALUE if \" \\n \" in doc : desc , doc = doc . split ( \" \\n \" , maxsplit = 1 ) else : desc = doc doc = None if doc : doc = doc . strip () return cls ( description = desc . strip (), doc = doc ) @classmethod def from_dict ( cls , data : typing . Mapping ): doc = data . get ( \"doc\" , None ) desc = data . get ( \"description\" , None ) if desc is None : desc = data . get ( \"desc\" , None ) if not doc and not desc : return cls . from_string ( DEFAULT_NO_DESC_VALUE ) elif doc and not desc : return cls . from_string ( doc ) elif desc and not doc : return cls . from_string ( desc ) else : return DocumentationMetadataModel ( description = desc , doc = doc ) @classmethod def create ( cls , item : typing . Any ): if not item : return cls . from_string ( DEFAULT_NO_DESC_VALUE ) elif isinstance ( item , DocumentationMetadataModel ): return item elif isinstance ( item , typing . Mapping ): return cls . from_dict ( item ) if isinstance ( item , type ): return cls . from_class_doc ( item ) elif isinstance ( item , str ): return cls . from_string ( item ) else : raise TypeError ( f \"Can't create documentation from type ' { type ( item ) } '.\" ) description : str = Field ( description = \"Short description of the item.\" , default = DEFAULT_NO_DESC_VALUE ) doc : typing . Optional [ str ] = Field ( description = \"Detailed documentation of the item (in markdown).\" , default = None ) @property def full_doc ( self ): if self . doc : return f \" { self . description } \\n\\n { self . doc } \" else : return self . description def create_renderable ( self , ** config : typing . Any ) -> RenderableType : return Markdown ( self . full_doc )","title":"DocumentationMetadataModel"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.DocumentationMetadataModel.description","text":"Short description of the item.","title":"description"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.DocumentationMetadataModel.doc","text":"Detailed documentation of the item (in markdown).","title":"doc"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.LinkModel","text":"Source code in kiara/metadata/core_models.py class LinkModel ( BaseModel ): url : AnyUrl = Field ( description = \"The url.\" ) desc : typing . Optional [ str ] = Field ( description = \"A short description of the link content.\" , default = DEFAULT_NO_DESC_VALUE , )","title":"LinkModel"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.LinkModel.desc","text":"A short description of the link content.","title":"desc"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.LinkModel.url","text":"The url.","title":"url"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.MetadataModelMetadata","text":"Source code in kiara/metadata/core_models.py class MetadataModelMetadata ( MetadataModel ): @classmethod def from_model_class ( cls , model_cls : typing . Type [ MetadataModel ]): origin_md = OriginMetadataModel . from_class ( model_cls ) doc = DocumentationMetadataModel . from_class_doc ( model_cls ) python_class = PythonClassMetadata . from_class ( model_cls ) properties_md = ContextMetadataModel . from_class ( model_cls ) return MetadataModelMetadata ( type_name = model_cls . _metadata_key , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" ) def create_fields_table ( self , show_header : bool = True , show_required : bool = True ) -> Table : type_cls = self . python_class . get_class () fields_table = Table ( show_header = show_header , box = box . SIMPLE ) fields_table . add_column ( \"Field name\" , style = \"i\" ) fields_table . add_column ( \"Type\" ) if show_required : fields_table . add_column ( \"Required\" ) fields_table . add_column ( \"Description\" ) for field_name , details in type_cls . __fields__ . items (): field_type = type_cls . schema ()[ \"properties\" ][ field_name ][ \"type\" ] info = details . field_info . description if show_required : req = \"yes\" if details . required else \"no\" fields_table . add_row ( field_name , field_type , req , info ) else : fields_table . add_row ( field_name , field_type , info ) return fields_table def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_schema = config . get ( \"display_schema\" , False ) include_doc = config . get ( \"include_doc\" , True ) table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) fields_table = self . create_fields_table () table . add_row ( \"Fields\" , fields_table ) if include_schema : json_str = Syntax ( self . python_class . get_class () . schema_json ( indent = 2 ), \"json\" , background_color = \"default\" , ) table . add_row ( \"Json Schema\" , json_str ) return table","title":"MetadataModelMetadata"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.MetadataModelMetadata.context","text":"Generic properties of this value type.","title":"context"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.MetadataModelMetadata.documentation","text":"Documentation for the value type.","title":"documentation"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.MetadataModelMetadata.origin","text":"Information about the creator of this value type.","title":"origin"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.MetadataModelMetadata.python_class","text":"The Python class for this value type.","title":"python_class"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.MetadataModelMetadata.type_name","text":"The registered name for this value type.","title":"type_name"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.OriginMetadataModel","text":"Source code in kiara/metadata/core_models.py class OriginMetadataModel ( MetadataModel ): _metadata_key = \"origin\" @classmethod def from_class ( cls , item_cls : typing . Type ): data = get_metadata_for_python_module_or_class ( item_cls ) # type: ignore merged = merge_dicts ( * data ) return cls . parse_obj ( merged ) authors : typing . List [ AuthorModel ] = Field ( description = \"The authors/creators of this item.\" , default_factory = list ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : table = Table ( show_header = False , box = box . SIMPLE ) table . add_column ( \"Key\" , style = \"i\" ) table . add_column ( \"Value\" ) authors = [] for author in reversed ( self . authors ): if author . email : authors . append ( f \" { author . name } ( { author . email } )\" ) else : authors . append ( author . name ) table . add_row ( \"Authors\" , \" \\n \" . join ( authors )) return table","title":"OriginMetadataModel"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.OriginMetadataModel.authors","text":"The authors/creators of this item.","title":"authors"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.PythonClassMetadata","text":"Python class and module information. Source code in kiara/metadata/core_models.py class PythonClassMetadata ( MetadataModel ): \"\"\"Python class and module information.\"\"\" _metadata_key : typing . ClassVar [ str ] = \"python_class\" @classmethod def from_class ( cls , item_cls : typing . Type ): conf : typing . Dict [ str , typing . Any ] = { \"class_name\" : item_cls . __name__ , \"module_name\" : item_cls . __module__ , \"full_name\" : f \" { item_cls . __module__ } . { item_cls . __name__ } \" , } return PythonClassMetadata ( ** conf ) class_name : str = Field ( description = \"The name of the Python class.\" ) module_name : str = Field ( description = \"The name of the Python module this class lives in.\" ) full_name : str = Field ( description = \"The full class namespace.\" ) def get_class ( self ) -> typing . Type : m = self . get_module () return getattr ( m , self . class_name ) def get_module ( self ) -> ModuleType : m = importlib . import_module ( self . module_name ) return m","title":"PythonClassMetadata"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.PythonClassMetadata.class_name","text":"The name of the Python class.","title":"class_name"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.PythonClassMetadata.full_name","text":"The full class namespace.","title":"full_name"},{"location":"reference/kiara/metadata/core_models/#kiara.metadata.core_models.PythonClassMetadata.module_name","text":"The name of the Python module this class lives in.","title":"module_name"},{"location":"reference/kiara/metadata/data/","text":"DeserializeConfig ( ModuleConfig ) pydantic-model \u00b6 Source code in kiara/metadata/data.py class DeserializeConfig ( ModuleConfig ): # value_id: str = Field(description=\"The id of the value.\") serialization_type : str = Field ( description = \"The serialization type.\" ) input : typing . Any = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) output_name : str = Field ( description = \"The name of the output field for the value.\" ) input : Any pydantic-field \u00b6 The inputs to use when running this module. output_name : str pydantic-field required \u00b6 The name of the output field for the value. serialization_type : str pydantic-field required \u00b6 The serialization type. LoadConfig ( ModuleConfig ) pydantic-model \u00b6 Source code in kiara/metadata/data.py class LoadConfig ( ModuleConfig ): value_id : str = Field ( description = \"The id of the value.\" ) base_path_input_name : typing . Optional [ str ] = Field ( description = \"The name of the input that stores the base_path where the value is saved.\" , default = None , ) inputs : typing . Dict [ str , typing . Any ] = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) output_name : str = Field ( description = \"The name of the output field for the value.\" ) base_path_input_name : str pydantic-field \u00b6 The name of the input that stores the base_path where the value is saved. inputs : Dict [ str , Any ] pydantic-field \u00b6 The inputs to use when running this module. output_name : str pydantic-field required \u00b6 The name of the output field for the value. value_id : str pydantic-field required \u00b6 The id of the value. SaveConfig ( ModuleConfig ) pydantic-model \u00b6 Source code in kiara/metadata/data.py class SaveConfig ( ModuleConfig ): inputs : typing . Dict [ str , typing . Any ] = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) load_config_output : str = Field ( description = \"The output name that will contain the load config output value.\" ) inputs : Dict [ str , Any ] pydantic-field \u00b6 The inputs to use when running this module. load_config_output : str pydantic-field required \u00b6 The output name that will contain the load config output value.","title":"data"},{"location":"reference/kiara/metadata/data/#kiara.metadata.data.DeserializeConfig","text":"Source code in kiara/metadata/data.py class DeserializeConfig ( ModuleConfig ): # value_id: str = Field(description=\"The id of the value.\") serialization_type : str = Field ( description = \"The serialization type.\" ) input : typing . Any = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) output_name : str = Field ( description = \"The name of the output field for the value.\" )","title":"DeserializeConfig"},{"location":"reference/kiara/metadata/data/#kiara.metadata.data.DeserializeConfig.input","text":"The inputs to use when running this module.","title":"input"},{"location":"reference/kiara/metadata/data/#kiara.metadata.data.DeserializeConfig.output_name","text":"The name of the output field for the value.","title":"output_name"},{"location":"reference/kiara/metadata/data/#kiara.metadata.data.DeserializeConfig.serialization_type","text":"The serialization type.","title":"serialization_type"},{"location":"reference/kiara/metadata/data/#kiara.metadata.data.LoadConfig","text":"Source code in kiara/metadata/data.py class LoadConfig ( ModuleConfig ): value_id : str = Field ( description = \"The id of the value.\" ) base_path_input_name : typing . Optional [ str ] = Field ( description = \"The name of the input that stores the base_path where the value is saved.\" , default = None , ) inputs : typing . Dict [ str , typing . Any ] = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) output_name : str = Field ( description = \"The name of the output field for the value.\" )","title":"LoadConfig"},{"location":"reference/kiara/metadata/data/#kiara.metadata.data.LoadConfig.base_path_input_name","text":"The name of the input that stores the base_path where the value is saved.","title":"base_path_input_name"},{"location":"reference/kiara/metadata/data/#kiara.metadata.data.LoadConfig.inputs","text":"The inputs to use when running this module.","title":"inputs"},{"location":"reference/kiara/metadata/data/#kiara.metadata.data.LoadConfig.output_name","text":"The name of the output field for the value.","title":"output_name"},{"location":"reference/kiara/metadata/data/#kiara.metadata.data.LoadConfig.value_id","text":"The id of the value.","title":"value_id"},{"location":"reference/kiara/metadata/data/#kiara.metadata.data.SaveConfig","text":"Source code in kiara/metadata/data.py class SaveConfig ( ModuleConfig ): inputs : typing . Dict [ str , typing . Any ] = Field ( description = \"The inputs to use when running this module.\" , default_factory = dict ) load_config_output : str = Field ( description = \"The output name that will contain the load config output value.\" )","title":"SaveConfig"},{"location":"reference/kiara/metadata/data/#kiara.metadata.data.SaveConfig.inputs","text":"The inputs to use when running this module.","title":"inputs"},{"location":"reference/kiara/metadata/data/#kiara.metadata.data.SaveConfig.load_config_output","text":"The output name that will contain the load config output value.","title":"load_config_output"},{"location":"reference/kiara/metadata/mgmt/","text":"","title":"mgmt"},{"location":"reference/kiara/metadata/module_models/","text":"KiaraModuleConfigMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/module_models.py class KiaraModuleConfigMetadata ( MetadataModel ): @classmethod def from_config_class ( cls , config_cls : typing . Type [ ModuleTypeConfigSchema ], remove_pipeline_config : bool = False , ): flat_models = get_flat_models_from_model ( config_cls ) model_name_map = get_model_name_map ( flat_models ) m_schema , _ , _ = model_process_schema ( config_cls , model_name_map = model_name_map ) fields = m_schema [ \"properties\" ] config_values = {} for field_name , details in fields . items (): if remove_pipeline_config and field_name in [ \"steps\" , \"input_aliases\" , \"output_aliases\" , \"doc\" , ]: continue type_str = \"-- n/a --\" if \"type\" in details . keys (): type_str = details [ \"type\" ] desc = details . get ( \"description\" , DEFAULT_NO_DESC_VALUE ) default = config_cls . __fields__ [ field_name ] . default default = config_cls . __fields__ [ field_name ] . default if default is None : if callable ( config_cls . __fields__ [ field_name ] . default_factory ): default = config_cls . __fields__ [ field_name ] . default_factory () # type: ignore req = config_cls . __fields__ [ field_name ] . required config_values [ field_name ] = ValueTypeAndDescription ( description = desc , type = type_str , value_default = default , required = req ) python_cls = PythonClassMetadata . from_class ( config_cls ) return KiaraModuleConfigMetadata ( python_class = python_cls , config_values = config_values ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this configuration.\" ) config_values : typing . Dict [ str , ValueTypeAndDescription ] = Field ( description = \"The available configuration values.\" ) config_values : Dict [ str , kiara . metadata . ValueTypeAndDescription ] pydantic-field required \u00b6 The available configuration values. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this configuration. KiaraModuleInstanceMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/module_models.py class KiaraModuleInstanceMetadata ( MetadataModel ): @classmethod def from_module_obj ( cls , obj : \"KiaraModule\" ): config = obj . config . dict () for x in [ \"steps\" , \"input_aliases\" , \"output_aliases\" , \"module_type_name\" , \"doc\" , \"metadata\" , ]: config . pop ( x , None ) type_metadata = KiaraModuleTypeMetadata . from_module_class ( obj . __class__ ) result = KiaraModuleInstanceMetadata ( type_metadata = type_metadata , config = config , inputs_schema = obj . input_schemas , outputs_schema = obj . output_schemas , ) return result type_metadata : KiaraModuleTypeMetadata = Field ( description = \"Metadata for the module type of this instance.\" ) config : typing . Dict [ str , typing . Any ] = Field ( description = \"Configuration that was used to create this module instance.\" ) inputs_schema : typing . Dict [ str , ValueSchema ] = Field ( description = \"The schema for the module inputs.\" ) outputs_schema : typing . Dict [ str , ValueSchema ] = Field ( description = \"The schema for the module outputs.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_desc = config . get ( \"include_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if include_desc : table . add_row ( \"Description\" , self . type_metadata . documentation . description ) table . add_row ( \"Origin\" , self . type_metadata . origin . create_renderable ()) table . add_row ( \"Type context\" , self . type_metadata . context . create_renderable ()) table . add_row ( \"Python class\" , self . type_metadata . python_class . create_renderable () ) conf = Syntax ( json . dumps ( self . config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Configuration\" , conf ) constants = self . config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . inputs_schema , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . outputs_schema , ) table . add_row ( \"Outputs\" , outputs_table ) # table.add_row(\"Source code\", self.type_metadata.process_src) return table config : Dict [ str , Any ] pydantic-field required \u00b6 Configuration that was used to create this module instance. inputs_schema : Dict [ str , kiara . data . values . ValueSchema ] pydantic-field required \u00b6 The schema for the module inputs. outputs_schema : Dict [ str , kiara . data . values . ValueSchema ] pydantic-field required \u00b6 The schema for the module outputs. type_metadata : KiaraModuleTypeMetadata pydantic-field required \u00b6 Metadata for the module type of this instance. KiaraModuleTypeMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/module_models.py class KiaraModuleTypeMetadata ( MetadataModel ): @classmethod def from_module_class ( cls , module_cls : typing . Type [ \"KiaraModule\" ]): module_attrs = cls . extract_module_attributes ( module_cls = module_cls ) return cls ( ** module_attrs ) @classmethod def extract_module_attributes ( self , module_cls : typing . Type [ \"KiaraModule\" ] ) -> typing . Dict [ str , typing . Any ]: if not hasattr ( module_cls , \"process\" ): raise Exception ( f \"Module class ' { module_cls } ' misses 'process' method.\" ) proc_src = textwrap . dedent ( inspect . getsource ( module_cls . process )) # type: ignore origin_md = OriginMetadataModel . from_class ( module_cls ) doc = DocumentationMetadataModel . from_class_doc ( module_cls ) python_class = PythonClassMetadata . from_class ( module_cls ) properties_md = ContextMetadataModel . from_class ( module_cls ) is_pipeline = module_cls . is_pipeline () doc_url = properties_md . get_url_for_reference ( \"documentation\" ) if doc_url : class_doc = calculate_class_doc_url ( doc_url , module_cls . _module_type_id , pipeline = is_pipeline ) # type: ignore properties_md . add_reference ( \"module_doc\" , class_doc , \"A link to the published, auto-generated module documentation.\" , ) if not is_pipeline : repo_url = properties_md . get_url_for_reference ( \"source_repo\" ) if repo_url is not None : src_url = calculate_class_source_url ( repo_url , python_class ) properties_md . add_reference ( \"source_url\" , src_url , \"A link to the published source file that contains this module.\" , ) config = KiaraModuleConfigMetadata . from_config_class ( module_cls . _config_cls ) pipeline_config = None if module_cls . _module_type_id != \"pipeline\" and is_pipeline : # type: ignore pipeline_config = module_cls . _base_pipeline_config # type: ignore return { \"type_name\" : module_cls . _module_type_name , # type: ignore \"type_id\" : module_cls . _module_type_id , # type: ignore \"documentation\" : doc , \"origin\" : origin_md , \"context\" : properties_md , \"python_class\" : python_class , \"config\" : config , \"is_pipeline\" : is_pipeline , \"pipeline_config\" : pipeline_config , \"process_src\" : proc_src , } type_name : str = Field ( description = \"The registered name for this module type.\" ) type_id : str = Field ( description = \"The full type id.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the module.\" ) origin : OriginMetadataModel = Field ( description = \"Information about authorship for the module type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this module (description, tags, labels, references, ...).\" ) python_class : PythonClassMetadata = Field ( description = \"Information about the Python class for this module type.\" ) config : KiaraModuleConfigMetadata = Field ( description = \"Details on how this module type can be configured.\" ) is_pipeline : bool = Field ( description = \"Whether the module type is a pipeline, or a core module.\" ) pipeline_config : typing . Optional [ PipelineConfig ] = Field ( description = \"If this module is a pipeline, this field contains the pipeline configuration.\" , default_factory = None , ) process_src : str = Field ( description = \"The source code of the process method of the module.\" ) @validator ( \"documentation\" , pre = True ) def validate_doc ( cls , value ): return DocumentationMetadataModel . create ( value ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_config_schema = config . get ( \"include_config_schema\" , True ) include_src = config . get ( \"include_src\" , True ) include_doc = config . get ( \"include_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , padding = ( 0 , 0 , 0 , 0 )) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) if include_config_schema : if self . config : config_cls = self . config . python_class . get_class () table . add_row ( \"Module config\" , create_table_from_base_model ( config_cls )) table . add_row ( \"Module config\" , \"-- no config --\" ) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) if include_src : if self . is_pipeline : json_str = self . pipeline_config . json ( indent = 2 ) # type: ignore _config : Syntax = Syntax ( json_str , \"json\" , background_color = \"default\" ) table . add_row ( \"Pipeline config\" , Panel ( _config , box = box . HORIZONTALS )) else : _config = Syntax ( self . process_src , \"python\" , background_color = \"default\" ) table . add_row ( \"Processing source code\" , Panel ( _config , box = box . HORIZONTALS ) ) return table config : KiaraModuleConfigMetadata pydantic-field required \u00b6 Details on how this module type can be configured. context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this module (description, tags, labels, references, ...). documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the module. is_pipeline : bool pydantic-field required \u00b6 Whether the module type is a pipeline, or a core module. origin : OriginMetadataModel pydantic-field required \u00b6 Information about authorship for the module type. pipeline_config : PipelineConfig pydantic-field \u00b6 If this module is a pipeline, this field contains the pipeline configuration. process_src : str pydantic-field required \u00b6 The source code of the process method of the module. python_class : PythonClassMetadata pydantic-field required \u00b6 Information about the Python class for this module type. type_id : str pydantic-field required \u00b6 The full type id. type_name : str pydantic-field required \u00b6 The registered name for this module type.","title":"module_models"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleConfigMetadata","text":"Source code in kiara/metadata/module_models.py class KiaraModuleConfigMetadata ( MetadataModel ): @classmethod def from_config_class ( cls , config_cls : typing . Type [ ModuleTypeConfigSchema ], remove_pipeline_config : bool = False , ): flat_models = get_flat_models_from_model ( config_cls ) model_name_map = get_model_name_map ( flat_models ) m_schema , _ , _ = model_process_schema ( config_cls , model_name_map = model_name_map ) fields = m_schema [ \"properties\" ] config_values = {} for field_name , details in fields . items (): if remove_pipeline_config and field_name in [ \"steps\" , \"input_aliases\" , \"output_aliases\" , \"doc\" , ]: continue type_str = \"-- n/a --\" if \"type\" in details . keys (): type_str = details [ \"type\" ] desc = details . get ( \"description\" , DEFAULT_NO_DESC_VALUE ) default = config_cls . __fields__ [ field_name ] . default default = config_cls . __fields__ [ field_name ] . default if default is None : if callable ( config_cls . __fields__ [ field_name ] . default_factory ): default = config_cls . __fields__ [ field_name ] . default_factory () # type: ignore req = config_cls . __fields__ [ field_name ] . required config_values [ field_name ] = ValueTypeAndDescription ( description = desc , type = type_str , value_default = default , required = req ) python_cls = PythonClassMetadata . from_class ( config_cls ) return KiaraModuleConfigMetadata ( python_class = python_cls , config_values = config_values ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this configuration.\" ) config_values : typing . Dict [ str , ValueTypeAndDescription ] = Field ( description = \"The available configuration values.\" )","title":"KiaraModuleConfigMetadata"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleConfigMetadata.config_values","text":"The available configuration values.","title":"config_values"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleConfigMetadata.python_class","text":"The Python class for this configuration.","title":"python_class"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleInstanceMetadata","text":"Source code in kiara/metadata/module_models.py class KiaraModuleInstanceMetadata ( MetadataModel ): @classmethod def from_module_obj ( cls , obj : \"KiaraModule\" ): config = obj . config . dict () for x in [ \"steps\" , \"input_aliases\" , \"output_aliases\" , \"module_type_name\" , \"doc\" , \"metadata\" , ]: config . pop ( x , None ) type_metadata = KiaraModuleTypeMetadata . from_module_class ( obj . __class__ ) result = KiaraModuleInstanceMetadata ( type_metadata = type_metadata , config = config , inputs_schema = obj . input_schemas , outputs_schema = obj . output_schemas , ) return result type_metadata : KiaraModuleTypeMetadata = Field ( description = \"Metadata for the module type of this instance.\" ) config : typing . Dict [ str , typing . Any ] = Field ( description = \"Configuration that was used to create this module instance.\" ) inputs_schema : typing . Dict [ str , ValueSchema ] = Field ( description = \"The schema for the module inputs.\" ) outputs_schema : typing . Dict [ str , ValueSchema ] = Field ( description = \"The schema for the module outputs.\" ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_desc = config . get ( \"include_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if include_desc : table . add_row ( \"Description\" , self . type_metadata . documentation . description ) table . add_row ( \"Origin\" , self . type_metadata . origin . create_renderable ()) table . add_row ( \"Type context\" , self . type_metadata . context . create_renderable ()) table . add_row ( \"Python class\" , self . type_metadata . python_class . create_renderable () ) conf = Syntax ( json . dumps ( self . config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Configuration\" , conf ) constants = self . config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . inputs_schema , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . outputs_schema , ) table . add_row ( \"Outputs\" , outputs_table ) # table.add_row(\"Source code\", self.type_metadata.process_src) return table","title":"KiaraModuleInstanceMetadata"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleInstanceMetadata.config","text":"Configuration that was used to create this module instance.","title":"config"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleInstanceMetadata.inputs_schema","text":"The schema for the module inputs.","title":"inputs_schema"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleInstanceMetadata.outputs_schema","text":"The schema for the module outputs.","title":"outputs_schema"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleInstanceMetadata.type_metadata","text":"Metadata for the module type of this instance.","title":"type_metadata"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata","text":"Source code in kiara/metadata/module_models.py class KiaraModuleTypeMetadata ( MetadataModel ): @classmethod def from_module_class ( cls , module_cls : typing . Type [ \"KiaraModule\" ]): module_attrs = cls . extract_module_attributes ( module_cls = module_cls ) return cls ( ** module_attrs ) @classmethod def extract_module_attributes ( self , module_cls : typing . Type [ \"KiaraModule\" ] ) -> typing . Dict [ str , typing . Any ]: if not hasattr ( module_cls , \"process\" ): raise Exception ( f \"Module class ' { module_cls } ' misses 'process' method.\" ) proc_src = textwrap . dedent ( inspect . getsource ( module_cls . process )) # type: ignore origin_md = OriginMetadataModel . from_class ( module_cls ) doc = DocumentationMetadataModel . from_class_doc ( module_cls ) python_class = PythonClassMetadata . from_class ( module_cls ) properties_md = ContextMetadataModel . from_class ( module_cls ) is_pipeline = module_cls . is_pipeline () doc_url = properties_md . get_url_for_reference ( \"documentation\" ) if doc_url : class_doc = calculate_class_doc_url ( doc_url , module_cls . _module_type_id , pipeline = is_pipeline ) # type: ignore properties_md . add_reference ( \"module_doc\" , class_doc , \"A link to the published, auto-generated module documentation.\" , ) if not is_pipeline : repo_url = properties_md . get_url_for_reference ( \"source_repo\" ) if repo_url is not None : src_url = calculate_class_source_url ( repo_url , python_class ) properties_md . add_reference ( \"source_url\" , src_url , \"A link to the published source file that contains this module.\" , ) config = KiaraModuleConfigMetadata . from_config_class ( module_cls . _config_cls ) pipeline_config = None if module_cls . _module_type_id != \"pipeline\" and is_pipeline : # type: ignore pipeline_config = module_cls . _base_pipeline_config # type: ignore return { \"type_name\" : module_cls . _module_type_name , # type: ignore \"type_id\" : module_cls . _module_type_id , # type: ignore \"documentation\" : doc , \"origin\" : origin_md , \"context\" : properties_md , \"python_class\" : python_class , \"config\" : config , \"is_pipeline\" : is_pipeline , \"pipeline_config\" : pipeline_config , \"process_src\" : proc_src , } type_name : str = Field ( description = \"The registered name for this module type.\" ) type_id : str = Field ( description = \"The full type id.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the module.\" ) origin : OriginMetadataModel = Field ( description = \"Information about authorship for the module type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this module (description, tags, labels, references, ...).\" ) python_class : PythonClassMetadata = Field ( description = \"Information about the Python class for this module type.\" ) config : KiaraModuleConfigMetadata = Field ( description = \"Details on how this module type can be configured.\" ) is_pipeline : bool = Field ( description = \"Whether the module type is a pipeline, or a core module.\" ) pipeline_config : typing . Optional [ PipelineConfig ] = Field ( description = \"If this module is a pipeline, this field contains the pipeline configuration.\" , default_factory = None , ) process_src : str = Field ( description = \"The source code of the process method of the module.\" ) @validator ( \"documentation\" , pre = True ) def validate_doc ( cls , value ): return DocumentationMetadataModel . create ( value ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : include_config_schema = config . get ( \"include_config_schema\" , True ) include_src = config . get ( \"include_src\" , True ) include_doc = config . get ( \"include_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , padding = ( 0 , 0 , 0 , 0 )) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) if include_doc : table . add_row ( \"Documentation\" , Panel ( self . documentation . create_renderable (), box = box . SIMPLE ), ) table . add_row ( \"Origin\" , self . origin . create_renderable ()) table . add_row ( \"Context\" , self . context . create_renderable ()) if include_config_schema : if self . config : config_cls = self . config . python_class . get_class () table . add_row ( \"Module config\" , create_table_from_base_model ( config_cls )) table . add_row ( \"Module config\" , \"-- no config --\" ) table . add_row ( \"Python class\" , self . python_class . create_renderable ()) if include_src : if self . is_pipeline : json_str = self . pipeline_config . json ( indent = 2 ) # type: ignore _config : Syntax = Syntax ( json_str , \"json\" , background_color = \"default\" ) table . add_row ( \"Pipeline config\" , Panel ( _config , box = box . HORIZONTALS )) else : _config = Syntax ( self . process_src , \"python\" , background_color = \"default\" ) table . add_row ( \"Processing source code\" , Panel ( _config , box = box . HORIZONTALS ) ) return table","title":"KiaraModuleTypeMetadata"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.config","text":"Details on how this module type can be configured.","title":"config"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.context","text":"Generic properties of this module (description, tags, labels, references, ...).","title":"context"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.documentation","text":"Documentation for the module.","title":"documentation"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.is_pipeline","text":"Whether the module type is a pipeline, or a core module.","title":"is_pipeline"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.origin","text":"Information about authorship for the module type.","title":"origin"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.pipeline_config","text":"If this module is a pipeline, this field contains the pipeline configuration.","title":"pipeline_config"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.process_src","text":"The source code of the process method of the module.","title":"process_src"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.python_class","text":"Information about the Python class for this module type.","title":"python_class"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.type_id","text":"The full type id.","title":"type_id"},{"location":"reference/kiara/metadata/module_models/#kiara.metadata.module_models.KiaraModuleTypeMetadata.type_name","text":"The registered name for this module type.","title":"type_name"},{"location":"reference/kiara/metadata/operation_models/","text":"OperationsMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/operation_models.py class OperationsMetadata ( MetadataModel ): @classmethod def create_all ( cls , kiara : \"Kiara\" ) -> typing . Dict [ str , \"OperationsMetadata\" ]: op_types = kiara . operation_mgmt . operation_types result = {} for op_type in op_types : op_type_cls = kiara . operation_mgmt . get_operations ( op_type ) result [ op_type ] = cls . from_operations_class ( op_type_cls . __class__ ) return result @classmethod def from_operations_class ( cls , operation_type_cls : typing . Type [ \"OperationType\" ] ) -> \"OperationsMetadata\" : origin_md = OriginMetadataModel . from_class ( operation_type_cls ) doc = DocumentationMetadataModel . from_class_doc ( operation_type_cls ) python_class = PythonClassMetadata . from_class ( operation_type_cls ) properties_md = ContextMetadataModel . from_class ( operation_type_cls ) return OperationsMetadata . construct ( type_name = operation_type_cls . _operation_type_name , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" ) context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type. documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about the creator of this value type. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this value type. type_name : str pydantic-field required \u00b6 The registered name for this value type.","title":"operation_models"},{"location":"reference/kiara/metadata/operation_models/#kiara.metadata.operation_models.OperationsMetadata","text":"Source code in kiara/metadata/operation_models.py class OperationsMetadata ( MetadataModel ): @classmethod def create_all ( cls , kiara : \"Kiara\" ) -> typing . Dict [ str , \"OperationsMetadata\" ]: op_types = kiara . operation_mgmt . operation_types result = {} for op_type in op_types : op_type_cls = kiara . operation_mgmt . get_operations ( op_type ) result [ op_type ] = cls . from_operations_class ( op_type_cls . __class__ ) return result @classmethod def from_operations_class ( cls , operation_type_cls : typing . Type [ \"OperationType\" ] ) -> \"OperationsMetadata\" : origin_md = OriginMetadataModel . from_class ( operation_type_cls ) doc = DocumentationMetadataModel . from_class_doc ( operation_type_cls ) python_class = PythonClassMetadata . from_class ( operation_type_cls ) properties_md = ContextMetadataModel . from_class ( operation_type_cls ) return OperationsMetadata . construct ( type_name = operation_type_cls . _operation_type_name , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" )","title":"OperationsMetadata"},{"location":"reference/kiara/metadata/operation_models/#kiara.metadata.operation_models.OperationsMetadata.context","text":"Generic properties of this value type.","title":"context"},{"location":"reference/kiara/metadata/operation_models/#kiara.metadata.operation_models.OperationsMetadata.documentation","text":"Documentation for the value type.","title":"documentation"},{"location":"reference/kiara/metadata/operation_models/#kiara.metadata.operation_models.OperationsMetadata.origin","text":"Information about the creator of this value type.","title":"origin"},{"location":"reference/kiara/metadata/operation_models/#kiara.metadata.operation_models.OperationsMetadata.python_class","text":"The Python class for this value type.","title":"python_class"},{"location":"reference/kiara/metadata/operation_models/#kiara.metadata.operation_models.OperationsMetadata.type_name","text":"The registered name for this value type.","title":"type_name"},{"location":"reference/kiara/metadata/type_models/","text":"ValueTypeMetadata ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/metadata/type_models.py class ValueTypeMetadata ( MetadataModel ): @classmethod def create_all ( cls , kiara : \"Kiara\" ) -> typing . Dict [ str , \"ValueTypeMetadata\" ]: result = {} for vt in kiara . value_types : t_cls = kiara . get_value_type_cls ( vt ) result [ vt ] = cls . from_value_type_class ( t_cls ) return result @classmethod def from_value_type_class ( cls , value_type_cls : typing . Type [ \"ValueType\" ]): origin_md = OriginMetadataModel . from_class ( value_type_cls ) doc = DocumentationMetadataModel . from_class_doc ( value_type_cls ) python_class = PythonClassMetadata . from_class ( value_type_cls ) properties_md = ContextMetadataModel . from_class ( value_type_cls ) return ValueTypeMetadata ( type_name = value_type_cls . _value_type_name , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" ) context : ContextMetadataModel pydantic-field required \u00b6 Generic properties of this value type. documentation : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the value type. origin : OriginMetadataModel pydantic-field required \u00b6 Information about the creator of this value type. python_class : PythonClassMetadata pydantic-field required \u00b6 The Python class for this value type. type_name : str pydantic-field required \u00b6 The registered name for this value type.","title":"type_models"},{"location":"reference/kiara/metadata/type_models/#kiara.metadata.type_models.ValueTypeMetadata","text":"Source code in kiara/metadata/type_models.py class ValueTypeMetadata ( MetadataModel ): @classmethod def create_all ( cls , kiara : \"Kiara\" ) -> typing . Dict [ str , \"ValueTypeMetadata\" ]: result = {} for vt in kiara . value_types : t_cls = kiara . get_value_type_cls ( vt ) result [ vt ] = cls . from_value_type_class ( t_cls ) return result @classmethod def from_value_type_class ( cls , value_type_cls : typing . Type [ \"ValueType\" ]): origin_md = OriginMetadataModel . from_class ( value_type_cls ) doc = DocumentationMetadataModel . from_class_doc ( value_type_cls ) python_class = PythonClassMetadata . from_class ( value_type_cls ) properties_md = ContextMetadataModel . from_class ( value_type_cls ) return ValueTypeMetadata ( type_name = value_type_cls . _value_type_name , # type: ignore documentation = doc , origin = origin_md , context = properties_md , python_class = python_class , ) type_name : str = Field ( description = \"The registered name for this value type.\" ) documentation : DocumentationMetadataModel = Field ( description = \"Documentation for the value type.\" ) origin : OriginMetadataModel = Field ( description = \"Information about the creator of this value type.\" ) context : ContextMetadataModel = Field ( description = \"Generic properties of this value type.\" ) python_class : PythonClassMetadata = Field ( description = \"The Python class for this value type.\" )","title":"ValueTypeMetadata"},{"location":"reference/kiara/metadata/type_models/#kiara.metadata.type_models.ValueTypeMetadata.context","text":"Generic properties of this value type.","title":"context"},{"location":"reference/kiara/metadata/type_models/#kiara.metadata.type_models.ValueTypeMetadata.documentation","text":"Documentation for the value type.","title":"documentation"},{"location":"reference/kiara/metadata/type_models/#kiara.metadata.type_models.ValueTypeMetadata.origin","text":"Information about the creator of this value type.","title":"origin"},{"location":"reference/kiara/metadata/type_models/#kiara.metadata.type_models.ValueTypeMetadata.python_class","text":"The Python class for this value type.","title":"python_class"},{"location":"reference/kiara/metadata/type_models/#kiara.metadata.type_models.ValueTypeMetadata.type_name","text":"The registered name for this value type.","title":"type_name"},{"location":"reference/kiara/module_mgmt/__init__/","text":"Base module for code that handles the import and management of KiaraModule sub-classes. merged \u00b6 MergedModuleManager ( ModuleManager ) \u00b6 Source code in kiara/module_mgmt/merged.py class MergedModuleManager ( ModuleManager ): def __init__ ( self , module_managers : typing . Optional [ typing . List [ typing . Union [ PythonModuleManagerConfig , PipelineModuleManagerConfig ] ] ] = None , extra_pipeline_folders : typing . Iterable [ str ] = None , ignore_errors : bool = False , ): self . _modules : typing . Optional [ typing . Dict [ str , ModuleManager ]] = None self . _module_cls_cache : typing . Dict [ str , typing . Type [ KiaraModule ]] = {} self . _default_python_mgr : typing . Optional [ PythonModuleManager ] = None self . _default_pipeline_mgr : typing . Optional [ PipelineModuleManager ] = None self . _custom_pipelines_mgr : typing . Optional [ PipelineModuleManager ] = None if extra_pipeline_folders is None : extra_pipeline_folders = [] self . _extra_pipeline_folders : typing . Iterable [ str ] = extra_pipeline_folders self . _ignore_errors : bool = ignore_errors if module_managers : raise NotImplementedError () # for mmc in module_managers: # mm = ModuleManager.from_config(mmc) # _mms.append(mm) self . _module_mgrs : typing . Optional [ typing . List [ ModuleManager ]] = None @property def module_managers ( self ) -> typing . Iterable [ ModuleManager ]: if self . _module_mgrs is not None : return self . _module_mgrs _mms = [ self . default_python_module_manager , self . default_pipeline_module_manager , self . default_custom_pipelines_manager , ] self . _module_mgrs = [] self . _modules = {} for mm in _mms : self . add_module_manager ( mm ) return self . _module_mgrs @property def module_map ( self ) -> typing . MutableMapping [ str , ModuleManager ]: if self . _modules is not None : return self . _modules # make sure module managers are initialized before this # this will also initialize the _modules attribute self . module_managers # noqa return self . _modules # type: ignore @property def default_python_module_manager ( self ) -> PythonModuleManager : if self . _default_python_mgr is None : self . _default_python_mgr = PythonModuleManager () return self . _default_python_mgr @property def default_pipeline_module_manager ( self ) -> PipelineModuleManager : if self . _default_pipeline_mgr is None : self . _default_pipeline_mgr = PipelineModuleManager ( folders = None , ignore_errors = self . _ignore_errors ) return self . _default_pipeline_mgr @property def default_custom_pipelines_manager ( self ) -> PipelineModuleManager : if self . _custom_pipelines_mgr is None : if self . _extra_pipeline_folders : folders = { \"extra\" : self . _extra_pipeline_folders } else : folders = {} self . _custom_pipelines_mgr = PipelineModuleManager ( folders = folders , ignore_errors = self . _ignore_errors ) return self . _custom_pipelines_mgr @property def available_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available modules\"\"\" return sorted ( set ( self . module_map . keys ())) @property def available_non_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return [ module_type for module_type in self . available_module_types if module_type != \"pipeline\" and not self . get_module_class ( module_type ) . is_pipeline () ] @property def available_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return [ module_type for module_type in self . available_module_types if module_type != \"pipeline\" and self . get_module_class ( module_type ) . is_pipeline () ] def get_module_types ( self ) -> typing . Iterable [ str ]: return self . available_module_types def is_pipeline_module ( self , module_type : str ): cls = self . get_module_class ( module_type = module_type ) return cls . is_pipeline () def add_module_manager ( self , module_manager : ModuleManager ): if self . _module_mgrs is None : self . module_managers # noqa for module_type in module_manager . get_module_types (): if module_type in self . module_map . keys (): log . warning ( f \"Duplicate module name ' { module_type } '. Ignoring all but the first.\" ) continue self . module_map [ module_type ] = module_manager self . _module_mgrs . append ( module_manager ) # type: ignore self . _value_types = None def register_pipeline_description ( self , data : typing . Union [ Path , str , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , raise_exception : bool = False , ) -> typing . Optional [ str ]: # making sure the module_map attribute is populated self . module_map # noqa name = self . default_custom_pipelines_manager . register_pipeline ( data = data , module_type_name = module_type_name , namespace = namespace ) if name in self . module_map . keys (): if raise_exception : raise Exception ( f \"Duplicate module name: { name } \" ) log . warning ( f \"Duplicate module name ' { name } '. Ignoring all but the first.\" ) return None else : self . module_map [ name ] = self . default_custom_pipelines_manager return name def get_module_class ( self , module_type : str ) -> typing . Type [ \"KiaraModule\" ]: if module_type == \"pipeline\" : from kiara import PipelineModule return PipelineModule mm = self . module_map . get ( module_type , None ) if mm is None : raise Exception ( f \"No module ' { module_type } ' available.\" ) if module_type in self . _module_cls_cache . keys (): return self . _module_cls_cache [ module_type ] cls = mm . get_module_class ( module_type ) if not hasattr ( cls , \"_module_type_name\" ): raise Exception ( f \"Class does not have a '_module_type_name' attribute: { cls } \" ) assert module_type . endswith ( cls . _module_type_name ) # type: ignore if hasattr ( cls , \"_module_type_id\" ) and cls . _module_type_id != \"pipeline\" and cls . _module_type_id != module_type : # type: ignore raise Exception ( f \"Can't create module class ' { cls } ', it already has a _module_type_id attribute and it's different to the module name ' { module_type } ': { cls . _module_type_id } \" # type: ignore ) self . _module_cls_cache [ module_type ] = cls setattr ( cls , \"_module_type_id\" , module_type ) return cls def create_module ( self , kiara : \"Kiara\" , id : typing . Optional [ str ], module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : mm = self . module_map . get ( module_type , None ) if mm is None : raise Exception ( f \"No module ' { module_type } ' registered. Available modules: { ', ' . join ( self . available_module_types ) } \" ) _ = self . get_module_class ( module_type ) # just to make sure the _module_type_id attribute is added return mm . create_module ( id = id , parent_id = parent_id , module_type = module_type , module_config = module_config , kiara = kiara , ) def find_modules_for_package ( self , package_name : str , include_core_modules : bool = True , include_pipelines : bool = True , ) -> typing . Dict [ str , typing . Type [ \"KiaraModule\" ]]: result = {} for module_type in self . available_module_types : if module_type == \"pipeline\" : continue module_cls = self . get_module_class ( module_type ) module_package = module_cls . get_type_metadata () . context . labels . get ( \"package\" , None ) if module_package != package_name : continue if module_cls . is_pipeline (): if include_pipelines : result [ module_type ] = module_cls else : if include_core_modules : result [ module_type ] = module_cls return result available_module_types : List [ str ] property readonly \u00b6 Return the names of all available modules available_non_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. available_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. MergedModuleManagerConfig ( BaseModel ) pydantic-model \u00b6 Source code in kiara/module_mgmt/merged.py class MergedModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"pipeline\" ] folders : typing . List [ str ] = Field ( description = \"A list of folders that contain pipeline descriptions.\" , default_factory = list , ) @validator ( \"folders\" , pre = True ) def _validate_folders ( cls , v ): if isinstance ( v , str ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( v , Path ): item = v . as_posix () assert isinstance ( item , str ) result . append ( item ) return result folders : List [ str ] pydantic-field \u00b6 A list of folders that contain pipeline descriptions. pipelines \u00b6 PipelineModuleManager ( ModuleManager ) \u00b6 Module manager that discovers pipeline descriptions, and create modules out of them. Parameters: Name Type Description Default folders Optional[Mapping[str, Union[str, pathlib.Path, Iterable[Union[str, pathlib.Path]]]]] a list of folders to search for pipeline descriptions, if 'None', 'kiara.pipelines' entrypoints are searched, as well as the user config None ignore_errors bool ignore any errors that occur during pipeline discovery (as much as that is possible) False Source code in kiara/module_mgmt/pipelines.py class PipelineModuleManager ( ModuleManager ): \"\"\"Module manager that discovers pipeline descriptions, and create modules out of them. Arguments: folders: a list of folders to search for pipeline descriptions, if 'None', 'kiara.pipelines' entrypoints are searched, as well as the user config ignore_errors: ignore any errors that occur during pipeline discovery (as much as that is possible) \"\"\" def __init__ ( self , folders : typing . Optional [ typing . Mapping [ str , typing . Union [ str , Path , typing . Iterable [ typing . Union [ str , Path ]]] ] ] = None , ignore_errors : bool = False , ): if folders is None : from kiara.utils.class_loading import find_all_kiara_pipeline_paths folders_map : typing . Dict [ str , typing . List [ typing . Tuple [ typing . Optional [ str ], str ]] ] = find_all_kiara_pipeline_paths ( skip_errors = ignore_errors ) if os . path . exists ( USER_PIPELINES_FOLDER ): folders_map [ \"user\" ] = [( None , USER_PIPELINES_FOLDER )] elif not folders : folders_map = {} else : assert isinstance ( folders , typing . Mapping ) assert \"user\" not in folders . keys () folders_map = {} for k , _folders in folders . items (): if isinstance ( _folders , str ) or not isinstance ( _folders , typing . Iterable ): raise NotImplementedError ( f \"Invalid folder configuration (must be an iterable): { _folders } \" ) for _f in _folders : # type: ignore if isinstance ( _f , Path ): _f = _f . as_posix () folders_map . setdefault ( k , []) . append (( None , _f )) self . _pipeline_desc_folders : typing . List [ Path ] = [] self . _pipeline_descs : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} self . _cached_classes : typing . Dict [ str , typing . Type [ PipelineModule ]] = {} for ns , paths in folders_map . items (): for path in paths : self . add_pipelines_path ( ns , path [ 1 ], path [ 0 ]) def register_pipeline ( self , data : typing . Union [ str , Path , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , ) -> str : \"\"\"Register a pipeline description to the pipeline pool. Arguments: data: the pipeline data (a dict, or a path to a file) module_type_name: the type name this pipeline should be registered as \"\"\" # TODO: verify that there is no conflict with module_type_name if isinstance ( data , str ): data = Path ( os . path . expanduser ( data )) if isinstance ( data , Path ): _name , _data = get_pipeline_details_from_path ( data ) _data = check_doc_sidecar ( data , _data ) if module_type_name : _name = module_type_name elif isinstance ( data , typing . Mapping ): _data = dict ( data ) if module_type_name : _data [ MODULE_TYPE_NAME_KEY ] = module_type_name _name = _data . get ( MODULE_TYPE_NAME_KEY , None ) if not _name : raise Exception ( f \"Can't register pipeline, no module type name available: { data } \" ) _data = { \"data\" : _data , \"source\" : data , \"source_type\" : \"dict\" } else : raise Exception ( f \"Can't register pipeline, must be dict-like data, not { type ( data ) } \" ) if not _name : raise Exception ( \"No pipeline name set.\" ) if not namespace : full_name = _name else : full_name = f \" { namespace } . { _name } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Can't register pipeline: duplicate workflow name ' { _name } '\" ) self . _pipeline_descs [ full_name ] = _data return full_name def add_pipelines_path ( self , namespace : str , path : typing . Union [ str , Path ], base_module : typing . Optional [ str ], ) -> typing . Iterable [ str ]: \"\"\"Add a pipeline description file or folder containing some to this manager. Arguments: namespace: the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted path: the path to a pipeline description file, or folder which contains some base_module: the base module the assembled pipeline modules under this path will be located at in the Python namespace Returns: a list of module type names that were added \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) elif isinstance ( path , typing . Iterable ): raise TypeError ( f \"Invalid type for path: { path } \" ) if not path . exists (): log . warning ( f \"Can't add pipeline path ' { path } ': path does not exist\" ) return [] elif path . is_dir (): files : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} for root , dirnames , filenames in os . walk ( path , topdown = True ): dirnames [:] = [ d for d in dirnames if d not in DEFAULT_EXCLUDE_DIRS ] for filename in [ f for f in filenames if os . path . isfile ( os . path . join ( root , f )) and any ( f . endswith ( ext ) for ext in VALID_PIPELINE_FILE_EXTENSIONS ) ]: try : full_path = os . path . join ( root , filename ) name , data = get_pipeline_details_from_path ( path = full_path , base_module = base_module ) data = check_doc_sidecar ( full_path , data ) rel_path = os . path . relpath ( os . path . dirname ( full_path ), path ) if not rel_path or rel_path == \".\" : ns_name = name else : _rel_path = rel_path . replace ( os . path . sep , \".\" ) ns_name = f \" { _rel_path } . { name } \" if not ns_name : raise Exception ( f \"Could not determine namespace for pipeline file ' { filename } '.\" ) if ns_name in files . keys (): raise Exception ( f \"Duplicate workflow name in namespace ' { namespace } ': { ns_name } \" ) files [ ns_name ] = data except Exception as e : log . warning ( f \"Ignoring invalid pipeline file ' { full_path } ': { e } \" ) elif path . is_file (): name , data = get_pipeline_details_from_path ( path = path , base_module = base_module ) data = check_doc_sidecar ( path , data ) if not name : raise Exception ( f \"Could not determine pipeline name for: { path } \" ) files = { name : data } result = {} for k , v in files . items (): if namespace . startswith ( \"_\" ): tokens = namespace . split ( \".\" ) if len ( tokens ) == 1 : _namespace = \"\" else : _namespace = \".\" . join ( tokens [ 1 :]) else : _namespace = namespace if not _namespace : full_name = k else : full_name = f \" { _namespace } . { k } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Duplicate workflow name: { name } \" ) result [ full_name ] = v self . _pipeline_descs . update ( result ) return result . keys () @property def pipeline_descs ( self ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: return self . _pipeline_descs def get_module_class ( self , module_type : str ) -> typing . Type [ \"PipelineModule\" ]: if module_type in self . _cached_classes . keys (): return self . _cached_classes [ module_type ] desc = self . _pipeline_descs . get ( module_type , None ) if desc is None : raise Exception ( f \"No pipeline with name ' { module_type } ' available.\" ) tokens = re . split ( r \"\\.|_\" , module_type ) cls_name = \"\" . join ( x . capitalize () or \"_\" for x in tokens ) if len ( tokens ) != 1 : full_name = \".\" . join ( tokens [ 0 : - 1 ] + [ cls_name ]) else : full_name = cls_name base_module = desc . get ( \"base_module\" , None ) cls = create_pipeline_class ( f \" { cls_name } PipelineModule\" , full_name , desc [ \"data\" ], base_module = base_module , ) setattr ( cls , \"_module_type_name\" , module_type ) self . _cached_classes [ module_type ] = cls return self . _cached_classes [ module_type ] def get_module_types ( self ) -> typing . Iterable [ str ]: return self . _pipeline_descs . keys () add_pipelines_path ( self , namespace , path , base_module ) \u00b6 Add a pipeline description file or folder containing some to this manager. Parameters: Name Type Description Default namespace str the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted required path Union[str, pathlib.Path] the path to a pipeline description file, or folder which contains some required base_module Optional[str] the base module the assembled pipeline modules under this path will be located at in the Python namespace required Returns: Type Description Iterable[str] a list of module type names that were added Source code in kiara/module_mgmt/pipelines.py def add_pipelines_path ( self , namespace : str , path : typing . Union [ str , Path ], base_module : typing . Optional [ str ], ) -> typing . Iterable [ str ]: \"\"\"Add a pipeline description file or folder containing some to this manager. Arguments: namespace: the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted path: the path to a pipeline description file, or folder which contains some base_module: the base module the assembled pipeline modules under this path will be located at in the Python namespace Returns: a list of module type names that were added \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) elif isinstance ( path , typing . Iterable ): raise TypeError ( f \"Invalid type for path: { path } \" ) if not path . exists (): log . warning ( f \"Can't add pipeline path ' { path } ': path does not exist\" ) return [] elif path . is_dir (): files : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} for root , dirnames , filenames in os . walk ( path , topdown = True ): dirnames [:] = [ d for d in dirnames if d not in DEFAULT_EXCLUDE_DIRS ] for filename in [ f for f in filenames if os . path . isfile ( os . path . join ( root , f )) and any ( f . endswith ( ext ) for ext in VALID_PIPELINE_FILE_EXTENSIONS ) ]: try : full_path = os . path . join ( root , filename ) name , data = get_pipeline_details_from_path ( path = full_path , base_module = base_module ) data = check_doc_sidecar ( full_path , data ) rel_path = os . path . relpath ( os . path . dirname ( full_path ), path ) if not rel_path or rel_path == \".\" : ns_name = name else : _rel_path = rel_path . replace ( os . path . sep , \".\" ) ns_name = f \" { _rel_path } . { name } \" if not ns_name : raise Exception ( f \"Could not determine namespace for pipeline file ' { filename } '.\" ) if ns_name in files . keys (): raise Exception ( f \"Duplicate workflow name in namespace ' { namespace } ': { ns_name } \" ) files [ ns_name ] = data except Exception as e : log . warning ( f \"Ignoring invalid pipeline file ' { full_path } ': { e } \" ) elif path . is_file (): name , data = get_pipeline_details_from_path ( path = path , base_module = base_module ) data = check_doc_sidecar ( path , data ) if not name : raise Exception ( f \"Could not determine pipeline name for: { path } \" ) files = { name : data } result = {} for k , v in files . items (): if namespace . startswith ( \"_\" ): tokens = namespace . split ( \".\" ) if len ( tokens ) == 1 : _namespace = \"\" else : _namespace = \".\" . join ( tokens [ 1 :]) else : _namespace = namespace if not _namespace : full_name = k else : full_name = f \" { _namespace } . { k } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Duplicate workflow name: { name } \" ) result [ full_name ] = v self . _pipeline_descs . update ( result ) return result . keys () register_pipeline ( self , data , module_type_name = None , namespace = None ) \u00b6 Register a pipeline description to the pipeline pool. Parameters: Name Type Description Default data Union[str, pathlib.Path, Mapping[str, Any]] the pipeline data (a dict, or a path to a file) required module_type_name Optional[str] the type name this pipeline should be registered as None Source code in kiara/module_mgmt/pipelines.py def register_pipeline ( self , data : typing . Union [ str , Path , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , ) -> str : \"\"\"Register a pipeline description to the pipeline pool. Arguments: data: the pipeline data (a dict, or a path to a file) module_type_name: the type name this pipeline should be registered as \"\"\" # TODO: verify that there is no conflict with module_type_name if isinstance ( data , str ): data = Path ( os . path . expanduser ( data )) if isinstance ( data , Path ): _name , _data = get_pipeline_details_from_path ( data ) _data = check_doc_sidecar ( data , _data ) if module_type_name : _name = module_type_name elif isinstance ( data , typing . Mapping ): _data = dict ( data ) if module_type_name : _data [ MODULE_TYPE_NAME_KEY ] = module_type_name _name = _data . get ( MODULE_TYPE_NAME_KEY , None ) if not _name : raise Exception ( f \"Can't register pipeline, no module type name available: { data } \" ) _data = { \"data\" : _data , \"source\" : data , \"source_type\" : \"dict\" } else : raise Exception ( f \"Can't register pipeline, must be dict-like data, not { type ( data ) } \" ) if not _name : raise Exception ( \"No pipeline name set.\" ) if not namespace : full_name = _name else : full_name = f \" { namespace } . { _name } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Can't register pipeline: duplicate workflow name ' { _name } '\" ) self . _pipeline_descs [ full_name ] = _data return full_name PipelineModuleManagerConfig ( BaseModel ) pydantic-model \u00b6 Source code in kiara/module_mgmt/pipelines.py class PipelineModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"pipeline\" ] folders : typing . List [ str ] = Field ( description = \"A list of folders that contain pipeline descriptions.\" , default_factory = list , ) @validator ( \"folders\" , pre = True ) def _validate_folders ( cls , v ): if isinstance ( v , str ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( v , Path ): item = v . as_posix () assert isinstance ( item , str ) result . append ( item ) return result folders : List [ str ] pydantic-field \u00b6 A list of folders that contain pipeline descriptions. get_pipeline_details_from_path ( path , module_type_name = None , base_module = None ) \u00b6 Load a pipeline description, save it's content, and determine it the pipeline base name. Parameters: Name Type Description Default path Union[str, pathlib.Path] the path to the pipeline file required module_type_name Optional[str] if specifies, overwrites any auto-detected or assigned pipeline name None base_module Optional[str] overrides the base module the assembled pipeline module will be located in the python hierarchy None Source code in kiara/module_mgmt/pipelines.py def get_pipeline_details_from_path ( path : typing . Union [ str , Path ], module_type_name : typing . Optional [ str ] = None , base_module : typing . Optional [ str ] = None , ) -> typing . Tuple [ typing . Optional [ str ], typing . Mapping [ str , typing . Any ]]: \"\"\"Load a pipeline description, save it's content, and determine it the pipeline base name. Arguments: path: the path to the pipeline file module_type_name: if specifies, overwrites any auto-detected or assigned pipeline name base_module: overrides the base module the assembled pipeline module will be located in the python hierarchy \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) if not path . is_file (): raise Exception ( f \"Can't add pipeline description ' { path . as_posix () } ': not a file\" ) data = get_data_from_file ( path ) if not data : raise Exception ( f \"Can't register pipeline file ' { path . as_posix () } ': no content.\" ) if module_type_name : data [ MODULE_TYPE_NAME_KEY ] = module_type_name filename = path . name if not isinstance ( data , typing . Mapping ): raise Exception ( \"Not a dictionary type.\" ) name = data . get ( MODULE_TYPE_NAME_KEY , None ) if name is None : name = filename . split ( \".\" , maxsplit = 1 )[ 0 ] result = { \"data\" : data , \"source\" : path . as_posix (), \"source_type\" : \"file\" } if base_module : result [ \"base_module\" ] = base_module return ( name , result ) python_classes \u00b6 PythonModuleManagerConfig ( BaseModel ) pydantic-model \u00b6 Source code in kiara/module_mgmt/python_classes.py class PythonModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"python\" ] module_classes : typing . Dict [ str , typing . Type [ \"KiaraModule\" ]] = Field ( description = \"The module classes this manager should hold.\" ) @validator ( \"module_classes\" , pre = True ) def _ensure_module_class_types ( cls , v ): _classes = [] if v : for _cls in v : if isinstance ( _cls , str ): try : module_name , cls_name = _cls . rsplit ( \".\" , maxsplit = 1 ) module = __import__ ( module_name ) _cls = getattr ( module , cls_name ) except Exception : raise ValueError ( f \"Can't parse value ' { _cls } ' into KiaraModule class.\" ) from kiara import KiaraModule # noqa if not issubclass ( _cls , KiaraModule ): raise ValueError ( f \"Not a KiaraModule sub-class: { _cls } \" ) _classes . append ( _cls ) return _classes module_classes : Dict [ str , Type [ KiaraModule ]] pydantic-field required \u00b6 The module classes this manager should hold.","title":"module_mgmt"},{"location":"reference/kiara/module_mgmt/__init__/#kiara.module_mgmt.merged","text":"","title":"merged"},{"location":"reference/kiara/module_mgmt/__init__/#kiara.module_mgmt.merged.MergedModuleManager","text":"Source code in kiara/module_mgmt/merged.py class MergedModuleManager ( ModuleManager ): def __init__ ( self , module_managers : typing . Optional [ typing . List [ typing . Union [ PythonModuleManagerConfig , PipelineModuleManagerConfig ] ] ] = None , extra_pipeline_folders : typing . Iterable [ str ] = None , ignore_errors : bool = False , ): self . _modules : typing . Optional [ typing . Dict [ str , ModuleManager ]] = None self . _module_cls_cache : typing . Dict [ str , typing . Type [ KiaraModule ]] = {} self . _default_python_mgr : typing . Optional [ PythonModuleManager ] = None self . _default_pipeline_mgr : typing . Optional [ PipelineModuleManager ] = None self . _custom_pipelines_mgr : typing . Optional [ PipelineModuleManager ] = None if extra_pipeline_folders is None : extra_pipeline_folders = [] self . _extra_pipeline_folders : typing . Iterable [ str ] = extra_pipeline_folders self . _ignore_errors : bool = ignore_errors if module_managers : raise NotImplementedError () # for mmc in module_managers: # mm = ModuleManager.from_config(mmc) # _mms.append(mm) self . _module_mgrs : typing . Optional [ typing . List [ ModuleManager ]] = None @property def module_managers ( self ) -> typing . Iterable [ ModuleManager ]: if self . _module_mgrs is not None : return self . _module_mgrs _mms = [ self . default_python_module_manager , self . default_pipeline_module_manager , self . default_custom_pipelines_manager , ] self . _module_mgrs = [] self . _modules = {} for mm in _mms : self . add_module_manager ( mm ) return self . _module_mgrs @property def module_map ( self ) -> typing . MutableMapping [ str , ModuleManager ]: if self . _modules is not None : return self . _modules # make sure module managers are initialized before this # this will also initialize the _modules attribute self . module_managers # noqa return self . _modules # type: ignore @property def default_python_module_manager ( self ) -> PythonModuleManager : if self . _default_python_mgr is None : self . _default_python_mgr = PythonModuleManager () return self . _default_python_mgr @property def default_pipeline_module_manager ( self ) -> PipelineModuleManager : if self . _default_pipeline_mgr is None : self . _default_pipeline_mgr = PipelineModuleManager ( folders = None , ignore_errors = self . _ignore_errors ) return self . _default_pipeline_mgr @property def default_custom_pipelines_manager ( self ) -> PipelineModuleManager : if self . _custom_pipelines_mgr is None : if self . _extra_pipeline_folders : folders = { \"extra\" : self . _extra_pipeline_folders } else : folders = {} self . _custom_pipelines_mgr = PipelineModuleManager ( folders = folders , ignore_errors = self . _ignore_errors ) return self . _custom_pipelines_mgr @property def available_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available modules\"\"\" return sorted ( set ( self . module_map . keys ())) @property def available_non_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return [ module_type for module_type in self . available_module_types if module_type != \"pipeline\" and not self . get_module_class ( module_type ) . is_pipeline () ] @property def available_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return [ module_type for module_type in self . available_module_types if module_type != \"pipeline\" and self . get_module_class ( module_type ) . is_pipeline () ] def get_module_types ( self ) -> typing . Iterable [ str ]: return self . available_module_types def is_pipeline_module ( self , module_type : str ): cls = self . get_module_class ( module_type = module_type ) return cls . is_pipeline () def add_module_manager ( self , module_manager : ModuleManager ): if self . _module_mgrs is None : self . module_managers # noqa for module_type in module_manager . get_module_types (): if module_type in self . module_map . keys (): log . warning ( f \"Duplicate module name ' { module_type } '. Ignoring all but the first.\" ) continue self . module_map [ module_type ] = module_manager self . _module_mgrs . append ( module_manager ) # type: ignore self . _value_types = None def register_pipeline_description ( self , data : typing . Union [ Path , str , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , raise_exception : bool = False , ) -> typing . Optional [ str ]: # making sure the module_map attribute is populated self . module_map # noqa name = self . default_custom_pipelines_manager . register_pipeline ( data = data , module_type_name = module_type_name , namespace = namespace ) if name in self . module_map . keys (): if raise_exception : raise Exception ( f \"Duplicate module name: { name } \" ) log . warning ( f \"Duplicate module name ' { name } '. Ignoring all but the first.\" ) return None else : self . module_map [ name ] = self . default_custom_pipelines_manager return name def get_module_class ( self , module_type : str ) -> typing . Type [ \"KiaraModule\" ]: if module_type == \"pipeline\" : from kiara import PipelineModule return PipelineModule mm = self . module_map . get ( module_type , None ) if mm is None : raise Exception ( f \"No module ' { module_type } ' available.\" ) if module_type in self . _module_cls_cache . keys (): return self . _module_cls_cache [ module_type ] cls = mm . get_module_class ( module_type ) if not hasattr ( cls , \"_module_type_name\" ): raise Exception ( f \"Class does not have a '_module_type_name' attribute: { cls } \" ) assert module_type . endswith ( cls . _module_type_name ) # type: ignore if hasattr ( cls , \"_module_type_id\" ) and cls . _module_type_id != \"pipeline\" and cls . _module_type_id != module_type : # type: ignore raise Exception ( f \"Can't create module class ' { cls } ', it already has a _module_type_id attribute and it's different to the module name ' { module_type } ': { cls . _module_type_id } \" # type: ignore ) self . _module_cls_cache [ module_type ] = cls setattr ( cls , \"_module_type_id\" , module_type ) return cls def create_module ( self , kiara : \"Kiara\" , id : typing . Optional [ str ], module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : mm = self . module_map . get ( module_type , None ) if mm is None : raise Exception ( f \"No module ' { module_type } ' registered. Available modules: { ', ' . join ( self . available_module_types ) } \" ) _ = self . get_module_class ( module_type ) # just to make sure the _module_type_id attribute is added return mm . create_module ( id = id , parent_id = parent_id , module_type = module_type , module_config = module_config , kiara = kiara , ) def find_modules_for_package ( self , package_name : str , include_core_modules : bool = True , include_pipelines : bool = True , ) -> typing . Dict [ str , typing . Type [ \"KiaraModule\" ]]: result = {} for module_type in self . available_module_types : if module_type == \"pipeline\" : continue module_cls = self . get_module_class ( module_type ) module_package = module_cls . get_type_metadata () . context . labels . get ( \"package\" , None ) if module_package != package_name : continue if module_cls . is_pipeline (): if include_pipelines : result [ module_type ] = module_cls else : if include_core_modules : result [ module_type ] = module_cls return result","title":"MergedModuleManager"},{"location":"reference/kiara/module_mgmt/__init__/#kiara.module_mgmt.merged.MergedModuleManagerConfig","text":"Source code in kiara/module_mgmt/merged.py class MergedModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"pipeline\" ] folders : typing . List [ str ] = Field ( description = \"A list of folders that contain pipeline descriptions.\" , default_factory = list , ) @validator ( \"folders\" , pre = True ) def _validate_folders ( cls , v ): if isinstance ( v , str ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( v , Path ): item = v . as_posix () assert isinstance ( item , str ) result . append ( item ) return result","title":"MergedModuleManagerConfig"},{"location":"reference/kiara/module_mgmt/__init__/#kiara.module_mgmt.pipelines","text":"","title":"pipelines"},{"location":"reference/kiara/module_mgmt/__init__/#kiara.module_mgmt.pipelines.PipelineModuleManager","text":"Module manager that discovers pipeline descriptions, and create modules out of them. Parameters: Name Type Description Default folders Optional[Mapping[str, Union[str, pathlib.Path, Iterable[Union[str, pathlib.Path]]]]] a list of folders to search for pipeline descriptions, if 'None', 'kiara.pipelines' entrypoints are searched, as well as the user config None ignore_errors bool ignore any errors that occur during pipeline discovery (as much as that is possible) False Source code in kiara/module_mgmt/pipelines.py class PipelineModuleManager ( ModuleManager ): \"\"\"Module manager that discovers pipeline descriptions, and create modules out of them. Arguments: folders: a list of folders to search for pipeline descriptions, if 'None', 'kiara.pipelines' entrypoints are searched, as well as the user config ignore_errors: ignore any errors that occur during pipeline discovery (as much as that is possible) \"\"\" def __init__ ( self , folders : typing . Optional [ typing . Mapping [ str , typing . Union [ str , Path , typing . Iterable [ typing . Union [ str , Path ]]] ] ] = None , ignore_errors : bool = False , ): if folders is None : from kiara.utils.class_loading import find_all_kiara_pipeline_paths folders_map : typing . Dict [ str , typing . List [ typing . Tuple [ typing . Optional [ str ], str ]] ] = find_all_kiara_pipeline_paths ( skip_errors = ignore_errors ) if os . path . exists ( USER_PIPELINES_FOLDER ): folders_map [ \"user\" ] = [( None , USER_PIPELINES_FOLDER )] elif not folders : folders_map = {} else : assert isinstance ( folders , typing . Mapping ) assert \"user\" not in folders . keys () folders_map = {} for k , _folders in folders . items (): if isinstance ( _folders , str ) or not isinstance ( _folders , typing . Iterable ): raise NotImplementedError ( f \"Invalid folder configuration (must be an iterable): { _folders } \" ) for _f in _folders : # type: ignore if isinstance ( _f , Path ): _f = _f . as_posix () folders_map . setdefault ( k , []) . append (( None , _f )) self . _pipeline_desc_folders : typing . List [ Path ] = [] self . _pipeline_descs : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} self . _cached_classes : typing . Dict [ str , typing . Type [ PipelineModule ]] = {} for ns , paths in folders_map . items (): for path in paths : self . add_pipelines_path ( ns , path [ 1 ], path [ 0 ]) def register_pipeline ( self , data : typing . Union [ str , Path , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , ) -> str : \"\"\"Register a pipeline description to the pipeline pool. Arguments: data: the pipeline data (a dict, or a path to a file) module_type_name: the type name this pipeline should be registered as \"\"\" # TODO: verify that there is no conflict with module_type_name if isinstance ( data , str ): data = Path ( os . path . expanduser ( data )) if isinstance ( data , Path ): _name , _data = get_pipeline_details_from_path ( data ) _data = check_doc_sidecar ( data , _data ) if module_type_name : _name = module_type_name elif isinstance ( data , typing . Mapping ): _data = dict ( data ) if module_type_name : _data [ MODULE_TYPE_NAME_KEY ] = module_type_name _name = _data . get ( MODULE_TYPE_NAME_KEY , None ) if not _name : raise Exception ( f \"Can't register pipeline, no module type name available: { data } \" ) _data = { \"data\" : _data , \"source\" : data , \"source_type\" : \"dict\" } else : raise Exception ( f \"Can't register pipeline, must be dict-like data, not { type ( data ) } \" ) if not _name : raise Exception ( \"No pipeline name set.\" ) if not namespace : full_name = _name else : full_name = f \" { namespace } . { _name } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Can't register pipeline: duplicate workflow name ' { _name } '\" ) self . _pipeline_descs [ full_name ] = _data return full_name def add_pipelines_path ( self , namespace : str , path : typing . Union [ str , Path ], base_module : typing . Optional [ str ], ) -> typing . Iterable [ str ]: \"\"\"Add a pipeline description file or folder containing some to this manager. Arguments: namespace: the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted path: the path to a pipeline description file, or folder which contains some base_module: the base module the assembled pipeline modules under this path will be located at in the Python namespace Returns: a list of module type names that were added \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) elif isinstance ( path , typing . Iterable ): raise TypeError ( f \"Invalid type for path: { path } \" ) if not path . exists (): log . warning ( f \"Can't add pipeline path ' { path } ': path does not exist\" ) return [] elif path . is_dir (): files : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} for root , dirnames , filenames in os . walk ( path , topdown = True ): dirnames [:] = [ d for d in dirnames if d not in DEFAULT_EXCLUDE_DIRS ] for filename in [ f for f in filenames if os . path . isfile ( os . path . join ( root , f )) and any ( f . endswith ( ext ) for ext in VALID_PIPELINE_FILE_EXTENSIONS ) ]: try : full_path = os . path . join ( root , filename ) name , data = get_pipeline_details_from_path ( path = full_path , base_module = base_module ) data = check_doc_sidecar ( full_path , data ) rel_path = os . path . relpath ( os . path . dirname ( full_path ), path ) if not rel_path or rel_path == \".\" : ns_name = name else : _rel_path = rel_path . replace ( os . path . sep , \".\" ) ns_name = f \" { _rel_path } . { name } \" if not ns_name : raise Exception ( f \"Could not determine namespace for pipeline file ' { filename } '.\" ) if ns_name in files . keys (): raise Exception ( f \"Duplicate workflow name in namespace ' { namespace } ': { ns_name } \" ) files [ ns_name ] = data except Exception as e : log . warning ( f \"Ignoring invalid pipeline file ' { full_path } ': { e } \" ) elif path . is_file (): name , data = get_pipeline_details_from_path ( path = path , base_module = base_module ) data = check_doc_sidecar ( path , data ) if not name : raise Exception ( f \"Could not determine pipeline name for: { path } \" ) files = { name : data } result = {} for k , v in files . items (): if namespace . startswith ( \"_\" ): tokens = namespace . split ( \".\" ) if len ( tokens ) == 1 : _namespace = \"\" else : _namespace = \".\" . join ( tokens [ 1 :]) else : _namespace = namespace if not _namespace : full_name = k else : full_name = f \" { _namespace } . { k } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Duplicate workflow name: { name } \" ) result [ full_name ] = v self . _pipeline_descs . update ( result ) return result . keys () @property def pipeline_descs ( self ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: return self . _pipeline_descs def get_module_class ( self , module_type : str ) -> typing . Type [ \"PipelineModule\" ]: if module_type in self . _cached_classes . keys (): return self . _cached_classes [ module_type ] desc = self . _pipeline_descs . get ( module_type , None ) if desc is None : raise Exception ( f \"No pipeline with name ' { module_type } ' available.\" ) tokens = re . split ( r \"\\.|_\" , module_type ) cls_name = \"\" . join ( x . capitalize () or \"_\" for x in tokens ) if len ( tokens ) != 1 : full_name = \".\" . join ( tokens [ 0 : - 1 ] + [ cls_name ]) else : full_name = cls_name base_module = desc . get ( \"base_module\" , None ) cls = create_pipeline_class ( f \" { cls_name } PipelineModule\" , full_name , desc [ \"data\" ], base_module = base_module , ) setattr ( cls , \"_module_type_name\" , module_type ) self . _cached_classes [ module_type ] = cls return self . _cached_classes [ module_type ] def get_module_types ( self ) -> typing . Iterable [ str ]: return self . _pipeline_descs . keys ()","title":"PipelineModuleManager"},{"location":"reference/kiara/module_mgmt/__init__/#kiara.module_mgmt.pipelines.PipelineModuleManagerConfig","text":"Source code in kiara/module_mgmt/pipelines.py class PipelineModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"pipeline\" ] folders : typing . List [ str ] = Field ( description = \"A list of folders that contain pipeline descriptions.\" , default_factory = list , ) @validator ( \"folders\" , pre = True ) def _validate_folders ( cls , v ): if isinstance ( v , str ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( v , Path ): item = v . as_posix () assert isinstance ( item , str ) result . append ( item ) return result","title":"PipelineModuleManagerConfig"},{"location":"reference/kiara/module_mgmt/__init__/#kiara.module_mgmt.pipelines.get_pipeline_details_from_path","text":"Load a pipeline description, save it's content, and determine it the pipeline base name. Parameters: Name Type Description Default path Union[str, pathlib.Path] the path to the pipeline file required module_type_name Optional[str] if specifies, overwrites any auto-detected or assigned pipeline name None base_module Optional[str] overrides the base module the assembled pipeline module will be located in the python hierarchy None Source code in kiara/module_mgmt/pipelines.py def get_pipeline_details_from_path ( path : typing . Union [ str , Path ], module_type_name : typing . Optional [ str ] = None , base_module : typing . Optional [ str ] = None , ) -> typing . Tuple [ typing . Optional [ str ], typing . Mapping [ str , typing . Any ]]: \"\"\"Load a pipeline description, save it's content, and determine it the pipeline base name. Arguments: path: the path to the pipeline file module_type_name: if specifies, overwrites any auto-detected or assigned pipeline name base_module: overrides the base module the assembled pipeline module will be located in the python hierarchy \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) if not path . is_file (): raise Exception ( f \"Can't add pipeline description ' { path . as_posix () } ': not a file\" ) data = get_data_from_file ( path ) if not data : raise Exception ( f \"Can't register pipeline file ' { path . as_posix () } ': no content.\" ) if module_type_name : data [ MODULE_TYPE_NAME_KEY ] = module_type_name filename = path . name if not isinstance ( data , typing . Mapping ): raise Exception ( \"Not a dictionary type.\" ) name = data . get ( MODULE_TYPE_NAME_KEY , None ) if name is None : name = filename . split ( \".\" , maxsplit = 1 )[ 0 ] result = { \"data\" : data , \"source\" : path . as_posix (), \"source_type\" : \"file\" } if base_module : result [ \"base_module\" ] = base_module return ( name , result )","title":"get_pipeline_details_from_path()"},{"location":"reference/kiara/module_mgmt/__init__/#kiara.module_mgmt.python_classes","text":"","title":"python_classes"},{"location":"reference/kiara/module_mgmt/__init__/#kiara.module_mgmt.python_classes.PythonModuleManagerConfig","text":"Source code in kiara/module_mgmt/python_classes.py class PythonModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"python\" ] module_classes : typing . Dict [ str , typing . Type [ \"KiaraModule\" ]] = Field ( description = \"The module classes this manager should hold.\" ) @validator ( \"module_classes\" , pre = True ) def _ensure_module_class_types ( cls , v ): _classes = [] if v : for _cls in v : if isinstance ( _cls , str ): try : module_name , cls_name = _cls . rsplit ( \".\" , maxsplit = 1 ) module = __import__ ( module_name ) _cls = getattr ( module , cls_name ) except Exception : raise ValueError ( f \"Can't parse value ' { _cls } ' into KiaraModule class.\" ) from kiara import KiaraModule # noqa if not issubclass ( _cls , KiaraModule ): raise ValueError ( f \"Not a KiaraModule sub-class: { _cls } \" ) _classes . append ( _cls ) return _classes","title":"PythonModuleManagerConfig"},{"location":"reference/kiara/module_mgmt/merged/","text":"MergedModuleManager ( ModuleManager ) \u00b6 Source code in kiara/module_mgmt/merged.py class MergedModuleManager ( ModuleManager ): def __init__ ( self , module_managers : typing . Optional [ typing . List [ typing . Union [ PythonModuleManagerConfig , PipelineModuleManagerConfig ] ] ] = None , extra_pipeline_folders : typing . Iterable [ str ] = None , ignore_errors : bool = False , ): self . _modules : typing . Optional [ typing . Dict [ str , ModuleManager ]] = None self . _module_cls_cache : typing . Dict [ str , typing . Type [ KiaraModule ]] = {} self . _default_python_mgr : typing . Optional [ PythonModuleManager ] = None self . _default_pipeline_mgr : typing . Optional [ PipelineModuleManager ] = None self . _custom_pipelines_mgr : typing . Optional [ PipelineModuleManager ] = None if extra_pipeline_folders is None : extra_pipeline_folders = [] self . _extra_pipeline_folders : typing . Iterable [ str ] = extra_pipeline_folders self . _ignore_errors : bool = ignore_errors if module_managers : raise NotImplementedError () # for mmc in module_managers: # mm = ModuleManager.from_config(mmc) # _mms.append(mm) self . _module_mgrs : typing . Optional [ typing . List [ ModuleManager ]] = None @property def module_managers ( self ) -> typing . Iterable [ ModuleManager ]: if self . _module_mgrs is not None : return self . _module_mgrs _mms = [ self . default_python_module_manager , self . default_pipeline_module_manager , self . default_custom_pipelines_manager , ] self . _module_mgrs = [] self . _modules = {} for mm in _mms : self . add_module_manager ( mm ) return self . _module_mgrs @property def module_map ( self ) -> typing . MutableMapping [ str , ModuleManager ]: if self . _modules is not None : return self . _modules # make sure module managers are initialized before this # this will also initialize the _modules attribute self . module_managers # noqa return self . _modules # type: ignore @property def default_python_module_manager ( self ) -> PythonModuleManager : if self . _default_python_mgr is None : self . _default_python_mgr = PythonModuleManager () return self . _default_python_mgr @property def default_pipeline_module_manager ( self ) -> PipelineModuleManager : if self . _default_pipeline_mgr is None : self . _default_pipeline_mgr = PipelineModuleManager ( folders = None , ignore_errors = self . _ignore_errors ) return self . _default_pipeline_mgr @property def default_custom_pipelines_manager ( self ) -> PipelineModuleManager : if self . _custom_pipelines_mgr is None : if self . _extra_pipeline_folders : folders = { \"extra\" : self . _extra_pipeline_folders } else : folders = {} self . _custom_pipelines_mgr = PipelineModuleManager ( folders = folders , ignore_errors = self . _ignore_errors ) return self . _custom_pipelines_mgr @property def available_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available modules\"\"\" return sorted ( set ( self . module_map . keys ())) @property def available_non_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return [ module_type for module_type in self . available_module_types if module_type != \"pipeline\" and not self . get_module_class ( module_type ) . is_pipeline () ] @property def available_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return [ module_type for module_type in self . available_module_types if module_type != \"pipeline\" and self . get_module_class ( module_type ) . is_pipeline () ] def get_module_types ( self ) -> typing . Iterable [ str ]: return self . available_module_types def is_pipeline_module ( self , module_type : str ): cls = self . get_module_class ( module_type = module_type ) return cls . is_pipeline () def add_module_manager ( self , module_manager : ModuleManager ): if self . _module_mgrs is None : self . module_managers # noqa for module_type in module_manager . get_module_types (): if module_type in self . module_map . keys (): log . warning ( f \"Duplicate module name ' { module_type } '. Ignoring all but the first.\" ) continue self . module_map [ module_type ] = module_manager self . _module_mgrs . append ( module_manager ) # type: ignore self . _value_types = None def register_pipeline_description ( self , data : typing . Union [ Path , str , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , raise_exception : bool = False , ) -> typing . Optional [ str ]: # making sure the module_map attribute is populated self . module_map # noqa name = self . default_custom_pipelines_manager . register_pipeline ( data = data , module_type_name = module_type_name , namespace = namespace ) if name in self . module_map . keys (): if raise_exception : raise Exception ( f \"Duplicate module name: { name } \" ) log . warning ( f \"Duplicate module name ' { name } '. Ignoring all but the first.\" ) return None else : self . module_map [ name ] = self . default_custom_pipelines_manager return name def get_module_class ( self , module_type : str ) -> typing . Type [ \"KiaraModule\" ]: if module_type == \"pipeline\" : from kiara import PipelineModule return PipelineModule mm = self . module_map . get ( module_type , None ) if mm is None : raise Exception ( f \"No module ' { module_type } ' available.\" ) if module_type in self . _module_cls_cache . keys (): return self . _module_cls_cache [ module_type ] cls = mm . get_module_class ( module_type ) if not hasattr ( cls , \"_module_type_name\" ): raise Exception ( f \"Class does not have a '_module_type_name' attribute: { cls } \" ) assert module_type . endswith ( cls . _module_type_name ) # type: ignore if hasattr ( cls , \"_module_type_id\" ) and cls . _module_type_id != \"pipeline\" and cls . _module_type_id != module_type : # type: ignore raise Exception ( f \"Can't create module class ' { cls } ', it already has a _module_type_id attribute and it's different to the module name ' { module_type } ': { cls . _module_type_id } \" # type: ignore ) self . _module_cls_cache [ module_type ] = cls setattr ( cls , \"_module_type_id\" , module_type ) return cls def create_module ( self , kiara : \"Kiara\" , id : typing . Optional [ str ], module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : mm = self . module_map . get ( module_type , None ) if mm is None : raise Exception ( f \"No module ' { module_type } ' registered. Available modules: { ', ' . join ( self . available_module_types ) } \" ) _ = self . get_module_class ( module_type ) # just to make sure the _module_type_id attribute is added return mm . create_module ( id = id , parent_id = parent_id , module_type = module_type , module_config = module_config , kiara = kiara , ) def find_modules_for_package ( self , package_name : str , include_core_modules : bool = True , include_pipelines : bool = True , ) -> typing . Dict [ str , typing . Type [ \"KiaraModule\" ]]: result = {} for module_type in self . available_module_types : if module_type == \"pipeline\" : continue module_cls = self . get_module_class ( module_type ) module_package = module_cls . get_type_metadata () . context . labels . get ( \"package\" , None ) if module_package != package_name : continue if module_cls . is_pipeline (): if include_pipelines : result [ module_type ] = module_cls else : if include_core_modules : result [ module_type ] = module_cls return result available_module_types : List [ str ] property readonly \u00b6 Return the names of all available modules available_non_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. available_pipeline_module_types : List [ str ] property readonly \u00b6 Return the names of all available pipeline-type modules. MergedModuleManagerConfig ( BaseModel ) pydantic-model \u00b6 Source code in kiara/module_mgmt/merged.py class MergedModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"pipeline\" ] folders : typing . List [ str ] = Field ( description = \"A list of folders that contain pipeline descriptions.\" , default_factory = list , ) @validator ( \"folders\" , pre = True ) def _validate_folders ( cls , v ): if isinstance ( v , str ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( v , Path ): item = v . as_posix () assert isinstance ( item , str ) result . append ( item ) return result folders : List [ str ] pydantic-field \u00b6 A list of folders that contain pipeline descriptions.","title":"merged"},{"location":"reference/kiara/module_mgmt/merged/#kiara.module_mgmt.merged.MergedModuleManager","text":"Source code in kiara/module_mgmt/merged.py class MergedModuleManager ( ModuleManager ): def __init__ ( self , module_managers : typing . Optional [ typing . List [ typing . Union [ PythonModuleManagerConfig , PipelineModuleManagerConfig ] ] ] = None , extra_pipeline_folders : typing . Iterable [ str ] = None , ignore_errors : bool = False , ): self . _modules : typing . Optional [ typing . Dict [ str , ModuleManager ]] = None self . _module_cls_cache : typing . Dict [ str , typing . Type [ KiaraModule ]] = {} self . _default_python_mgr : typing . Optional [ PythonModuleManager ] = None self . _default_pipeline_mgr : typing . Optional [ PipelineModuleManager ] = None self . _custom_pipelines_mgr : typing . Optional [ PipelineModuleManager ] = None if extra_pipeline_folders is None : extra_pipeline_folders = [] self . _extra_pipeline_folders : typing . Iterable [ str ] = extra_pipeline_folders self . _ignore_errors : bool = ignore_errors if module_managers : raise NotImplementedError () # for mmc in module_managers: # mm = ModuleManager.from_config(mmc) # _mms.append(mm) self . _module_mgrs : typing . Optional [ typing . List [ ModuleManager ]] = None @property def module_managers ( self ) -> typing . Iterable [ ModuleManager ]: if self . _module_mgrs is not None : return self . _module_mgrs _mms = [ self . default_python_module_manager , self . default_pipeline_module_manager , self . default_custom_pipelines_manager , ] self . _module_mgrs = [] self . _modules = {} for mm in _mms : self . add_module_manager ( mm ) return self . _module_mgrs @property def module_map ( self ) -> typing . MutableMapping [ str , ModuleManager ]: if self . _modules is not None : return self . _modules # make sure module managers are initialized before this # this will also initialize the _modules attribute self . module_managers # noqa return self . _modules # type: ignore @property def default_python_module_manager ( self ) -> PythonModuleManager : if self . _default_python_mgr is None : self . _default_python_mgr = PythonModuleManager () return self . _default_python_mgr @property def default_pipeline_module_manager ( self ) -> PipelineModuleManager : if self . _default_pipeline_mgr is None : self . _default_pipeline_mgr = PipelineModuleManager ( folders = None , ignore_errors = self . _ignore_errors ) return self . _default_pipeline_mgr @property def default_custom_pipelines_manager ( self ) -> PipelineModuleManager : if self . _custom_pipelines_mgr is None : if self . _extra_pipeline_folders : folders = { \"extra\" : self . _extra_pipeline_folders } else : folders = {} self . _custom_pipelines_mgr = PipelineModuleManager ( folders = folders , ignore_errors = self . _ignore_errors ) return self . _custom_pipelines_mgr @property def available_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available modules\"\"\" return sorted ( set ( self . module_map . keys ())) @property def available_non_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return [ module_type for module_type in self . available_module_types if module_type != \"pipeline\" and not self . get_module_class ( module_type ) . is_pipeline () ] @property def available_pipeline_module_types ( self ) -> typing . List [ str ]: \"\"\"Return the names of all available pipeline-type modules.\"\"\" return [ module_type for module_type in self . available_module_types if module_type != \"pipeline\" and self . get_module_class ( module_type ) . is_pipeline () ] def get_module_types ( self ) -> typing . Iterable [ str ]: return self . available_module_types def is_pipeline_module ( self , module_type : str ): cls = self . get_module_class ( module_type = module_type ) return cls . is_pipeline () def add_module_manager ( self , module_manager : ModuleManager ): if self . _module_mgrs is None : self . module_managers # noqa for module_type in module_manager . get_module_types (): if module_type in self . module_map . keys (): log . warning ( f \"Duplicate module name ' { module_type } '. Ignoring all but the first.\" ) continue self . module_map [ module_type ] = module_manager self . _module_mgrs . append ( module_manager ) # type: ignore self . _value_types = None def register_pipeline_description ( self , data : typing . Union [ Path , str , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , raise_exception : bool = False , ) -> typing . Optional [ str ]: # making sure the module_map attribute is populated self . module_map # noqa name = self . default_custom_pipelines_manager . register_pipeline ( data = data , module_type_name = module_type_name , namespace = namespace ) if name in self . module_map . keys (): if raise_exception : raise Exception ( f \"Duplicate module name: { name } \" ) log . warning ( f \"Duplicate module name ' { name } '. Ignoring all but the first.\" ) return None else : self . module_map [ name ] = self . default_custom_pipelines_manager return name def get_module_class ( self , module_type : str ) -> typing . Type [ \"KiaraModule\" ]: if module_type == \"pipeline\" : from kiara import PipelineModule return PipelineModule mm = self . module_map . get ( module_type , None ) if mm is None : raise Exception ( f \"No module ' { module_type } ' available.\" ) if module_type in self . _module_cls_cache . keys (): return self . _module_cls_cache [ module_type ] cls = mm . get_module_class ( module_type ) if not hasattr ( cls , \"_module_type_name\" ): raise Exception ( f \"Class does not have a '_module_type_name' attribute: { cls } \" ) assert module_type . endswith ( cls . _module_type_name ) # type: ignore if hasattr ( cls , \"_module_type_id\" ) and cls . _module_type_id != \"pipeline\" and cls . _module_type_id != module_type : # type: ignore raise Exception ( f \"Can't create module class ' { cls } ', it already has a _module_type_id attribute and it's different to the module name ' { module_type } ': { cls . _module_type_id } \" # type: ignore ) self . _module_cls_cache [ module_type ] = cls setattr ( cls , \"_module_type_id\" , module_type ) return cls def create_module ( self , kiara : \"Kiara\" , id : typing . Optional [ str ], module_type : str , module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , parent_id : typing . Optional [ str ] = None , ) -> \"KiaraModule\" : mm = self . module_map . get ( module_type , None ) if mm is None : raise Exception ( f \"No module ' { module_type } ' registered. Available modules: { ', ' . join ( self . available_module_types ) } \" ) _ = self . get_module_class ( module_type ) # just to make sure the _module_type_id attribute is added return mm . create_module ( id = id , parent_id = parent_id , module_type = module_type , module_config = module_config , kiara = kiara , ) def find_modules_for_package ( self , package_name : str , include_core_modules : bool = True , include_pipelines : bool = True , ) -> typing . Dict [ str , typing . Type [ \"KiaraModule\" ]]: result = {} for module_type in self . available_module_types : if module_type == \"pipeline\" : continue module_cls = self . get_module_class ( module_type ) module_package = module_cls . get_type_metadata () . context . labels . get ( \"package\" , None ) if module_package != package_name : continue if module_cls . is_pipeline (): if include_pipelines : result [ module_type ] = module_cls else : if include_core_modules : result [ module_type ] = module_cls return result","title":"MergedModuleManager"},{"location":"reference/kiara/module_mgmt/merged/#kiara.module_mgmt.merged.MergedModuleManager.available_module_types","text":"Return the names of all available modules","title":"available_module_types"},{"location":"reference/kiara/module_mgmt/merged/#kiara.module_mgmt.merged.MergedModuleManager.available_non_pipeline_module_types","text":"Return the names of all available pipeline-type modules.","title":"available_non_pipeline_module_types"},{"location":"reference/kiara/module_mgmt/merged/#kiara.module_mgmt.merged.MergedModuleManager.available_pipeline_module_types","text":"Return the names of all available pipeline-type modules.","title":"available_pipeline_module_types"},{"location":"reference/kiara/module_mgmt/merged/#kiara.module_mgmt.merged.MergedModuleManagerConfig","text":"Source code in kiara/module_mgmt/merged.py class MergedModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"pipeline\" ] folders : typing . List [ str ] = Field ( description = \"A list of folders that contain pipeline descriptions.\" , default_factory = list , ) @validator ( \"folders\" , pre = True ) def _validate_folders ( cls , v ): if isinstance ( v , str ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( v , Path ): item = v . as_posix () assert isinstance ( item , str ) result . append ( item ) return result","title":"MergedModuleManagerConfig"},{"location":"reference/kiara/module_mgmt/merged/#kiara.module_mgmt.merged.MergedModuleManagerConfig.folders","text":"A list of folders that contain pipeline descriptions.","title":"folders"},{"location":"reference/kiara/module_mgmt/pipelines/","text":"PipelineModuleManager ( ModuleManager ) \u00b6 Module manager that discovers pipeline descriptions, and create modules out of them. Parameters: Name Type Description Default folders Optional[Mapping[str, Union[str, pathlib.Path, Iterable[Union[str, pathlib.Path]]]]] a list of folders to search for pipeline descriptions, if 'None', 'kiara.pipelines' entrypoints are searched, as well as the user config None ignore_errors bool ignore any errors that occur during pipeline discovery (as much as that is possible) False Source code in kiara/module_mgmt/pipelines.py class PipelineModuleManager ( ModuleManager ): \"\"\"Module manager that discovers pipeline descriptions, and create modules out of them. Arguments: folders: a list of folders to search for pipeline descriptions, if 'None', 'kiara.pipelines' entrypoints are searched, as well as the user config ignore_errors: ignore any errors that occur during pipeline discovery (as much as that is possible) \"\"\" def __init__ ( self , folders : typing . Optional [ typing . Mapping [ str , typing . Union [ str , Path , typing . Iterable [ typing . Union [ str , Path ]]] ] ] = None , ignore_errors : bool = False , ): if folders is None : from kiara.utils.class_loading import find_all_kiara_pipeline_paths folders_map : typing . Dict [ str , typing . List [ typing . Tuple [ typing . Optional [ str ], str ]] ] = find_all_kiara_pipeline_paths ( skip_errors = ignore_errors ) if os . path . exists ( USER_PIPELINES_FOLDER ): folders_map [ \"user\" ] = [( None , USER_PIPELINES_FOLDER )] elif not folders : folders_map = {} else : assert isinstance ( folders , typing . Mapping ) assert \"user\" not in folders . keys () folders_map = {} for k , _folders in folders . items (): if isinstance ( _folders , str ) or not isinstance ( _folders , typing . Iterable ): raise NotImplementedError ( f \"Invalid folder configuration (must be an iterable): { _folders } \" ) for _f in _folders : # type: ignore if isinstance ( _f , Path ): _f = _f . as_posix () folders_map . setdefault ( k , []) . append (( None , _f )) self . _pipeline_desc_folders : typing . List [ Path ] = [] self . _pipeline_descs : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} self . _cached_classes : typing . Dict [ str , typing . Type [ PipelineModule ]] = {} for ns , paths in folders_map . items (): for path in paths : self . add_pipelines_path ( ns , path [ 1 ], path [ 0 ]) def register_pipeline ( self , data : typing . Union [ str , Path , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , ) -> str : \"\"\"Register a pipeline description to the pipeline pool. Arguments: data: the pipeline data (a dict, or a path to a file) module_type_name: the type name this pipeline should be registered as \"\"\" # TODO: verify that there is no conflict with module_type_name if isinstance ( data , str ): data = Path ( os . path . expanduser ( data )) if isinstance ( data , Path ): _name , _data = get_pipeline_details_from_path ( data ) _data = check_doc_sidecar ( data , _data ) if module_type_name : _name = module_type_name elif isinstance ( data , typing . Mapping ): _data = dict ( data ) if module_type_name : _data [ MODULE_TYPE_NAME_KEY ] = module_type_name _name = _data . get ( MODULE_TYPE_NAME_KEY , None ) if not _name : raise Exception ( f \"Can't register pipeline, no module type name available: { data } \" ) _data = { \"data\" : _data , \"source\" : data , \"source_type\" : \"dict\" } else : raise Exception ( f \"Can't register pipeline, must be dict-like data, not { type ( data ) } \" ) if not _name : raise Exception ( \"No pipeline name set.\" ) if not namespace : full_name = _name else : full_name = f \" { namespace } . { _name } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Can't register pipeline: duplicate workflow name ' { _name } '\" ) self . _pipeline_descs [ full_name ] = _data return full_name def add_pipelines_path ( self , namespace : str , path : typing . Union [ str , Path ], base_module : typing . Optional [ str ], ) -> typing . Iterable [ str ]: \"\"\"Add a pipeline description file or folder containing some to this manager. Arguments: namespace: the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted path: the path to a pipeline description file, or folder which contains some base_module: the base module the assembled pipeline modules under this path will be located at in the Python namespace Returns: a list of module type names that were added \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) elif isinstance ( path , typing . Iterable ): raise TypeError ( f \"Invalid type for path: { path } \" ) if not path . exists (): log . warning ( f \"Can't add pipeline path ' { path } ': path does not exist\" ) return [] elif path . is_dir (): files : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} for root , dirnames , filenames in os . walk ( path , topdown = True ): dirnames [:] = [ d for d in dirnames if d not in DEFAULT_EXCLUDE_DIRS ] for filename in [ f for f in filenames if os . path . isfile ( os . path . join ( root , f )) and any ( f . endswith ( ext ) for ext in VALID_PIPELINE_FILE_EXTENSIONS ) ]: try : full_path = os . path . join ( root , filename ) name , data = get_pipeline_details_from_path ( path = full_path , base_module = base_module ) data = check_doc_sidecar ( full_path , data ) rel_path = os . path . relpath ( os . path . dirname ( full_path ), path ) if not rel_path or rel_path == \".\" : ns_name = name else : _rel_path = rel_path . replace ( os . path . sep , \".\" ) ns_name = f \" { _rel_path } . { name } \" if not ns_name : raise Exception ( f \"Could not determine namespace for pipeline file ' { filename } '.\" ) if ns_name in files . keys (): raise Exception ( f \"Duplicate workflow name in namespace ' { namespace } ': { ns_name } \" ) files [ ns_name ] = data except Exception as e : log . warning ( f \"Ignoring invalid pipeline file ' { full_path } ': { e } \" ) elif path . is_file (): name , data = get_pipeline_details_from_path ( path = path , base_module = base_module ) data = check_doc_sidecar ( path , data ) if not name : raise Exception ( f \"Could not determine pipeline name for: { path } \" ) files = { name : data } result = {} for k , v in files . items (): if namespace . startswith ( \"_\" ): tokens = namespace . split ( \".\" ) if len ( tokens ) == 1 : _namespace = \"\" else : _namespace = \".\" . join ( tokens [ 1 :]) else : _namespace = namespace if not _namespace : full_name = k else : full_name = f \" { _namespace } . { k } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Duplicate workflow name: { name } \" ) result [ full_name ] = v self . _pipeline_descs . update ( result ) return result . keys () @property def pipeline_descs ( self ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: return self . _pipeline_descs def get_module_class ( self , module_type : str ) -> typing . Type [ \"PipelineModule\" ]: if module_type in self . _cached_classes . keys (): return self . _cached_classes [ module_type ] desc = self . _pipeline_descs . get ( module_type , None ) if desc is None : raise Exception ( f \"No pipeline with name ' { module_type } ' available.\" ) tokens = re . split ( r \"\\.|_\" , module_type ) cls_name = \"\" . join ( x . capitalize () or \"_\" for x in tokens ) if len ( tokens ) != 1 : full_name = \".\" . join ( tokens [ 0 : - 1 ] + [ cls_name ]) else : full_name = cls_name base_module = desc . get ( \"base_module\" , None ) cls = create_pipeline_class ( f \" { cls_name } PipelineModule\" , full_name , desc [ \"data\" ], base_module = base_module , ) setattr ( cls , \"_module_type_name\" , module_type ) self . _cached_classes [ module_type ] = cls return self . _cached_classes [ module_type ] def get_module_types ( self ) -> typing . Iterable [ str ]: return self . _pipeline_descs . keys () add_pipelines_path ( self , namespace , path , base_module ) \u00b6 Add a pipeline description file or folder containing some to this manager. Parameters: Name Type Description Default namespace str the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted required path Union[str, pathlib.Path] the path to a pipeline description file, or folder which contains some required base_module Optional[str] the base module the assembled pipeline modules under this path will be located at in the Python namespace required Returns: Type Description Iterable[str] a list of module type names that were added Source code in kiara/module_mgmt/pipelines.py def add_pipelines_path ( self , namespace : str , path : typing . Union [ str , Path ], base_module : typing . Optional [ str ], ) -> typing . Iterable [ str ]: \"\"\"Add a pipeline description file or folder containing some to this manager. Arguments: namespace: the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted path: the path to a pipeline description file, or folder which contains some base_module: the base module the assembled pipeline modules under this path will be located at in the Python namespace Returns: a list of module type names that were added \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) elif isinstance ( path , typing . Iterable ): raise TypeError ( f \"Invalid type for path: { path } \" ) if not path . exists (): log . warning ( f \"Can't add pipeline path ' { path } ': path does not exist\" ) return [] elif path . is_dir (): files : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} for root , dirnames , filenames in os . walk ( path , topdown = True ): dirnames [:] = [ d for d in dirnames if d not in DEFAULT_EXCLUDE_DIRS ] for filename in [ f for f in filenames if os . path . isfile ( os . path . join ( root , f )) and any ( f . endswith ( ext ) for ext in VALID_PIPELINE_FILE_EXTENSIONS ) ]: try : full_path = os . path . join ( root , filename ) name , data = get_pipeline_details_from_path ( path = full_path , base_module = base_module ) data = check_doc_sidecar ( full_path , data ) rel_path = os . path . relpath ( os . path . dirname ( full_path ), path ) if not rel_path or rel_path == \".\" : ns_name = name else : _rel_path = rel_path . replace ( os . path . sep , \".\" ) ns_name = f \" { _rel_path } . { name } \" if not ns_name : raise Exception ( f \"Could not determine namespace for pipeline file ' { filename } '.\" ) if ns_name in files . keys (): raise Exception ( f \"Duplicate workflow name in namespace ' { namespace } ': { ns_name } \" ) files [ ns_name ] = data except Exception as e : log . warning ( f \"Ignoring invalid pipeline file ' { full_path } ': { e } \" ) elif path . is_file (): name , data = get_pipeline_details_from_path ( path = path , base_module = base_module ) data = check_doc_sidecar ( path , data ) if not name : raise Exception ( f \"Could not determine pipeline name for: { path } \" ) files = { name : data } result = {} for k , v in files . items (): if namespace . startswith ( \"_\" ): tokens = namespace . split ( \".\" ) if len ( tokens ) == 1 : _namespace = \"\" else : _namespace = \".\" . join ( tokens [ 1 :]) else : _namespace = namespace if not _namespace : full_name = k else : full_name = f \" { _namespace } . { k } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Duplicate workflow name: { name } \" ) result [ full_name ] = v self . _pipeline_descs . update ( result ) return result . keys () register_pipeline ( self , data , module_type_name = None , namespace = None ) \u00b6 Register a pipeline description to the pipeline pool. Parameters: Name Type Description Default data Union[str, pathlib.Path, Mapping[str, Any]] the pipeline data (a dict, or a path to a file) required module_type_name Optional[str] the type name this pipeline should be registered as None Source code in kiara/module_mgmt/pipelines.py def register_pipeline ( self , data : typing . Union [ str , Path , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , ) -> str : \"\"\"Register a pipeline description to the pipeline pool. Arguments: data: the pipeline data (a dict, or a path to a file) module_type_name: the type name this pipeline should be registered as \"\"\" # TODO: verify that there is no conflict with module_type_name if isinstance ( data , str ): data = Path ( os . path . expanduser ( data )) if isinstance ( data , Path ): _name , _data = get_pipeline_details_from_path ( data ) _data = check_doc_sidecar ( data , _data ) if module_type_name : _name = module_type_name elif isinstance ( data , typing . Mapping ): _data = dict ( data ) if module_type_name : _data [ MODULE_TYPE_NAME_KEY ] = module_type_name _name = _data . get ( MODULE_TYPE_NAME_KEY , None ) if not _name : raise Exception ( f \"Can't register pipeline, no module type name available: { data } \" ) _data = { \"data\" : _data , \"source\" : data , \"source_type\" : \"dict\" } else : raise Exception ( f \"Can't register pipeline, must be dict-like data, not { type ( data ) } \" ) if not _name : raise Exception ( \"No pipeline name set.\" ) if not namespace : full_name = _name else : full_name = f \" { namespace } . { _name } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Can't register pipeline: duplicate workflow name ' { _name } '\" ) self . _pipeline_descs [ full_name ] = _data return full_name PipelineModuleManagerConfig ( BaseModel ) pydantic-model \u00b6 Source code in kiara/module_mgmt/pipelines.py class PipelineModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"pipeline\" ] folders : typing . List [ str ] = Field ( description = \"A list of folders that contain pipeline descriptions.\" , default_factory = list , ) @validator ( \"folders\" , pre = True ) def _validate_folders ( cls , v ): if isinstance ( v , str ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( v , Path ): item = v . as_posix () assert isinstance ( item , str ) result . append ( item ) return result folders : List [ str ] pydantic-field \u00b6 A list of folders that contain pipeline descriptions. get_pipeline_details_from_path ( path , module_type_name = None , base_module = None ) \u00b6 Load a pipeline description, save it's content, and determine it the pipeline base name. Parameters: Name Type Description Default path Union[str, pathlib.Path] the path to the pipeline file required module_type_name Optional[str] if specifies, overwrites any auto-detected or assigned pipeline name None base_module Optional[str] overrides the base module the assembled pipeline module will be located in the python hierarchy None Source code in kiara/module_mgmt/pipelines.py def get_pipeline_details_from_path ( path : typing . Union [ str , Path ], module_type_name : typing . Optional [ str ] = None , base_module : typing . Optional [ str ] = None , ) -> typing . Tuple [ typing . Optional [ str ], typing . Mapping [ str , typing . Any ]]: \"\"\"Load a pipeline description, save it's content, and determine it the pipeline base name. Arguments: path: the path to the pipeline file module_type_name: if specifies, overwrites any auto-detected or assigned pipeline name base_module: overrides the base module the assembled pipeline module will be located in the python hierarchy \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) if not path . is_file (): raise Exception ( f \"Can't add pipeline description ' { path . as_posix () } ': not a file\" ) data = get_data_from_file ( path ) if not data : raise Exception ( f \"Can't register pipeline file ' { path . as_posix () } ': no content.\" ) if module_type_name : data [ MODULE_TYPE_NAME_KEY ] = module_type_name filename = path . name if not isinstance ( data , typing . Mapping ): raise Exception ( \"Not a dictionary type.\" ) name = data . get ( MODULE_TYPE_NAME_KEY , None ) if name is None : name = filename . split ( \".\" , maxsplit = 1 )[ 0 ] result = { \"data\" : data , \"source\" : path . as_posix (), \"source_type\" : \"file\" } if base_module : result [ \"base_module\" ] = base_module return ( name , result )","title":"pipelines"},{"location":"reference/kiara/module_mgmt/pipelines/#kiara.module_mgmt.pipelines.PipelineModuleManager","text":"Module manager that discovers pipeline descriptions, and create modules out of them. Parameters: Name Type Description Default folders Optional[Mapping[str, Union[str, pathlib.Path, Iterable[Union[str, pathlib.Path]]]]] a list of folders to search for pipeline descriptions, if 'None', 'kiara.pipelines' entrypoints are searched, as well as the user config None ignore_errors bool ignore any errors that occur during pipeline discovery (as much as that is possible) False Source code in kiara/module_mgmt/pipelines.py class PipelineModuleManager ( ModuleManager ): \"\"\"Module manager that discovers pipeline descriptions, and create modules out of them. Arguments: folders: a list of folders to search for pipeline descriptions, if 'None', 'kiara.pipelines' entrypoints are searched, as well as the user config ignore_errors: ignore any errors that occur during pipeline discovery (as much as that is possible) \"\"\" def __init__ ( self , folders : typing . Optional [ typing . Mapping [ str , typing . Union [ str , Path , typing . Iterable [ typing . Union [ str , Path ]]] ] ] = None , ignore_errors : bool = False , ): if folders is None : from kiara.utils.class_loading import find_all_kiara_pipeline_paths folders_map : typing . Dict [ str , typing . List [ typing . Tuple [ typing . Optional [ str ], str ]] ] = find_all_kiara_pipeline_paths ( skip_errors = ignore_errors ) if os . path . exists ( USER_PIPELINES_FOLDER ): folders_map [ \"user\" ] = [( None , USER_PIPELINES_FOLDER )] elif not folders : folders_map = {} else : assert isinstance ( folders , typing . Mapping ) assert \"user\" not in folders . keys () folders_map = {} for k , _folders in folders . items (): if isinstance ( _folders , str ) or not isinstance ( _folders , typing . Iterable ): raise NotImplementedError ( f \"Invalid folder configuration (must be an iterable): { _folders } \" ) for _f in _folders : # type: ignore if isinstance ( _f , Path ): _f = _f . as_posix () folders_map . setdefault ( k , []) . append (( None , _f )) self . _pipeline_desc_folders : typing . List [ Path ] = [] self . _pipeline_descs : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} self . _cached_classes : typing . Dict [ str , typing . Type [ PipelineModule ]] = {} for ns , paths in folders_map . items (): for path in paths : self . add_pipelines_path ( ns , path [ 1 ], path [ 0 ]) def register_pipeline ( self , data : typing . Union [ str , Path , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , ) -> str : \"\"\"Register a pipeline description to the pipeline pool. Arguments: data: the pipeline data (a dict, or a path to a file) module_type_name: the type name this pipeline should be registered as \"\"\" # TODO: verify that there is no conflict with module_type_name if isinstance ( data , str ): data = Path ( os . path . expanduser ( data )) if isinstance ( data , Path ): _name , _data = get_pipeline_details_from_path ( data ) _data = check_doc_sidecar ( data , _data ) if module_type_name : _name = module_type_name elif isinstance ( data , typing . Mapping ): _data = dict ( data ) if module_type_name : _data [ MODULE_TYPE_NAME_KEY ] = module_type_name _name = _data . get ( MODULE_TYPE_NAME_KEY , None ) if not _name : raise Exception ( f \"Can't register pipeline, no module type name available: { data } \" ) _data = { \"data\" : _data , \"source\" : data , \"source_type\" : \"dict\" } else : raise Exception ( f \"Can't register pipeline, must be dict-like data, not { type ( data ) } \" ) if not _name : raise Exception ( \"No pipeline name set.\" ) if not namespace : full_name = _name else : full_name = f \" { namespace } . { _name } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Can't register pipeline: duplicate workflow name ' { _name } '\" ) self . _pipeline_descs [ full_name ] = _data return full_name def add_pipelines_path ( self , namespace : str , path : typing . Union [ str , Path ], base_module : typing . Optional [ str ], ) -> typing . Iterable [ str ]: \"\"\"Add a pipeline description file or folder containing some to this manager. Arguments: namespace: the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted path: the path to a pipeline description file, or folder which contains some base_module: the base module the assembled pipeline modules under this path will be located at in the Python namespace Returns: a list of module type names that were added \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) elif isinstance ( path , typing . Iterable ): raise TypeError ( f \"Invalid type for path: { path } \" ) if not path . exists (): log . warning ( f \"Can't add pipeline path ' { path } ': path does not exist\" ) return [] elif path . is_dir (): files : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} for root , dirnames , filenames in os . walk ( path , topdown = True ): dirnames [:] = [ d for d in dirnames if d not in DEFAULT_EXCLUDE_DIRS ] for filename in [ f for f in filenames if os . path . isfile ( os . path . join ( root , f )) and any ( f . endswith ( ext ) for ext in VALID_PIPELINE_FILE_EXTENSIONS ) ]: try : full_path = os . path . join ( root , filename ) name , data = get_pipeline_details_from_path ( path = full_path , base_module = base_module ) data = check_doc_sidecar ( full_path , data ) rel_path = os . path . relpath ( os . path . dirname ( full_path ), path ) if not rel_path or rel_path == \".\" : ns_name = name else : _rel_path = rel_path . replace ( os . path . sep , \".\" ) ns_name = f \" { _rel_path } . { name } \" if not ns_name : raise Exception ( f \"Could not determine namespace for pipeline file ' { filename } '.\" ) if ns_name in files . keys (): raise Exception ( f \"Duplicate workflow name in namespace ' { namespace } ': { ns_name } \" ) files [ ns_name ] = data except Exception as e : log . warning ( f \"Ignoring invalid pipeline file ' { full_path } ': { e } \" ) elif path . is_file (): name , data = get_pipeline_details_from_path ( path = path , base_module = base_module ) data = check_doc_sidecar ( path , data ) if not name : raise Exception ( f \"Could not determine pipeline name for: { path } \" ) files = { name : data } result = {} for k , v in files . items (): if namespace . startswith ( \"_\" ): tokens = namespace . split ( \".\" ) if len ( tokens ) == 1 : _namespace = \"\" else : _namespace = \".\" . join ( tokens [ 1 :]) else : _namespace = namespace if not _namespace : full_name = k else : full_name = f \" { _namespace } . { k } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Duplicate workflow name: { name } \" ) result [ full_name ] = v self . _pipeline_descs . update ( result ) return result . keys () @property def pipeline_descs ( self ) -> typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]: return self . _pipeline_descs def get_module_class ( self , module_type : str ) -> typing . Type [ \"PipelineModule\" ]: if module_type in self . _cached_classes . keys (): return self . _cached_classes [ module_type ] desc = self . _pipeline_descs . get ( module_type , None ) if desc is None : raise Exception ( f \"No pipeline with name ' { module_type } ' available.\" ) tokens = re . split ( r \"\\.|_\" , module_type ) cls_name = \"\" . join ( x . capitalize () or \"_\" for x in tokens ) if len ( tokens ) != 1 : full_name = \".\" . join ( tokens [ 0 : - 1 ] + [ cls_name ]) else : full_name = cls_name base_module = desc . get ( \"base_module\" , None ) cls = create_pipeline_class ( f \" { cls_name } PipelineModule\" , full_name , desc [ \"data\" ], base_module = base_module , ) setattr ( cls , \"_module_type_name\" , module_type ) self . _cached_classes [ module_type ] = cls return self . _cached_classes [ module_type ] def get_module_types ( self ) -> typing . Iterable [ str ]: return self . _pipeline_descs . keys ()","title":"PipelineModuleManager"},{"location":"reference/kiara/module_mgmt/pipelines/#kiara.module_mgmt.pipelines.PipelineModuleManager.add_pipelines_path","text":"Add a pipeline description file or folder containing some to this manager. Parameters: Name Type Description Default namespace str the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted required path Union[str, pathlib.Path] the path to a pipeline description file, or folder which contains some required base_module Optional[str] the base module the assembled pipeline modules under this path will be located at in the Python namespace required Returns: Type Description Iterable[str] a list of module type names that were added Source code in kiara/module_mgmt/pipelines.py def add_pipelines_path ( self , namespace : str , path : typing . Union [ str , Path ], base_module : typing . Optional [ str ], ) -> typing . Iterable [ str ]: \"\"\"Add a pipeline description file or folder containing some to this manager. Arguments: namespace: the namespace the pipeline modules found under this path will be part of, if it starts with '_' it will be omitted path: the path to a pipeline description file, or folder which contains some base_module: the base module the assembled pipeline modules under this path will be located at in the Python namespace Returns: a list of module type names that were added \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) elif isinstance ( path , typing . Iterable ): raise TypeError ( f \"Invalid type for path: { path } \" ) if not path . exists (): log . warning ( f \"Can't add pipeline path ' { path } ': path does not exist\" ) return [] elif path . is_dir (): files : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = {} for root , dirnames , filenames in os . walk ( path , topdown = True ): dirnames [:] = [ d for d in dirnames if d not in DEFAULT_EXCLUDE_DIRS ] for filename in [ f for f in filenames if os . path . isfile ( os . path . join ( root , f )) and any ( f . endswith ( ext ) for ext in VALID_PIPELINE_FILE_EXTENSIONS ) ]: try : full_path = os . path . join ( root , filename ) name , data = get_pipeline_details_from_path ( path = full_path , base_module = base_module ) data = check_doc_sidecar ( full_path , data ) rel_path = os . path . relpath ( os . path . dirname ( full_path ), path ) if not rel_path or rel_path == \".\" : ns_name = name else : _rel_path = rel_path . replace ( os . path . sep , \".\" ) ns_name = f \" { _rel_path } . { name } \" if not ns_name : raise Exception ( f \"Could not determine namespace for pipeline file ' { filename } '.\" ) if ns_name in files . keys (): raise Exception ( f \"Duplicate workflow name in namespace ' { namespace } ': { ns_name } \" ) files [ ns_name ] = data except Exception as e : log . warning ( f \"Ignoring invalid pipeline file ' { full_path } ': { e } \" ) elif path . is_file (): name , data = get_pipeline_details_from_path ( path = path , base_module = base_module ) data = check_doc_sidecar ( path , data ) if not name : raise Exception ( f \"Could not determine pipeline name for: { path } \" ) files = { name : data } result = {} for k , v in files . items (): if namespace . startswith ( \"_\" ): tokens = namespace . split ( \".\" ) if len ( tokens ) == 1 : _namespace = \"\" else : _namespace = \".\" . join ( tokens [ 1 :]) else : _namespace = namespace if not _namespace : full_name = k else : full_name = f \" { _namespace } . { k } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Duplicate workflow name: { name } \" ) result [ full_name ] = v self . _pipeline_descs . update ( result ) return result . keys ()","title":"add_pipelines_path()"},{"location":"reference/kiara/module_mgmt/pipelines/#kiara.module_mgmt.pipelines.PipelineModuleManager.register_pipeline","text":"Register a pipeline description to the pipeline pool. Parameters: Name Type Description Default data Union[str, pathlib.Path, Mapping[str, Any]] the pipeline data (a dict, or a path to a file) required module_type_name Optional[str] the type name this pipeline should be registered as None Source code in kiara/module_mgmt/pipelines.py def register_pipeline ( self , data : typing . Union [ str , Path , typing . Mapping [ str , typing . Any ]], module_type_name : typing . Optional [ str ] = None , namespace : typing . Optional [ str ] = None , ) -> str : \"\"\"Register a pipeline description to the pipeline pool. Arguments: data: the pipeline data (a dict, or a path to a file) module_type_name: the type name this pipeline should be registered as \"\"\" # TODO: verify that there is no conflict with module_type_name if isinstance ( data , str ): data = Path ( os . path . expanduser ( data )) if isinstance ( data , Path ): _name , _data = get_pipeline_details_from_path ( data ) _data = check_doc_sidecar ( data , _data ) if module_type_name : _name = module_type_name elif isinstance ( data , typing . Mapping ): _data = dict ( data ) if module_type_name : _data [ MODULE_TYPE_NAME_KEY ] = module_type_name _name = _data . get ( MODULE_TYPE_NAME_KEY , None ) if not _name : raise Exception ( f \"Can't register pipeline, no module type name available: { data } \" ) _data = { \"data\" : _data , \"source\" : data , \"source_type\" : \"dict\" } else : raise Exception ( f \"Can't register pipeline, must be dict-like data, not { type ( data ) } \" ) if not _name : raise Exception ( \"No pipeline name set.\" ) if not namespace : full_name = _name else : full_name = f \" { namespace } . { _name } \" if full_name . startswith ( \"core.\" ): full_name = full_name [ 5 :] if full_name in self . _pipeline_descs . keys (): raise Exception ( f \"Can't register pipeline: duplicate workflow name ' { _name } '\" ) self . _pipeline_descs [ full_name ] = _data return full_name","title":"register_pipeline()"},{"location":"reference/kiara/module_mgmt/pipelines/#kiara.module_mgmt.pipelines.PipelineModuleManagerConfig","text":"Source code in kiara/module_mgmt/pipelines.py class PipelineModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"pipeline\" ] folders : typing . List [ str ] = Field ( description = \"A list of folders that contain pipeline descriptions.\" , default_factory = list , ) @validator ( \"folders\" , pre = True ) def _validate_folders ( cls , v ): if isinstance ( v , str ): v = [ v ] assert isinstance ( v , typing . Iterable ) result = [] for item in v : if isinstance ( v , Path ): item = v . as_posix () assert isinstance ( item , str ) result . append ( item ) return result","title":"PipelineModuleManagerConfig"},{"location":"reference/kiara/module_mgmt/pipelines/#kiara.module_mgmt.pipelines.PipelineModuleManagerConfig.folders","text":"A list of folders that contain pipeline descriptions.","title":"folders"},{"location":"reference/kiara/module_mgmt/pipelines/#kiara.module_mgmt.pipelines.get_pipeline_details_from_path","text":"Load a pipeline description, save it's content, and determine it the pipeline base name. Parameters: Name Type Description Default path Union[str, pathlib.Path] the path to the pipeline file required module_type_name Optional[str] if specifies, overwrites any auto-detected or assigned pipeline name None base_module Optional[str] overrides the base module the assembled pipeline module will be located in the python hierarchy None Source code in kiara/module_mgmt/pipelines.py def get_pipeline_details_from_path ( path : typing . Union [ str , Path ], module_type_name : typing . Optional [ str ] = None , base_module : typing . Optional [ str ] = None , ) -> typing . Tuple [ typing . Optional [ str ], typing . Mapping [ str , typing . Any ]]: \"\"\"Load a pipeline description, save it's content, and determine it the pipeline base name. Arguments: path: the path to the pipeline file module_type_name: if specifies, overwrites any auto-detected or assigned pipeline name base_module: overrides the base module the assembled pipeline module will be located in the python hierarchy \"\"\" if isinstance ( path , str ): path = Path ( os . path . expanduser ( path )) if not path . is_file (): raise Exception ( f \"Can't add pipeline description ' { path . as_posix () } ': not a file\" ) data = get_data_from_file ( path ) if not data : raise Exception ( f \"Can't register pipeline file ' { path . as_posix () } ': no content.\" ) if module_type_name : data [ MODULE_TYPE_NAME_KEY ] = module_type_name filename = path . name if not isinstance ( data , typing . Mapping ): raise Exception ( \"Not a dictionary type.\" ) name = data . get ( MODULE_TYPE_NAME_KEY , None ) if name is None : name = filename . split ( \".\" , maxsplit = 1 )[ 0 ] result = { \"data\" : data , \"source\" : path . as_posix (), \"source_type\" : \"file\" } if base_module : result [ \"base_module\" ] = base_module return ( name , result )","title":"get_pipeline_details_from_path()"},{"location":"reference/kiara/module_mgmt/python_classes/","text":"PythonModuleManagerConfig ( BaseModel ) pydantic-model \u00b6 Source code in kiara/module_mgmt/python_classes.py class PythonModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"python\" ] module_classes : typing . Dict [ str , typing . Type [ \"KiaraModule\" ]] = Field ( description = \"The module classes this manager should hold.\" ) @validator ( \"module_classes\" , pre = True ) def _ensure_module_class_types ( cls , v ): _classes = [] if v : for _cls in v : if isinstance ( _cls , str ): try : module_name , cls_name = _cls . rsplit ( \".\" , maxsplit = 1 ) module = __import__ ( module_name ) _cls = getattr ( module , cls_name ) except Exception : raise ValueError ( f \"Can't parse value ' { _cls } ' into KiaraModule class.\" ) from kiara import KiaraModule # noqa if not issubclass ( _cls , KiaraModule ): raise ValueError ( f \"Not a KiaraModule sub-class: { _cls } \" ) _classes . append ( _cls ) return _classes module_classes : Dict [ str , Type [ KiaraModule ]] pydantic-field required \u00b6 The module classes this manager should hold.","title":"python_classes"},{"location":"reference/kiara/module_mgmt/python_classes/#kiara.module_mgmt.python_classes.PythonModuleManagerConfig","text":"Source code in kiara/module_mgmt/python_classes.py class PythonModuleManagerConfig ( BaseModel ): module_manager_type : Literal [ \"python\" ] module_classes : typing . Dict [ str , typing . Type [ \"KiaraModule\" ]] = Field ( description = \"The module classes this manager should hold.\" ) @validator ( \"module_classes\" , pre = True ) def _ensure_module_class_types ( cls , v ): _classes = [] if v : for _cls in v : if isinstance ( _cls , str ): try : module_name , cls_name = _cls . rsplit ( \".\" , maxsplit = 1 ) module = __import__ ( module_name ) _cls = getattr ( module , cls_name ) except Exception : raise ValueError ( f \"Can't parse value ' { _cls } ' into KiaraModule class.\" ) from kiara import KiaraModule # noqa if not issubclass ( _cls , KiaraModule ): raise ValueError ( f \"Not a KiaraModule sub-class: { _cls } \" ) _classes . append ( _cls ) return _classes","title":"PythonModuleManagerConfig"},{"location":"reference/kiara/module_mgmt/python_classes/#kiara.module_mgmt.python_classes.PythonModuleManagerConfig.module_classes","text":"The module classes this manager should hold.","title":"module_classes"},{"location":"reference/kiara/modules/__init__/","text":"Base module under which the 'official' KiaraModule implementations live. metadata \u00b6 ExtractPythonClass ( ExtractMetadataModule ) \u00b6 Extract metadata about the Python type of a value. Source code in kiara/modules/metadata.py class ExtractPythonClass ( ExtractMetadataModule ): \"\"\"Extract metadata about the Python type of a value.\"\"\" _module_type_name = \"metadata.python_class\" @classmethod def _get_supported_types ( cls ) -> typing . Union [ str , typing . Iterable [ str ]]: return \"*\" @classmethod def get_metadata_key ( cls ) -> str : return \"python_class\" def _get_metadata_schema ( self , type : str ) -> typing . Union [ str , typing . Type [ BaseModel ]]: return PythonClassMetadata def extract_metadata ( self , value : Value ) -> typing . Mapping [ str , typing . Any ]: item = value . get_value_data () cls = item . __class__ return { \"class_name\" : cls . __name__ , \"module_name\" : cls . __module__ , \"full_name\" : f \" { cls . __module__ } . { cls . __name__ } \" , } pipelines special \u00b6 Base module that holds PipelineModule classes that are auto-generated from pipeline descriptions in the pipelines folder. type_conversion \u00b6 OldTypeConversionModule ( KiaraModule ) \u00b6 Source code in kiara/modules/type_conversion.py class OldTypeConversionModule ( KiaraModule ): _config_cls = TypeConversionModuleConfig @classmethod @abc . abstractmethod def _get_supported_source_types ( self ) -> typing . Union [ typing . Iterable [ str ], str ]: pass @classmethod @abc . abstractmethod def _get_target_types ( self ) -> typing . Union [ typing . Iterable [ str ], str ]: pass @classmethod def get_supported_source_types ( self ) -> typing . Set [ str ]: _types : typing . Iterable [ str ] = self . _get_supported_source_types () if isinstance ( _types , str ): _types = [ _types ] if \"config\" in _types : raise Exception ( \"Invalid source type, type name 'config' is invalid.\" ) return set ( _types ) @classmethod def get_supported_target_types ( self ) -> typing . Set [ str ]: _types : typing . Iterable [ str ] = self . _get_target_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @property def source_type ( self ) -> str : data_type = self . get_config_value ( \"source_type\" ) supported = self . get_supported_source_types () if \"*\" not in supported and data_type not in supported : raise ValueError ( f \"Invalid module configuration, source type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_source_types ()) } .\" ) return data_type @property def target_type ( self ) -> str : data_type = self . get_config_value ( \"target_type\" ) if data_type not in self . get_supported_target_types (): raise ValueError ( f \"Invalid module configuration, target type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_target_types ()) } .\" ) return data_type def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"source_value\" : { \"type\" : self . source_type , \"doc\" : f \"A value of type ' { self . source_type } '.\" , }, \"config\" : { \"type\" : \"dict\" , \"doc\" : \"The configuration for the transformation.\" , \"optional\" : True , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"target_value\" : { \"type\" : self . target_type , \"doc\" : f \"A value of type ' { self . target_type } '.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value = inputs . get_value_obj ( \"source_value\" ) if value . value_schema . type != self . source_type : raise KiaraProcessingException ( f \"Can't convert value of source type ' { value . value_schema . type } '. Expected type ' { self . source_type } '.\" ) config = inputs . get_value_data ( \"config\" ) if config is None : config = {} target_value = self . convert ( value = value , config = config ) # TODO: validate value? outputs . set_value ( \"target_value\" , target_value ) @abc . abstractmethod def convert ( self , value : Value , config : typing . Mapping [ str , typing . Any ] ) -> typing . Any : pass create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/type_conversion.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"source_value\" : { \"type\" : self . source_type , \"doc\" : f \"A value of type ' { self . source_type } '.\" , }, \"config\" : { \"type\" : \"dict\" , \"doc\" : \"The configuration for the transformation.\" , \"optional\" : True , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/type_conversion.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"target_value\" : { \"type\" : self . target_type , \"doc\" : f \"A value of type ' { self . target_type } '.\" , } } return outputs TypeConversionModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/modules/type_conversion.py class TypeConversionModuleConfig ( ModuleTypeConfigSchema ): source_type : str = Field ( description = \"The source type.\" ) target_type : str = Field ( description = \"The target type.\" ) source_type : str pydantic-field required \u00b6 The source type. target_type : str pydantic-field required \u00b6 The target type.","title":"modules"},{"location":"reference/kiara/modules/__init__/#kiara.modules.metadata","text":"","title":"metadata"},{"location":"reference/kiara/modules/__init__/#kiara.modules.metadata.ExtractPythonClass","text":"Extract metadata about the Python type of a value. Source code in kiara/modules/metadata.py class ExtractPythonClass ( ExtractMetadataModule ): \"\"\"Extract metadata about the Python type of a value.\"\"\" _module_type_name = \"metadata.python_class\" @classmethod def _get_supported_types ( cls ) -> typing . Union [ str , typing . Iterable [ str ]]: return \"*\" @classmethod def get_metadata_key ( cls ) -> str : return \"python_class\" def _get_metadata_schema ( self , type : str ) -> typing . Union [ str , typing . Type [ BaseModel ]]: return PythonClassMetadata def extract_metadata ( self , value : Value ) -> typing . Mapping [ str , typing . Any ]: item = value . get_value_data () cls = item . __class__ return { \"class_name\" : cls . __name__ , \"module_name\" : cls . __module__ , \"full_name\" : f \" { cls . __module__ } . { cls . __name__ } \" , }","title":"ExtractPythonClass"},{"location":"reference/kiara/modules/__init__/#kiara.modules.pipelines","text":"Base module that holds PipelineModule classes that are auto-generated from pipeline descriptions in the pipelines folder.","title":"pipelines"},{"location":"reference/kiara/modules/__init__/#kiara.modules.type_conversion","text":"","title":"type_conversion"},{"location":"reference/kiara/modules/__init__/#kiara.modules.type_conversion.OldTypeConversionModule","text":"Source code in kiara/modules/type_conversion.py class OldTypeConversionModule ( KiaraModule ): _config_cls = TypeConversionModuleConfig @classmethod @abc . abstractmethod def _get_supported_source_types ( self ) -> typing . Union [ typing . Iterable [ str ], str ]: pass @classmethod @abc . abstractmethod def _get_target_types ( self ) -> typing . Union [ typing . Iterable [ str ], str ]: pass @classmethod def get_supported_source_types ( self ) -> typing . Set [ str ]: _types : typing . Iterable [ str ] = self . _get_supported_source_types () if isinstance ( _types , str ): _types = [ _types ] if \"config\" in _types : raise Exception ( \"Invalid source type, type name 'config' is invalid.\" ) return set ( _types ) @classmethod def get_supported_target_types ( self ) -> typing . Set [ str ]: _types : typing . Iterable [ str ] = self . _get_target_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @property def source_type ( self ) -> str : data_type = self . get_config_value ( \"source_type\" ) supported = self . get_supported_source_types () if \"*\" not in supported and data_type not in supported : raise ValueError ( f \"Invalid module configuration, source type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_source_types ()) } .\" ) return data_type @property def target_type ( self ) -> str : data_type = self . get_config_value ( \"target_type\" ) if data_type not in self . get_supported_target_types (): raise ValueError ( f \"Invalid module configuration, target type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_target_types ()) } .\" ) return data_type def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"source_value\" : { \"type\" : self . source_type , \"doc\" : f \"A value of type ' { self . source_type } '.\" , }, \"config\" : { \"type\" : \"dict\" , \"doc\" : \"The configuration for the transformation.\" , \"optional\" : True , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"target_value\" : { \"type\" : self . target_type , \"doc\" : f \"A value of type ' { self . target_type } '.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value = inputs . get_value_obj ( \"source_value\" ) if value . value_schema . type != self . source_type : raise KiaraProcessingException ( f \"Can't convert value of source type ' { value . value_schema . type } '. Expected type ' { self . source_type } '.\" ) config = inputs . get_value_data ( \"config\" ) if config is None : config = {} target_value = self . convert ( value = value , config = config ) # TODO: validate value? outputs . set_value ( \"target_value\" , target_value ) @abc . abstractmethod def convert ( self , value : Value , config : typing . Mapping [ str , typing . Any ] ) -> typing . Any : pass","title":"OldTypeConversionModule"},{"location":"reference/kiara/modules/__init__/#kiara.modules.type_conversion.TypeConversionModuleConfig","text":"Source code in kiara/modules/type_conversion.py class TypeConversionModuleConfig ( ModuleTypeConfigSchema ): source_type : str = Field ( description = \"The source type.\" ) target_type : str = Field ( description = \"The target type.\" )","title":"TypeConversionModuleConfig"},{"location":"reference/kiara/modules/metadata/","text":"ExtractPythonClass ( ExtractMetadataModule ) \u00b6 Extract metadata about the Python type of a value. Source code in kiara/modules/metadata.py class ExtractPythonClass ( ExtractMetadataModule ): \"\"\"Extract metadata about the Python type of a value.\"\"\" _module_type_name = \"metadata.python_class\" @classmethod def _get_supported_types ( cls ) -> typing . Union [ str , typing . Iterable [ str ]]: return \"*\" @classmethod def get_metadata_key ( cls ) -> str : return \"python_class\" def _get_metadata_schema ( self , type : str ) -> typing . Union [ str , typing . Type [ BaseModel ]]: return PythonClassMetadata def extract_metadata ( self , value : Value ) -> typing . Mapping [ str , typing . Any ]: item = value . get_value_data () cls = item . __class__ return { \"class_name\" : cls . __name__ , \"module_name\" : cls . __module__ , \"full_name\" : f \" { cls . __module__ } . { cls . __name__ } \" , }","title":"metadata"},{"location":"reference/kiara/modules/metadata/#kiara.modules.metadata.ExtractPythonClass","text":"Extract metadata about the Python type of a value. Source code in kiara/modules/metadata.py class ExtractPythonClass ( ExtractMetadataModule ): \"\"\"Extract metadata about the Python type of a value.\"\"\" _module_type_name = \"metadata.python_class\" @classmethod def _get_supported_types ( cls ) -> typing . Union [ str , typing . Iterable [ str ]]: return \"*\" @classmethod def get_metadata_key ( cls ) -> str : return \"python_class\" def _get_metadata_schema ( self , type : str ) -> typing . Union [ str , typing . Type [ BaseModel ]]: return PythonClassMetadata def extract_metadata ( self , value : Value ) -> typing . Mapping [ str , typing . Any ]: item = value . get_value_data () cls = item . __class__ return { \"class_name\" : cls . __name__ , \"module_name\" : cls . __module__ , \"full_name\" : f \" { cls . __module__ } . { cls . __name__ } \" , }","title":"ExtractPythonClass"},{"location":"reference/kiara/modules/save_value/","text":"","title":"save_value"},{"location":"reference/kiara/modules/type_conversion/","text":"OldTypeConversionModule ( KiaraModule ) \u00b6 Source code in kiara/modules/type_conversion.py class OldTypeConversionModule ( KiaraModule ): _config_cls = TypeConversionModuleConfig @classmethod @abc . abstractmethod def _get_supported_source_types ( self ) -> typing . Union [ typing . Iterable [ str ], str ]: pass @classmethod @abc . abstractmethod def _get_target_types ( self ) -> typing . Union [ typing . Iterable [ str ], str ]: pass @classmethod def get_supported_source_types ( self ) -> typing . Set [ str ]: _types : typing . Iterable [ str ] = self . _get_supported_source_types () if isinstance ( _types , str ): _types = [ _types ] if \"config\" in _types : raise Exception ( \"Invalid source type, type name 'config' is invalid.\" ) return set ( _types ) @classmethod def get_supported_target_types ( self ) -> typing . Set [ str ]: _types : typing . Iterable [ str ] = self . _get_target_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @property def source_type ( self ) -> str : data_type = self . get_config_value ( \"source_type\" ) supported = self . get_supported_source_types () if \"*\" not in supported and data_type not in supported : raise ValueError ( f \"Invalid module configuration, source type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_source_types ()) } .\" ) return data_type @property def target_type ( self ) -> str : data_type = self . get_config_value ( \"target_type\" ) if data_type not in self . get_supported_target_types (): raise ValueError ( f \"Invalid module configuration, target type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_target_types ()) } .\" ) return data_type def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"source_value\" : { \"type\" : self . source_type , \"doc\" : f \"A value of type ' { self . source_type } '.\" , }, \"config\" : { \"type\" : \"dict\" , \"doc\" : \"The configuration for the transformation.\" , \"optional\" : True , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"target_value\" : { \"type\" : self . target_type , \"doc\" : f \"A value of type ' { self . target_type } '.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value = inputs . get_value_obj ( \"source_value\" ) if value . value_schema . type != self . source_type : raise KiaraProcessingException ( f \"Can't convert value of source type ' { value . value_schema . type } '. Expected type ' { self . source_type } '.\" ) config = inputs . get_value_data ( \"config\" ) if config is None : config = {} target_value = self . convert ( value = value , config = config ) # TODO: validate value? outputs . set_value ( \"target_value\" , target_value ) @abc . abstractmethod def convert ( self , value : Value , config : typing . Mapping [ str , typing . Any ] ) -> typing . Any : pass create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/type_conversion.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"source_value\" : { \"type\" : self . source_type , \"doc\" : f \"A value of type ' { self . source_type } '.\" , }, \"config\" : { \"type\" : \"dict\" , \"doc\" : \"The configuration for the transformation.\" , \"optional\" : True , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/type_conversion.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"target_value\" : { \"type\" : self . target_type , \"doc\" : f \"A value of type ' { self . target_type } '.\" , } } return outputs TypeConversionModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/modules/type_conversion.py class TypeConversionModuleConfig ( ModuleTypeConfigSchema ): source_type : str = Field ( description = \"The source type.\" ) target_type : str = Field ( description = \"The target type.\" ) source_type : str pydantic-field required \u00b6 The source type. target_type : str pydantic-field required \u00b6 The target type.","title":"type_conversion"},{"location":"reference/kiara/modules/type_conversion/#kiara.modules.type_conversion.OldTypeConversionModule","text":"Source code in kiara/modules/type_conversion.py class OldTypeConversionModule ( KiaraModule ): _config_cls = TypeConversionModuleConfig @classmethod @abc . abstractmethod def _get_supported_source_types ( self ) -> typing . Union [ typing . Iterable [ str ], str ]: pass @classmethod @abc . abstractmethod def _get_target_types ( self ) -> typing . Union [ typing . Iterable [ str ], str ]: pass @classmethod def get_supported_source_types ( self ) -> typing . Set [ str ]: _types : typing . Iterable [ str ] = self . _get_supported_source_types () if isinstance ( _types , str ): _types = [ _types ] if \"config\" in _types : raise Exception ( \"Invalid source type, type name 'config' is invalid.\" ) return set ( _types ) @classmethod def get_supported_target_types ( self ) -> typing . Set [ str ]: _types : typing . Iterable [ str ] = self . _get_target_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) @property def source_type ( self ) -> str : data_type = self . get_config_value ( \"source_type\" ) supported = self . get_supported_source_types () if \"*\" not in supported and data_type not in supported : raise ValueError ( f \"Invalid module configuration, source type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_source_types ()) } .\" ) return data_type @property def target_type ( self ) -> str : data_type = self . get_config_value ( \"target_type\" ) if data_type not in self . get_supported_target_types (): raise ValueError ( f \"Invalid module configuration, target type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_target_types ()) } .\" ) return data_type def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"source_value\" : { \"type\" : self . source_type , \"doc\" : f \"A value of type ' { self . source_type } '.\" , }, \"config\" : { \"type\" : \"dict\" , \"doc\" : \"The configuration for the transformation.\" , \"optional\" : True , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"target_value\" : { \"type\" : self . target_type , \"doc\" : f \"A value of type ' { self . target_type } '.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value = inputs . get_value_obj ( \"source_value\" ) if value . value_schema . type != self . source_type : raise KiaraProcessingException ( f \"Can't convert value of source type ' { value . value_schema . type } '. Expected type ' { self . source_type } '.\" ) config = inputs . get_value_data ( \"config\" ) if config is None : config = {} target_value = self . convert ( value = value , config = config ) # TODO: validate value? outputs . set_value ( \"target_value\" , target_value ) @abc . abstractmethod def convert ( self , value : Value , config : typing . Mapping [ str , typing . Any ] ) -> typing . Any : pass","title":"OldTypeConversionModule"},{"location":"reference/kiara/modules/type_conversion/#kiara.modules.type_conversion.OldTypeConversionModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/type_conversion.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: inputs : typing . Mapping [ str , typing . Any ] = { \"source_value\" : { \"type\" : self . source_type , \"doc\" : f \"A value of type ' { self . source_type } '.\" , }, \"config\" : { \"type\" : \"dict\" , \"doc\" : \"The configuration for the transformation.\" , \"optional\" : True , }, } return inputs","title":"create_input_schema()"},{"location":"reference/kiara/modules/type_conversion/#kiara.modules.type_conversion.OldTypeConversionModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/modules/type_conversion.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"target_value\" : { \"type\" : self . target_type , \"doc\" : f \"A value of type ' { self . target_type } '.\" , } } return outputs","title":"create_output_schema()"},{"location":"reference/kiara/modules/type_conversion/#kiara.modules.type_conversion.TypeConversionModuleConfig","text":"Source code in kiara/modules/type_conversion.py class TypeConversionModuleConfig ( ModuleTypeConfigSchema ): source_type : str = Field ( description = \"The source type.\" ) target_type : str = Field ( description = \"The target type.\" )","title":"TypeConversionModuleConfig"},{"location":"reference/kiara/modules/type_conversion/#kiara.modules.type_conversion.TypeConversionModuleConfig.source_type","text":"The source type.","title":"source_type"},{"location":"reference/kiara/modules/type_conversion/#kiara.modules.type_conversion.TypeConversionModuleConfig.target_type","text":"The target type.","title":"target_type"},{"location":"reference/kiara/modules/pipelines/__init__/","text":"Base module that holds PipelineModule classes that are auto-generated from pipeline descriptions in the pipelines folder.","title":"pipelines"},{"location":"reference/kiara/operations/__init__/","text":"Operation ( ModuleConfig ) pydantic-model \u00b6 Source code in kiara/operations/__init__.py class Operation ( ModuleConfig ): @classmethod def create_operation ( cls , kiara : \"Kiara\" , operation_id : str , config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , ) -> \"Operation\" : _config = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) _config_dict = _config . dict () _config_dict [ \"id\" ] = operation_id op_config = cls ( ** _config_dict ) op_config . _kiara = kiara return op_config _kiara : typing . Optional [ \"Kiara\" ] = PrivateAttr ( default = None ) _module : typing . Optional [ \"KiaraModule\" ] id : str = Field ( description = \"The operation id.\" ) type_category : typing . Optional [ str ] = Field ( description = \"The operation category this belongs to.\" , default = None ) @property def kiara ( self ) -> \"Kiara\" : if self . _kiara is None : raise Exception ( \"Kiara context not set for operation.\" ) return self . _kiara @property def module ( self ) -> \"KiaraModule\" : if self . _module is None : self . _module = self . create_module ( kiara = self . kiara ) return self . _module @property def input_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . module . input_schemas @property def output_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . module . output_schemas @property def module_cls ( self ) -> typing . Type [ \"KiaraModule\" ]: return self . kiara . get_module_class ( self . module_type ) def run ( self , _attach_lineage : bool = True , ** inputs : typing . Any ) -> ValueSet : return self . module . run ( _attach_lineage = _attach_lineage , ** inputs ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code \"\"\" include_full_doc = config . get ( \"include_full_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . doc : if include_full_doc : table . add_row ( \"Documentation\" , self . doc . full_doc ) else : table . add_row ( \"Description\" , self . doc . description ) module_type_md = self . module . get_type_metadata () table . add_row ( \"Module type\" , self . module_type ) conf = Syntax ( json . dumps ( self . module_config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Module config\" , conf ) constants = self . module_config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . module . input_schemas , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . module . output_schemas , ) table . add_row ( \"Outputs\" , outputs_table ) m_md_o = module_type_md . origin . create_renderable () m_md_c = module_type_md . context . create_renderable () m_md = RenderGroup ( m_md_o , m_md_c ) table . add_row ( \"Module metadata\" , m_md ) if config . get ( \"include_src\" , False ): table . add_row ( \"Source code\" , module_type_md . process_src ) return table id : str pydantic-field required \u00b6 The operation id. type_category : str pydantic-field \u00b6 The operation category this belongs to. create_renderable ( self , ** config ) \u00b6 Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code Source code in kiara/operations/__init__.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code \"\"\" include_full_doc = config . get ( \"include_full_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . doc : if include_full_doc : table . add_row ( \"Documentation\" , self . doc . full_doc ) else : table . add_row ( \"Description\" , self . doc . description ) module_type_md = self . module . get_type_metadata () table . add_row ( \"Module type\" , self . module_type ) conf = Syntax ( json . dumps ( self . module_config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Module config\" , conf ) constants = self . module_config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . module . input_schemas , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . module . output_schemas , ) table . add_row ( \"Outputs\" , outputs_table ) m_md_o = module_type_md . origin . create_renderable () m_md_c = module_type_md . context . create_renderable () m_md = RenderGroup ( m_md_o , m_md_c ) table . add_row ( \"Module metadata\" , m_md ) if config . get ( \"include_src\" , False ): table . add_row ( \"Source code\" , module_type_md . process_src ) return table calculate_hash \u00b6 CalculateHashOperationType ( OperationType ) \u00b6 Operation type to calculate hash(es) for data. This is mostly used internally, so users usually won't be exposed to operations of this type. Source code in kiara/operations/calculate_hash.py class CalculateHashOperationType ( OperationType ): \"\"\"Operation type to calculate hash(es) for data. This is mostly used internally, so users usually won't be exposed to operations of this type. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return op_config . module_cls == CalculateValueHashModule def get_hash_operations_for_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: result = {} for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] == value_type : result [ op_config . module_config [ \"hash_type\" ]] = op_config return result CalculateValueHashModule ( KiaraModule ) \u00b6 Calculate the hash of a value. Source code in kiara/operations/calculate_hash.py class CalculateValueHashModule ( KiaraModule ): \"\"\"Calculate the hash of a value.\"\"\" _module_type_name = \"value.hash\" _config_cls = CalculateValueHashesConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type in kiara . type_mgmt . value_types . items (): hash_types = value_type . get_supported_hash_types () for ht in hash_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"hash_type\" : ht }, \"doc\" : f \"Calculate ' { ht } ' hash for value type ' { value_type_name } '.\" , } all_metadata_profiles [ f \"calculate_hash. { ht } .for. { value_type_name } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , } } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"hash\" : { \"type\" : \"string\" , \"doc\" : \"The hash string.\" }} def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" value : Value = inputs . get_value_obj ( input_name ) value_hash = value . get_hash ( hash_type = self . get_config_value ( \"hash_type\" )) outputs . set_value ( \"hash\" , value_hash . hash ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/calculate_hash.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , } } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/calculate_hash.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"hash\" : { \"type\" : \"string\" , \"doc\" : \"The hash string.\" }} retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/calculate_hash.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type in kiara . type_mgmt . value_types . items (): hash_types = value_type . get_supported_hash_types () for ht in hash_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"hash_type\" : ht }, \"doc\" : f \"Calculate ' { ht } ' hash for value type ' { value_type_name } '.\" , } all_metadata_profiles [ f \"calculate_hash. { ht } .for. { value_type_name } \" ] = op_config return all_metadata_profiles CalculateValueHashesConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/calculate_hash.py class CalculateValueHashesConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to calculate the hash for.\" ) hash_type : str = Field ( description = \"The hash type.\" ) hash_type : str pydantic-field required \u00b6 The hash type. value_type : str pydantic-field required \u00b6 The type of the value to calculate the hash for. create_value \u00b6 CreateValueModule ( KiaraModule ) \u00b6 Base class for 'create' value type operations. Source code in kiara/operations/create_value.py class CreateValueModule ( KiaraModule ): \"\"\"Base class for 'create' value type operations.\"\"\" _config_cls = CreateValueModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} # value_types: typing.Iterable[str] = cls.get_supported_value_types() # if \"*\" in value_types: # value_types = kiara.type_mgmt.value_type_names # # for value_type in value_types: target_type = cls . get_target_value_type () if target_type not in kiara . type_mgmt . value_type_names : raise Exception ( f \"Can't assemble type convert operations for target type ' { target_type } ': type not available\" ) for source_profile in cls . get_source_value_profiles (): mod_conf = { \"source_profile\" : source_profile , \"target_type\" : target_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Create a ' { target_type } ' value from a ' { source_profile } '.\" , } # key = f\"{value_type}.convert_from.{target_type}\" # type: ignore # if key in all_metadata_profiles.keys(): # raise Exception(f\"Duplicate profile key: {key}\") # all_metadata_profiles[key] = op_config key = f \"create. { target_type } .from. { source_profile } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles # @classmethod # def get_supported_value_types(cls) -> typing.Set[str]: # # _types = cls._get_supported_value_types() # if isinstance(_types, str): # _types = [_types] # # return set(_types) # @classmethod @abc . abstractmethod def get_target_value_type ( cls ) -> str : pass @classmethod def get_source_value_profiles ( cls ) -> typing . Iterable [ str ]: # supported = cls.get_supported_value_types() types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"from_\" ) and callable ( attr ): v_type = attr_name [ 5 :] # if v_type in supported: # raise Exception( # f\"Invalid configuration for type conversion module class {cls.__name__}: conversion source type can't be '{v_type}' because this is already registered as a supported type of the class.\" # ) types . append ( v_type ) return types # @classmethod # def get_target_value_type(cls) -> str: # # supported = cls.get_supported_value_types() # # types = [] # for attr_name, attr in cls.__dict__.items(): # # if attr_name.startswith(\"to_\") and callable(attr): # v_type = attr_name[3:] # if v_type in supported: # raise Exception( # f\"Invalid configuration for type conversion module class {cls.__name__}: conversion target type can't be '{v_type}' because this is already registered as a supported type of the class.\" # ) # types.append(attr_name[3:]) # # return types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) source_config = dict ( self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) ) source_config [ \"doc\" ] = f \"The ' { source_profile } ' value to be converted.\" schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { source_profile : source_config } if self . get_config_value ( \"allow_none_input\" ): schema [ source_config [ \"type\" ]][ \"optional\" ] = True return schema def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) target_type = self . get_config_value ( \"target_type\" ) schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { target_type : { \"type\" : target_type , \"doc\" : f \"The converted ' { source_profile } ' value as ' { target_type } '.\" , } } if self . get_config_value ( \"allow_none_input\" ): schema [ target_type ][ \"optional\" ] = True return schema def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : source_profile : str = self . get_config_value ( \"source_profile\" ) source_config : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) source_type = source_config [ \"type\" ] target_type : str = self . get_config_value ( \"target_type\" ) allow_none : bool = self . get_config_value ( \"allow_none_input\" ) source : Value = inputs . get_value_obj ( source_profile ) if source_type != source . type_name : raise KiaraProcessingException ( f \"Invalid type ( { source . type_name } ) of source value: expected ' { source_type } ' (source profile name: { source_profile } ).\" ) if not source . is_set or source . is_none : if allow_none : outputs . set_value ( \"value_item\" , None ) return else : raise KiaraProcessingException ( \"No source value set.\" ) if not hasattr ( self , f \"from_ { source_profile } \" ): raise Exception ( f \"Module ' { self . _module_type_id } ' can't convert ' { source_type } ' into ' { target_type } ': missing method 'from_ { source_type } '. This is a bug.\" # type: ignore ) func = getattr ( self , f \"from_ { source_profile } \" ) converted = func ( source ) outputs . set_value ( target_type , converted ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/create_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) source_config = dict ( self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) ) source_config [ \"doc\" ] = f \"The ' { source_profile } ' value to be converted.\" schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { source_profile : source_config } if self . get_config_value ( \"allow_none_input\" ): schema [ source_config [ \"type\" ]][ \"optional\" ] = True return schema create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/create_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) target_type = self . get_config_value ( \"target_type\" ) schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { target_type : { \"type\" : target_type , \"doc\" : f \"The converted ' { source_profile } ' value as ' { target_type } '.\" , } } if self . get_config_value ( \"allow_none_input\" ): schema [ target_type ][ \"optional\" ] = True return schema retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/create_value.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} # value_types: typing.Iterable[str] = cls.get_supported_value_types() # if \"*\" in value_types: # value_types = kiara.type_mgmt.value_type_names # # for value_type in value_types: target_type = cls . get_target_value_type () if target_type not in kiara . type_mgmt . value_type_names : raise Exception ( f \"Can't assemble type convert operations for target type ' { target_type } ': type not available\" ) for source_profile in cls . get_source_value_profiles (): mod_conf = { \"source_profile\" : source_profile , \"target_type\" : target_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Create a ' { target_type } ' value from a ' { source_profile } '.\" , } # key = f\"{value_type}.convert_from.{target_type}\" # type: ignore # if key in all_metadata_profiles.keys(): # raise Exception(f\"Duplicate profile key: {key}\") # all_metadata_profiles[key] = op_config key = f \"create. { target_type } .from. { source_profile } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles CreateValueModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/create_value.py class CreateValueModuleConfig ( ModuleTypeConfigSchema ): source_profile : str = Field ( description = \"The profile of the source value.\" ) target_type : str = Field ( description = \"The type of the value to convert to.\" ) allow_none_input : bool = Field ( description = \"Whether to allow 'none' source values, if one is encountered 'none' is returned.\" , default = False , ) allow_none_input : bool pydantic-field \u00b6 Whether to allow 'none' source values, if one is encountered 'none' is returned. source_profile : str pydantic-field required \u00b6 The profile of the source value. target_type : str pydantic-field required \u00b6 The type of the value to convert to. CreateValueOperationType ( OperationType ) \u00b6 Operations that create values of specific types from values of certain value profiles. In most cases, source profiles will be 'file' or 'file_bundle' values of some sort, and the created values will exhibit more inherent structure and stricter specs than their sources. The 'create' operation type differs from 'import' in that it expects values that are already onboarded and are all information in the value is stored in a kiara data registry (in most cases the kiara data store ). Source code in kiara/operations/create_value.py class CreateValueOperationType ( OperationType ): \"\"\"Operations that create values of specific types from values of certain value profiles. In most cases, source profiles will be 'file' or 'file_bundle' values of some sort, and the created values will exhibit more inherent structure and stricter specs than their sources. The 'create' operation type differs from 'import' in that it expects values that are already onboarded and are all information in the value is stored in a *kiara* data registry (in most cases the *kiara data store*). \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , CreateValueModule ) # def get_operations_for_source_type( # self, value_type: str # ) -> typing.Dict[str, Operation]: # \"\"\"Find all operations that transform from the specified type. # # The result dict uses the target type of the conversion as key, and the operation itself as value. # \"\"\" # # result: typing.Dict[str, Operation] = {} # for o_id, op in self.operations.items(): # source_type = op.module_config[\"source_type\"] # if source_type == value_type: # target_type = op.module_config[\"target_type\"] # if target_type in result.keys(): # raise Exception( # f\"Multiple operations to transform from '{source_type}' to {target_type}\" # ) # result[target_type] = op # # return result def get_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): target_type = op . module_config [ \"target_type\" ] if target_type == value_type : source_type = op . module_config [ \"source_type\" ] if source_type in result . keys (): raise Exception ( f \"Multiple operations to transform from ' { source_type } ' to { target_type } \" ) result [ source_type ] = op return result get_operations_for_target_type ( self , value_type ) \u00b6 Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. Source code in kiara/operations/create_value.py def get_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): target_type = op . module_config [ \"target_type\" ] if target_type == value_type : source_type = op . module_config [ \"source_type\" ] if source_type in result . keys (): raise Exception ( f \"Multiple operations to transform from ' { source_type } ' to { target_type } \" ) result [ source_type ] = op return result data_export \u00b6 DataExportModule ( KiaraModule ) \u00b6 Source code in kiara/operations/data_export.py class DataExportModule ( KiaraModule ): _config_cls = DataExportModuleConfig @classmethod @abc . abstractmethod def get_source_value_type ( cls ) -> str : pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_source_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data export operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"export_as__\" ): continue target_profile = attr [ 11 :] op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_type\" : sup_type , \"target_profile\" : target_profile , }, \"doc\" : f \"Export data of type ' { sup_type } ' as: { target_profile } .\" , } all_metadata_profiles [ f \"export. { sup_type } .as. { target_profile } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_type = self . get_config_value ( \"source_type\" ) inputs : typing . Dict [ str , typing . Any ] = { source_type : { \"type\" : source_type , \"doc\" : f \"A value of type ' { source_type } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The directory to export the file(s) to.\" , \"optional\" : True , }, \"name\" : { \"type\" : \"string\" , \"doc\" : \"The (base) name of the exported file(s).\" , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"export_details\" : { \"type\" : \"dict\" , \"doc\" : \"Details about the exported files/folders.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : target_profile : str = self . get_config_value ( \"target_profile\" ) source_type : str = self . get_config_value ( \"source_type\" ) source = inputs . get_value_data ( source_type ) func_name = f \"export_as__ { target_profile } \" if not hasattr ( self , func_name ): raise Exception ( f \"Can't export ' { source_type } ' value: missing function ' { func_name } ' in class ' { self . __class__ . __name__ } '. Please check this modules documentation or source code to determine which source types and profiles are supported.\" ) base_path = inputs . get_value_data ( \"base_path\" ) if base_path is None : base_path = os . getcwd () name = inputs . get_value_data ( \"name\" ) func = getattr ( self , func_name ) # TODO: check signature? base_path = os . path . abspath ( base_path ) os . makedirs ( base_path , exist_ok = True ) result = func ( value = source , base_path = base_path , name = name ) # schema = ValueSchema(type=self.get_target_value_type(), doc=\"Imported dataset.\") # value_lineage = ValueLineage.from_module_and_inputs( # module=self, output_name=output_key, inputs=inputs # ) # value: Value = self._kiara.data_registry.register_data( # value_data=result, value_schema=schema, lineage=None # ) outputs . set_value ( \"export_details\" , result ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_export.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_type = self . get_config_value ( \"source_type\" ) inputs : typing . Dict [ str , typing . Any ] = { source_type : { \"type\" : source_type , \"doc\" : f \"A value of type ' { source_type } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The directory to export the file(s) to.\" , \"optional\" : True , }, \"name\" : { \"type\" : \"string\" , \"doc\" : \"The (base) name of the exported file(s).\" , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_export.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"export_details\" : { \"type\" : \"dict\" , \"doc\" : \"Details about the exported files/folders.\" , } } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/data_export.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_source_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data export operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"export_as__\" ): continue target_profile = attr [ 11 :] op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_type\" : sup_type , \"target_profile\" : target_profile , }, \"doc\" : f \"Export data of type ' { sup_type } ' as: { target_profile } .\" , } all_metadata_profiles [ f \"export. { sup_type } .as. { target_profile } \" ] = op_config return all_metadata_profiles DataExportModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/data_export.py class DataExportModuleConfig ( ModuleTypeConfigSchema ): target_profile : str = Field ( description = \"The name of the target profile. Used to distinguish different target formats for the same data type.\" ) source_type : str = Field ( description = \"The type of the source data that is going to be exported.\" ) source_type : str pydantic-field required \u00b6 The type of the source data that is going to be exported. target_profile : str pydantic-field required \u00b6 The name of the target profile. Used to distinguish different target formats for the same data type. ExportDataOperationType ( OperationType ) \u00b6 Export data from kiara . Operations of this type use internally handled datasets, and export it to the local file system. Export operations are created by implementing a class that inherits from DataExportModule , kiara will register it under an operation id following this template: <SOURCE_DATA_TYPE>.export_as.<EXPORT_PROFILE> The meaning of the templated fields is: EXPORTED_DATA_TYPE : the data type of the value to export EXPORT_PROFILE : a short, free-form description of the format the data will be exported as Source code in kiara/operations/data_export.py class ExportDataOperationType ( OperationType ): \"\"\"Export data from *kiara*. Operations of this type use internally handled datasets, and export it to the local file system. Export operations are created by implementing a class that inherits from [DataExportModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.data_export/#kiara.operations.data_export.DataExportModule), *kiara* will register it under an operation id following this template: ``` <SOURCE_DATA_TYPE>.export_as.<EXPORT_PROFILE> ``` The meaning of the templated fields is: - `EXPORTED_DATA_TYPE`: the data type of the value to export - `EXPORT_PROFILE`: a short, free-form description of the format the data will be exported as \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : match = issubclass ( op_config . module_cls , DataExportModule ) return match def get_export_operations_per_source_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_source_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] target_profile = op_config . module_config [ \"target_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ target_profile ] = op_config return result def get_export_operations_for_source_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_export_operations_per_source_type () . get ( value_type , {}) get_export_operations_for_source_type ( self , value_type ) \u00b6 Return all available import operations that produce data of the specified type. Source code in kiara/operations/data_export.py def get_export_operations_for_source_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_export_operations_per_source_type () . get ( value_type , {}) get_export_operations_per_source_type ( self ) \u00b6 Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. Source code in kiara/operations/data_export.py def get_export_operations_per_source_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_source_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] target_profile = op_config . module_config [ \"target_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ target_profile ] = op_config return result FileBundleImportModule ( DataExportModule ) \u00b6 Import a file, optionally saving it to the data store. Source code in kiara/operations/data_export.py class FileBundleImportModule ( DataExportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file_bundle\" FileExportModule ( DataExportModule ) \u00b6 Import a file, optionally saving it to the data store. Source code in kiara/operations/data_export.py class FileExportModule ( DataExportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file\" data_import \u00b6 DataImportModule ( KiaraModule ) \u00b6 Source code in kiara/operations/data_import.py class DataImportModule ( KiaraModule ): _config_cls = DataImportModuleConfig @classmethod @abc . abstractmethod def get_target_value_type ( cls ) -> str : pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_target_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data import operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"import_from__\" ): continue tokens = attr [ 13 :] . rsplit ( \"__\" , maxsplit = 1 ) if len ( tokens ) != 2 : log_message ( f \"Can't determine source name and type from string in module { cls . _module_type_id } , ignoring method: { attr } \" # type: ignore ) source_profile , source_type = tokens op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_profile\" : source_profile , \"source_type\" : source_type , }, \"doc\" : f \"Import data of type ' { sup_type } ' from a { source_profile } { source_type } and save it to the kiara data store.\" , } all_metadata_profiles [ f \"import. { sup_type } .from. { source_profile } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"source_profile\" ) inputs : typing . Dict [ str , typing . Any ] = { input_name : { \"type\" : self . get_config_value ( \"source_type\" ), \"doc\" : f \"A { self . get_config_value ( 'source_profile' ) } ' { self . get_config_value ( 'source_type' ) } ' value.\" , }, } # allow_save = self.get_config_value(\"allow_save_input\") # save_default = self.get_config_value(\"save_default\") # if allow_save: # inputs[\"save\"] = { # \"type\": \"boolean\", # \"doc\": \"Whether to save the imported value, or not.\", # \"default\": save_default, # } # # allow_aliases: typing.Optional[bool] = self.get_config_value( # \"allow_aliases_input\" # ) # if allow_aliases is None: # allow_aliases = allow_save # # if allow_aliases and not allow_save and not save_default: # raise Exception( # \"Invalid module configuration: allowing aliases input does not make sense if save is disabled.\" # ) # # if allow_aliases: # default_aliases = self.get_config_value(\"aliases_default\") # inputs[\"aliases\"] = { # \"type\": \"list\", # \"doc\": \"A list of aliases to use when storing the value (only applicable if 'save' is set).\", # \"default\": default_aliases, # } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: output_name = self . get_target_value_type () if output_name == \"any\" : output_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { output_name : { \"type\" : self . get_target_value_type (), \"doc\" : f \"The imported { self . get_target_value_type () } value.\" , }, } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : source_profile : str = self . get_config_value ( \"source_profile\" ) source_type : str = self . get_config_value ( \"source_type\" ) source = inputs . get_value_data ( source_profile ) if self . get_target_value_type () == \"any\" : output_key : str = \"value_item\" else : output_key = self . get_target_value_type () func_name = f \"import_from__ { source_profile } __ { source_type } \" if not hasattr ( self , func_name ): raise Exception ( f \"Can't import ' { source_type } ' value: missing function ' { func_name } ' in class ' { self . __class__ . __name__ } '. Please check this modules documentation or source code to determine which source types and profiles are supported.\" ) func = getattr ( self , func_name ) # TODO: check signature? result = func ( source ) # schema = ValueSchema(type=self.get_target_value_type(), doc=\"Imported dataset.\") # value_lineage = ValueLineage.from_module_and_inputs( # module=self, output_name=output_key, inputs=inputs # ) # value: Value = self._kiara.data_registry.register_data( # value_data=result, value_schema=schema, lineage=None # ) outputs . set_value ( output_key , result ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_import.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"source_profile\" ) inputs : typing . Dict [ str , typing . Any ] = { input_name : { \"type\" : self . get_config_value ( \"source_type\" ), \"doc\" : f \"A { self . get_config_value ( 'source_profile' ) } ' { self . get_config_value ( 'source_type' ) } ' value.\" , }, } # allow_save = self.get_config_value(\"allow_save_input\") # save_default = self.get_config_value(\"save_default\") # if allow_save: # inputs[\"save\"] = { # \"type\": \"boolean\", # \"doc\": \"Whether to save the imported value, or not.\", # \"default\": save_default, # } # # allow_aliases: typing.Optional[bool] = self.get_config_value( # \"allow_aliases_input\" # ) # if allow_aliases is None: # allow_aliases = allow_save # # if allow_aliases and not allow_save and not save_default: # raise Exception( # \"Invalid module configuration: allowing aliases input does not make sense if save is disabled.\" # ) # # if allow_aliases: # default_aliases = self.get_config_value(\"aliases_default\") # inputs[\"aliases\"] = { # \"type\": \"list\", # \"doc\": \"A list of aliases to use when storing the value (only applicable if 'save' is set).\", # \"default\": default_aliases, # } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_import.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: output_name = self . get_target_value_type () if output_name == \"any\" : output_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { output_name : { \"type\" : self . get_target_value_type (), \"doc\" : f \"The imported { self . get_target_value_type () } value.\" , }, } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/data_import.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_target_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data import operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"import_from__\" ): continue tokens = attr [ 13 :] . rsplit ( \"__\" , maxsplit = 1 ) if len ( tokens ) != 2 : log_message ( f \"Can't determine source name and type from string in module { cls . _module_type_id } , ignoring method: { attr } \" # type: ignore ) source_profile , source_type = tokens op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_profile\" : source_profile , \"source_type\" : source_type , }, \"doc\" : f \"Import data of type ' { sup_type } ' from a { source_profile } { source_type } and save it to the kiara data store.\" , } all_metadata_profiles [ f \"import. { sup_type } .from. { source_profile } \" ] = op_config return all_metadata_profiles DataImportModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/data_import.py class DataImportModuleConfig ( ModuleTypeConfigSchema ): # value_type: str = Field(description=\"The type of the value to be imported.\") source_profile : str = Field ( description = \"The name of the source profile. Used to distinguish different input categories for the same input type.\" ) source_type : str = Field ( description = \"The type of the source to import from.\" ) # allow_save_input: bool = Field( # description=\"Allow the user to choose whether to save the imported item or not.\", # default=True, # ) # save_default: bool = Field( # description=\"The default of the 'save' input if not specified by the user.\", # default=False, # ) # allow_aliases_input: typing.Optional[bool] = Field( # description=\"Allow the user to choose aliases for the saved value.\", # default=None, # ) # aliases_default: typing.List[str] = Field( # description=\"Default value for aliases.\", default_factory=list # ) source_profile : str pydantic-field required \u00b6 The name of the source profile. Used to distinguish different input categories for the same input type. source_type : str pydantic-field required \u00b6 The type of the source to import from. FileBundleImportModule ( DataImportModule ) \u00b6 Import a file, optionally saving it to the data store. Source code in kiara/operations/data_import.py class FileBundleImportModule ( DataImportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file_bundle\" FileImportModule ( DataImportModule ) \u00b6 Import a file, optionally saving it to the data store. Source code in kiara/operations/data_import.py class FileImportModule ( DataImportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file\" ImportDataOperationType ( OperationType ) \u00b6 Import data into kiara . Operations of this type take external data, and register it into kiara . External data is different in that it usually does not come with any metadata on how it was created, who created it, when, etc. Import operations are created by implementing a class that inherits from DataImportModule , kiara will register it under an operation id following this template: <IMPORTED_DATA_TYPE>.import_from.<IMPORT_PROFILE>.<INPUT_TYPE> The meaning of the templated fields is: IMPORTED_DATA_TYPE : the data type of the imported value IMPORT_PROFILE : a short, free-form description of where from (or how) the data is imported INPUT_TYPE : the data type of the user input that points to the data (like a file path, url, query, etc.) -- in most cases this will be some form of a string or uri There are two main scenarios when an operation of this type is used: 'onboard' data that was created by a 3rd party, or using external processes 're-import' data that as created in kiara , then exported to be transformed in an external process, and then imported again into kiara In both of those scenarios, we'll need to have a way to add metadata to fill out 'holes' in the metadata 'chold chain'. We don't have a concept yet as to how to do that, but that is planned for the future. Source code in kiara/operations/data_import.py class ImportDataOperationType ( OperationType ): \"\"\"Import data into *kiara*. Operations of this type take external data, and register it into *kiara*. External data is different in that it usually does not come with any metadata on how it was created, who created it, when, etc. Import operations are created by implementing a class that inherits from [DataImportModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModule), *kiara* will register it under an operation id following this template: ``` <IMPORTED_DATA_TYPE>.import_from.<IMPORT_PROFILE>.<INPUT_TYPE> ``` The meaning of the templated fields is: - `IMPORTED_DATA_TYPE`: the data type of the imported value - `IMPORT_PROFILE`: a short, free-form description of where from (or how) the data is imported - `INPUT_TYPE`: the data type of the user input that points to the data (like a file path, url, query, etc.) -- in most cases this will be some form of a string or uri There are two main scenarios when an operation of this type is used: - 'onboard' data that was created by a 3rd party, or using external processes - 're-import' data that as created in *kiara*, then exported to be transformed in an external process, and then imported again into *kiara* In both of those scenarios, we'll need to have a way to add metadata to fill out 'holes' in the metadata 'chold chain'. We don't have a concept yet as to how to do that, but that is planned for the future. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , DataImportModule ) def get_import_operations_per_target_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_target_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] source_profile = op_config . module_config [ \"source_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ source_profile ] = op_config return result def get_import_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_import_operations_per_target_type () . get ( value_type , {}) get_import_operations_for_target_type ( self , value_type ) \u00b6 Return all available import operations that produce data of the specified type. Source code in kiara/operations/data_import.py def get_import_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_import_operations_per_target_type () . get ( value_type , {}) get_import_operations_per_target_type ( self ) \u00b6 Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. Source code in kiara/operations/data_import.py def get_import_operations_per_target_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_target_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] source_profile = op_config . module_config [ \"source_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ source_profile ] = op_config return result extract_metadata \u00b6 ExtractMetadataModule ( KiaraModule ) \u00b6 Base class to use when writing a module to extract metadata from a file. It's possible to use any arbitrary kiara module for this purpose, but sub-classing this makes it easier. Source code in kiara/operations/extract_metadata.py class ExtractMetadataModule ( KiaraModule ): \"\"\"Base class to use when writing a module to extract metadata from a file. It's possible to use any arbitrary *kiara* module for this purpose, but sub-classing this makes it easier. \"\"\" _config_cls = MetadataModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_types : typing . Iterable = cls . get_supported_value_types () if \"*\" in value_types : value_types = kiara . type_mgmt . value_type_names metadata_key = cls . get_metadata_key () all_value_types = set () for value_type in value_types : if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring metadata-extract operation (metadata key: { metadata_key } ) for type ' { value_type } ': type not available\" ) continue all_value_types . add ( value_type ) sub_types = kiara . type_mgmt . get_sub_types ( value_type ) all_value_types . update ( sub_types ) for value_type in all_value_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type }, \"doc\" : f \"Extract ' { metadata_key } ' metadata for value of type ' { value_type } '.\" , } all_metadata_profiles [ f \"extract. { metadata_key } .metadata.from. { value_type } \" ] = op_config return all_metadata_profiles @classmethod @abc . abstractmethod def _get_supported_types ( self ) -> typing . Union [ str , typing . Iterable [ str ]]: pass @classmethod def get_metadata_key ( cls ) -> str : return cls . _module_type_name # type: ignore @classmethod def get_supported_value_types ( cls ) -> typing . Set [ str ]: _types = cls . _get_supported_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) def __init__ ( self , * args , ** kwargs ): self . _metadata_schema : typing . Optional [ str ] = None super () . __init__ ( * args , ** kwargs ) @property def value_type ( self ) -> str : data_type = self . get_config_value ( \"value_type\" ) sup_types = self . get_supported_value_types () if \"*\" not in sup_types and data_type not in sup_types : match = False for sup_type in sup_types : for sub_type in self . _kiara . type_mgmt . get_sub_types ( sup_type ): if sub_type == data_type : match = True break if not match : raise ValueError ( f \"Invalid module configuration, type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_value_types ()) } .\" ) return data_type @property def metadata_schema ( self ) -> str : if self . _metadata_schema is not None : return self . _metadata_schema schema = self . _get_metadata_schema ( type = self . value_type ) if isinstance ( schema , type ) and issubclass ( schema , BaseModel ): schema = schema . schema_json () elif not isinstance ( schema , str ): raise TypeError ( f \"Invalid type for metadata schema: { type ( schema ) } \" ) self . _metadata_schema = schema return self . _metadata_schema @abc . abstractmethod def _get_metadata_schema ( self , type : str ) -> typing . Union [ str , typing . Type [ BaseModel ]]: \"\"\"Create the metadata schema for the configured type.\"\"\" @abc . abstractmethod def extract_metadata ( self , value : Value ) -> typing . Union [ typing . Mapping [ str , typing . Any ], BaseModel ]: pass def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : { \"type\" : self . value_type , \"doc\" : f \"A value of type ' { self . value_type } '\" , \"optional\" : False , } } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"metadata_item\" : { \"type\" : \"dict\" , \"doc\" : \"The metadata for the provided value.\" , }, \"metadata_item_schema\" : { \"type\" : \"string\" , \"doc\" : \"The (json) schema for the metadata.\" , }, } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" value = inputs . get_value_obj ( input_name ) if self . value_type != \"any\" and value . type_name != self . value_type : raise KiaraProcessingException ( f \"Can't extract metadata for value of type ' { value . value_schema . type } '. Expected type ' { self . value_type } '.\" ) # TODO: if type 'any', validate that the data is actually of the right type? outputs . set_value ( \"metadata_item_schema\" , self . metadata_schema ) metadata = self . extract_metadata ( value ) if isinstance ( metadata , BaseModel ): metadata = metadata . dict ( exclude_none = True ) # TODO: validate metadata? outputs . set_value ( \"metadata_item\" , metadata ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/extract_metadata.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : { \"type\" : self . value_type , \"doc\" : f \"A value of type ' { self . value_type } '\" , \"optional\" : False , } } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/extract_metadata.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"metadata_item\" : { \"type\" : \"dict\" , \"doc\" : \"The metadata for the provided value.\" , }, \"metadata_item_schema\" : { \"type\" : \"string\" , \"doc\" : \"The (json) schema for the metadata.\" , }, } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/extract_metadata.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_types : typing . Iterable = cls . get_supported_value_types () if \"*\" in value_types : value_types = kiara . type_mgmt . value_type_names metadata_key = cls . get_metadata_key () all_value_types = set () for value_type in value_types : if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring metadata-extract operation (metadata key: { metadata_key } ) for type ' { value_type } ': type not available\" ) continue all_value_types . add ( value_type ) sub_types = kiara . type_mgmt . get_sub_types ( value_type ) all_value_types . update ( sub_types ) for value_type in all_value_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type }, \"doc\" : f \"Extract ' { metadata_key } ' metadata for value of type ' { value_type } '.\" , } all_metadata_profiles [ f \"extract. { metadata_key } .metadata.from. { value_type } \" ] = op_config return all_metadata_profiles ExtractMetadataOperationType ( OperationType ) \u00b6 Extract metadata from a dataset. The purpose of this operation type is to be able to extract arbitrary, type-specific metadata from value data. In general, kiara wants to collect (and store along the value) as much metadata related to data as possible, but the extraction process should not take a lot of processing time (since this is done whenever a value is registered into a data registry). As its hard to predict all the types of metadata of a specific type that could be interesting in specific scenarios, kiara supports a pluggable mechanism to add new metadata extraction processes by extending the base class ExtractMetadataModule and adding that implementation somewhere kiara can find it. Once that is done, kiara will automatically add a new operation with an id that follows this template: <VALUE_TYPE>.extract_metadata.<METADATA_KEY> , where METADATA_KEY is a name under which the metadata will be stored within the value object. By default, every value type should have at least one metadata extraction module where the METADATA_KEY is the same as the value type name, and which contains basic, type-specific metadata (e.g. for a 'table', that would be number of rows, column names, column types, etc.). Source code in kiara/operations/extract_metadata.py class ExtractMetadataOperationType ( OperationType ): \"\"\"Extract metadata from a dataset. The purpose of this operation type is to be able to extract arbitrary, type-specific metadata from value data. In general, *kiara* wants to collect (and store along the value) as much metadata related to data as possible, but the extraction process should not take a lot of processing time (since this is done whenever a value is registered into a data registry). As its hard to predict all the types of metadata of a specific type that could be interesting in specific scenarios, *kiara* supports a pluggable mechanism to add new metadata extraction processes by extending the base class [`ExtractMetadataModule`](http://dharpa.org/kiara/latest/api_reference/kiara.operations.extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataModule) and adding that implementation somewhere *kiara* can find it. Once that is done, *kiara* will automatically add a new operation with an id that follows this template: `<VALUE_TYPE>.extract_metadata.<METADATA_KEY>`, where `METADATA_KEY` is a name under which the metadata will be stored within the value object. By default, every value type should have at least one metadata extraction module where the `METADATA_KEY` is the same as the value type name, and which contains basic, type-specific metadata (e.g. for a 'table', that would be number of rows, column names, column types, etc.). \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , ExtractMetadataModule ) def get_all_operations_for_type ( self , value_type : str ) -> typing . Mapping [ str , Operation ]: result = {} for op_config in self . operations . values (): v_t = op_config . module_config [ \"value_type\" ] if v_t != value_type : continue module_cls : ExtractMetadataModule = op_config . module_cls # type: ignore md_key = module_cls . get_metadata_key () result [ md_key ] = op_config return result MetadataModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/extract_metadata.py class MetadataModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The data type this module will be used for.\" ) value_type : str pydantic-field required \u00b6 The data type this module will be used for. merge_values \u00b6 ValueMergeModule ( KiaraModule ) \u00b6 Base class for operations that merge several values into one. NOT USED YET. Source code in kiara/operations/merge_values.py class ValueMergeModule ( KiaraModule ): \"\"\"Base class for operations that merge several values into one. NOT USED YET. \"\"\" def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_schema_dicts : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . get_config_value ( \"input_schemas\" ) if not input_schema_dicts : raise Exception ( \"No input schemas provided.\" ) input_schemas : typing . Dict [ str , ValueSchema ] = {} for k , v in input_schema_dicts . items (): input_schemas [ k ] = ValueSchema ( ** v ) return input_schemas def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"merged_value\" : { \"type\" : self . get_config_value ( \"output_type\" )}} create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/merge_values.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_schema_dicts : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . get_config_value ( \"input_schemas\" ) if not input_schema_dicts : raise Exception ( \"No input schemas provided.\" ) input_schemas : typing . Dict [ str , ValueSchema ] = {} for k , v in input_schema_dicts . items (): input_schemas [ k ] = ValueSchema ( ** v ) return input_schemas create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/merge_values.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"merged_value\" : { \"type\" : self . get_config_value ( \"output_type\" )}} ValueMergeModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/merge_values.py class ValueMergeModuleConfig ( ModuleTypeConfigSchema ): input_schemas : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = Field ( description = \"The schemas for all of the expected inputs.\" ) output_type : str = Field ( description = \"The result type of the merged value.\" ) input_schemas : Dict [ str , Mapping [ str , Any ]] pydantic-field required \u00b6 The schemas for all of the expected inputs. output_type : str pydantic-field required \u00b6 The result type of the merged value. pretty_print \u00b6 PrettyPrintModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/pretty_print.py class PrettyPrintModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to save.\" ) target_type : str = Field ( description = \"The target to print the value to.\" , default = \"string\" ) target_type : str pydantic-field \u00b6 The target to print the value to. value_type : str pydantic-field required \u00b6 The type of the value to save. PrettyPrintOperationType ( OperationType ) \u00b6 This operation type renders values of any type into human readable output for different targets (e.g. terminal, html, ...). The main purpose of this operation type is to show the user as much content of the data as possible for the specific rendering target, without giving any guarantees that all information contained in the data is shown. It may print out the whole content (if the content is small, or the available space large enough), or just a few bits and pieces from the start and/or end of the data, whatever makes most sense for the data itself. For example, for large tables it might be a good idea to print out the first and last 30 rows, so users can see which columns are available, and get a rough overview of the content itself. For network graphs it might make sense to print out some graph properties (number of nodes, edges, available attributes, ...) on the terminal, but render the graph itself when using html as target. Currently, it is only possible to implement a pretty_print renderer if you have access to the source code of the value type you want to render. To add a new render method, add a new instance method to the ValueType class in question, in the format: def pretty_print_as_<TARGET_TYPE>(value: Value, print_config: typing.Mapping[str, typing.Any]) -> typing.Any: ... ... kiara will look at all available ValueType classes for methods that match this signature, and auto-generate operations following this naming template: <SOURCE_TYPE>.pretty_print_as.<TARGET_TYPE> . Currently, only the type renderables is implemented as target type (for printing out to the terminal). It is planned to add output suitable for use in Jupyter notebooks in the near future. Source code in kiara/operations/pretty_print.py class PrettyPrintOperationType ( OperationType ): \"\"\"This operation type renders values of any type into human readable output for different targets (e.g. terminal, html, ...). The main purpose of this operation type is to show the user as much content of the data as possible for the specific rendering target, without giving any guarantees that all information contained in the data is shown. It may print out the whole content (if the content is small, or the available space large enough), or just a few bits and pieces from the start and/or end of the data, whatever makes most sense for the data itself. For example, for large tables it might be a good idea to print out the first and last 30 rows, so users can see which columns are available, and get a rough overview of the content itself. For network graphs it might make sense to print out some graph properties (number of nodes, edges, available attributes, ...) on the terminal, but render the graph itself when using html as target. Currently, it is only possible to implement a `pretty_print` renderer if you have access to the source code of the value type you want to render. To add a new render method, add a new instance method to the `ValueType` class in question, in the format: ``` def pretty_print_as_<TARGET_TYPE>(value: Value, print_config: typing.Mapping[str, typing.Any]) -> typing.Any: ... ... ``` *kiara* will look at all available `ValueType` classes for methods that match this signature, and auto-generate operations following this naming template: `<SOURCE_TYPE>.pretty_print_as.<TARGET_TYPE>`. Currently, only the type `renderables` is implemented as target type (for printing out to the terminal). It is planned to add output suitable for use in Jupyter notebooks in the near future. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , PrettyPrintValueModule ) def get_pretty_print_operation ( self , value_type : str , target_type : str ) -> Operation : result = [] for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] != value_type : continue if op_config . module_config [ \"target_type\" ] != target_type : continue result . append ( op_config ) if not result : raise Exception ( f \"No pretty print operation for value type ' { value_type } ' and output ' { target_type } ' registered.\" ) elif len ( result ) != 1 : raise Exception ( f \"Multiple pretty print operations for value type ' { value_type } ' and output ' { target_type } ' registered.\" ) return result [ 0 ] def pretty_print ( self , value : Value , target_type : str , print_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ) -> typing . Any : ops_config = self . get_pretty_print_operation ( value_type = value . type_name , target_type = target_type ) inputs : typing . Mapping [ str , typing . Any ] = { value . type_name : value , \"print_config\" : print_config , } result = ops_config . module . run ( ** inputs ) printed = result . get_value_data ( \"printed\" ) return printed PrettyPrintValueModule ( KiaraModule ) \u00b6 Source code in kiara/operations/pretty_print.py class PrettyPrintValueModule ( KiaraModule ): _config_cls = PrettyPrintModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type_cls in kiara . type_mgmt . value_types . items (): for attr in dir ( value_type_cls ): if not attr . startswith ( \"pretty_print_as_\" ): continue target_type = attr [ 16 :] if target_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Pretty print target type ' { target_type } ' for source type ' { value_type_name } ' not valid, ignoring.\" ) continue op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"target_type\" : target_type , }, \"doc\" : f \"Pretty print a value of type ' { value_type_name } ' as ' { target_type } '.\" , } all_metadata_profiles [ f \"pretty_print. { value_type_name } .as. { target_type } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type = self . get_config_value ( \"value_type\" ) if value_type == \"all\" : input_name = \"value_item\" doc = \"A value of any type.\" else : input_name = value_type doc = f \"A value of type ' { value_type } '.\" inputs : typing . Mapping [ str , typing . Any ] = { input_name : { \"type\" : value_type , \"doc\" : doc }, \"print_config\" : { \"type\" : \"dict\" , \"doc\" : \"Optional print configuration.\" , \"optional\" : True , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"printed\" : { \"type\" : self . get_config_value ( \"target_type\" ), \"doc\" : f \"The printed value as ' { self . get_config_value ( 'target_type' ) } '.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_type : str = self . get_config_value ( \"value_type\" ) target_type : str = self . get_config_value ( \"target_type\" ) if value_type == \"all\" : input_name = \"value_item\" else : input_name = value_type value_obj : Value = inputs . get_value_obj ( input_name ) print_config = dict ( DEFAULT_PRETTY_PRINT_CONFIG ) config : typing . Mapping = inputs . get_value_data ( \"print_config\" ) if config : print_config . update ( config ) func_name = f \"pretty_print_as_ { target_type } \" if not hasattr ( value_obj . type_obj , func_name ): raise Exception ( f \"Type ' { value_type } ' can't be pretty printed as ' { target_type } '. This is most likely a bug.\" ) if not value_obj . is_set : printed = \"-- not set --\" else : func = getattr ( value_obj . type_obj , func_name ) # TODO: check signature try : printed = func ( value = value_obj , print_config = print_config ) except Exception as e : if is_debug (): import traceback traceback . print_exc () raise KiaraProcessingException ( str ( e )) outputs . set_value ( \"printed\" , printed ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/pretty_print.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type = self . get_config_value ( \"value_type\" ) if value_type == \"all\" : input_name = \"value_item\" doc = \"A value of any type.\" else : input_name = value_type doc = f \"A value of type ' { value_type } '.\" inputs : typing . Mapping [ str , typing . Any ] = { input_name : { \"type\" : value_type , \"doc\" : doc }, \"print_config\" : { \"type\" : \"dict\" , \"doc\" : \"Optional print configuration.\" , \"optional\" : True , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/pretty_print.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"printed\" : { \"type\" : self . get_config_value ( \"target_type\" ), \"doc\" : f \"The printed value as ' { self . get_config_value ( 'target_type' ) } '.\" , } } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/pretty_print.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type_cls in kiara . type_mgmt . value_types . items (): for attr in dir ( value_type_cls ): if not attr . startswith ( \"pretty_print_as_\" ): continue target_type = attr [ 16 :] if target_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Pretty print target type ' { target_type } ' for source type ' { value_type_name } ' not valid, ignoring.\" ) continue op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"target_type\" : target_type , }, \"doc\" : f \"Pretty print a value of type ' { value_type_name } ' as ' { target_type } '.\" , } all_metadata_profiles [ f \"pretty_print. { value_type_name } .as. { target_type } \" ] = op_config return all_metadata_profiles sample \u00b6 SampleValueModule ( KiaraModule ) \u00b6 Base class for operations that take samples of data. Source code in kiara/operations/sample.py class SampleValueModule ( KiaraModule ): \"\"\"Base class for operations that take samples of data.\"\"\" _config_cls = SampleValueModuleConfig @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : \"\"\"Return the value type for this sample module.\"\"\" @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring sample operation for source type ' { value_type } ': type not available\" ) return {} for sample_type in cls . get_supported_sample_types (): op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"sample_type\" : sample_type }, \"doc\" : f \"Sample value of type ' { value_type } ' using method: { sample_type } .\" , } key = f \"sample. { value_type } . { sample_type } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles @classmethod def get_supported_sample_types ( cls ) -> typing . Iterable [ str ]: types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"sample_\" ) and callable ( attr ): sample_type = attr_name [ 7 :] if sample_type in types : raise Exception ( f \"Error in sample module ' { cls . __name__ } ': multiple sample methods for type ' { sample_type } '.\" ) types . append ( sample_type ) return types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_value_type (), \"doc\" : \"The value to sample.\" , }, \"sample_size\" : { \"type\" : \"integer\" , \"doc\" : \"The sample size.\" , \"default\" : 10 , }, } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"sampled_value\" : { \"type\" : self . get_value_type (), \"doc\" : \"The sampled value\" } } def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : sample_size : int = inputs . get_value_data ( \"sample_size\" ) sample_type : str = self . get_config_value ( \"sample_type\" ) if sample_size < 0 : raise KiaraProcessingException ( f \"Invalid sample size ' { sample_size } ': can't be negative.\" ) input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" value : Value = inputs . get_value_obj ( input_name ) func = getattr ( self , f \"sample_ { sample_type } \" ) result = func ( value = value , sample_size = sample_size ) outputs . set_value ( \"sampled_value\" , result ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/sample.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_value_type (), \"doc\" : \"The value to sample.\" , }, \"sample_size\" : { \"type\" : \"integer\" , \"doc\" : \"The sample size.\" , \"default\" : 10 , }, } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/sample.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"sampled_value\" : { \"type\" : self . get_value_type (), \"doc\" : \"The sampled value\" } } get_value_type () classmethod \u00b6 Return the value type for this sample module. Source code in kiara/operations/sample.py @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : \"\"\"Return the value type for this sample module.\"\"\" retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/sample.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring sample operation for source type ' { value_type } ': type not available\" ) return {} for sample_type in cls . get_supported_sample_types (): op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"sample_type\" : sample_type }, \"doc\" : f \"Sample value of type ' { value_type } ' using method: { sample_type } .\" , } key = f \"sample. { value_type } . { sample_type } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles SampleValueModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/sample.py class SampleValueModuleConfig ( ModuleTypeConfigSchema ): sample_type : str = Field ( description = \"The sample method.\" ) sample_type : str pydantic-field required \u00b6 The sample method. SampleValueOperationType ( OperationType ) \u00b6 Operation type for sampling data of different types. This is useful to reduce the size of some datasets in previews and test-runs, while adjusting parameters and the like. Operations of this type can implement very simple or complex ways to take samples of the data they are fed. The most important ones are sampling operations relating to tables and arrays, but it might also make sense to sample texts, image-sets and so on. Operations of this type take the value to be sampled as input, as well as a sample size (integer), which can be, for example, a number of rows, or percentage of the whole dataset, depending on sample type. Modules that implement sampling should inherit from SampleValueModule , and will get auto-registered with operation ids following this template: <VALUE_TYPE>.sample.<SAMPLE_TYPE_NAME> , where SAMPLE_TYPE_NAME is a descriptive name what will be sampled, or how sampling will be done. Source code in kiara/operations/sample.py class SampleValueOperationType ( OperationType ): \"\"\"Operation type for sampling data of different types. This is useful to reduce the size of some datasets in previews and test-runs, while adjusting parameters and the like. Operations of this type can implement very simple or complex ways to take samples of the data they are fed. The most important ones are sampling operations relating to tables and arrays, but it might also make sense to sample texts, image-sets and so on. Operations of this type take the value to be sampled as input, as well as a sample size (integer), which can be, for example, a number of rows, or percentage of the whole dataset, depending on sample type. Modules that implement sampling should inherit from [SampleValueModule](https://dharpa.org/kiara/latest/api_reference/kiara.operations.sample/#kiara.operations.sample.SampleValueModule), and will get auto-registered with operation ids following this template: `<VALUE_TYPE>.sample.<SAMPLE_TYPE_NAME>`, where `SAMPLE_TYPE_NAME` is a descriptive name what will be sampled, or how sampling will be done. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , SampleValueModule ) def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): sample_op_module_cls : typing . Type [ SampleValueModule ] = op . module_cls # type: ignore source_type = sample_op_module_cls . get_value_type () if source_type == value_type : target_type = op . module_config [ \"sample_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to sample ' { source_type } ' using { target_type } \" ) result [ target_type ] = op return result get_operations_for_value_type ( self , value_type ) \u00b6 Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. Source code in kiara/operations/sample.py def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): sample_op_module_cls : typing . Type [ SampleValueModule ] = op . module_cls # type: ignore source_type = sample_op_module_cls . get_value_type () if source_type == value_type : target_type = op . module_config [ \"sample_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to sample ' { source_type } ' using { target_type } \" ) result [ target_type ] = op return result serialize \u00b6 SerializeValueModule ( KiaraModule ) \u00b6 Base class for 'serialize' operations. Source code in kiara/operations/serialize.py class SerializeValueModule ( KiaraModule ): \"\"\"Base class for 'serialize' operations.\"\"\" _config_cls = SerializeValueModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring serialization operation for source type ' { value_type } ': type not available\" ) return {} for serialization_type in cls . get_supported_serialization_types (): mod_conf = { \"value_type\" : value_type , \"serialization_type\" : serialization_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Serialize value of type ' { value_type } ' to ' { serialization_type } '.\" , } key = f \"serialize. { value_type } .as. { serialization_type } \" if key in all_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_profiles [ key ] = op_config return all_profiles @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : pass @classmethod def get_supported_serialization_types ( cls ) -> typing . Iterable [ str ]: serialize_types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"to_\" ) and callable ( attr ): serialize_types . append ( attr_name [ 3 :]) return serialize_types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) # serialization_type: str = self.get_config_value(\"serialization_type\") return { \"value_item\" : { \"type\" : value_type , \"doc\" : f \"The ' { value_type } ' value to be serialized.\" , } } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) return { \"value_info\" : { \"type\" : \"value_info\" , \"doc\" : \"Information about the (original) serialized value (can be used to re-constitute the value, incl. its original id).\" , }, \"deserialize_config\" : { \"type\" : \"deserialize_config\" , \"doc\" : f \"The config to use to deserialize the value of type ' { value_type } '.\" , }, } def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_type : str = self . get_config_value ( \"value_type\" ) value_obj = inputs . get_value_obj ( \"value_item\" ) serialization_type = self . get_config_value ( \"serialization_type\" ) if value_type != value_obj . type_name : raise KiaraProcessingException ( f \"Invalid type ( { value_obj . type_name } ) of source value: expected ' { value_type } '.\" ) if not hasattr ( self , f \"to_ { serialization_type } \" ): # this can never happen, I think raise Exception ( f \"Module ' { self . _module_type_id } ' can't serialize ' { value_type } ' to ' { serialization_type } ': missing method 'to_ { serialization_type } '. This is a bug.\" # type: ignore ) func = getattr ( self , f \"to_ { serialization_type } \" ) serialized = func ( value_obj ) if isinstance ( serialized , typing . Mapping ): serialized = DeserializeConfig ( ** serialized ) if not isinstance ( serialized , DeserializeConfig ): raise KiaraProcessingException ( f \"Invalid serialization result type: { type ( serialized ) } \" ) outputs . set_values ( deserialize_config = serialized , value_info = value_obj . get_info () ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/serialize.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) # serialization_type: str = self.get_config_value(\"serialization_type\") return { \"value_item\" : { \"type\" : value_type , \"doc\" : f \"The ' { value_type } ' value to be serialized.\" , } } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/serialize.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) return { \"value_info\" : { \"type\" : \"value_info\" , \"doc\" : \"Information about the (original) serialized value (can be used to re-constitute the value, incl. its original id).\" , }, \"deserialize_config\" : { \"type\" : \"deserialize_config\" , \"doc\" : f \"The config to use to deserialize the value of type ' { value_type } '.\" , }, } retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/serialize.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring serialization operation for source type ' { value_type } ': type not available\" ) return {} for serialization_type in cls . get_supported_serialization_types (): mod_conf = { \"value_type\" : value_type , \"serialization_type\" : serialization_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Serialize value of type ' { value_type } ' to ' { serialization_type } '.\" , } key = f \"serialize. { value_type } .as. { serialization_type } \" if key in all_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_profiles [ key ] = op_config return all_profiles SerializeValueModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/serialize.py class SerializeValueModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the source value.\" ) serialization_type : str = Field ( description = \"The type of the converted value.\" ) serialization_type : str pydantic-field required \u00b6 The type of the converted value. value_type : str pydantic-field required \u00b6 The type of the source value. SerializeValueOperationType ( OperationType ) \u00b6 Operations that serialize data into formats that can be used for data exchange. NOT USED YET Source code in kiara/operations/serialize.py class SerializeValueOperationType ( OperationType ): \"\"\"Operations that serialize data into formats that can be used for data exchange. NOT USED YET \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , SerializeValueModule ) def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): source_type = op . module_config [ \"value_type\" ] if source_type == value_type : target_type = op . module_config [ \"serialization_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to serialize ' { source_type } ' to { target_type } \" ) result [ target_type ] = op return result get_operations_for_value_type ( self , value_type ) \u00b6 Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. Source code in kiara/operations/serialize.py def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): source_type = op . module_config [ \"value_type\" ] if source_type == value_type : target_type = op . module_config [ \"serialization_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to serialize ' { source_type } ' to { target_type } \" ) result [ target_type ] = op return result store_value \u00b6 StoreOperationType ( OperationType ) \u00b6 Store a value into a local data store. This is a special operation type, that is used internally by the [LocalDataStore](http://dharpa.org/kiara/latest/api_reference/kiara.data.registry.store/#kiara.data.registry.store.LocalDataStore] data registry implementation. For each value type that should be supported by the persistent kiara data store, there must be an implementation of the StoreValueTypeModule class, which handles the actual persisting on disk. In most cases, end users won't need to interact with this type of operation. Source code in kiara/operations/store_value.py class StoreOperationType ( OperationType ): \"\"\"Store a value into a local data store. This is a special operation type, that is used internally by the [LocalDataStore](http://dharpa.org/kiara/latest/api_reference/kiara.data.registry.store/#kiara.data.registry.store.LocalDataStore] data registry implementation. For each value type that should be supported by the persistent *kiara* data store, there must be an implementation of the [StoreValueTypeModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.store_value/#kiara.operations.store_value.StoreValueTypeModule) class, which handles the actual persisting on disk. In most cases, end users won't need to interact with this type of operation. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , StoreValueTypeModule ) def get_store_operation_for_type ( self , value_type : str ) -> Operation : result = [] for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] == value_type : result . append ( op_config ) if not result : raise Exception ( f \"No 'store_value' operation for type ' { value_type } ' registered.\" ) elif len ( result ) != 1 : pass for r in result : print ( r . json ( indent = 2 )) raise Exception ( f \"Multiple 'store_value' operations for type ' { value_type } ' registered.\" ) return result [ 0 ] StoreValueModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/store_value.py class StoreValueModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to save.\" ) value_type : str pydantic-field required \u00b6 The type of the value to save. StoreValueTypeModule ( KiaraModule ) \u00b6 Store a specific value type. This is used internally. Source code in kiara/operations/store_value.py class StoreValueTypeModule ( KiaraModule ): \"\"\"Store a specific value type. This is used internally. \"\"\" _config_cls = StoreValueModuleConfig @classmethod def get_supported_value_types ( cls ) -> typing . Set [ str ]: _types = cls . retrieve_supported_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) @classmethod @abc . abstractmethod def retrieve_supported_types ( cls ) -> typing . Union [ str , typing . Iterable [ str ]]: pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} all_types_and_subtypes = set () for sup_type in cls . get_supported_value_types (): if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring save operation for type ' { sup_type } ': type not available\" ) continue all_types_and_subtypes . add ( sup_type ) sub_types = kiara . type_mgmt . get_sub_types ( sup_type ) all_types_and_subtypes . update ( sub_types ) for sup_type in all_types_and_subtypes : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : sup_type }, \"doc\" : f \"Store a value of type ' { sup_type } '.\" , } all_metadata_profiles [ f \"store. { sup_type } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" inputs : typing . Mapping [ str , typing . Any ] = { \"value_id\" : { \"type\" : \"string\" , \"doc\" : \"The id to use when saving the value.\" , }, field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The base path to save the value to.\" , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : \"The original or cloned (if applicable) value that was saved.\" , }, \"load_config\" : { \"type\" : \"load_config\" , \"doc\" : \"The configuration to use with kiara to load the saved value.\" , }, } return outputs @abc . abstractmethod def store_value ( self , value : Value , base_path : str ) -> typing . Union [ typing . Tuple [ typing . Dict [ str , typing . Any ], typing . Any ], typing . Dict [ str , typing . Any ], ]: \"\"\"Save the value, and return the load config needed to load it again.\"\"\" def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_id : str = inputs . get_value_data ( \"value_id\" ) if not value_id : raise KiaraProcessingException ( \"No value id provided.\" ) field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" value_obj : Value = inputs . get_value_obj ( field_name ) base_path : str = inputs . get_value_data ( \"base_path\" ) result = self . store_value ( value = value_obj , base_path = base_path ) if isinstance ( result , typing . Mapping ): load_config = result result_value = value_obj elif isinstance ( result , tuple ): load_config = result [ 0 ] if result [ 1 ]: result_value = result [ 1 ] else : result_value = value_obj else : raise KiaraProcessingException ( f \"Invalid result type for 'store_value' method in class ' { self . __class__ . __name__ } '. This is a bug.\" ) load_config [ \"value_id\" ] = value_id lc = LoadConfig ( ** load_config ) if lc . base_path_input_name and lc . base_path_input_name not in lc . inputs . keys (): raise KiaraProcessingException ( f \"Invalid load config: base path ' { lc . base_path_input_name } ' not part of inputs.\" ) outputs . set_values ( metadata = None , lineage = None , ** { \"load_config\" : lc , field_name : result_value } ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/store_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" inputs : typing . Mapping [ str , typing . Any ] = { \"value_id\" : { \"type\" : \"string\" , \"doc\" : \"The id to use when saving the value.\" , }, field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The base path to save the value to.\" , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/store_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : \"The original or cloned (if applicable) value that was saved.\" , }, \"load_config\" : { \"type\" : \"load_config\" , \"doc\" : \"The configuration to use with kiara to load the saved value.\" , }, } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/store_value.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} all_types_and_subtypes = set () for sup_type in cls . get_supported_value_types (): if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring save operation for type ' { sup_type } ': type not available\" ) continue all_types_and_subtypes . add ( sup_type ) sub_types = kiara . type_mgmt . get_sub_types ( sup_type ) all_types_and_subtypes . update ( sub_types ) for sup_type in all_types_and_subtypes : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : sup_type }, \"doc\" : f \"Store a value of type ' { sup_type } '.\" , } all_metadata_profiles [ f \"store. { sup_type } \" ] = op_config return all_metadata_profiles store_value ( self , value , base_path ) \u00b6 Save the value, and return the load config needed to load it again. Source code in kiara/operations/store_value.py @abc . abstractmethod def store_value ( self , value : Value , base_path : str ) -> typing . Union [ typing . Tuple [ typing . Dict [ str , typing . Any ], typing . Any ], typing . Dict [ str , typing . Any ], ]: \"\"\"Save the value, and return the load config needed to load it again.\"\"\"","title":"operations"},{"location":"reference/kiara/operations/__init__/#kiara.operations.Operation","text":"Source code in kiara/operations/__init__.py class Operation ( ModuleConfig ): @classmethod def create_operation ( cls , kiara : \"Kiara\" , operation_id : str , config : typing . Union [ \"ModuleConfig\" , typing . Mapping , str ], module_config : typing . Union [ None , typing . Mapping [ str , typing . Any ]] = None , ) -> \"Operation\" : _config = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) _config_dict = _config . dict () _config_dict [ \"id\" ] = operation_id op_config = cls ( ** _config_dict ) op_config . _kiara = kiara return op_config _kiara : typing . Optional [ \"Kiara\" ] = PrivateAttr ( default = None ) _module : typing . Optional [ \"KiaraModule\" ] id : str = Field ( description = \"The operation id.\" ) type_category : typing . Optional [ str ] = Field ( description = \"The operation category this belongs to.\" , default = None ) @property def kiara ( self ) -> \"Kiara\" : if self . _kiara is None : raise Exception ( \"Kiara context not set for operation.\" ) return self . _kiara @property def module ( self ) -> \"KiaraModule\" : if self . _module is None : self . _module = self . create_module ( kiara = self . kiara ) return self . _module @property def input_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . module . input_schemas @property def output_schemas ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . module . output_schemas @property def module_cls ( self ) -> typing . Type [ \"KiaraModule\" ]: return self . kiara . get_module_class ( self . module_type ) def run ( self , _attach_lineage : bool = True , ** inputs : typing . Any ) -> ValueSet : return self . module . run ( _attach_lineage = _attach_lineage , ** inputs ) def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code \"\"\" include_full_doc = config . get ( \"include_full_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . doc : if include_full_doc : table . add_row ( \"Documentation\" , self . doc . full_doc ) else : table . add_row ( \"Description\" , self . doc . description ) module_type_md = self . module . get_type_metadata () table . add_row ( \"Module type\" , self . module_type ) conf = Syntax ( json . dumps ( self . module_config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Module config\" , conf ) constants = self . module_config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . module . input_schemas , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . module . output_schemas , ) table . add_row ( \"Outputs\" , outputs_table ) m_md_o = module_type_md . origin . create_renderable () m_md_c = module_type_md . context . create_renderable () m_md = RenderGroup ( m_md_o , m_md_c ) table . add_row ( \"Module metadata\" , m_md ) if config . get ( \"include_src\" , False ): table . add_row ( \"Source code\" , module_type_md . process_src ) return table","title":"Operation"},{"location":"reference/kiara/operations/__init__/#kiara.operations.Operation.id","text":"The operation id.","title":"id"},{"location":"reference/kiara/operations/__init__/#kiara.operations.Operation.type_category","text":"The operation category this belongs to.","title":"type_category"},{"location":"reference/kiara/operations/__init__/#kiara.operations.Operation.create_renderable","text":"Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code Source code in kiara/operations/__init__.py def create_renderable ( self , ** config : typing . Any ) -> RenderableType : \"\"\"Create a printable overview of this operations details. Available config options: - 'include_full_doc' (default: True): whether to include the full documentation, or just a description - 'include_src' (default: False): whether to include the module source code \"\"\" include_full_doc = config . get ( \"include_full_doc\" , True ) table = Table ( box = box . SIMPLE , show_header = False , show_lines = True ) table . add_column ( \"Property\" , style = \"i\" ) table . add_column ( \"Value\" ) if self . doc : if include_full_doc : table . add_row ( \"Documentation\" , self . doc . full_doc ) else : table . add_row ( \"Description\" , self . doc . description ) module_type_md = self . module . get_type_metadata () table . add_row ( \"Module type\" , self . module_type ) conf = Syntax ( json . dumps ( self . module_config , indent = 2 ), \"json\" , background_color = \"default\" ) table . add_row ( \"Module config\" , conf ) constants = self . module_config . get ( \"constants\" ) inputs_table = create_table_from_field_schemas ( _add_required = True , _add_default = True , _show_header = True , _constants = constants , ** self . module . input_schemas , ) table . add_row ( \"Inputs\" , inputs_table ) outputs_table = create_table_from_field_schemas ( _add_required = False , _add_default = False , _show_header = True , _constants = None , ** self . module . output_schemas , ) table . add_row ( \"Outputs\" , outputs_table ) m_md_o = module_type_md . origin . create_renderable () m_md_c = module_type_md . context . create_renderable () m_md = RenderGroup ( m_md_o , m_md_c ) table . add_row ( \"Module metadata\" , m_md ) if config . get ( \"include_src\" , False ): table . add_row ( \"Source code\" , module_type_md . process_src ) return table","title":"create_renderable()"},{"location":"reference/kiara/operations/__init__/#kiara.operations.calculate_hash","text":"","title":"calculate_hash"},{"location":"reference/kiara/operations/__init__/#kiara.operations.calculate_hash.CalculateHashOperationType","text":"Operation type to calculate hash(es) for data. This is mostly used internally, so users usually won't be exposed to operations of this type. Source code in kiara/operations/calculate_hash.py class CalculateHashOperationType ( OperationType ): \"\"\"Operation type to calculate hash(es) for data. This is mostly used internally, so users usually won't be exposed to operations of this type. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return op_config . module_cls == CalculateValueHashModule def get_hash_operations_for_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: result = {} for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] == value_type : result [ op_config . module_config [ \"hash_type\" ]] = op_config return result","title":"CalculateHashOperationType"},{"location":"reference/kiara/operations/__init__/#kiara.operations.calculate_hash.CalculateValueHashModule","text":"Calculate the hash of a value. Source code in kiara/operations/calculate_hash.py class CalculateValueHashModule ( KiaraModule ): \"\"\"Calculate the hash of a value.\"\"\" _module_type_name = \"value.hash\" _config_cls = CalculateValueHashesConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type in kiara . type_mgmt . value_types . items (): hash_types = value_type . get_supported_hash_types () for ht in hash_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"hash_type\" : ht }, \"doc\" : f \"Calculate ' { ht } ' hash for value type ' { value_type_name } '.\" , } all_metadata_profiles [ f \"calculate_hash. { ht } .for. { value_type_name } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , } } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"hash\" : { \"type\" : \"string\" , \"doc\" : \"The hash string.\" }} def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" value : Value = inputs . get_value_obj ( input_name ) value_hash = value . get_hash ( hash_type = self . get_config_value ( \"hash_type\" )) outputs . set_value ( \"hash\" , value_hash . hash )","title":"CalculateValueHashModule"},{"location":"reference/kiara/operations/__init__/#kiara.operations.calculate_hash.CalculateValueHashesConfig","text":"Source code in kiara/operations/calculate_hash.py class CalculateValueHashesConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to calculate the hash for.\" ) hash_type : str = Field ( description = \"The hash type.\" )","title":"CalculateValueHashesConfig"},{"location":"reference/kiara/operations/__init__/#kiara.operations.create_value","text":"","title":"create_value"},{"location":"reference/kiara/operations/__init__/#kiara.operations.create_value.CreateValueModule","text":"Base class for 'create' value type operations. Source code in kiara/operations/create_value.py class CreateValueModule ( KiaraModule ): \"\"\"Base class for 'create' value type operations.\"\"\" _config_cls = CreateValueModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} # value_types: typing.Iterable[str] = cls.get_supported_value_types() # if \"*\" in value_types: # value_types = kiara.type_mgmt.value_type_names # # for value_type in value_types: target_type = cls . get_target_value_type () if target_type not in kiara . type_mgmt . value_type_names : raise Exception ( f \"Can't assemble type convert operations for target type ' { target_type } ': type not available\" ) for source_profile in cls . get_source_value_profiles (): mod_conf = { \"source_profile\" : source_profile , \"target_type\" : target_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Create a ' { target_type } ' value from a ' { source_profile } '.\" , } # key = f\"{value_type}.convert_from.{target_type}\" # type: ignore # if key in all_metadata_profiles.keys(): # raise Exception(f\"Duplicate profile key: {key}\") # all_metadata_profiles[key] = op_config key = f \"create. { target_type } .from. { source_profile } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles # @classmethod # def get_supported_value_types(cls) -> typing.Set[str]: # # _types = cls._get_supported_value_types() # if isinstance(_types, str): # _types = [_types] # # return set(_types) # @classmethod @abc . abstractmethod def get_target_value_type ( cls ) -> str : pass @classmethod def get_source_value_profiles ( cls ) -> typing . Iterable [ str ]: # supported = cls.get_supported_value_types() types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"from_\" ) and callable ( attr ): v_type = attr_name [ 5 :] # if v_type in supported: # raise Exception( # f\"Invalid configuration for type conversion module class {cls.__name__}: conversion source type can't be '{v_type}' because this is already registered as a supported type of the class.\" # ) types . append ( v_type ) return types # @classmethod # def get_target_value_type(cls) -> str: # # supported = cls.get_supported_value_types() # # types = [] # for attr_name, attr in cls.__dict__.items(): # # if attr_name.startswith(\"to_\") and callable(attr): # v_type = attr_name[3:] # if v_type in supported: # raise Exception( # f\"Invalid configuration for type conversion module class {cls.__name__}: conversion target type can't be '{v_type}' because this is already registered as a supported type of the class.\" # ) # types.append(attr_name[3:]) # # return types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) source_config = dict ( self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) ) source_config [ \"doc\" ] = f \"The ' { source_profile } ' value to be converted.\" schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { source_profile : source_config } if self . get_config_value ( \"allow_none_input\" ): schema [ source_config [ \"type\" ]][ \"optional\" ] = True return schema def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) target_type = self . get_config_value ( \"target_type\" ) schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { target_type : { \"type\" : target_type , \"doc\" : f \"The converted ' { source_profile } ' value as ' { target_type } '.\" , } } if self . get_config_value ( \"allow_none_input\" ): schema [ target_type ][ \"optional\" ] = True return schema def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : source_profile : str = self . get_config_value ( \"source_profile\" ) source_config : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) source_type = source_config [ \"type\" ] target_type : str = self . get_config_value ( \"target_type\" ) allow_none : bool = self . get_config_value ( \"allow_none_input\" ) source : Value = inputs . get_value_obj ( source_profile ) if source_type != source . type_name : raise KiaraProcessingException ( f \"Invalid type ( { source . type_name } ) of source value: expected ' { source_type } ' (source profile name: { source_profile } ).\" ) if not source . is_set or source . is_none : if allow_none : outputs . set_value ( \"value_item\" , None ) return else : raise KiaraProcessingException ( \"No source value set.\" ) if not hasattr ( self , f \"from_ { source_profile } \" ): raise Exception ( f \"Module ' { self . _module_type_id } ' can't convert ' { source_type } ' into ' { target_type } ': missing method 'from_ { source_type } '. This is a bug.\" # type: ignore ) func = getattr ( self , f \"from_ { source_profile } \" ) converted = func ( source ) outputs . set_value ( target_type , converted )","title":"CreateValueModule"},{"location":"reference/kiara/operations/__init__/#kiara.operations.create_value.CreateValueModuleConfig","text":"Source code in kiara/operations/create_value.py class CreateValueModuleConfig ( ModuleTypeConfigSchema ): source_profile : str = Field ( description = \"The profile of the source value.\" ) target_type : str = Field ( description = \"The type of the value to convert to.\" ) allow_none_input : bool = Field ( description = \"Whether to allow 'none' source values, if one is encountered 'none' is returned.\" , default = False , )","title":"CreateValueModuleConfig"},{"location":"reference/kiara/operations/__init__/#kiara.operations.create_value.CreateValueOperationType","text":"Operations that create values of specific types from values of certain value profiles. In most cases, source profiles will be 'file' or 'file_bundle' values of some sort, and the created values will exhibit more inherent structure and stricter specs than their sources. The 'create' operation type differs from 'import' in that it expects values that are already onboarded and are all information in the value is stored in a kiara data registry (in most cases the kiara data store ). Source code in kiara/operations/create_value.py class CreateValueOperationType ( OperationType ): \"\"\"Operations that create values of specific types from values of certain value profiles. In most cases, source profiles will be 'file' or 'file_bundle' values of some sort, and the created values will exhibit more inherent structure and stricter specs than their sources. The 'create' operation type differs from 'import' in that it expects values that are already onboarded and are all information in the value is stored in a *kiara* data registry (in most cases the *kiara data store*). \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , CreateValueModule ) # def get_operations_for_source_type( # self, value_type: str # ) -> typing.Dict[str, Operation]: # \"\"\"Find all operations that transform from the specified type. # # The result dict uses the target type of the conversion as key, and the operation itself as value. # \"\"\" # # result: typing.Dict[str, Operation] = {} # for o_id, op in self.operations.items(): # source_type = op.module_config[\"source_type\"] # if source_type == value_type: # target_type = op.module_config[\"target_type\"] # if target_type in result.keys(): # raise Exception( # f\"Multiple operations to transform from '{source_type}' to {target_type}\" # ) # result[target_type] = op # # return result def get_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): target_type = op . module_config [ \"target_type\" ] if target_type == value_type : source_type = op . module_config [ \"source_type\" ] if source_type in result . keys (): raise Exception ( f \"Multiple operations to transform from ' { source_type } ' to { target_type } \" ) result [ source_type ] = op return result","title":"CreateValueOperationType"},{"location":"reference/kiara/operations/__init__/#kiara.operations.data_export","text":"","title":"data_export"},{"location":"reference/kiara/operations/__init__/#kiara.operations.data_export.DataExportModule","text":"Source code in kiara/operations/data_export.py class DataExportModule ( KiaraModule ): _config_cls = DataExportModuleConfig @classmethod @abc . abstractmethod def get_source_value_type ( cls ) -> str : pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_source_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data export operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"export_as__\" ): continue target_profile = attr [ 11 :] op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_type\" : sup_type , \"target_profile\" : target_profile , }, \"doc\" : f \"Export data of type ' { sup_type } ' as: { target_profile } .\" , } all_metadata_profiles [ f \"export. { sup_type } .as. { target_profile } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_type = self . get_config_value ( \"source_type\" ) inputs : typing . Dict [ str , typing . Any ] = { source_type : { \"type\" : source_type , \"doc\" : f \"A value of type ' { source_type } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The directory to export the file(s) to.\" , \"optional\" : True , }, \"name\" : { \"type\" : \"string\" , \"doc\" : \"The (base) name of the exported file(s).\" , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"export_details\" : { \"type\" : \"dict\" , \"doc\" : \"Details about the exported files/folders.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : target_profile : str = self . get_config_value ( \"target_profile\" ) source_type : str = self . get_config_value ( \"source_type\" ) source = inputs . get_value_data ( source_type ) func_name = f \"export_as__ { target_profile } \" if not hasattr ( self , func_name ): raise Exception ( f \"Can't export ' { source_type } ' value: missing function ' { func_name } ' in class ' { self . __class__ . __name__ } '. Please check this modules documentation or source code to determine which source types and profiles are supported.\" ) base_path = inputs . get_value_data ( \"base_path\" ) if base_path is None : base_path = os . getcwd () name = inputs . get_value_data ( \"name\" ) func = getattr ( self , func_name ) # TODO: check signature? base_path = os . path . abspath ( base_path ) os . makedirs ( base_path , exist_ok = True ) result = func ( value = source , base_path = base_path , name = name ) # schema = ValueSchema(type=self.get_target_value_type(), doc=\"Imported dataset.\") # value_lineage = ValueLineage.from_module_and_inputs( # module=self, output_name=output_key, inputs=inputs # ) # value: Value = self._kiara.data_registry.register_data( # value_data=result, value_schema=schema, lineage=None # ) outputs . set_value ( \"export_details\" , result )","title":"DataExportModule"},{"location":"reference/kiara/operations/__init__/#kiara.operations.data_export.DataExportModuleConfig","text":"Source code in kiara/operations/data_export.py class DataExportModuleConfig ( ModuleTypeConfigSchema ): target_profile : str = Field ( description = \"The name of the target profile. Used to distinguish different target formats for the same data type.\" ) source_type : str = Field ( description = \"The type of the source data that is going to be exported.\" )","title":"DataExportModuleConfig"},{"location":"reference/kiara/operations/__init__/#kiara.operations.data_export.ExportDataOperationType","text":"Export data from kiara . Operations of this type use internally handled datasets, and export it to the local file system. Export operations are created by implementing a class that inherits from DataExportModule , kiara will register it under an operation id following this template: <SOURCE_DATA_TYPE>.export_as.<EXPORT_PROFILE> The meaning of the templated fields is: EXPORTED_DATA_TYPE : the data type of the value to export EXPORT_PROFILE : a short, free-form description of the format the data will be exported as Source code in kiara/operations/data_export.py class ExportDataOperationType ( OperationType ): \"\"\"Export data from *kiara*. Operations of this type use internally handled datasets, and export it to the local file system. Export operations are created by implementing a class that inherits from [DataExportModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.data_export/#kiara.operations.data_export.DataExportModule), *kiara* will register it under an operation id following this template: ``` <SOURCE_DATA_TYPE>.export_as.<EXPORT_PROFILE> ``` The meaning of the templated fields is: - `EXPORTED_DATA_TYPE`: the data type of the value to export - `EXPORT_PROFILE`: a short, free-form description of the format the data will be exported as \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : match = issubclass ( op_config . module_cls , DataExportModule ) return match def get_export_operations_per_source_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_source_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] target_profile = op_config . module_config [ \"target_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ target_profile ] = op_config return result def get_export_operations_for_source_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_export_operations_per_source_type () . get ( value_type , {})","title":"ExportDataOperationType"},{"location":"reference/kiara/operations/__init__/#kiara.operations.data_export.FileBundleImportModule","text":"Import a file, optionally saving it to the data store. Source code in kiara/operations/data_export.py class FileBundleImportModule ( DataExportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file_bundle\"","title":"FileBundleImportModule"},{"location":"reference/kiara/operations/__init__/#kiara.operations.data_export.FileExportModule","text":"Import a file, optionally saving it to the data store. Source code in kiara/operations/data_export.py class FileExportModule ( DataExportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file\"","title":"FileExportModule"},{"location":"reference/kiara/operations/__init__/#kiara.operations.data_import","text":"","title":"data_import"},{"location":"reference/kiara/operations/__init__/#kiara.operations.data_import.DataImportModule","text":"Source code in kiara/operations/data_import.py class DataImportModule ( KiaraModule ): _config_cls = DataImportModuleConfig @classmethod @abc . abstractmethod def get_target_value_type ( cls ) -> str : pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_target_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data import operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"import_from__\" ): continue tokens = attr [ 13 :] . rsplit ( \"__\" , maxsplit = 1 ) if len ( tokens ) != 2 : log_message ( f \"Can't determine source name and type from string in module { cls . _module_type_id } , ignoring method: { attr } \" # type: ignore ) source_profile , source_type = tokens op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_profile\" : source_profile , \"source_type\" : source_type , }, \"doc\" : f \"Import data of type ' { sup_type } ' from a { source_profile } { source_type } and save it to the kiara data store.\" , } all_metadata_profiles [ f \"import. { sup_type } .from. { source_profile } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"source_profile\" ) inputs : typing . Dict [ str , typing . Any ] = { input_name : { \"type\" : self . get_config_value ( \"source_type\" ), \"doc\" : f \"A { self . get_config_value ( 'source_profile' ) } ' { self . get_config_value ( 'source_type' ) } ' value.\" , }, } # allow_save = self.get_config_value(\"allow_save_input\") # save_default = self.get_config_value(\"save_default\") # if allow_save: # inputs[\"save\"] = { # \"type\": \"boolean\", # \"doc\": \"Whether to save the imported value, or not.\", # \"default\": save_default, # } # # allow_aliases: typing.Optional[bool] = self.get_config_value( # \"allow_aliases_input\" # ) # if allow_aliases is None: # allow_aliases = allow_save # # if allow_aliases and not allow_save and not save_default: # raise Exception( # \"Invalid module configuration: allowing aliases input does not make sense if save is disabled.\" # ) # # if allow_aliases: # default_aliases = self.get_config_value(\"aliases_default\") # inputs[\"aliases\"] = { # \"type\": \"list\", # \"doc\": \"A list of aliases to use when storing the value (only applicable if 'save' is set).\", # \"default\": default_aliases, # } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: output_name = self . get_target_value_type () if output_name == \"any\" : output_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { output_name : { \"type\" : self . get_target_value_type (), \"doc\" : f \"The imported { self . get_target_value_type () } value.\" , }, } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : source_profile : str = self . get_config_value ( \"source_profile\" ) source_type : str = self . get_config_value ( \"source_type\" ) source = inputs . get_value_data ( source_profile ) if self . get_target_value_type () == \"any\" : output_key : str = \"value_item\" else : output_key = self . get_target_value_type () func_name = f \"import_from__ { source_profile } __ { source_type } \" if not hasattr ( self , func_name ): raise Exception ( f \"Can't import ' { source_type } ' value: missing function ' { func_name } ' in class ' { self . __class__ . __name__ } '. Please check this modules documentation or source code to determine which source types and profiles are supported.\" ) func = getattr ( self , func_name ) # TODO: check signature? result = func ( source ) # schema = ValueSchema(type=self.get_target_value_type(), doc=\"Imported dataset.\") # value_lineage = ValueLineage.from_module_and_inputs( # module=self, output_name=output_key, inputs=inputs # ) # value: Value = self._kiara.data_registry.register_data( # value_data=result, value_schema=schema, lineage=None # ) outputs . set_value ( output_key , result )","title":"DataImportModule"},{"location":"reference/kiara/operations/__init__/#kiara.operations.data_import.DataImportModuleConfig","text":"Source code in kiara/operations/data_import.py class DataImportModuleConfig ( ModuleTypeConfigSchema ): # value_type: str = Field(description=\"The type of the value to be imported.\") source_profile : str = Field ( description = \"The name of the source profile. Used to distinguish different input categories for the same input type.\" ) source_type : str = Field ( description = \"The type of the source to import from.\" ) # allow_save_input: bool = Field( # description=\"Allow the user to choose whether to save the imported item or not.\", # default=True, # ) # save_default: bool = Field( # description=\"The default of the 'save' input if not specified by the user.\", # default=False, # ) # allow_aliases_input: typing.Optional[bool] = Field( # description=\"Allow the user to choose aliases for the saved value.\", # default=None, # ) # aliases_default: typing.List[str] = Field( # description=\"Default value for aliases.\", default_factory=list # )","title":"DataImportModuleConfig"},{"location":"reference/kiara/operations/__init__/#kiara.operations.data_import.FileBundleImportModule","text":"Import a file, optionally saving it to the data store. Source code in kiara/operations/data_import.py class FileBundleImportModule ( DataImportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file_bundle\"","title":"FileBundleImportModule"},{"location":"reference/kiara/operations/__init__/#kiara.operations.data_import.FileImportModule","text":"Import a file, optionally saving it to the data store. Source code in kiara/operations/data_import.py class FileImportModule ( DataImportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file\"","title":"FileImportModule"},{"location":"reference/kiara/operations/__init__/#kiara.operations.data_import.ImportDataOperationType","text":"Import data into kiara . Operations of this type take external data, and register it into kiara . External data is different in that it usually does not come with any metadata on how it was created, who created it, when, etc. Import operations are created by implementing a class that inherits from DataImportModule , kiara will register it under an operation id following this template: <IMPORTED_DATA_TYPE>.import_from.<IMPORT_PROFILE>.<INPUT_TYPE> The meaning of the templated fields is: IMPORTED_DATA_TYPE : the data type of the imported value IMPORT_PROFILE : a short, free-form description of where from (or how) the data is imported INPUT_TYPE : the data type of the user input that points to the data (like a file path, url, query, etc.) -- in most cases this will be some form of a string or uri There are two main scenarios when an operation of this type is used: 'onboard' data that was created by a 3rd party, or using external processes 're-import' data that as created in kiara , then exported to be transformed in an external process, and then imported again into kiara In both of those scenarios, we'll need to have a way to add metadata to fill out 'holes' in the metadata 'chold chain'. We don't have a concept yet as to how to do that, but that is planned for the future. Source code in kiara/operations/data_import.py class ImportDataOperationType ( OperationType ): \"\"\"Import data into *kiara*. Operations of this type take external data, and register it into *kiara*. External data is different in that it usually does not come with any metadata on how it was created, who created it, when, etc. Import operations are created by implementing a class that inherits from [DataImportModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModule), *kiara* will register it under an operation id following this template: ``` <IMPORTED_DATA_TYPE>.import_from.<IMPORT_PROFILE>.<INPUT_TYPE> ``` The meaning of the templated fields is: - `IMPORTED_DATA_TYPE`: the data type of the imported value - `IMPORT_PROFILE`: a short, free-form description of where from (or how) the data is imported - `INPUT_TYPE`: the data type of the user input that points to the data (like a file path, url, query, etc.) -- in most cases this will be some form of a string or uri There are two main scenarios when an operation of this type is used: - 'onboard' data that was created by a 3rd party, or using external processes - 're-import' data that as created in *kiara*, then exported to be transformed in an external process, and then imported again into *kiara* In both of those scenarios, we'll need to have a way to add metadata to fill out 'holes' in the metadata 'chold chain'. We don't have a concept yet as to how to do that, but that is planned for the future. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , DataImportModule ) def get_import_operations_per_target_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_target_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] source_profile = op_config . module_config [ \"source_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ source_profile ] = op_config return result def get_import_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_import_operations_per_target_type () . get ( value_type , {})","title":"ImportDataOperationType"},{"location":"reference/kiara/operations/__init__/#kiara.operations.extract_metadata","text":"","title":"extract_metadata"},{"location":"reference/kiara/operations/__init__/#kiara.operations.extract_metadata.ExtractMetadataModule","text":"Base class to use when writing a module to extract metadata from a file. It's possible to use any arbitrary kiara module for this purpose, but sub-classing this makes it easier. Source code in kiara/operations/extract_metadata.py class ExtractMetadataModule ( KiaraModule ): \"\"\"Base class to use when writing a module to extract metadata from a file. It's possible to use any arbitrary *kiara* module for this purpose, but sub-classing this makes it easier. \"\"\" _config_cls = MetadataModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_types : typing . Iterable = cls . get_supported_value_types () if \"*\" in value_types : value_types = kiara . type_mgmt . value_type_names metadata_key = cls . get_metadata_key () all_value_types = set () for value_type in value_types : if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring metadata-extract operation (metadata key: { metadata_key } ) for type ' { value_type } ': type not available\" ) continue all_value_types . add ( value_type ) sub_types = kiara . type_mgmt . get_sub_types ( value_type ) all_value_types . update ( sub_types ) for value_type in all_value_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type }, \"doc\" : f \"Extract ' { metadata_key } ' metadata for value of type ' { value_type } '.\" , } all_metadata_profiles [ f \"extract. { metadata_key } .metadata.from. { value_type } \" ] = op_config return all_metadata_profiles @classmethod @abc . abstractmethod def _get_supported_types ( self ) -> typing . Union [ str , typing . Iterable [ str ]]: pass @classmethod def get_metadata_key ( cls ) -> str : return cls . _module_type_name # type: ignore @classmethod def get_supported_value_types ( cls ) -> typing . Set [ str ]: _types = cls . _get_supported_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) def __init__ ( self , * args , ** kwargs ): self . _metadata_schema : typing . Optional [ str ] = None super () . __init__ ( * args , ** kwargs ) @property def value_type ( self ) -> str : data_type = self . get_config_value ( \"value_type\" ) sup_types = self . get_supported_value_types () if \"*\" not in sup_types and data_type not in sup_types : match = False for sup_type in sup_types : for sub_type in self . _kiara . type_mgmt . get_sub_types ( sup_type ): if sub_type == data_type : match = True break if not match : raise ValueError ( f \"Invalid module configuration, type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_value_types ()) } .\" ) return data_type @property def metadata_schema ( self ) -> str : if self . _metadata_schema is not None : return self . _metadata_schema schema = self . _get_metadata_schema ( type = self . value_type ) if isinstance ( schema , type ) and issubclass ( schema , BaseModel ): schema = schema . schema_json () elif not isinstance ( schema , str ): raise TypeError ( f \"Invalid type for metadata schema: { type ( schema ) } \" ) self . _metadata_schema = schema return self . _metadata_schema @abc . abstractmethod def _get_metadata_schema ( self , type : str ) -> typing . Union [ str , typing . Type [ BaseModel ]]: \"\"\"Create the metadata schema for the configured type.\"\"\" @abc . abstractmethod def extract_metadata ( self , value : Value ) -> typing . Union [ typing . Mapping [ str , typing . Any ], BaseModel ]: pass def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : { \"type\" : self . value_type , \"doc\" : f \"A value of type ' { self . value_type } '\" , \"optional\" : False , } } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"metadata_item\" : { \"type\" : \"dict\" , \"doc\" : \"The metadata for the provided value.\" , }, \"metadata_item_schema\" : { \"type\" : \"string\" , \"doc\" : \"The (json) schema for the metadata.\" , }, } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" value = inputs . get_value_obj ( input_name ) if self . value_type != \"any\" and value . type_name != self . value_type : raise KiaraProcessingException ( f \"Can't extract metadata for value of type ' { value . value_schema . type } '. Expected type ' { self . value_type } '.\" ) # TODO: if type 'any', validate that the data is actually of the right type? outputs . set_value ( \"metadata_item_schema\" , self . metadata_schema ) metadata = self . extract_metadata ( value ) if isinstance ( metadata , BaseModel ): metadata = metadata . dict ( exclude_none = True ) # TODO: validate metadata? outputs . set_value ( \"metadata_item\" , metadata )","title":"ExtractMetadataModule"},{"location":"reference/kiara/operations/__init__/#kiara.operations.extract_metadata.ExtractMetadataOperationType","text":"Extract metadata from a dataset. The purpose of this operation type is to be able to extract arbitrary, type-specific metadata from value data. In general, kiara wants to collect (and store along the value) as much metadata related to data as possible, but the extraction process should not take a lot of processing time (since this is done whenever a value is registered into a data registry). As its hard to predict all the types of metadata of a specific type that could be interesting in specific scenarios, kiara supports a pluggable mechanism to add new metadata extraction processes by extending the base class ExtractMetadataModule and adding that implementation somewhere kiara can find it. Once that is done, kiara will automatically add a new operation with an id that follows this template: <VALUE_TYPE>.extract_metadata.<METADATA_KEY> , where METADATA_KEY is a name under which the metadata will be stored within the value object. By default, every value type should have at least one metadata extraction module where the METADATA_KEY is the same as the value type name, and which contains basic, type-specific metadata (e.g. for a 'table', that would be number of rows, column names, column types, etc.). Source code in kiara/operations/extract_metadata.py class ExtractMetadataOperationType ( OperationType ): \"\"\"Extract metadata from a dataset. The purpose of this operation type is to be able to extract arbitrary, type-specific metadata from value data. In general, *kiara* wants to collect (and store along the value) as much metadata related to data as possible, but the extraction process should not take a lot of processing time (since this is done whenever a value is registered into a data registry). As its hard to predict all the types of metadata of a specific type that could be interesting in specific scenarios, *kiara* supports a pluggable mechanism to add new metadata extraction processes by extending the base class [`ExtractMetadataModule`](http://dharpa.org/kiara/latest/api_reference/kiara.operations.extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataModule) and adding that implementation somewhere *kiara* can find it. Once that is done, *kiara* will automatically add a new operation with an id that follows this template: `<VALUE_TYPE>.extract_metadata.<METADATA_KEY>`, where `METADATA_KEY` is a name under which the metadata will be stored within the value object. By default, every value type should have at least one metadata extraction module where the `METADATA_KEY` is the same as the value type name, and which contains basic, type-specific metadata (e.g. for a 'table', that would be number of rows, column names, column types, etc.). \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , ExtractMetadataModule ) def get_all_operations_for_type ( self , value_type : str ) -> typing . Mapping [ str , Operation ]: result = {} for op_config in self . operations . values (): v_t = op_config . module_config [ \"value_type\" ] if v_t != value_type : continue module_cls : ExtractMetadataModule = op_config . module_cls # type: ignore md_key = module_cls . get_metadata_key () result [ md_key ] = op_config return result","title":"ExtractMetadataOperationType"},{"location":"reference/kiara/operations/__init__/#kiara.operations.extract_metadata.MetadataModuleConfig","text":"Source code in kiara/operations/extract_metadata.py class MetadataModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The data type this module will be used for.\" )","title":"MetadataModuleConfig"},{"location":"reference/kiara/operations/__init__/#kiara.operations.merge_values","text":"","title":"merge_values"},{"location":"reference/kiara/operations/__init__/#kiara.operations.merge_values.ValueMergeModule","text":"Base class for operations that merge several values into one. NOT USED YET. Source code in kiara/operations/merge_values.py class ValueMergeModule ( KiaraModule ): \"\"\"Base class for operations that merge several values into one. NOT USED YET. \"\"\" def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_schema_dicts : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . get_config_value ( \"input_schemas\" ) if not input_schema_dicts : raise Exception ( \"No input schemas provided.\" ) input_schemas : typing . Dict [ str , ValueSchema ] = {} for k , v in input_schema_dicts . items (): input_schemas [ k ] = ValueSchema ( ** v ) return input_schemas def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"merged_value\" : { \"type\" : self . get_config_value ( \"output_type\" )}}","title":"ValueMergeModule"},{"location":"reference/kiara/operations/__init__/#kiara.operations.merge_values.ValueMergeModuleConfig","text":"Source code in kiara/operations/merge_values.py class ValueMergeModuleConfig ( ModuleTypeConfigSchema ): input_schemas : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = Field ( description = \"The schemas for all of the expected inputs.\" ) output_type : str = Field ( description = \"The result type of the merged value.\" )","title":"ValueMergeModuleConfig"},{"location":"reference/kiara/operations/__init__/#kiara.operations.pretty_print","text":"","title":"pretty_print"},{"location":"reference/kiara/operations/__init__/#kiara.operations.pretty_print.PrettyPrintModuleConfig","text":"Source code in kiara/operations/pretty_print.py class PrettyPrintModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to save.\" ) target_type : str = Field ( description = \"The target to print the value to.\" , default = \"string\" )","title":"PrettyPrintModuleConfig"},{"location":"reference/kiara/operations/__init__/#kiara.operations.pretty_print.PrettyPrintOperationType","text":"This operation type renders values of any type into human readable output for different targets (e.g. terminal, html, ...). The main purpose of this operation type is to show the user as much content of the data as possible for the specific rendering target, without giving any guarantees that all information contained in the data is shown. It may print out the whole content (if the content is small, or the available space large enough), or just a few bits and pieces from the start and/or end of the data, whatever makes most sense for the data itself. For example, for large tables it might be a good idea to print out the first and last 30 rows, so users can see which columns are available, and get a rough overview of the content itself. For network graphs it might make sense to print out some graph properties (number of nodes, edges, available attributes, ...) on the terminal, but render the graph itself when using html as target. Currently, it is only possible to implement a pretty_print renderer if you have access to the source code of the value type you want to render. To add a new render method, add a new instance method to the ValueType class in question, in the format: def pretty_print_as_<TARGET_TYPE>(value: Value, print_config: typing.Mapping[str, typing.Any]) -> typing.Any: ... ... kiara will look at all available ValueType classes for methods that match this signature, and auto-generate operations following this naming template: <SOURCE_TYPE>.pretty_print_as.<TARGET_TYPE> . Currently, only the type renderables is implemented as target type (for printing out to the terminal). It is planned to add output suitable for use in Jupyter notebooks in the near future. Source code in kiara/operations/pretty_print.py class PrettyPrintOperationType ( OperationType ): \"\"\"This operation type renders values of any type into human readable output for different targets (e.g. terminal, html, ...). The main purpose of this operation type is to show the user as much content of the data as possible for the specific rendering target, without giving any guarantees that all information contained in the data is shown. It may print out the whole content (if the content is small, or the available space large enough), or just a few bits and pieces from the start and/or end of the data, whatever makes most sense for the data itself. For example, for large tables it might be a good idea to print out the first and last 30 rows, so users can see which columns are available, and get a rough overview of the content itself. For network graphs it might make sense to print out some graph properties (number of nodes, edges, available attributes, ...) on the terminal, but render the graph itself when using html as target. Currently, it is only possible to implement a `pretty_print` renderer if you have access to the source code of the value type you want to render. To add a new render method, add a new instance method to the `ValueType` class in question, in the format: ``` def pretty_print_as_<TARGET_TYPE>(value: Value, print_config: typing.Mapping[str, typing.Any]) -> typing.Any: ... ... ``` *kiara* will look at all available `ValueType` classes for methods that match this signature, and auto-generate operations following this naming template: `<SOURCE_TYPE>.pretty_print_as.<TARGET_TYPE>`. Currently, only the type `renderables` is implemented as target type (for printing out to the terminal). It is planned to add output suitable for use in Jupyter notebooks in the near future. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , PrettyPrintValueModule ) def get_pretty_print_operation ( self , value_type : str , target_type : str ) -> Operation : result = [] for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] != value_type : continue if op_config . module_config [ \"target_type\" ] != target_type : continue result . append ( op_config ) if not result : raise Exception ( f \"No pretty print operation for value type ' { value_type } ' and output ' { target_type } ' registered.\" ) elif len ( result ) != 1 : raise Exception ( f \"Multiple pretty print operations for value type ' { value_type } ' and output ' { target_type } ' registered.\" ) return result [ 0 ] def pretty_print ( self , value : Value , target_type : str , print_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ) -> typing . Any : ops_config = self . get_pretty_print_operation ( value_type = value . type_name , target_type = target_type ) inputs : typing . Mapping [ str , typing . Any ] = { value . type_name : value , \"print_config\" : print_config , } result = ops_config . module . run ( ** inputs ) printed = result . get_value_data ( \"printed\" ) return printed","title":"PrettyPrintOperationType"},{"location":"reference/kiara/operations/__init__/#kiara.operations.pretty_print.PrettyPrintValueModule","text":"Source code in kiara/operations/pretty_print.py class PrettyPrintValueModule ( KiaraModule ): _config_cls = PrettyPrintModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type_cls in kiara . type_mgmt . value_types . items (): for attr in dir ( value_type_cls ): if not attr . startswith ( \"pretty_print_as_\" ): continue target_type = attr [ 16 :] if target_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Pretty print target type ' { target_type } ' for source type ' { value_type_name } ' not valid, ignoring.\" ) continue op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"target_type\" : target_type , }, \"doc\" : f \"Pretty print a value of type ' { value_type_name } ' as ' { target_type } '.\" , } all_metadata_profiles [ f \"pretty_print. { value_type_name } .as. { target_type } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type = self . get_config_value ( \"value_type\" ) if value_type == \"all\" : input_name = \"value_item\" doc = \"A value of any type.\" else : input_name = value_type doc = f \"A value of type ' { value_type } '.\" inputs : typing . Mapping [ str , typing . Any ] = { input_name : { \"type\" : value_type , \"doc\" : doc }, \"print_config\" : { \"type\" : \"dict\" , \"doc\" : \"Optional print configuration.\" , \"optional\" : True , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"printed\" : { \"type\" : self . get_config_value ( \"target_type\" ), \"doc\" : f \"The printed value as ' { self . get_config_value ( 'target_type' ) } '.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_type : str = self . get_config_value ( \"value_type\" ) target_type : str = self . get_config_value ( \"target_type\" ) if value_type == \"all\" : input_name = \"value_item\" else : input_name = value_type value_obj : Value = inputs . get_value_obj ( input_name ) print_config = dict ( DEFAULT_PRETTY_PRINT_CONFIG ) config : typing . Mapping = inputs . get_value_data ( \"print_config\" ) if config : print_config . update ( config ) func_name = f \"pretty_print_as_ { target_type } \" if not hasattr ( value_obj . type_obj , func_name ): raise Exception ( f \"Type ' { value_type } ' can't be pretty printed as ' { target_type } '. This is most likely a bug.\" ) if not value_obj . is_set : printed = \"-- not set --\" else : func = getattr ( value_obj . type_obj , func_name ) # TODO: check signature try : printed = func ( value = value_obj , print_config = print_config ) except Exception as e : if is_debug (): import traceback traceback . print_exc () raise KiaraProcessingException ( str ( e )) outputs . set_value ( \"printed\" , printed )","title":"PrettyPrintValueModule"},{"location":"reference/kiara/operations/__init__/#kiara.operations.sample","text":"","title":"sample"},{"location":"reference/kiara/operations/__init__/#kiara.operations.sample.SampleValueModule","text":"Base class for operations that take samples of data. Source code in kiara/operations/sample.py class SampleValueModule ( KiaraModule ): \"\"\"Base class for operations that take samples of data.\"\"\" _config_cls = SampleValueModuleConfig @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : \"\"\"Return the value type for this sample module.\"\"\" @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring sample operation for source type ' { value_type } ': type not available\" ) return {} for sample_type in cls . get_supported_sample_types (): op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"sample_type\" : sample_type }, \"doc\" : f \"Sample value of type ' { value_type } ' using method: { sample_type } .\" , } key = f \"sample. { value_type } . { sample_type } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles @classmethod def get_supported_sample_types ( cls ) -> typing . Iterable [ str ]: types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"sample_\" ) and callable ( attr ): sample_type = attr_name [ 7 :] if sample_type in types : raise Exception ( f \"Error in sample module ' { cls . __name__ } ': multiple sample methods for type ' { sample_type } '.\" ) types . append ( sample_type ) return types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_value_type (), \"doc\" : \"The value to sample.\" , }, \"sample_size\" : { \"type\" : \"integer\" , \"doc\" : \"The sample size.\" , \"default\" : 10 , }, } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"sampled_value\" : { \"type\" : self . get_value_type (), \"doc\" : \"The sampled value\" } } def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : sample_size : int = inputs . get_value_data ( \"sample_size\" ) sample_type : str = self . get_config_value ( \"sample_type\" ) if sample_size < 0 : raise KiaraProcessingException ( f \"Invalid sample size ' { sample_size } ': can't be negative.\" ) input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" value : Value = inputs . get_value_obj ( input_name ) func = getattr ( self , f \"sample_ { sample_type } \" ) result = func ( value = value , sample_size = sample_size ) outputs . set_value ( \"sampled_value\" , result )","title":"SampleValueModule"},{"location":"reference/kiara/operations/__init__/#kiara.operations.sample.SampleValueModuleConfig","text":"Source code in kiara/operations/sample.py class SampleValueModuleConfig ( ModuleTypeConfigSchema ): sample_type : str = Field ( description = \"The sample method.\" )","title":"SampleValueModuleConfig"},{"location":"reference/kiara/operations/__init__/#kiara.operations.sample.SampleValueOperationType","text":"Operation type for sampling data of different types. This is useful to reduce the size of some datasets in previews and test-runs, while adjusting parameters and the like. Operations of this type can implement very simple or complex ways to take samples of the data they are fed. The most important ones are sampling operations relating to tables and arrays, but it might also make sense to sample texts, image-sets and so on. Operations of this type take the value to be sampled as input, as well as a sample size (integer), which can be, for example, a number of rows, or percentage of the whole dataset, depending on sample type. Modules that implement sampling should inherit from SampleValueModule , and will get auto-registered with operation ids following this template: <VALUE_TYPE>.sample.<SAMPLE_TYPE_NAME> , where SAMPLE_TYPE_NAME is a descriptive name what will be sampled, or how sampling will be done. Source code in kiara/operations/sample.py class SampleValueOperationType ( OperationType ): \"\"\"Operation type for sampling data of different types. This is useful to reduce the size of some datasets in previews and test-runs, while adjusting parameters and the like. Operations of this type can implement very simple or complex ways to take samples of the data they are fed. The most important ones are sampling operations relating to tables and arrays, but it might also make sense to sample texts, image-sets and so on. Operations of this type take the value to be sampled as input, as well as a sample size (integer), which can be, for example, a number of rows, or percentage of the whole dataset, depending on sample type. Modules that implement sampling should inherit from [SampleValueModule](https://dharpa.org/kiara/latest/api_reference/kiara.operations.sample/#kiara.operations.sample.SampleValueModule), and will get auto-registered with operation ids following this template: `<VALUE_TYPE>.sample.<SAMPLE_TYPE_NAME>`, where `SAMPLE_TYPE_NAME` is a descriptive name what will be sampled, or how sampling will be done. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , SampleValueModule ) def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): sample_op_module_cls : typing . Type [ SampleValueModule ] = op . module_cls # type: ignore source_type = sample_op_module_cls . get_value_type () if source_type == value_type : target_type = op . module_config [ \"sample_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to sample ' { source_type } ' using { target_type } \" ) result [ target_type ] = op return result","title":"SampleValueOperationType"},{"location":"reference/kiara/operations/__init__/#kiara.operations.serialize","text":"","title":"serialize"},{"location":"reference/kiara/operations/__init__/#kiara.operations.serialize.SerializeValueModule","text":"Base class for 'serialize' operations. Source code in kiara/operations/serialize.py class SerializeValueModule ( KiaraModule ): \"\"\"Base class for 'serialize' operations.\"\"\" _config_cls = SerializeValueModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring serialization operation for source type ' { value_type } ': type not available\" ) return {} for serialization_type in cls . get_supported_serialization_types (): mod_conf = { \"value_type\" : value_type , \"serialization_type\" : serialization_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Serialize value of type ' { value_type } ' to ' { serialization_type } '.\" , } key = f \"serialize. { value_type } .as. { serialization_type } \" if key in all_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_profiles [ key ] = op_config return all_profiles @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : pass @classmethod def get_supported_serialization_types ( cls ) -> typing . Iterable [ str ]: serialize_types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"to_\" ) and callable ( attr ): serialize_types . append ( attr_name [ 3 :]) return serialize_types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) # serialization_type: str = self.get_config_value(\"serialization_type\") return { \"value_item\" : { \"type\" : value_type , \"doc\" : f \"The ' { value_type } ' value to be serialized.\" , } } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) return { \"value_info\" : { \"type\" : \"value_info\" , \"doc\" : \"Information about the (original) serialized value (can be used to re-constitute the value, incl. its original id).\" , }, \"deserialize_config\" : { \"type\" : \"deserialize_config\" , \"doc\" : f \"The config to use to deserialize the value of type ' { value_type } '.\" , }, } def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_type : str = self . get_config_value ( \"value_type\" ) value_obj = inputs . get_value_obj ( \"value_item\" ) serialization_type = self . get_config_value ( \"serialization_type\" ) if value_type != value_obj . type_name : raise KiaraProcessingException ( f \"Invalid type ( { value_obj . type_name } ) of source value: expected ' { value_type } '.\" ) if not hasattr ( self , f \"to_ { serialization_type } \" ): # this can never happen, I think raise Exception ( f \"Module ' { self . _module_type_id } ' can't serialize ' { value_type } ' to ' { serialization_type } ': missing method 'to_ { serialization_type } '. This is a bug.\" # type: ignore ) func = getattr ( self , f \"to_ { serialization_type } \" ) serialized = func ( value_obj ) if isinstance ( serialized , typing . Mapping ): serialized = DeserializeConfig ( ** serialized ) if not isinstance ( serialized , DeserializeConfig ): raise KiaraProcessingException ( f \"Invalid serialization result type: { type ( serialized ) } \" ) outputs . set_values ( deserialize_config = serialized , value_info = value_obj . get_info () )","title":"SerializeValueModule"},{"location":"reference/kiara/operations/__init__/#kiara.operations.serialize.SerializeValueModuleConfig","text":"Source code in kiara/operations/serialize.py class SerializeValueModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the source value.\" ) serialization_type : str = Field ( description = \"The type of the converted value.\" )","title":"SerializeValueModuleConfig"},{"location":"reference/kiara/operations/__init__/#kiara.operations.serialize.SerializeValueOperationType","text":"Operations that serialize data into formats that can be used for data exchange. NOT USED YET Source code in kiara/operations/serialize.py class SerializeValueOperationType ( OperationType ): \"\"\"Operations that serialize data into formats that can be used for data exchange. NOT USED YET \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , SerializeValueModule ) def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): source_type = op . module_config [ \"value_type\" ] if source_type == value_type : target_type = op . module_config [ \"serialization_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to serialize ' { source_type } ' to { target_type } \" ) result [ target_type ] = op return result","title":"SerializeValueOperationType"},{"location":"reference/kiara/operations/__init__/#kiara.operations.store_value","text":"","title":"store_value"},{"location":"reference/kiara/operations/__init__/#kiara.operations.store_value.StoreOperationType","text":"Store a value into a local data store. This is a special operation type, that is used internally by the [LocalDataStore](http://dharpa.org/kiara/latest/api_reference/kiara.data.registry.store/#kiara.data.registry.store.LocalDataStore] data registry implementation. For each value type that should be supported by the persistent kiara data store, there must be an implementation of the StoreValueTypeModule class, which handles the actual persisting on disk. In most cases, end users won't need to interact with this type of operation. Source code in kiara/operations/store_value.py class StoreOperationType ( OperationType ): \"\"\"Store a value into a local data store. This is a special operation type, that is used internally by the [LocalDataStore](http://dharpa.org/kiara/latest/api_reference/kiara.data.registry.store/#kiara.data.registry.store.LocalDataStore] data registry implementation. For each value type that should be supported by the persistent *kiara* data store, there must be an implementation of the [StoreValueTypeModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.store_value/#kiara.operations.store_value.StoreValueTypeModule) class, which handles the actual persisting on disk. In most cases, end users won't need to interact with this type of operation. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , StoreValueTypeModule ) def get_store_operation_for_type ( self , value_type : str ) -> Operation : result = [] for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] == value_type : result . append ( op_config ) if not result : raise Exception ( f \"No 'store_value' operation for type ' { value_type } ' registered.\" ) elif len ( result ) != 1 : pass for r in result : print ( r . json ( indent = 2 )) raise Exception ( f \"Multiple 'store_value' operations for type ' { value_type } ' registered.\" ) return result [ 0 ]","title":"StoreOperationType"},{"location":"reference/kiara/operations/__init__/#kiara.operations.store_value.StoreValueModuleConfig","text":"Source code in kiara/operations/store_value.py class StoreValueModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to save.\" )","title":"StoreValueModuleConfig"},{"location":"reference/kiara/operations/__init__/#kiara.operations.store_value.StoreValueTypeModule","text":"Store a specific value type. This is used internally. Source code in kiara/operations/store_value.py class StoreValueTypeModule ( KiaraModule ): \"\"\"Store a specific value type. This is used internally. \"\"\" _config_cls = StoreValueModuleConfig @classmethod def get_supported_value_types ( cls ) -> typing . Set [ str ]: _types = cls . retrieve_supported_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) @classmethod @abc . abstractmethod def retrieve_supported_types ( cls ) -> typing . Union [ str , typing . Iterable [ str ]]: pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} all_types_and_subtypes = set () for sup_type in cls . get_supported_value_types (): if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring save operation for type ' { sup_type } ': type not available\" ) continue all_types_and_subtypes . add ( sup_type ) sub_types = kiara . type_mgmt . get_sub_types ( sup_type ) all_types_and_subtypes . update ( sub_types ) for sup_type in all_types_and_subtypes : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : sup_type }, \"doc\" : f \"Store a value of type ' { sup_type } '.\" , } all_metadata_profiles [ f \"store. { sup_type } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" inputs : typing . Mapping [ str , typing . Any ] = { \"value_id\" : { \"type\" : \"string\" , \"doc\" : \"The id to use when saving the value.\" , }, field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The base path to save the value to.\" , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : \"The original or cloned (if applicable) value that was saved.\" , }, \"load_config\" : { \"type\" : \"load_config\" , \"doc\" : \"The configuration to use with kiara to load the saved value.\" , }, } return outputs @abc . abstractmethod def store_value ( self , value : Value , base_path : str ) -> typing . Union [ typing . Tuple [ typing . Dict [ str , typing . Any ], typing . Any ], typing . Dict [ str , typing . Any ], ]: \"\"\"Save the value, and return the load config needed to load it again.\"\"\" def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_id : str = inputs . get_value_data ( \"value_id\" ) if not value_id : raise KiaraProcessingException ( \"No value id provided.\" ) field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" value_obj : Value = inputs . get_value_obj ( field_name ) base_path : str = inputs . get_value_data ( \"base_path\" ) result = self . store_value ( value = value_obj , base_path = base_path ) if isinstance ( result , typing . Mapping ): load_config = result result_value = value_obj elif isinstance ( result , tuple ): load_config = result [ 0 ] if result [ 1 ]: result_value = result [ 1 ] else : result_value = value_obj else : raise KiaraProcessingException ( f \"Invalid result type for 'store_value' method in class ' { self . __class__ . __name__ } '. This is a bug.\" ) load_config [ \"value_id\" ] = value_id lc = LoadConfig ( ** load_config ) if lc . base_path_input_name and lc . base_path_input_name not in lc . inputs . keys (): raise KiaraProcessingException ( f \"Invalid load config: base path ' { lc . base_path_input_name } ' not part of inputs.\" ) outputs . set_values ( metadata = None , lineage = None , ** { \"load_config\" : lc , field_name : result_value } )","title":"StoreValueTypeModule"},{"location":"reference/kiara/operations/calculate_hash/","text":"CalculateHashOperationType ( OperationType ) \u00b6 Operation type to calculate hash(es) for data. This is mostly used internally, so users usually won't be exposed to operations of this type. Source code in kiara/operations/calculate_hash.py class CalculateHashOperationType ( OperationType ): \"\"\"Operation type to calculate hash(es) for data. This is mostly used internally, so users usually won't be exposed to operations of this type. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return op_config . module_cls == CalculateValueHashModule def get_hash_operations_for_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: result = {} for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] == value_type : result [ op_config . module_config [ \"hash_type\" ]] = op_config return result CalculateValueHashModule ( KiaraModule ) \u00b6 Calculate the hash of a value. Source code in kiara/operations/calculate_hash.py class CalculateValueHashModule ( KiaraModule ): \"\"\"Calculate the hash of a value.\"\"\" _module_type_name = \"value.hash\" _config_cls = CalculateValueHashesConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type in kiara . type_mgmt . value_types . items (): hash_types = value_type . get_supported_hash_types () for ht in hash_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"hash_type\" : ht }, \"doc\" : f \"Calculate ' { ht } ' hash for value type ' { value_type_name } '.\" , } all_metadata_profiles [ f \"calculate_hash. { ht } .for. { value_type_name } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , } } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"hash\" : { \"type\" : \"string\" , \"doc\" : \"The hash string.\" }} def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" value : Value = inputs . get_value_obj ( input_name ) value_hash = value . get_hash ( hash_type = self . get_config_value ( \"hash_type\" )) outputs . set_value ( \"hash\" , value_hash . hash ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/calculate_hash.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , } } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/calculate_hash.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"hash\" : { \"type\" : \"string\" , \"doc\" : \"The hash string.\" }} retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/calculate_hash.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type in kiara . type_mgmt . value_types . items (): hash_types = value_type . get_supported_hash_types () for ht in hash_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"hash_type\" : ht }, \"doc\" : f \"Calculate ' { ht } ' hash for value type ' { value_type_name } '.\" , } all_metadata_profiles [ f \"calculate_hash. { ht } .for. { value_type_name } \" ] = op_config return all_metadata_profiles CalculateValueHashesConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/calculate_hash.py class CalculateValueHashesConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to calculate the hash for.\" ) hash_type : str = Field ( description = \"The hash type.\" ) hash_type : str pydantic-field required \u00b6 The hash type. value_type : str pydantic-field required \u00b6 The type of the value to calculate the hash for.","title":"calculate_hash"},{"location":"reference/kiara/operations/calculate_hash/#kiara.operations.calculate_hash.CalculateHashOperationType","text":"Operation type to calculate hash(es) for data. This is mostly used internally, so users usually won't be exposed to operations of this type. Source code in kiara/operations/calculate_hash.py class CalculateHashOperationType ( OperationType ): \"\"\"Operation type to calculate hash(es) for data. This is mostly used internally, so users usually won't be exposed to operations of this type. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return op_config . module_cls == CalculateValueHashModule def get_hash_operations_for_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: result = {} for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] == value_type : result [ op_config . module_config [ \"hash_type\" ]] = op_config return result","title":"CalculateHashOperationType"},{"location":"reference/kiara/operations/calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashModule","text":"Calculate the hash of a value. Source code in kiara/operations/calculate_hash.py class CalculateValueHashModule ( KiaraModule ): \"\"\"Calculate the hash of a value.\"\"\" _module_type_name = \"value.hash\" _config_cls = CalculateValueHashesConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type in kiara . type_mgmt . value_types . items (): hash_types = value_type . get_supported_hash_types () for ht in hash_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"hash_type\" : ht }, \"doc\" : f \"Calculate ' { ht } ' hash for value type ' { value_type_name } '.\" , } all_metadata_profiles [ f \"calculate_hash. { ht } .for. { value_type_name } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , } } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"hash\" : { \"type\" : \"string\" , \"doc\" : \"The hash string.\" }} def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" value : Value = inputs . get_value_obj ( input_name ) value_hash = value . get_hash ( hash_type = self . get_config_value ( \"hash_type\" )) outputs . set_value ( \"hash\" , value_hash . hash )","title":"CalculateValueHashModule"},{"location":"reference/kiara/operations/calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/calculate_hash.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"value_type\" ) if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , } }","title":"create_input_schema()"},{"location":"reference/kiara/operations/calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/calculate_hash.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"hash\" : { \"type\" : \"string\" , \"doc\" : \"The hash string.\" }}","title":"create_output_schema()"},{"location":"reference/kiara/operations/calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/calculate_hash.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type in kiara . type_mgmt . value_types . items (): hash_types = value_type . get_supported_hash_types () for ht in hash_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"hash_type\" : ht }, \"doc\" : f \"Calculate ' { ht } ' hash for value type ' { value_type_name } '.\" , } all_metadata_profiles [ f \"calculate_hash. { ht } .for. { value_type_name } \" ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"reference/kiara/operations/calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashesConfig","text":"Source code in kiara/operations/calculate_hash.py class CalculateValueHashesConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to calculate the hash for.\" ) hash_type : str = Field ( description = \"The hash type.\" )","title":"CalculateValueHashesConfig"},{"location":"reference/kiara/operations/calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashesConfig.hash_type","text":"The hash type.","title":"hash_type"},{"location":"reference/kiara/operations/calculate_hash/#kiara.operations.calculate_hash.CalculateValueHashesConfig.value_type","text":"The type of the value to calculate the hash for.","title":"value_type"},{"location":"reference/kiara/operations/create_value/","text":"CreateValueModule ( KiaraModule ) \u00b6 Base class for 'create' value type operations. Source code in kiara/operations/create_value.py class CreateValueModule ( KiaraModule ): \"\"\"Base class for 'create' value type operations.\"\"\" _config_cls = CreateValueModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} # value_types: typing.Iterable[str] = cls.get_supported_value_types() # if \"*\" in value_types: # value_types = kiara.type_mgmt.value_type_names # # for value_type in value_types: target_type = cls . get_target_value_type () if target_type not in kiara . type_mgmt . value_type_names : raise Exception ( f \"Can't assemble type convert operations for target type ' { target_type } ': type not available\" ) for source_profile in cls . get_source_value_profiles (): mod_conf = { \"source_profile\" : source_profile , \"target_type\" : target_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Create a ' { target_type } ' value from a ' { source_profile } '.\" , } # key = f\"{value_type}.convert_from.{target_type}\" # type: ignore # if key in all_metadata_profiles.keys(): # raise Exception(f\"Duplicate profile key: {key}\") # all_metadata_profiles[key] = op_config key = f \"create. { target_type } .from. { source_profile } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles # @classmethod # def get_supported_value_types(cls) -> typing.Set[str]: # # _types = cls._get_supported_value_types() # if isinstance(_types, str): # _types = [_types] # # return set(_types) # @classmethod @abc . abstractmethod def get_target_value_type ( cls ) -> str : pass @classmethod def get_source_value_profiles ( cls ) -> typing . Iterable [ str ]: # supported = cls.get_supported_value_types() types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"from_\" ) and callable ( attr ): v_type = attr_name [ 5 :] # if v_type in supported: # raise Exception( # f\"Invalid configuration for type conversion module class {cls.__name__}: conversion source type can't be '{v_type}' because this is already registered as a supported type of the class.\" # ) types . append ( v_type ) return types # @classmethod # def get_target_value_type(cls) -> str: # # supported = cls.get_supported_value_types() # # types = [] # for attr_name, attr in cls.__dict__.items(): # # if attr_name.startswith(\"to_\") and callable(attr): # v_type = attr_name[3:] # if v_type in supported: # raise Exception( # f\"Invalid configuration for type conversion module class {cls.__name__}: conversion target type can't be '{v_type}' because this is already registered as a supported type of the class.\" # ) # types.append(attr_name[3:]) # # return types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) source_config = dict ( self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) ) source_config [ \"doc\" ] = f \"The ' { source_profile } ' value to be converted.\" schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { source_profile : source_config } if self . get_config_value ( \"allow_none_input\" ): schema [ source_config [ \"type\" ]][ \"optional\" ] = True return schema def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) target_type = self . get_config_value ( \"target_type\" ) schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { target_type : { \"type\" : target_type , \"doc\" : f \"The converted ' { source_profile } ' value as ' { target_type } '.\" , } } if self . get_config_value ( \"allow_none_input\" ): schema [ target_type ][ \"optional\" ] = True return schema def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : source_profile : str = self . get_config_value ( \"source_profile\" ) source_config : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) source_type = source_config [ \"type\" ] target_type : str = self . get_config_value ( \"target_type\" ) allow_none : bool = self . get_config_value ( \"allow_none_input\" ) source : Value = inputs . get_value_obj ( source_profile ) if source_type != source . type_name : raise KiaraProcessingException ( f \"Invalid type ( { source . type_name } ) of source value: expected ' { source_type } ' (source profile name: { source_profile } ).\" ) if not source . is_set or source . is_none : if allow_none : outputs . set_value ( \"value_item\" , None ) return else : raise KiaraProcessingException ( \"No source value set.\" ) if not hasattr ( self , f \"from_ { source_profile } \" ): raise Exception ( f \"Module ' { self . _module_type_id } ' can't convert ' { source_type } ' into ' { target_type } ': missing method 'from_ { source_type } '. This is a bug.\" # type: ignore ) func = getattr ( self , f \"from_ { source_profile } \" ) converted = func ( source ) outputs . set_value ( target_type , converted ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/create_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) source_config = dict ( self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) ) source_config [ \"doc\" ] = f \"The ' { source_profile } ' value to be converted.\" schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { source_profile : source_config } if self . get_config_value ( \"allow_none_input\" ): schema [ source_config [ \"type\" ]][ \"optional\" ] = True return schema create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/create_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) target_type = self . get_config_value ( \"target_type\" ) schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { target_type : { \"type\" : target_type , \"doc\" : f \"The converted ' { source_profile } ' value as ' { target_type } '.\" , } } if self . get_config_value ( \"allow_none_input\" ): schema [ target_type ][ \"optional\" ] = True return schema retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/create_value.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} # value_types: typing.Iterable[str] = cls.get_supported_value_types() # if \"*\" in value_types: # value_types = kiara.type_mgmt.value_type_names # # for value_type in value_types: target_type = cls . get_target_value_type () if target_type not in kiara . type_mgmt . value_type_names : raise Exception ( f \"Can't assemble type convert operations for target type ' { target_type } ': type not available\" ) for source_profile in cls . get_source_value_profiles (): mod_conf = { \"source_profile\" : source_profile , \"target_type\" : target_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Create a ' { target_type } ' value from a ' { source_profile } '.\" , } # key = f\"{value_type}.convert_from.{target_type}\" # type: ignore # if key in all_metadata_profiles.keys(): # raise Exception(f\"Duplicate profile key: {key}\") # all_metadata_profiles[key] = op_config key = f \"create. { target_type } .from. { source_profile } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles CreateValueModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/create_value.py class CreateValueModuleConfig ( ModuleTypeConfigSchema ): source_profile : str = Field ( description = \"The profile of the source value.\" ) target_type : str = Field ( description = \"The type of the value to convert to.\" ) allow_none_input : bool = Field ( description = \"Whether to allow 'none' source values, if one is encountered 'none' is returned.\" , default = False , ) allow_none_input : bool pydantic-field \u00b6 Whether to allow 'none' source values, if one is encountered 'none' is returned. source_profile : str pydantic-field required \u00b6 The profile of the source value. target_type : str pydantic-field required \u00b6 The type of the value to convert to. CreateValueOperationType ( OperationType ) \u00b6 Operations that create values of specific types from values of certain value profiles. In most cases, source profiles will be 'file' or 'file_bundle' values of some sort, and the created values will exhibit more inherent structure and stricter specs than their sources. The 'create' operation type differs from 'import' in that it expects values that are already onboarded and are all information in the value is stored in a kiara data registry (in most cases the kiara data store ). Source code in kiara/operations/create_value.py class CreateValueOperationType ( OperationType ): \"\"\"Operations that create values of specific types from values of certain value profiles. In most cases, source profiles will be 'file' or 'file_bundle' values of some sort, and the created values will exhibit more inherent structure and stricter specs than their sources. The 'create' operation type differs from 'import' in that it expects values that are already onboarded and are all information in the value is stored in a *kiara* data registry (in most cases the *kiara data store*). \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , CreateValueModule ) # def get_operations_for_source_type( # self, value_type: str # ) -> typing.Dict[str, Operation]: # \"\"\"Find all operations that transform from the specified type. # # The result dict uses the target type of the conversion as key, and the operation itself as value. # \"\"\" # # result: typing.Dict[str, Operation] = {} # for o_id, op in self.operations.items(): # source_type = op.module_config[\"source_type\"] # if source_type == value_type: # target_type = op.module_config[\"target_type\"] # if target_type in result.keys(): # raise Exception( # f\"Multiple operations to transform from '{source_type}' to {target_type}\" # ) # result[target_type] = op # # return result def get_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): target_type = op . module_config [ \"target_type\" ] if target_type == value_type : source_type = op . module_config [ \"source_type\" ] if source_type in result . keys (): raise Exception ( f \"Multiple operations to transform from ' { source_type } ' to { target_type } \" ) result [ source_type ] = op return result get_operations_for_target_type ( self , value_type ) \u00b6 Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. Source code in kiara/operations/create_value.py def get_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): target_type = op . module_config [ \"target_type\" ] if target_type == value_type : source_type = op . module_config [ \"source_type\" ] if source_type in result . keys (): raise Exception ( f \"Multiple operations to transform from ' { source_type } ' to { target_type } \" ) result [ source_type ] = op return result","title":"create_value"},{"location":"reference/kiara/operations/create_value/#kiara.operations.create_value.CreateValueModule","text":"Base class for 'create' value type operations. Source code in kiara/operations/create_value.py class CreateValueModule ( KiaraModule ): \"\"\"Base class for 'create' value type operations.\"\"\" _config_cls = CreateValueModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} # value_types: typing.Iterable[str] = cls.get_supported_value_types() # if \"*\" in value_types: # value_types = kiara.type_mgmt.value_type_names # # for value_type in value_types: target_type = cls . get_target_value_type () if target_type not in kiara . type_mgmt . value_type_names : raise Exception ( f \"Can't assemble type convert operations for target type ' { target_type } ': type not available\" ) for source_profile in cls . get_source_value_profiles (): mod_conf = { \"source_profile\" : source_profile , \"target_type\" : target_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Create a ' { target_type } ' value from a ' { source_profile } '.\" , } # key = f\"{value_type}.convert_from.{target_type}\" # type: ignore # if key in all_metadata_profiles.keys(): # raise Exception(f\"Duplicate profile key: {key}\") # all_metadata_profiles[key] = op_config key = f \"create. { target_type } .from. { source_profile } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles # @classmethod # def get_supported_value_types(cls) -> typing.Set[str]: # # _types = cls._get_supported_value_types() # if isinstance(_types, str): # _types = [_types] # # return set(_types) # @classmethod @abc . abstractmethod def get_target_value_type ( cls ) -> str : pass @classmethod def get_source_value_profiles ( cls ) -> typing . Iterable [ str ]: # supported = cls.get_supported_value_types() types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"from_\" ) and callable ( attr ): v_type = attr_name [ 5 :] # if v_type in supported: # raise Exception( # f\"Invalid configuration for type conversion module class {cls.__name__}: conversion source type can't be '{v_type}' because this is already registered as a supported type of the class.\" # ) types . append ( v_type ) return types # @classmethod # def get_target_value_type(cls) -> str: # # supported = cls.get_supported_value_types() # # types = [] # for attr_name, attr in cls.__dict__.items(): # # if attr_name.startswith(\"to_\") and callable(attr): # v_type = attr_name[3:] # if v_type in supported: # raise Exception( # f\"Invalid configuration for type conversion module class {cls.__name__}: conversion target type can't be '{v_type}' because this is already registered as a supported type of the class.\" # ) # types.append(attr_name[3:]) # # return types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) source_config = dict ( self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) ) source_config [ \"doc\" ] = f \"The ' { source_profile } ' value to be converted.\" schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { source_profile : source_config } if self . get_config_value ( \"allow_none_input\" ): schema [ source_config [ \"type\" ]][ \"optional\" ] = True return schema def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) target_type = self . get_config_value ( \"target_type\" ) schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { target_type : { \"type\" : target_type , \"doc\" : f \"The converted ' { source_profile } ' value as ' { target_type } '.\" , } } if self . get_config_value ( \"allow_none_input\" ): schema [ target_type ][ \"optional\" ] = True return schema def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : source_profile : str = self . get_config_value ( \"source_profile\" ) source_config : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) source_type = source_config [ \"type\" ] target_type : str = self . get_config_value ( \"target_type\" ) allow_none : bool = self . get_config_value ( \"allow_none_input\" ) source : Value = inputs . get_value_obj ( source_profile ) if source_type != source . type_name : raise KiaraProcessingException ( f \"Invalid type ( { source . type_name } ) of source value: expected ' { source_type } ' (source profile name: { source_profile } ).\" ) if not source . is_set or source . is_none : if allow_none : outputs . set_value ( \"value_item\" , None ) return else : raise KiaraProcessingException ( \"No source value set.\" ) if not hasattr ( self , f \"from_ { source_profile } \" ): raise Exception ( f \"Module ' { self . _module_type_id } ' can't convert ' { source_type } ' into ' { target_type } ': missing method 'from_ { source_type } '. This is a bug.\" # type: ignore ) func = getattr ( self , f \"from_ { source_profile } \" ) converted = func ( source ) outputs . set_value ( target_type , converted )","title":"CreateValueModule"},{"location":"reference/kiara/operations/create_value/#kiara.operations.create_value.CreateValueModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/create_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) source_config = dict ( self . _kiara . type_mgmt . get_type_config_for_data_profile ( source_profile ) ) source_config [ \"doc\" ] = f \"The ' { source_profile } ' value to be converted.\" schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { source_profile : source_config } if self . get_config_value ( \"allow_none_input\" ): schema [ source_config [ \"type\" ]][ \"optional\" ] = True return schema","title":"create_input_schema()"},{"location":"reference/kiara/operations/create_value/#kiara.operations.create_value.CreateValueModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/create_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_profile = self . get_config_value ( \"source_profile\" ) target_type = self . get_config_value ( \"target_type\" ) schema : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = { target_type : { \"type\" : target_type , \"doc\" : f \"The converted ' { source_profile } ' value as ' { target_type } '.\" , } } if self . get_config_value ( \"allow_none_input\" ): schema [ target_type ][ \"optional\" ] = True return schema","title":"create_output_schema()"},{"location":"reference/kiara/operations/create_value/#kiara.operations.create_value.CreateValueModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/create_value.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} # value_types: typing.Iterable[str] = cls.get_supported_value_types() # if \"*\" in value_types: # value_types = kiara.type_mgmt.value_type_names # # for value_type in value_types: target_type = cls . get_target_value_type () if target_type not in kiara . type_mgmt . value_type_names : raise Exception ( f \"Can't assemble type convert operations for target type ' { target_type } ': type not available\" ) for source_profile in cls . get_source_value_profiles (): mod_conf = { \"source_profile\" : source_profile , \"target_type\" : target_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Create a ' { target_type } ' value from a ' { source_profile } '.\" , } # key = f\"{value_type}.convert_from.{target_type}\" # type: ignore # if key in all_metadata_profiles.keys(): # raise Exception(f\"Duplicate profile key: {key}\") # all_metadata_profiles[key] = op_config key = f \"create. { target_type } .from. { source_profile } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"reference/kiara/operations/create_value/#kiara.operations.create_value.CreateValueModuleConfig","text":"Source code in kiara/operations/create_value.py class CreateValueModuleConfig ( ModuleTypeConfigSchema ): source_profile : str = Field ( description = \"The profile of the source value.\" ) target_type : str = Field ( description = \"The type of the value to convert to.\" ) allow_none_input : bool = Field ( description = \"Whether to allow 'none' source values, if one is encountered 'none' is returned.\" , default = False , )","title":"CreateValueModuleConfig"},{"location":"reference/kiara/operations/create_value/#kiara.operations.create_value.CreateValueModuleConfig.allow_none_input","text":"Whether to allow 'none' source values, if one is encountered 'none' is returned.","title":"allow_none_input"},{"location":"reference/kiara/operations/create_value/#kiara.operations.create_value.CreateValueModuleConfig.source_profile","text":"The profile of the source value.","title":"source_profile"},{"location":"reference/kiara/operations/create_value/#kiara.operations.create_value.CreateValueModuleConfig.target_type","text":"The type of the value to convert to.","title":"target_type"},{"location":"reference/kiara/operations/create_value/#kiara.operations.create_value.CreateValueOperationType","text":"Operations that create values of specific types from values of certain value profiles. In most cases, source profiles will be 'file' or 'file_bundle' values of some sort, and the created values will exhibit more inherent structure and stricter specs than their sources. The 'create' operation type differs from 'import' in that it expects values that are already onboarded and are all information in the value is stored in a kiara data registry (in most cases the kiara data store ). Source code in kiara/operations/create_value.py class CreateValueOperationType ( OperationType ): \"\"\"Operations that create values of specific types from values of certain value profiles. In most cases, source profiles will be 'file' or 'file_bundle' values of some sort, and the created values will exhibit more inherent structure and stricter specs than their sources. The 'create' operation type differs from 'import' in that it expects values that are already onboarded and are all information in the value is stored in a *kiara* data registry (in most cases the *kiara data store*). \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , CreateValueModule ) # def get_operations_for_source_type( # self, value_type: str # ) -> typing.Dict[str, Operation]: # \"\"\"Find all operations that transform from the specified type. # # The result dict uses the target type of the conversion as key, and the operation itself as value. # \"\"\" # # result: typing.Dict[str, Operation] = {} # for o_id, op in self.operations.items(): # source_type = op.module_config[\"source_type\"] # if source_type == value_type: # target_type = op.module_config[\"target_type\"] # if target_type in result.keys(): # raise Exception( # f\"Multiple operations to transform from '{source_type}' to {target_type}\" # ) # result[target_type] = op # # return result def get_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): target_type = op . module_config [ \"target_type\" ] if target_type == value_type : source_type = op . module_config [ \"source_type\" ] if source_type in result . keys (): raise Exception ( f \"Multiple operations to transform from ' { source_type } ' to { target_type } \" ) result [ source_type ] = op return result","title":"CreateValueOperationType"},{"location":"reference/kiara/operations/create_value/#kiara.operations.create_value.CreateValueOperationType.get_operations_for_target_type","text":"Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. Source code in kiara/operations/create_value.py def get_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that transform to the specified type. The result dict uses the source type of the conversion as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): target_type = op . module_config [ \"target_type\" ] if target_type == value_type : source_type = op . module_config [ \"source_type\" ] if source_type in result . keys (): raise Exception ( f \"Multiple operations to transform from ' { source_type } ' to { target_type } \" ) result [ source_type ] = op return result","title":"get_operations_for_target_type()"},{"location":"reference/kiara/operations/data_export/","text":"DataExportModule ( KiaraModule ) \u00b6 Source code in kiara/operations/data_export.py class DataExportModule ( KiaraModule ): _config_cls = DataExportModuleConfig @classmethod @abc . abstractmethod def get_source_value_type ( cls ) -> str : pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_source_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data export operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"export_as__\" ): continue target_profile = attr [ 11 :] op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_type\" : sup_type , \"target_profile\" : target_profile , }, \"doc\" : f \"Export data of type ' { sup_type } ' as: { target_profile } .\" , } all_metadata_profiles [ f \"export. { sup_type } .as. { target_profile } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_type = self . get_config_value ( \"source_type\" ) inputs : typing . Dict [ str , typing . Any ] = { source_type : { \"type\" : source_type , \"doc\" : f \"A value of type ' { source_type } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The directory to export the file(s) to.\" , \"optional\" : True , }, \"name\" : { \"type\" : \"string\" , \"doc\" : \"The (base) name of the exported file(s).\" , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"export_details\" : { \"type\" : \"dict\" , \"doc\" : \"Details about the exported files/folders.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : target_profile : str = self . get_config_value ( \"target_profile\" ) source_type : str = self . get_config_value ( \"source_type\" ) source = inputs . get_value_data ( source_type ) func_name = f \"export_as__ { target_profile } \" if not hasattr ( self , func_name ): raise Exception ( f \"Can't export ' { source_type } ' value: missing function ' { func_name } ' in class ' { self . __class__ . __name__ } '. Please check this modules documentation or source code to determine which source types and profiles are supported.\" ) base_path = inputs . get_value_data ( \"base_path\" ) if base_path is None : base_path = os . getcwd () name = inputs . get_value_data ( \"name\" ) func = getattr ( self , func_name ) # TODO: check signature? base_path = os . path . abspath ( base_path ) os . makedirs ( base_path , exist_ok = True ) result = func ( value = source , base_path = base_path , name = name ) # schema = ValueSchema(type=self.get_target_value_type(), doc=\"Imported dataset.\") # value_lineage = ValueLineage.from_module_and_inputs( # module=self, output_name=output_key, inputs=inputs # ) # value: Value = self._kiara.data_registry.register_data( # value_data=result, value_schema=schema, lineage=None # ) outputs . set_value ( \"export_details\" , result ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_export.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_type = self . get_config_value ( \"source_type\" ) inputs : typing . Dict [ str , typing . Any ] = { source_type : { \"type\" : source_type , \"doc\" : f \"A value of type ' { source_type } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The directory to export the file(s) to.\" , \"optional\" : True , }, \"name\" : { \"type\" : \"string\" , \"doc\" : \"The (base) name of the exported file(s).\" , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_export.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"export_details\" : { \"type\" : \"dict\" , \"doc\" : \"Details about the exported files/folders.\" , } } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/data_export.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_source_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data export operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"export_as__\" ): continue target_profile = attr [ 11 :] op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_type\" : sup_type , \"target_profile\" : target_profile , }, \"doc\" : f \"Export data of type ' { sup_type } ' as: { target_profile } .\" , } all_metadata_profiles [ f \"export. { sup_type } .as. { target_profile } \" ] = op_config return all_metadata_profiles DataExportModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/data_export.py class DataExportModuleConfig ( ModuleTypeConfigSchema ): target_profile : str = Field ( description = \"The name of the target profile. Used to distinguish different target formats for the same data type.\" ) source_type : str = Field ( description = \"The type of the source data that is going to be exported.\" ) source_type : str pydantic-field required \u00b6 The type of the source data that is going to be exported. target_profile : str pydantic-field required \u00b6 The name of the target profile. Used to distinguish different target formats for the same data type. ExportDataOperationType ( OperationType ) \u00b6 Export data from kiara . Operations of this type use internally handled datasets, and export it to the local file system. Export operations are created by implementing a class that inherits from DataExportModule , kiara will register it under an operation id following this template: <SOURCE_DATA_TYPE>.export_as.<EXPORT_PROFILE> The meaning of the templated fields is: EXPORTED_DATA_TYPE : the data type of the value to export EXPORT_PROFILE : a short, free-form description of the format the data will be exported as Source code in kiara/operations/data_export.py class ExportDataOperationType ( OperationType ): \"\"\"Export data from *kiara*. Operations of this type use internally handled datasets, and export it to the local file system. Export operations are created by implementing a class that inherits from [DataExportModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.data_export/#kiara.operations.data_export.DataExportModule), *kiara* will register it under an operation id following this template: ``` <SOURCE_DATA_TYPE>.export_as.<EXPORT_PROFILE> ``` The meaning of the templated fields is: - `EXPORTED_DATA_TYPE`: the data type of the value to export - `EXPORT_PROFILE`: a short, free-form description of the format the data will be exported as \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : match = issubclass ( op_config . module_cls , DataExportModule ) return match def get_export_operations_per_source_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_source_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] target_profile = op_config . module_config [ \"target_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ target_profile ] = op_config return result def get_export_operations_for_source_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_export_operations_per_source_type () . get ( value_type , {}) get_export_operations_for_source_type ( self , value_type ) \u00b6 Return all available import operations that produce data of the specified type. Source code in kiara/operations/data_export.py def get_export_operations_for_source_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_export_operations_per_source_type () . get ( value_type , {}) get_export_operations_per_source_type ( self ) \u00b6 Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. Source code in kiara/operations/data_export.py def get_export_operations_per_source_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_source_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] target_profile = op_config . module_config [ \"target_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ target_profile ] = op_config return result FileBundleImportModule ( DataExportModule ) \u00b6 Import a file, optionally saving it to the data store. Source code in kiara/operations/data_export.py class FileBundleImportModule ( DataExportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file_bundle\" FileExportModule ( DataExportModule ) \u00b6 Import a file, optionally saving it to the data store. Source code in kiara/operations/data_export.py class FileExportModule ( DataExportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file\"","title":"data_export"},{"location":"reference/kiara/operations/data_export/#kiara.operations.data_export.DataExportModule","text":"Source code in kiara/operations/data_export.py class DataExportModule ( KiaraModule ): _config_cls = DataExportModuleConfig @classmethod @abc . abstractmethod def get_source_value_type ( cls ) -> str : pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_source_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data export operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"export_as__\" ): continue target_profile = attr [ 11 :] op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_type\" : sup_type , \"target_profile\" : target_profile , }, \"doc\" : f \"Export data of type ' { sup_type } ' as: { target_profile } .\" , } all_metadata_profiles [ f \"export. { sup_type } .as. { target_profile } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_type = self . get_config_value ( \"source_type\" ) inputs : typing . Dict [ str , typing . Any ] = { source_type : { \"type\" : source_type , \"doc\" : f \"A value of type ' { source_type } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The directory to export the file(s) to.\" , \"optional\" : True , }, \"name\" : { \"type\" : \"string\" , \"doc\" : \"The (base) name of the exported file(s).\" , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"export_details\" : { \"type\" : \"dict\" , \"doc\" : \"Details about the exported files/folders.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : target_profile : str = self . get_config_value ( \"target_profile\" ) source_type : str = self . get_config_value ( \"source_type\" ) source = inputs . get_value_data ( source_type ) func_name = f \"export_as__ { target_profile } \" if not hasattr ( self , func_name ): raise Exception ( f \"Can't export ' { source_type } ' value: missing function ' { func_name } ' in class ' { self . __class__ . __name__ } '. Please check this modules documentation or source code to determine which source types and profiles are supported.\" ) base_path = inputs . get_value_data ( \"base_path\" ) if base_path is None : base_path = os . getcwd () name = inputs . get_value_data ( \"name\" ) func = getattr ( self , func_name ) # TODO: check signature? base_path = os . path . abspath ( base_path ) os . makedirs ( base_path , exist_ok = True ) result = func ( value = source , base_path = base_path , name = name ) # schema = ValueSchema(type=self.get_target_value_type(), doc=\"Imported dataset.\") # value_lineage = ValueLineage.from_module_and_inputs( # module=self, output_name=output_key, inputs=inputs # ) # value: Value = self._kiara.data_registry.register_data( # value_data=result, value_schema=schema, lineage=None # ) outputs . set_value ( \"export_details\" , result )","title":"DataExportModule"},{"location":"reference/kiara/operations/data_export/#kiara.operations.data_export.DataExportModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_export.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: source_type = self . get_config_value ( \"source_type\" ) inputs : typing . Dict [ str , typing . Any ] = { source_type : { \"type\" : source_type , \"doc\" : f \"A value of type ' { source_type } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The directory to export the file(s) to.\" , \"optional\" : True , }, \"name\" : { \"type\" : \"string\" , \"doc\" : \"The (base) name of the exported file(s).\" , }, } return inputs","title":"create_input_schema()"},{"location":"reference/kiara/operations/data_export/#kiara.operations.data_export.DataExportModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_export.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"export_details\" : { \"type\" : \"dict\" , \"doc\" : \"Details about the exported files/folders.\" , } } return outputs","title":"create_output_schema()"},{"location":"reference/kiara/operations/data_export/#kiara.operations.data_export.DataExportModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/data_export.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_source_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data export operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"export_as__\" ): continue target_profile = attr [ 11 :] op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_type\" : sup_type , \"target_profile\" : target_profile , }, \"doc\" : f \"Export data of type ' { sup_type } ' as: { target_profile } .\" , } all_metadata_profiles [ f \"export. { sup_type } .as. { target_profile } \" ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"reference/kiara/operations/data_export/#kiara.operations.data_export.DataExportModuleConfig","text":"Source code in kiara/operations/data_export.py class DataExportModuleConfig ( ModuleTypeConfigSchema ): target_profile : str = Field ( description = \"The name of the target profile. Used to distinguish different target formats for the same data type.\" ) source_type : str = Field ( description = \"The type of the source data that is going to be exported.\" )","title":"DataExportModuleConfig"},{"location":"reference/kiara/operations/data_export/#kiara.operations.data_export.DataExportModuleConfig.source_type","text":"The type of the source data that is going to be exported.","title":"source_type"},{"location":"reference/kiara/operations/data_export/#kiara.operations.data_export.DataExportModuleConfig.target_profile","text":"The name of the target profile. Used to distinguish different target formats for the same data type.","title":"target_profile"},{"location":"reference/kiara/operations/data_export/#kiara.operations.data_export.ExportDataOperationType","text":"Export data from kiara . Operations of this type use internally handled datasets, and export it to the local file system. Export operations are created by implementing a class that inherits from DataExportModule , kiara will register it under an operation id following this template: <SOURCE_DATA_TYPE>.export_as.<EXPORT_PROFILE> The meaning of the templated fields is: EXPORTED_DATA_TYPE : the data type of the value to export EXPORT_PROFILE : a short, free-form description of the format the data will be exported as Source code in kiara/operations/data_export.py class ExportDataOperationType ( OperationType ): \"\"\"Export data from *kiara*. Operations of this type use internally handled datasets, and export it to the local file system. Export operations are created by implementing a class that inherits from [DataExportModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.data_export/#kiara.operations.data_export.DataExportModule), *kiara* will register it under an operation id following this template: ``` <SOURCE_DATA_TYPE>.export_as.<EXPORT_PROFILE> ``` The meaning of the templated fields is: - `EXPORTED_DATA_TYPE`: the data type of the value to export - `EXPORT_PROFILE`: a short, free-form description of the format the data will be exported as \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : match = issubclass ( op_config . module_cls , DataExportModule ) return match def get_export_operations_per_source_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_source_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] target_profile = op_config . module_config [ \"target_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ target_profile ] = op_config return result def get_export_operations_for_source_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_export_operations_per_source_type () . get ( value_type , {})","title":"ExportDataOperationType"},{"location":"reference/kiara/operations/data_export/#kiara.operations.data_export.ExportDataOperationType.get_export_operations_for_source_type","text":"Return all available import operations that produce data of the specified type. Source code in kiara/operations/data_export.py def get_export_operations_for_source_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_export_operations_per_source_type () . get ( value_type , {})","title":"get_export_operations_for_source_type()"},{"location":"reference/kiara/operations/data_export/#kiara.operations.data_export.ExportDataOperationType.get_export_operations_per_source_type","text":"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. Source code in kiara/operations/data_export.py def get_export_operations_per_source_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_source_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] target_profile = op_config . module_config [ \"target_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ target_profile ] = op_config return result","title":"get_export_operations_per_source_type()"},{"location":"reference/kiara/operations/data_export/#kiara.operations.data_export.FileBundleImportModule","text":"Import a file, optionally saving it to the data store. Source code in kiara/operations/data_export.py class FileBundleImportModule ( DataExportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file_bundle\"","title":"FileBundleImportModule"},{"location":"reference/kiara/operations/data_export/#kiara.operations.data_export.FileExportModule","text":"Import a file, optionally saving it to the data store. Source code in kiara/operations/data_export.py class FileExportModule ( DataExportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file\"","title":"FileExportModule"},{"location":"reference/kiara/operations/data_import/","text":"DataImportModule ( KiaraModule ) \u00b6 Source code in kiara/operations/data_import.py class DataImportModule ( KiaraModule ): _config_cls = DataImportModuleConfig @classmethod @abc . abstractmethod def get_target_value_type ( cls ) -> str : pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_target_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data import operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"import_from__\" ): continue tokens = attr [ 13 :] . rsplit ( \"__\" , maxsplit = 1 ) if len ( tokens ) != 2 : log_message ( f \"Can't determine source name and type from string in module { cls . _module_type_id } , ignoring method: { attr } \" # type: ignore ) source_profile , source_type = tokens op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_profile\" : source_profile , \"source_type\" : source_type , }, \"doc\" : f \"Import data of type ' { sup_type } ' from a { source_profile } { source_type } and save it to the kiara data store.\" , } all_metadata_profiles [ f \"import. { sup_type } .from. { source_profile } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"source_profile\" ) inputs : typing . Dict [ str , typing . Any ] = { input_name : { \"type\" : self . get_config_value ( \"source_type\" ), \"doc\" : f \"A { self . get_config_value ( 'source_profile' ) } ' { self . get_config_value ( 'source_type' ) } ' value.\" , }, } # allow_save = self.get_config_value(\"allow_save_input\") # save_default = self.get_config_value(\"save_default\") # if allow_save: # inputs[\"save\"] = { # \"type\": \"boolean\", # \"doc\": \"Whether to save the imported value, or not.\", # \"default\": save_default, # } # # allow_aliases: typing.Optional[bool] = self.get_config_value( # \"allow_aliases_input\" # ) # if allow_aliases is None: # allow_aliases = allow_save # # if allow_aliases and not allow_save and not save_default: # raise Exception( # \"Invalid module configuration: allowing aliases input does not make sense if save is disabled.\" # ) # # if allow_aliases: # default_aliases = self.get_config_value(\"aliases_default\") # inputs[\"aliases\"] = { # \"type\": \"list\", # \"doc\": \"A list of aliases to use when storing the value (only applicable if 'save' is set).\", # \"default\": default_aliases, # } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: output_name = self . get_target_value_type () if output_name == \"any\" : output_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { output_name : { \"type\" : self . get_target_value_type (), \"doc\" : f \"The imported { self . get_target_value_type () } value.\" , }, } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : source_profile : str = self . get_config_value ( \"source_profile\" ) source_type : str = self . get_config_value ( \"source_type\" ) source = inputs . get_value_data ( source_profile ) if self . get_target_value_type () == \"any\" : output_key : str = \"value_item\" else : output_key = self . get_target_value_type () func_name = f \"import_from__ { source_profile } __ { source_type } \" if not hasattr ( self , func_name ): raise Exception ( f \"Can't import ' { source_type } ' value: missing function ' { func_name } ' in class ' { self . __class__ . __name__ } '. Please check this modules documentation or source code to determine which source types and profiles are supported.\" ) func = getattr ( self , func_name ) # TODO: check signature? result = func ( source ) # schema = ValueSchema(type=self.get_target_value_type(), doc=\"Imported dataset.\") # value_lineage = ValueLineage.from_module_and_inputs( # module=self, output_name=output_key, inputs=inputs # ) # value: Value = self._kiara.data_registry.register_data( # value_data=result, value_schema=schema, lineage=None # ) outputs . set_value ( output_key , result ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_import.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"source_profile\" ) inputs : typing . Dict [ str , typing . Any ] = { input_name : { \"type\" : self . get_config_value ( \"source_type\" ), \"doc\" : f \"A { self . get_config_value ( 'source_profile' ) } ' { self . get_config_value ( 'source_type' ) } ' value.\" , }, } # allow_save = self.get_config_value(\"allow_save_input\") # save_default = self.get_config_value(\"save_default\") # if allow_save: # inputs[\"save\"] = { # \"type\": \"boolean\", # \"doc\": \"Whether to save the imported value, or not.\", # \"default\": save_default, # } # # allow_aliases: typing.Optional[bool] = self.get_config_value( # \"allow_aliases_input\" # ) # if allow_aliases is None: # allow_aliases = allow_save # # if allow_aliases and not allow_save and not save_default: # raise Exception( # \"Invalid module configuration: allowing aliases input does not make sense if save is disabled.\" # ) # # if allow_aliases: # default_aliases = self.get_config_value(\"aliases_default\") # inputs[\"aliases\"] = { # \"type\": \"list\", # \"doc\": \"A list of aliases to use when storing the value (only applicable if 'save' is set).\", # \"default\": default_aliases, # } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_import.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: output_name = self . get_target_value_type () if output_name == \"any\" : output_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { output_name : { \"type\" : self . get_target_value_type (), \"doc\" : f \"The imported { self . get_target_value_type () } value.\" , }, } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/data_import.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_target_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data import operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"import_from__\" ): continue tokens = attr [ 13 :] . rsplit ( \"__\" , maxsplit = 1 ) if len ( tokens ) != 2 : log_message ( f \"Can't determine source name and type from string in module { cls . _module_type_id } , ignoring method: { attr } \" # type: ignore ) source_profile , source_type = tokens op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_profile\" : source_profile , \"source_type\" : source_type , }, \"doc\" : f \"Import data of type ' { sup_type } ' from a { source_profile } { source_type } and save it to the kiara data store.\" , } all_metadata_profiles [ f \"import. { sup_type } .from. { source_profile } \" ] = op_config return all_metadata_profiles DataImportModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/data_import.py class DataImportModuleConfig ( ModuleTypeConfigSchema ): # value_type: str = Field(description=\"The type of the value to be imported.\") source_profile : str = Field ( description = \"The name of the source profile. Used to distinguish different input categories for the same input type.\" ) source_type : str = Field ( description = \"The type of the source to import from.\" ) # allow_save_input: bool = Field( # description=\"Allow the user to choose whether to save the imported item or not.\", # default=True, # ) # save_default: bool = Field( # description=\"The default of the 'save' input if not specified by the user.\", # default=False, # ) # allow_aliases_input: typing.Optional[bool] = Field( # description=\"Allow the user to choose aliases for the saved value.\", # default=None, # ) # aliases_default: typing.List[str] = Field( # description=\"Default value for aliases.\", default_factory=list # ) source_profile : str pydantic-field required \u00b6 The name of the source profile. Used to distinguish different input categories for the same input type. source_type : str pydantic-field required \u00b6 The type of the source to import from. FileBundleImportModule ( DataImportModule ) \u00b6 Import a file, optionally saving it to the data store. Source code in kiara/operations/data_import.py class FileBundleImportModule ( DataImportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file_bundle\" FileImportModule ( DataImportModule ) \u00b6 Import a file, optionally saving it to the data store. Source code in kiara/operations/data_import.py class FileImportModule ( DataImportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file\" ImportDataOperationType ( OperationType ) \u00b6 Import data into kiara . Operations of this type take external data, and register it into kiara . External data is different in that it usually does not come with any metadata on how it was created, who created it, when, etc. Import operations are created by implementing a class that inherits from DataImportModule , kiara will register it under an operation id following this template: <IMPORTED_DATA_TYPE>.import_from.<IMPORT_PROFILE>.<INPUT_TYPE> The meaning of the templated fields is: IMPORTED_DATA_TYPE : the data type of the imported value IMPORT_PROFILE : a short, free-form description of where from (or how) the data is imported INPUT_TYPE : the data type of the user input that points to the data (like a file path, url, query, etc.) -- in most cases this will be some form of a string or uri There are two main scenarios when an operation of this type is used: 'onboard' data that was created by a 3rd party, or using external processes 're-import' data that as created in kiara , then exported to be transformed in an external process, and then imported again into kiara In both of those scenarios, we'll need to have a way to add metadata to fill out 'holes' in the metadata 'chold chain'. We don't have a concept yet as to how to do that, but that is planned for the future. Source code in kiara/operations/data_import.py class ImportDataOperationType ( OperationType ): \"\"\"Import data into *kiara*. Operations of this type take external data, and register it into *kiara*. External data is different in that it usually does not come with any metadata on how it was created, who created it, when, etc. Import operations are created by implementing a class that inherits from [DataImportModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModule), *kiara* will register it under an operation id following this template: ``` <IMPORTED_DATA_TYPE>.import_from.<IMPORT_PROFILE>.<INPUT_TYPE> ``` The meaning of the templated fields is: - `IMPORTED_DATA_TYPE`: the data type of the imported value - `IMPORT_PROFILE`: a short, free-form description of where from (or how) the data is imported - `INPUT_TYPE`: the data type of the user input that points to the data (like a file path, url, query, etc.) -- in most cases this will be some form of a string or uri There are two main scenarios when an operation of this type is used: - 'onboard' data that was created by a 3rd party, or using external processes - 're-import' data that as created in *kiara*, then exported to be transformed in an external process, and then imported again into *kiara* In both of those scenarios, we'll need to have a way to add metadata to fill out 'holes' in the metadata 'chold chain'. We don't have a concept yet as to how to do that, but that is planned for the future. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , DataImportModule ) def get_import_operations_per_target_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_target_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] source_profile = op_config . module_config [ \"source_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ source_profile ] = op_config return result def get_import_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_import_operations_per_target_type () . get ( value_type , {}) get_import_operations_for_target_type ( self , value_type ) \u00b6 Return all available import operations that produce data of the specified type. Source code in kiara/operations/data_import.py def get_import_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_import_operations_per_target_type () . get ( value_type , {}) get_import_operations_per_target_type ( self ) \u00b6 Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. Source code in kiara/operations/data_import.py def get_import_operations_per_target_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_target_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] source_profile = op_config . module_config [ \"source_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ source_profile ] = op_config return result","title":"data_import"},{"location":"reference/kiara/operations/data_import/#kiara.operations.data_import.DataImportModule","text":"Source code in kiara/operations/data_import.py class DataImportModule ( KiaraModule ): _config_cls = DataImportModuleConfig @classmethod @abc . abstractmethod def get_target_value_type ( cls ) -> str : pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_target_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data import operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"import_from__\" ): continue tokens = attr [ 13 :] . rsplit ( \"__\" , maxsplit = 1 ) if len ( tokens ) != 2 : log_message ( f \"Can't determine source name and type from string in module { cls . _module_type_id } , ignoring method: { attr } \" # type: ignore ) source_profile , source_type = tokens op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_profile\" : source_profile , \"source_type\" : source_type , }, \"doc\" : f \"Import data of type ' { sup_type } ' from a { source_profile } { source_type } and save it to the kiara data store.\" , } all_metadata_profiles [ f \"import. { sup_type } .from. { source_profile } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"source_profile\" ) inputs : typing . Dict [ str , typing . Any ] = { input_name : { \"type\" : self . get_config_value ( \"source_type\" ), \"doc\" : f \"A { self . get_config_value ( 'source_profile' ) } ' { self . get_config_value ( 'source_type' ) } ' value.\" , }, } # allow_save = self.get_config_value(\"allow_save_input\") # save_default = self.get_config_value(\"save_default\") # if allow_save: # inputs[\"save\"] = { # \"type\": \"boolean\", # \"doc\": \"Whether to save the imported value, or not.\", # \"default\": save_default, # } # # allow_aliases: typing.Optional[bool] = self.get_config_value( # \"allow_aliases_input\" # ) # if allow_aliases is None: # allow_aliases = allow_save # # if allow_aliases and not allow_save and not save_default: # raise Exception( # \"Invalid module configuration: allowing aliases input does not make sense if save is disabled.\" # ) # # if allow_aliases: # default_aliases = self.get_config_value(\"aliases_default\") # inputs[\"aliases\"] = { # \"type\": \"list\", # \"doc\": \"A list of aliases to use when storing the value (only applicable if 'save' is set).\", # \"default\": default_aliases, # } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: output_name = self . get_target_value_type () if output_name == \"any\" : output_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { output_name : { \"type\" : self . get_target_value_type (), \"doc\" : f \"The imported { self . get_target_value_type () } value.\" , }, } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : source_profile : str = self . get_config_value ( \"source_profile\" ) source_type : str = self . get_config_value ( \"source_type\" ) source = inputs . get_value_data ( source_profile ) if self . get_target_value_type () == \"any\" : output_key : str = \"value_item\" else : output_key = self . get_target_value_type () func_name = f \"import_from__ { source_profile } __ { source_type } \" if not hasattr ( self , func_name ): raise Exception ( f \"Can't import ' { source_type } ' value: missing function ' { func_name } ' in class ' { self . __class__ . __name__ } '. Please check this modules documentation or source code to determine which source types and profiles are supported.\" ) func = getattr ( self , func_name ) # TODO: check signature? result = func ( source ) # schema = ValueSchema(type=self.get_target_value_type(), doc=\"Imported dataset.\") # value_lineage = ValueLineage.from_module_and_inputs( # module=self, output_name=output_key, inputs=inputs # ) # value: Value = self._kiara.data_registry.register_data( # value_data=result, value_schema=schema, lineage=None # ) outputs . set_value ( output_key , result )","title":"DataImportModule"},{"location":"reference/kiara/operations/data_import/#kiara.operations.data_import.DataImportModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_import.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_config_value ( \"source_profile\" ) inputs : typing . Dict [ str , typing . Any ] = { input_name : { \"type\" : self . get_config_value ( \"source_type\" ), \"doc\" : f \"A { self . get_config_value ( 'source_profile' ) } ' { self . get_config_value ( 'source_type' ) } ' value.\" , }, } # allow_save = self.get_config_value(\"allow_save_input\") # save_default = self.get_config_value(\"save_default\") # if allow_save: # inputs[\"save\"] = { # \"type\": \"boolean\", # \"doc\": \"Whether to save the imported value, or not.\", # \"default\": save_default, # } # # allow_aliases: typing.Optional[bool] = self.get_config_value( # \"allow_aliases_input\" # ) # if allow_aliases is None: # allow_aliases = allow_save # # if allow_aliases and not allow_save and not save_default: # raise Exception( # \"Invalid module configuration: allowing aliases input does not make sense if save is disabled.\" # ) # # if allow_aliases: # default_aliases = self.get_config_value(\"aliases_default\") # inputs[\"aliases\"] = { # \"type\": \"list\", # \"doc\": \"A list of aliases to use when storing the value (only applicable if 'save' is set).\", # \"default\": default_aliases, # } return inputs","title":"create_input_schema()"},{"location":"reference/kiara/operations/data_import/#kiara.operations.data_import.DataImportModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/data_import.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: output_name = self . get_target_value_type () if output_name == \"any\" : output_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { output_name : { \"type\" : self . get_target_value_type (), \"doc\" : f \"The imported { self . get_target_value_type () } value.\" , }, } return outputs","title":"create_output_schema()"},{"location":"reference/kiara/operations/data_import/#kiara.operations.data_import.DataImportModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/data_import.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} sup_type = cls . get_target_value_type () if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring data import operation for type ' { sup_type } ': type not available\" ) return {} for attr in dir ( cls ): if not attr . startswith ( \"import_from__\" ): continue tokens = attr [ 13 :] . rsplit ( \"__\" , maxsplit = 1 ) if len ( tokens ) != 2 : log_message ( f \"Can't determine source name and type from string in module { cls . _module_type_id } , ignoring method: { attr } \" # type: ignore ) source_profile , source_type = tokens op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"source_profile\" : source_profile , \"source_type\" : source_type , }, \"doc\" : f \"Import data of type ' { sup_type } ' from a { source_profile } { source_type } and save it to the kiara data store.\" , } all_metadata_profiles [ f \"import. { sup_type } .from. { source_profile } \" ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"reference/kiara/operations/data_import/#kiara.operations.data_import.DataImportModuleConfig","text":"Source code in kiara/operations/data_import.py class DataImportModuleConfig ( ModuleTypeConfigSchema ): # value_type: str = Field(description=\"The type of the value to be imported.\") source_profile : str = Field ( description = \"The name of the source profile. Used to distinguish different input categories for the same input type.\" ) source_type : str = Field ( description = \"The type of the source to import from.\" ) # allow_save_input: bool = Field( # description=\"Allow the user to choose whether to save the imported item or not.\", # default=True, # ) # save_default: bool = Field( # description=\"The default of the 'save' input if not specified by the user.\", # default=False, # ) # allow_aliases_input: typing.Optional[bool] = Field( # description=\"Allow the user to choose aliases for the saved value.\", # default=None, # ) # aliases_default: typing.List[str] = Field( # description=\"Default value for aliases.\", default_factory=list # )","title":"DataImportModuleConfig"},{"location":"reference/kiara/operations/data_import/#kiara.operations.data_import.DataImportModuleConfig.source_profile","text":"The name of the source profile. Used to distinguish different input categories for the same input type.","title":"source_profile"},{"location":"reference/kiara/operations/data_import/#kiara.operations.data_import.DataImportModuleConfig.source_type","text":"The type of the source to import from.","title":"source_type"},{"location":"reference/kiara/operations/data_import/#kiara.operations.data_import.FileBundleImportModule","text":"Import a file, optionally saving it to the data store. Source code in kiara/operations/data_import.py class FileBundleImportModule ( DataImportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file_bundle\"","title":"FileBundleImportModule"},{"location":"reference/kiara/operations/data_import/#kiara.operations.data_import.FileImportModule","text":"Import a file, optionally saving it to the data store. Source code in kiara/operations/data_import.py class FileImportModule ( DataImportModule ): \"\"\"Import a file, optionally saving it to the data store.\"\"\" @classmethod def get_target_value_type ( cls ) -> str : return \"file\"","title":"FileImportModule"},{"location":"reference/kiara/operations/data_import/#kiara.operations.data_import.ImportDataOperationType","text":"Import data into kiara . Operations of this type take external data, and register it into kiara . External data is different in that it usually does not come with any metadata on how it was created, who created it, when, etc. Import operations are created by implementing a class that inherits from DataImportModule , kiara will register it under an operation id following this template: <IMPORTED_DATA_TYPE>.import_from.<IMPORT_PROFILE>.<INPUT_TYPE> The meaning of the templated fields is: IMPORTED_DATA_TYPE : the data type of the imported value IMPORT_PROFILE : a short, free-form description of where from (or how) the data is imported INPUT_TYPE : the data type of the user input that points to the data (like a file path, url, query, etc.) -- in most cases this will be some form of a string or uri There are two main scenarios when an operation of this type is used: 'onboard' data that was created by a 3rd party, or using external processes 're-import' data that as created in kiara , then exported to be transformed in an external process, and then imported again into kiara In both of those scenarios, we'll need to have a way to add metadata to fill out 'holes' in the metadata 'chold chain'. We don't have a concept yet as to how to do that, but that is planned for the future. Source code in kiara/operations/data_import.py class ImportDataOperationType ( OperationType ): \"\"\"Import data into *kiara*. Operations of this type take external data, and register it into *kiara*. External data is different in that it usually does not come with any metadata on how it was created, who created it, when, etc. Import operations are created by implementing a class that inherits from [DataImportModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.data_import/#kiara.operations.data_import.DataImportModule), *kiara* will register it under an operation id following this template: ``` <IMPORTED_DATA_TYPE>.import_from.<IMPORT_PROFILE>.<INPUT_TYPE> ``` The meaning of the templated fields is: - `IMPORTED_DATA_TYPE`: the data type of the imported value - `IMPORT_PROFILE`: a short, free-form description of where from (or how) the data is imported - `INPUT_TYPE`: the data type of the user input that points to the data (like a file path, url, query, etc.) -- in most cases this will be some form of a string or uri There are two main scenarios when an operation of this type is used: - 'onboard' data that was created by a 3rd party, or using external processes - 're-import' data that as created in *kiara*, then exported to be transformed in an external process, and then imported again into *kiara* In both of those scenarios, we'll need to have a way to add metadata to fill out 'holes' in the metadata 'chold chain'. We don't have a concept yet as to how to do that, but that is planned for the future. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , DataImportModule ) def get_import_operations_per_target_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_target_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] source_profile = op_config . module_config [ \"source_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ source_profile ] = op_config return result def get_import_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_import_operations_per_target_type () . get ( value_type , {})","title":"ImportDataOperationType"},{"location":"reference/kiara/operations/data_import/#kiara.operations.data_import.ImportDataOperationType.get_import_operations_for_target_type","text":"Return all available import operations that produce data of the specified type. Source code in kiara/operations/data_import.py def get_import_operations_for_target_type ( self , value_type : str ) -> typing . Dict [ str , typing . Dict [ str , Operation ]]: \"\"\"Return all available import operations that produce data of the specified type.\"\"\" return self . get_import_operations_per_target_type () . get ( value_type , {})","title":"get_import_operations_for_target_type()"},{"location":"reference/kiara/operations/data_import/#kiara.operations.data_import.ImportDataOperationType.get_import_operations_per_target_type","text":"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. Source code in kiara/operations/data_import.py def get_import_operations_per_target_type ( self , ) -> typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]]: \"\"\"Return all available import operations per value type. The result dictionary uses the source type as first level key, a source name/description as 2nd level key, and the Operation object as value. \"\"\" result : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Operation ]]] = {} for op_config in self . operations . values (): target_type : str = op_config . module_cls . get_target_value_type () # type: ignore source_type = op_config . module_config [ \"source_type\" ] source_profile = op_config . module_config [ \"source_profile\" ] result . setdefault ( target_type , {}) . setdefault ( source_type , {})[ source_profile ] = op_config return result","title":"get_import_operations_per_target_type()"},{"location":"reference/kiara/operations/extract_metadata/","text":"ExtractMetadataModule ( KiaraModule ) \u00b6 Base class to use when writing a module to extract metadata from a file. It's possible to use any arbitrary kiara module for this purpose, but sub-classing this makes it easier. Source code in kiara/operations/extract_metadata.py class ExtractMetadataModule ( KiaraModule ): \"\"\"Base class to use when writing a module to extract metadata from a file. It's possible to use any arbitrary *kiara* module for this purpose, but sub-classing this makes it easier. \"\"\" _config_cls = MetadataModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_types : typing . Iterable = cls . get_supported_value_types () if \"*\" in value_types : value_types = kiara . type_mgmt . value_type_names metadata_key = cls . get_metadata_key () all_value_types = set () for value_type in value_types : if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring metadata-extract operation (metadata key: { metadata_key } ) for type ' { value_type } ': type not available\" ) continue all_value_types . add ( value_type ) sub_types = kiara . type_mgmt . get_sub_types ( value_type ) all_value_types . update ( sub_types ) for value_type in all_value_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type }, \"doc\" : f \"Extract ' { metadata_key } ' metadata for value of type ' { value_type } '.\" , } all_metadata_profiles [ f \"extract. { metadata_key } .metadata.from. { value_type } \" ] = op_config return all_metadata_profiles @classmethod @abc . abstractmethod def _get_supported_types ( self ) -> typing . Union [ str , typing . Iterable [ str ]]: pass @classmethod def get_metadata_key ( cls ) -> str : return cls . _module_type_name # type: ignore @classmethod def get_supported_value_types ( cls ) -> typing . Set [ str ]: _types = cls . _get_supported_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) def __init__ ( self , * args , ** kwargs ): self . _metadata_schema : typing . Optional [ str ] = None super () . __init__ ( * args , ** kwargs ) @property def value_type ( self ) -> str : data_type = self . get_config_value ( \"value_type\" ) sup_types = self . get_supported_value_types () if \"*\" not in sup_types and data_type not in sup_types : match = False for sup_type in sup_types : for sub_type in self . _kiara . type_mgmt . get_sub_types ( sup_type ): if sub_type == data_type : match = True break if not match : raise ValueError ( f \"Invalid module configuration, type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_value_types ()) } .\" ) return data_type @property def metadata_schema ( self ) -> str : if self . _metadata_schema is not None : return self . _metadata_schema schema = self . _get_metadata_schema ( type = self . value_type ) if isinstance ( schema , type ) and issubclass ( schema , BaseModel ): schema = schema . schema_json () elif not isinstance ( schema , str ): raise TypeError ( f \"Invalid type for metadata schema: { type ( schema ) } \" ) self . _metadata_schema = schema return self . _metadata_schema @abc . abstractmethod def _get_metadata_schema ( self , type : str ) -> typing . Union [ str , typing . Type [ BaseModel ]]: \"\"\"Create the metadata schema for the configured type.\"\"\" @abc . abstractmethod def extract_metadata ( self , value : Value ) -> typing . Union [ typing . Mapping [ str , typing . Any ], BaseModel ]: pass def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : { \"type\" : self . value_type , \"doc\" : f \"A value of type ' { self . value_type } '\" , \"optional\" : False , } } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"metadata_item\" : { \"type\" : \"dict\" , \"doc\" : \"The metadata for the provided value.\" , }, \"metadata_item_schema\" : { \"type\" : \"string\" , \"doc\" : \"The (json) schema for the metadata.\" , }, } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" value = inputs . get_value_obj ( input_name ) if self . value_type != \"any\" and value . type_name != self . value_type : raise KiaraProcessingException ( f \"Can't extract metadata for value of type ' { value . value_schema . type } '. Expected type ' { self . value_type } '.\" ) # TODO: if type 'any', validate that the data is actually of the right type? outputs . set_value ( \"metadata_item_schema\" , self . metadata_schema ) metadata = self . extract_metadata ( value ) if isinstance ( metadata , BaseModel ): metadata = metadata . dict ( exclude_none = True ) # TODO: validate metadata? outputs . set_value ( \"metadata_item\" , metadata ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/extract_metadata.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : { \"type\" : self . value_type , \"doc\" : f \"A value of type ' { self . value_type } '\" , \"optional\" : False , } } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/extract_metadata.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"metadata_item\" : { \"type\" : \"dict\" , \"doc\" : \"The metadata for the provided value.\" , }, \"metadata_item_schema\" : { \"type\" : \"string\" , \"doc\" : \"The (json) schema for the metadata.\" , }, } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/extract_metadata.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_types : typing . Iterable = cls . get_supported_value_types () if \"*\" in value_types : value_types = kiara . type_mgmt . value_type_names metadata_key = cls . get_metadata_key () all_value_types = set () for value_type in value_types : if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring metadata-extract operation (metadata key: { metadata_key } ) for type ' { value_type } ': type not available\" ) continue all_value_types . add ( value_type ) sub_types = kiara . type_mgmt . get_sub_types ( value_type ) all_value_types . update ( sub_types ) for value_type in all_value_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type }, \"doc\" : f \"Extract ' { metadata_key } ' metadata for value of type ' { value_type } '.\" , } all_metadata_profiles [ f \"extract. { metadata_key } .metadata.from. { value_type } \" ] = op_config return all_metadata_profiles ExtractMetadataOperationType ( OperationType ) \u00b6 Extract metadata from a dataset. The purpose of this operation type is to be able to extract arbitrary, type-specific metadata from value data. In general, kiara wants to collect (and store along the value) as much metadata related to data as possible, but the extraction process should not take a lot of processing time (since this is done whenever a value is registered into a data registry). As its hard to predict all the types of metadata of a specific type that could be interesting in specific scenarios, kiara supports a pluggable mechanism to add new metadata extraction processes by extending the base class ExtractMetadataModule and adding that implementation somewhere kiara can find it. Once that is done, kiara will automatically add a new operation with an id that follows this template: <VALUE_TYPE>.extract_metadata.<METADATA_KEY> , where METADATA_KEY is a name under which the metadata will be stored within the value object. By default, every value type should have at least one metadata extraction module where the METADATA_KEY is the same as the value type name, and which contains basic, type-specific metadata (e.g. for a 'table', that would be number of rows, column names, column types, etc.). Source code in kiara/operations/extract_metadata.py class ExtractMetadataOperationType ( OperationType ): \"\"\"Extract metadata from a dataset. The purpose of this operation type is to be able to extract arbitrary, type-specific metadata from value data. In general, *kiara* wants to collect (and store along the value) as much metadata related to data as possible, but the extraction process should not take a lot of processing time (since this is done whenever a value is registered into a data registry). As its hard to predict all the types of metadata of a specific type that could be interesting in specific scenarios, *kiara* supports a pluggable mechanism to add new metadata extraction processes by extending the base class [`ExtractMetadataModule`](http://dharpa.org/kiara/latest/api_reference/kiara.operations.extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataModule) and adding that implementation somewhere *kiara* can find it. Once that is done, *kiara* will automatically add a new operation with an id that follows this template: `<VALUE_TYPE>.extract_metadata.<METADATA_KEY>`, where `METADATA_KEY` is a name under which the metadata will be stored within the value object. By default, every value type should have at least one metadata extraction module where the `METADATA_KEY` is the same as the value type name, and which contains basic, type-specific metadata (e.g. for a 'table', that would be number of rows, column names, column types, etc.). \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , ExtractMetadataModule ) def get_all_operations_for_type ( self , value_type : str ) -> typing . Mapping [ str , Operation ]: result = {} for op_config in self . operations . values (): v_t = op_config . module_config [ \"value_type\" ] if v_t != value_type : continue module_cls : ExtractMetadataModule = op_config . module_cls # type: ignore md_key = module_cls . get_metadata_key () result [ md_key ] = op_config return result MetadataModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/extract_metadata.py class MetadataModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The data type this module will be used for.\" ) value_type : str pydantic-field required \u00b6 The data type this module will be used for.","title":"extract_metadata"},{"location":"reference/kiara/operations/extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataModule","text":"Base class to use when writing a module to extract metadata from a file. It's possible to use any arbitrary kiara module for this purpose, but sub-classing this makes it easier. Source code in kiara/operations/extract_metadata.py class ExtractMetadataModule ( KiaraModule ): \"\"\"Base class to use when writing a module to extract metadata from a file. It's possible to use any arbitrary *kiara* module for this purpose, but sub-classing this makes it easier. \"\"\" _config_cls = MetadataModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_types : typing . Iterable = cls . get_supported_value_types () if \"*\" in value_types : value_types = kiara . type_mgmt . value_type_names metadata_key = cls . get_metadata_key () all_value_types = set () for value_type in value_types : if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring metadata-extract operation (metadata key: { metadata_key } ) for type ' { value_type } ': type not available\" ) continue all_value_types . add ( value_type ) sub_types = kiara . type_mgmt . get_sub_types ( value_type ) all_value_types . update ( sub_types ) for value_type in all_value_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type }, \"doc\" : f \"Extract ' { metadata_key } ' metadata for value of type ' { value_type } '.\" , } all_metadata_profiles [ f \"extract. { metadata_key } .metadata.from. { value_type } \" ] = op_config return all_metadata_profiles @classmethod @abc . abstractmethod def _get_supported_types ( self ) -> typing . Union [ str , typing . Iterable [ str ]]: pass @classmethod def get_metadata_key ( cls ) -> str : return cls . _module_type_name # type: ignore @classmethod def get_supported_value_types ( cls ) -> typing . Set [ str ]: _types = cls . _get_supported_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) def __init__ ( self , * args , ** kwargs ): self . _metadata_schema : typing . Optional [ str ] = None super () . __init__ ( * args , ** kwargs ) @property def value_type ( self ) -> str : data_type = self . get_config_value ( \"value_type\" ) sup_types = self . get_supported_value_types () if \"*\" not in sup_types and data_type not in sup_types : match = False for sup_type in sup_types : for sub_type in self . _kiara . type_mgmt . get_sub_types ( sup_type ): if sub_type == data_type : match = True break if not match : raise ValueError ( f \"Invalid module configuration, type ' { data_type } ' not supported. Supported types: { ', ' . join ( self . get_supported_value_types ()) } .\" ) return data_type @property def metadata_schema ( self ) -> str : if self . _metadata_schema is not None : return self . _metadata_schema schema = self . _get_metadata_schema ( type = self . value_type ) if isinstance ( schema , type ) and issubclass ( schema , BaseModel ): schema = schema . schema_json () elif not isinstance ( schema , str ): raise TypeError ( f \"Invalid type for metadata schema: { type ( schema ) } \" ) self . _metadata_schema = schema return self . _metadata_schema @abc . abstractmethod def _get_metadata_schema ( self , type : str ) -> typing . Union [ str , typing . Type [ BaseModel ]]: \"\"\"Create the metadata schema for the configured type.\"\"\" @abc . abstractmethod def extract_metadata ( self , value : Value ) -> typing . Union [ typing . Mapping [ str , typing . Any ], BaseModel ]: pass def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : { \"type\" : self . value_type , \"doc\" : f \"A value of type ' { self . value_type } '\" , \"optional\" : False , } } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"metadata_item\" : { \"type\" : \"dict\" , \"doc\" : \"The metadata for the provided value.\" , }, \"metadata_item_schema\" : { \"type\" : \"string\" , \"doc\" : \"The (json) schema for the metadata.\" , }, } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" value = inputs . get_value_obj ( input_name ) if self . value_type != \"any\" and value . type_name != self . value_type : raise KiaraProcessingException ( f \"Can't extract metadata for value of type ' { value . value_schema . type } '. Expected type ' { self . value_type } '.\" ) # TODO: if type 'any', validate that the data is actually of the right type? outputs . set_value ( \"metadata_item_schema\" , self . metadata_schema ) metadata = self . extract_metadata ( value ) if isinstance ( metadata , BaseModel ): metadata = metadata . dict ( exclude_none = True ) # TODO: validate metadata? outputs . set_value ( \"metadata_item\" , metadata )","title":"ExtractMetadataModule"},{"location":"reference/kiara/operations/extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/extract_metadata.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . value_type if input_name == \"any\" : input_name = \"value_item\" inputs = { input_name : { \"type\" : self . value_type , \"doc\" : f \"A value of type ' { self . value_type } '\" , \"optional\" : False , } } return inputs","title":"create_input_schema()"},{"location":"reference/kiara/operations/extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/extract_metadata.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs = { \"metadata_item\" : { \"type\" : \"dict\" , \"doc\" : \"The metadata for the provided value.\" , }, \"metadata_item_schema\" : { \"type\" : \"string\" , \"doc\" : \"The (json) schema for the metadata.\" , }, } return outputs","title":"create_output_schema()"},{"location":"reference/kiara/operations/extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/extract_metadata.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_types : typing . Iterable = cls . get_supported_value_types () if \"*\" in value_types : value_types = kiara . type_mgmt . value_type_names metadata_key = cls . get_metadata_key () all_value_types = set () for value_type in value_types : if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring metadata-extract operation (metadata key: { metadata_key } ) for type ' { value_type } ': type not available\" ) continue all_value_types . add ( value_type ) sub_types = kiara . type_mgmt . get_sub_types ( value_type ) all_value_types . update ( sub_types ) for value_type in all_value_types : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type }, \"doc\" : f \"Extract ' { metadata_key } ' metadata for value of type ' { value_type } '.\" , } all_metadata_profiles [ f \"extract. { metadata_key } .metadata.from. { value_type } \" ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"reference/kiara/operations/extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataOperationType","text":"Extract metadata from a dataset. The purpose of this operation type is to be able to extract arbitrary, type-specific metadata from value data. In general, kiara wants to collect (and store along the value) as much metadata related to data as possible, but the extraction process should not take a lot of processing time (since this is done whenever a value is registered into a data registry). As its hard to predict all the types of metadata of a specific type that could be interesting in specific scenarios, kiara supports a pluggable mechanism to add new metadata extraction processes by extending the base class ExtractMetadataModule and adding that implementation somewhere kiara can find it. Once that is done, kiara will automatically add a new operation with an id that follows this template: <VALUE_TYPE>.extract_metadata.<METADATA_KEY> , where METADATA_KEY is a name under which the metadata will be stored within the value object. By default, every value type should have at least one metadata extraction module where the METADATA_KEY is the same as the value type name, and which contains basic, type-specific metadata (e.g. for a 'table', that would be number of rows, column names, column types, etc.). Source code in kiara/operations/extract_metadata.py class ExtractMetadataOperationType ( OperationType ): \"\"\"Extract metadata from a dataset. The purpose of this operation type is to be able to extract arbitrary, type-specific metadata from value data. In general, *kiara* wants to collect (and store along the value) as much metadata related to data as possible, but the extraction process should not take a lot of processing time (since this is done whenever a value is registered into a data registry). As its hard to predict all the types of metadata of a specific type that could be interesting in specific scenarios, *kiara* supports a pluggable mechanism to add new metadata extraction processes by extending the base class [`ExtractMetadataModule`](http://dharpa.org/kiara/latest/api_reference/kiara.operations.extract_metadata/#kiara.operations.extract_metadata.ExtractMetadataModule) and adding that implementation somewhere *kiara* can find it. Once that is done, *kiara* will automatically add a new operation with an id that follows this template: `<VALUE_TYPE>.extract_metadata.<METADATA_KEY>`, where `METADATA_KEY` is a name under which the metadata will be stored within the value object. By default, every value type should have at least one metadata extraction module where the `METADATA_KEY` is the same as the value type name, and which contains basic, type-specific metadata (e.g. for a 'table', that would be number of rows, column names, column types, etc.). \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , ExtractMetadataModule ) def get_all_operations_for_type ( self , value_type : str ) -> typing . Mapping [ str , Operation ]: result = {} for op_config in self . operations . values (): v_t = op_config . module_config [ \"value_type\" ] if v_t != value_type : continue module_cls : ExtractMetadataModule = op_config . module_cls # type: ignore md_key = module_cls . get_metadata_key () result [ md_key ] = op_config return result","title":"ExtractMetadataOperationType"},{"location":"reference/kiara/operations/extract_metadata/#kiara.operations.extract_metadata.MetadataModuleConfig","text":"Source code in kiara/operations/extract_metadata.py class MetadataModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The data type this module will be used for.\" )","title":"MetadataModuleConfig"},{"location":"reference/kiara/operations/extract_metadata/#kiara.operations.extract_metadata.MetadataModuleConfig.value_type","text":"The data type this module will be used for.","title":"value_type"},{"location":"reference/kiara/operations/merge_values/","text":"ValueMergeModule ( KiaraModule ) \u00b6 Base class for operations that merge several values into one. NOT USED YET. Source code in kiara/operations/merge_values.py class ValueMergeModule ( KiaraModule ): \"\"\"Base class for operations that merge several values into one. NOT USED YET. \"\"\" def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_schema_dicts : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . get_config_value ( \"input_schemas\" ) if not input_schema_dicts : raise Exception ( \"No input schemas provided.\" ) input_schemas : typing . Dict [ str , ValueSchema ] = {} for k , v in input_schema_dicts . items (): input_schemas [ k ] = ValueSchema ( ** v ) return input_schemas def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"merged_value\" : { \"type\" : self . get_config_value ( \"output_type\" )}} create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/merge_values.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_schema_dicts : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . get_config_value ( \"input_schemas\" ) if not input_schema_dicts : raise Exception ( \"No input schemas provided.\" ) input_schemas : typing . Dict [ str , ValueSchema ] = {} for k , v in input_schema_dicts . items (): input_schemas [ k ] = ValueSchema ( ** v ) return input_schemas create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/merge_values.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"merged_value\" : { \"type\" : self . get_config_value ( \"output_type\" )}} ValueMergeModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/merge_values.py class ValueMergeModuleConfig ( ModuleTypeConfigSchema ): input_schemas : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = Field ( description = \"The schemas for all of the expected inputs.\" ) output_type : str = Field ( description = \"The result type of the merged value.\" ) input_schemas : Dict [ str , Mapping [ str , Any ]] pydantic-field required \u00b6 The schemas for all of the expected inputs. output_type : str pydantic-field required \u00b6 The result type of the merged value.","title":"merge_values"},{"location":"reference/kiara/operations/merge_values/#kiara.operations.merge_values.ValueMergeModule","text":"Base class for operations that merge several values into one. NOT USED YET. Source code in kiara/operations/merge_values.py class ValueMergeModule ( KiaraModule ): \"\"\"Base class for operations that merge several values into one. NOT USED YET. \"\"\" def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_schema_dicts : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . get_config_value ( \"input_schemas\" ) if not input_schema_dicts : raise Exception ( \"No input schemas provided.\" ) input_schemas : typing . Dict [ str , ValueSchema ] = {} for k , v in input_schema_dicts . items (): input_schemas [ k ] = ValueSchema ( ** v ) return input_schemas def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"merged_value\" : { \"type\" : self . get_config_value ( \"output_type\" )}}","title":"ValueMergeModule"},{"location":"reference/kiara/operations/merge_values/#kiara.operations.merge_values.ValueMergeModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/merge_values.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_schema_dicts : typing . Mapping [ str , typing . Mapping [ str , typing . Any ] ] = self . get_config_value ( \"input_schemas\" ) if not input_schema_dicts : raise Exception ( \"No input schemas provided.\" ) input_schemas : typing . Dict [ str , ValueSchema ] = {} for k , v in input_schema_dicts . items (): input_schemas [ k ] = ValueSchema ( ** v ) return input_schemas","title":"create_input_schema()"},{"location":"reference/kiara/operations/merge_values/#kiara.operations.merge_values.ValueMergeModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/merge_values.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"merged_value\" : { \"type\" : self . get_config_value ( \"output_type\" )}}","title":"create_output_schema()"},{"location":"reference/kiara/operations/merge_values/#kiara.operations.merge_values.ValueMergeModuleConfig","text":"Source code in kiara/operations/merge_values.py class ValueMergeModuleConfig ( ModuleTypeConfigSchema ): input_schemas : typing . Dict [ str , typing . Mapping [ str , typing . Any ]] = Field ( description = \"The schemas for all of the expected inputs.\" ) output_type : str = Field ( description = \"The result type of the merged value.\" )","title":"ValueMergeModuleConfig"},{"location":"reference/kiara/operations/merge_values/#kiara.operations.merge_values.ValueMergeModuleConfig.input_schemas","text":"The schemas for all of the expected inputs.","title":"input_schemas"},{"location":"reference/kiara/operations/merge_values/#kiara.operations.merge_values.ValueMergeModuleConfig.output_type","text":"The result type of the merged value.","title":"output_type"},{"location":"reference/kiara/operations/pretty_print/","text":"PrettyPrintModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/pretty_print.py class PrettyPrintModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to save.\" ) target_type : str = Field ( description = \"The target to print the value to.\" , default = \"string\" ) target_type : str pydantic-field \u00b6 The target to print the value to. value_type : str pydantic-field required \u00b6 The type of the value to save. PrettyPrintOperationType ( OperationType ) \u00b6 This operation type renders values of any type into human readable output for different targets (e.g. terminal, html, ...). The main purpose of this operation type is to show the user as much content of the data as possible for the specific rendering target, without giving any guarantees that all information contained in the data is shown. It may print out the whole content (if the content is small, or the available space large enough), or just a few bits and pieces from the start and/or end of the data, whatever makes most sense for the data itself. For example, for large tables it might be a good idea to print out the first and last 30 rows, so users can see which columns are available, and get a rough overview of the content itself. For network graphs it might make sense to print out some graph properties (number of nodes, edges, available attributes, ...) on the terminal, but render the graph itself when using html as target. Currently, it is only possible to implement a pretty_print renderer if you have access to the source code of the value type you want to render. To add a new render method, add a new instance method to the ValueType class in question, in the format: def pretty_print_as_<TARGET_TYPE>(value: Value, print_config: typing.Mapping[str, typing.Any]) -> typing.Any: ... ... kiara will look at all available ValueType classes for methods that match this signature, and auto-generate operations following this naming template: <SOURCE_TYPE>.pretty_print_as.<TARGET_TYPE> . Currently, only the type renderables is implemented as target type (for printing out to the terminal). It is planned to add output suitable for use in Jupyter notebooks in the near future. Source code in kiara/operations/pretty_print.py class PrettyPrintOperationType ( OperationType ): \"\"\"This operation type renders values of any type into human readable output for different targets (e.g. terminal, html, ...). The main purpose of this operation type is to show the user as much content of the data as possible for the specific rendering target, without giving any guarantees that all information contained in the data is shown. It may print out the whole content (if the content is small, or the available space large enough), or just a few bits and pieces from the start and/or end of the data, whatever makes most sense for the data itself. For example, for large tables it might be a good idea to print out the first and last 30 rows, so users can see which columns are available, and get a rough overview of the content itself. For network graphs it might make sense to print out some graph properties (number of nodes, edges, available attributes, ...) on the terminal, but render the graph itself when using html as target. Currently, it is only possible to implement a `pretty_print` renderer if you have access to the source code of the value type you want to render. To add a new render method, add a new instance method to the `ValueType` class in question, in the format: ``` def pretty_print_as_<TARGET_TYPE>(value: Value, print_config: typing.Mapping[str, typing.Any]) -> typing.Any: ... ... ``` *kiara* will look at all available `ValueType` classes for methods that match this signature, and auto-generate operations following this naming template: `<SOURCE_TYPE>.pretty_print_as.<TARGET_TYPE>`. Currently, only the type `renderables` is implemented as target type (for printing out to the terminal). It is planned to add output suitable for use in Jupyter notebooks in the near future. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , PrettyPrintValueModule ) def get_pretty_print_operation ( self , value_type : str , target_type : str ) -> Operation : result = [] for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] != value_type : continue if op_config . module_config [ \"target_type\" ] != target_type : continue result . append ( op_config ) if not result : raise Exception ( f \"No pretty print operation for value type ' { value_type } ' and output ' { target_type } ' registered.\" ) elif len ( result ) != 1 : raise Exception ( f \"Multiple pretty print operations for value type ' { value_type } ' and output ' { target_type } ' registered.\" ) return result [ 0 ] def pretty_print ( self , value : Value , target_type : str , print_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ) -> typing . Any : ops_config = self . get_pretty_print_operation ( value_type = value . type_name , target_type = target_type ) inputs : typing . Mapping [ str , typing . Any ] = { value . type_name : value , \"print_config\" : print_config , } result = ops_config . module . run ( ** inputs ) printed = result . get_value_data ( \"printed\" ) return printed PrettyPrintValueModule ( KiaraModule ) \u00b6 Source code in kiara/operations/pretty_print.py class PrettyPrintValueModule ( KiaraModule ): _config_cls = PrettyPrintModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type_cls in kiara . type_mgmt . value_types . items (): for attr in dir ( value_type_cls ): if not attr . startswith ( \"pretty_print_as_\" ): continue target_type = attr [ 16 :] if target_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Pretty print target type ' { target_type } ' for source type ' { value_type_name } ' not valid, ignoring.\" ) continue op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"target_type\" : target_type , }, \"doc\" : f \"Pretty print a value of type ' { value_type_name } ' as ' { target_type } '.\" , } all_metadata_profiles [ f \"pretty_print. { value_type_name } .as. { target_type } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type = self . get_config_value ( \"value_type\" ) if value_type == \"all\" : input_name = \"value_item\" doc = \"A value of any type.\" else : input_name = value_type doc = f \"A value of type ' { value_type } '.\" inputs : typing . Mapping [ str , typing . Any ] = { input_name : { \"type\" : value_type , \"doc\" : doc }, \"print_config\" : { \"type\" : \"dict\" , \"doc\" : \"Optional print configuration.\" , \"optional\" : True , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"printed\" : { \"type\" : self . get_config_value ( \"target_type\" ), \"doc\" : f \"The printed value as ' { self . get_config_value ( 'target_type' ) } '.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_type : str = self . get_config_value ( \"value_type\" ) target_type : str = self . get_config_value ( \"target_type\" ) if value_type == \"all\" : input_name = \"value_item\" else : input_name = value_type value_obj : Value = inputs . get_value_obj ( input_name ) print_config = dict ( DEFAULT_PRETTY_PRINT_CONFIG ) config : typing . Mapping = inputs . get_value_data ( \"print_config\" ) if config : print_config . update ( config ) func_name = f \"pretty_print_as_ { target_type } \" if not hasattr ( value_obj . type_obj , func_name ): raise Exception ( f \"Type ' { value_type } ' can't be pretty printed as ' { target_type } '. This is most likely a bug.\" ) if not value_obj . is_set : printed = \"-- not set --\" else : func = getattr ( value_obj . type_obj , func_name ) # TODO: check signature try : printed = func ( value = value_obj , print_config = print_config ) except Exception as e : if is_debug (): import traceback traceback . print_exc () raise KiaraProcessingException ( str ( e )) outputs . set_value ( \"printed\" , printed ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/pretty_print.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type = self . get_config_value ( \"value_type\" ) if value_type == \"all\" : input_name = \"value_item\" doc = \"A value of any type.\" else : input_name = value_type doc = f \"A value of type ' { value_type } '.\" inputs : typing . Mapping [ str , typing . Any ] = { input_name : { \"type\" : value_type , \"doc\" : doc }, \"print_config\" : { \"type\" : \"dict\" , \"doc\" : \"Optional print configuration.\" , \"optional\" : True , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/pretty_print.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"printed\" : { \"type\" : self . get_config_value ( \"target_type\" ), \"doc\" : f \"The printed value as ' { self . get_config_value ( 'target_type' ) } '.\" , } } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/pretty_print.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type_cls in kiara . type_mgmt . value_types . items (): for attr in dir ( value_type_cls ): if not attr . startswith ( \"pretty_print_as_\" ): continue target_type = attr [ 16 :] if target_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Pretty print target type ' { target_type } ' for source type ' { value_type_name } ' not valid, ignoring.\" ) continue op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"target_type\" : target_type , }, \"doc\" : f \"Pretty print a value of type ' { value_type_name } ' as ' { target_type } '.\" , } all_metadata_profiles [ f \"pretty_print. { value_type_name } .as. { target_type } \" ] = op_config return all_metadata_profiles","title":"pretty_print"},{"location":"reference/kiara/operations/pretty_print/#kiara.operations.pretty_print.PrettyPrintModuleConfig","text":"Source code in kiara/operations/pretty_print.py class PrettyPrintModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to save.\" ) target_type : str = Field ( description = \"The target to print the value to.\" , default = \"string\" )","title":"PrettyPrintModuleConfig"},{"location":"reference/kiara/operations/pretty_print/#kiara.operations.pretty_print.PrettyPrintModuleConfig.target_type","text":"The target to print the value to.","title":"target_type"},{"location":"reference/kiara/operations/pretty_print/#kiara.operations.pretty_print.PrettyPrintModuleConfig.value_type","text":"The type of the value to save.","title":"value_type"},{"location":"reference/kiara/operations/pretty_print/#kiara.operations.pretty_print.PrettyPrintOperationType","text":"This operation type renders values of any type into human readable output for different targets (e.g. terminal, html, ...). The main purpose of this operation type is to show the user as much content of the data as possible for the specific rendering target, without giving any guarantees that all information contained in the data is shown. It may print out the whole content (if the content is small, or the available space large enough), or just a few bits and pieces from the start and/or end of the data, whatever makes most sense for the data itself. For example, for large tables it might be a good idea to print out the first and last 30 rows, so users can see which columns are available, and get a rough overview of the content itself. For network graphs it might make sense to print out some graph properties (number of nodes, edges, available attributes, ...) on the terminal, but render the graph itself when using html as target. Currently, it is only possible to implement a pretty_print renderer if you have access to the source code of the value type you want to render. To add a new render method, add a new instance method to the ValueType class in question, in the format: def pretty_print_as_<TARGET_TYPE>(value: Value, print_config: typing.Mapping[str, typing.Any]) -> typing.Any: ... ... kiara will look at all available ValueType classes for methods that match this signature, and auto-generate operations following this naming template: <SOURCE_TYPE>.pretty_print_as.<TARGET_TYPE> . Currently, only the type renderables is implemented as target type (for printing out to the terminal). It is planned to add output suitable for use in Jupyter notebooks in the near future. Source code in kiara/operations/pretty_print.py class PrettyPrintOperationType ( OperationType ): \"\"\"This operation type renders values of any type into human readable output for different targets (e.g. terminal, html, ...). The main purpose of this operation type is to show the user as much content of the data as possible for the specific rendering target, without giving any guarantees that all information contained in the data is shown. It may print out the whole content (if the content is small, or the available space large enough), or just a few bits and pieces from the start and/or end of the data, whatever makes most sense for the data itself. For example, for large tables it might be a good idea to print out the first and last 30 rows, so users can see which columns are available, and get a rough overview of the content itself. For network graphs it might make sense to print out some graph properties (number of nodes, edges, available attributes, ...) on the terminal, but render the graph itself when using html as target. Currently, it is only possible to implement a `pretty_print` renderer if you have access to the source code of the value type you want to render. To add a new render method, add a new instance method to the `ValueType` class in question, in the format: ``` def pretty_print_as_<TARGET_TYPE>(value: Value, print_config: typing.Mapping[str, typing.Any]) -> typing.Any: ... ... ``` *kiara* will look at all available `ValueType` classes for methods that match this signature, and auto-generate operations following this naming template: `<SOURCE_TYPE>.pretty_print_as.<TARGET_TYPE>`. Currently, only the type `renderables` is implemented as target type (for printing out to the terminal). It is planned to add output suitable for use in Jupyter notebooks in the near future. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , PrettyPrintValueModule ) def get_pretty_print_operation ( self , value_type : str , target_type : str ) -> Operation : result = [] for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] != value_type : continue if op_config . module_config [ \"target_type\" ] != target_type : continue result . append ( op_config ) if not result : raise Exception ( f \"No pretty print operation for value type ' { value_type } ' and output ' { target_type } ' registered.\" ) elif len ( result ) != 1 : raise Exception ( f \"Multiple pretty print operations for value type ' { value_type } ' and output ' { target_type } ' registered.\" ) return result [ 0 ] def pretty_print ( self , value : Value , target_type : str , print_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ) -> typing . Any : ops_config = self . get_pretty_print_operation ( value_type = value . type_name , target_type = target_type ) inputs : typing . Mapping [ str , typing . Any ] = { value . type_name : value , \"print_config\" : print_config , } result = ops_config . module . run ( ** inputs ) printed = result . get_value_data ( \"printed\" ) return printed","title":"PrettyPrintOperationType"},{"location":"reference/kiara/operations/pretty_print/#kiara.operations.pretty_print.PrettyPrintValueModule","text":"Source code in kiara/operations/pretty_print.py class PrettyPrintValueModule ( KiaraModule ): _config_cls = PrettyPrintModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type_cls in kiara . type_mgmt . value_types . items (): for attr in dir ( value_type_cls ): if not attr . startswith ( \"pretty_print_as_\" ): continue target_type = attr [ 16 :] if target_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Pretty print target type ' { target_type } ' for source type ' { value_type_name } ' not valid, ignoring.\" ) continue op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"target_type\" : target_type , }, \"doc\" : f \"Pretty print a value of type ' { value_type_name } ' as ' { target_type } '.\" , } all_metadata_profiles [ f \"pretty_print. { value_type_name } .as. { target_type } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type = self . get_config_value ( \"value_type\" ) if value_type == \"all\" : input_name = \"value_item\" doc = \"A value of any type.\" else : input_name = value_type doc = f \"A value of type ' { value_type } '.\" inputs : typing . Mapping [ str , typing . Any ] = { input_name : { \"type\" : value_type , \"doc\" : doc }, \"print_config\" : { \"type\" : \"dict\" , \"doc\" : \"Optional print configuration.\" , \"optional\" : True , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"printed\" : { \"type\" : self . get_config_value ( \"target_type\" ), \"doc\" : f \"The printed value as ' { self . get_config_value ( 'target_type' ) } '.\" , } } return outputs def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_type : str = self . get_config_value ( \"value_type\" ) target_type : str = self . get_config_value ( \"target_type\" ) if value_type == \"all\" : input_name = \"value_item\" else : input_name = value_type value_obj : Value = inputs . get_value_obj ( input_name ) print_config = dict ( DEFAULT_PRETTY_PRINT_CONFIG ) config : typing . Mapping = inputs . get_value_data ( \"print_config\" ) if config : print_config . update ( config ) func_name = f \"pretty_print_as_ { target_type } \" if not hasattr ( value_obj . type_obj , func_name ): raise Exception ( f \"Type ' { value_type } ' can't be pretty printed as ' { target_type } '. This is most likely a bug.\" ) if not value_obj . is_set : printed = \"-- not set --\" else : func = getattr ( value_obj . type_obj , func_name ) # TODO: check signature try : printed = func ( value = value_obj , print_config = print_config ) except Exception as e : if is_debug (): import traceback traceback . print_exc () raise KiaraProcessingException ( str ( e )) outputs . set_value ( \"printed\" , printed )","title":"PrettyPrintValueModule"},{"location":"reference/kiara/operations/pretty_print/#kiara.operations.pretty_print.PrettyPrintValueModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/pretty_print.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type = self . get_config_value ( \"value_type\" ) if value_type == \"all\" : input_name = \"value_item\" doc = \"A value of any type.\" else : input_name = value_type doc = f \"A value of type ' { value_type } '.\" inputs : typing . Mapping [ str , typing . Any ] = { input_name : { \"type\" : value_type , \"doc\" : doc }, \"print_config\" : { \"type\" : \"dict\" , \"doc\" : \"Optional print configuration.\" , \"optional\" : True , }, } return inputs","title":"create_input_schema()"},{"location":"reference/kiara/operations/pretty_print/#kiara.operations.pretty_print.PrettyPrintValueModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/pretty_print.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: outputs : typing . Mapping [ str , typing . Any ] = { \"printed\" : { \"type\" : self . get_config_value ( \"target_type\" ), \"doc\" : f \"The printed value as ' { self . get_config_value ( 'target_type' ) } '.\" , } } return outputs","title":"create_output_schema()"},{"location":"reference/kiara/operations/pretty_print/#kiara.operations.pretty_print.PrettyPrintValueModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/pretty_print.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} for value_type_name , value_type_cls in kiara . type_mgmt . value_types . items (): for attr in dir ( value_type_cls ): if not attr . startswith ( \"pretty_print_as_\" ): continue target_type = attr [ 16 :] if target_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Pretty print target type ' { target_type } ' for source type ' { value_type_name } ' not valid, ignoring.\" ) continue op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : value_type_name , \"target_type\" : target_type , }, \"doc\" : f \"Pretty print a value of type ' { value_type_name } ' as ' { target_type } '.\" , } all_metadata_profiles [ f \"pretty_print. { value_type_name } .as. { target_type } \" ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"reference/kiara/operations/sample/","text":"SampleValueModule ( KiaraModule ) \u00b6 Base class for operations that take samples of data. Source code in kiara/operations/sample.py class SampleValueModule ( KiaraModule ): \"\"\"Base class for operations that take samples of data.\"\"\" _config_cls = SampleValueModuleConfig @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : \"\"\"Return the value type for this sample module.\"\"\" @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring sample operation for source type ' { value_type } ': type not available\" ) return {} for sample_type in cls . get_supported_sample_types (): op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"sample_type\" : sample_type }, \"doc\" : f \"Sample value of type ' { value_type } ' using method: { sample_type } .\" , } key = f \"sample. { value_type } . { sample_type } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles @classmethod def get_supported_sample_types ( cls ) -> typing . Iterable [ str ]: types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"sample_\" ) and callable ( attr ): sample_type = attr_name [ 7 :] if sample_type in types : raise Exception ( f \"Error in sample module ' { cls . __name__ } ': multiple sample methods for type ' { sample_type } '.\" ) types . append ( sample_type ) return types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_value_type (), \"doc\" : \"The value to sample.\" , }, \"sample_size\" : { \"type\" : \"integer\" , \"doc\" : \"The sample size.\" , \"default\" : 10 , }, } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"sampled_value\" : { \"type\" : self . get_value_type (), \"doc\" : \"The sampled value\" } } def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : sample_size : int = inputs . get_value_data ( \"sample_size\" ) sample_type : str = self . get_config_value ( \"sample_type\" ) if sample_size < 0 : raise KiaraProcessingException ( f \"Invalid sample size ' { sample_size } ': can't be negative.\" ) input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" value : Value = inputs . get_value_obj ( input_name ) func = getattr ( self , f \"sample_ { sample_type } \" ) result = func ( value = value , sample_size = sample_size ) outputs . set_value ( \"sampled_value\" , result ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/sample.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_value_type (), \"doc\" : \"The value to sample.\" , }, \"sample_size\" : { \"type\" : \"integer\" , \"doc\" : \"The sample size.\" , \"default\" : 10 , }, } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/sample.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"sampled_value\" : { \"type\" : self . get_value_type (), \"doc\" : \"The sampled value\" } } get_value_type () classmethod \u00b6 Return the value type for this sample module. Source code in kiara/operations/sample.py @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : \"\"\"Return the value type for this sample module.\"\"\" retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/sample.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring sample operation for source type ' { value_type } ': type not available\" ) return {} for sample_type in cls . get_supported_sample_types (): op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"sample_type\" : sample_type }, \"doc\" : f \"Sample value of type ' { value_type } ' using method: { sample_type } .\" , } key = f \"sample. { value_type } . { sample_type } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles SampleValueModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/sample.py class SampleValueModuleConfig ( ModuleTypeConfigSchema ): sample_type : str = Field ( description = \"The sample method.\" ) sample_type : str pydantic-field required \u00b6 The sample method. SampleValueOperationType ( OperationType ) \u00b6 Operation type for sampling data of different types. This is useful to reduce the size of some datasets in previews and test-runs, while adjusting parameters and the like. Operations of this type can implement very simple or complex ways to take samples of the data they are fed. The most important ones are sampling operations relating to tables and arrays, but it might also make sense to sample texts, image-sets and so on. Operations of this type take the value to be sampled as input, as well as a sample size (integer), which can be, for example, a number of rows, or percentage of the whole dataset, depending on sample type. Modules that implement sampling should inherit from SampleValueModule , and will get auto-registered with operation ids following this template: <VALUE_TYPE>.sample.<SAMPLE_TYPE_NAME> , where SAMPLE_TYPE_NAME is a descriptive name what will be sampled, or how sampling will be done. Source code in kiara/operations/sample.py class SampleValueOperationType ( OperationType ): \"\"\"Operation type for sampling data of different types. This is useful to reduce the size of some datasets in previews and test-runs, while adjusting parameters and the like. Operations of this type can implement very simple or complex ways to take samples of the data they are fed. The most important ones are sampling operations relating to tables and arrays, but it might also make sense to sample texts, image-sets and so on. Operations of this type take the value to be sampled as input, as well as a sample size (integer), which can be, for example, a number of rows, or percentage of the whole dataset, depending on sample type. Modules that implement sampling should inherit from [SampleValueModule](https://dharpa.org/kiara/latest/api_reference/kiara.operations.sample/#kiara.operations.sample.SampleValueModule), and will get auto-registered with operation ids following this template: `<VALUE_TYPE>.sample.<SAMPLE_TYPE_NAME>`, where `SAMPLE_TYPE_NAME` is a descriptive name what will be sampled, or how sampling will be done. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , SampleValueModule ) def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): sample_op_module_cls : typing . Type [ SampleValueModule ] = op . module_cls # type: ignore source_type = sample_op_module_cls . get_value_type () if source_type == value_type : target_type = op . module_config [ \"sample_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to sample ' { source_type } ' using { target_type } \" ) result [ target_type ] = op return result get_operations_for_value_type ( self , value_type ) \u00b6 Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. Source code in kiara/operations/sample.py def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): sample_op_module_cls : typing . Type [ SampleValueModule ] = op . module_cls # type: ignore source_type = sample_op_module_cls . get_value_type () if source_type == value_type : target_type = op . module_config [ \"sample_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to sample ' { source_type } ' using { target_type } \" ) result [ target_type ] = op return result","title":"sample"},{"location":"reference/kiara/operations/sample/#kiara.operations.sample.SampleValueModule","text":"Base class for operations that take samples of data. Source code in kiara/operations/sample.py class SampleValueModule ( KiaraModule ): \"\"\"Base class for operations that take samples of data.\"\"\" _config_cls = SampleValueModuleConfig @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : \"\"\"Return the value type for this sample module.\"\"\" @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring sample operation for source type ' { value_type } ': type not available\" ) return {} for sample_type in cls . get_supported_sample_types (): op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"sample_type\" : sample_type }, \"doc\" : f \"Sample value of type ' { value_type } ' using method: { sample_type } .\" , } key = f \"sample. { value_type } . { sample_type } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles @classmethod def get_supported_sample_types ( cls ) -> typing . Iterable [ str ]: types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"sample_\" ) and callable ( attr ): sample_type = attr_name [ 7 :] if sample_type in types : raise Exception ( f \"Error in sample module ' { cls . __name__ } ': multiple sample methods for type ' { sample_type } '.\" ) types . append ( sample_type ) return types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_value_type (), \"doc\" : \"The value to sample.\" , }, \"sample_size\" : { \"type\" : \"integer\" , \"doc\" : \"The sample size.\" , \"default\" : 10 , }, } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"sampled_value\" : { \"type\" : self . get_value_type (), \"doc\" : \"The sampled value\" } } def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : sample_size : int = inputs . get_value_data ( \"sample_size\" ) sample_type : str = self . get_config_value ( \"sample_type\" ) if sample_size < 0 : raise KiaraProcessingException ( f \"Invalid sample size ' { sample_size } ': can't be negative.\" ) input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" value : Value = inputs . get_value_obj ( input_name ) func = getattr ( self , f \"sample_ { sample_type } \" ) result = func ( value = value , sample_size = sample_size ) outputs . set_value ( \"sampled_value\" , result )","title":"SampleValueModule"},{"location":"reference/kiara/operations/sample/#kiara.operations.sample.SampleValueModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/sample.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: input_name = self . get_value_type () if input_name == \"any\" : input_name = \"value_item\" return { input_name : { \"type\" : self . get_value_type (), \"doc\" : \"The value to sample.\" , }, \"sample_size\" : { \"type\" : \"integer\" , \"doc\" : \"The sample size.\" , \"default\" : 10 , }, }","title":"create_input_schema()"},{"location":"reference/kiara/operations/sample/#kiara.operations.sample.SampleValueModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/sample.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: return { \"sampled_value\" : { \"type\" : self . get_value_type (), \"doc\" : \"The sampled value\" } }","title":"create_output_schema()"},{"location":"reference/kiara/operations/sample/#kiara.operations.sample.SampleValueModule.get_value_type","text":"Return the value type for this sample module. Source code in kiara/operations/sample.py @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : \"\"\"Return the value type for this sample module.\"\"\"","title":"get_value_type()"},{"location":"reference/kiara/operations/sample/#kiara.operations.sample.SampleValueModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/sample.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring sample operation for source type ' { value_type } ': type not available\" ) return {} for sample_type in cls . get_supported_sample_types (): op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"sample_type\" : sample_type }, \"doc\" : f \"Sample value of type ' { value_type } ' using method: { sample_type } .\" , } key = f \"sample. { value_type } . { sample_type } \" if key in all_metadata_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_metadata_profiles [ key ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"reference/kiara/operations/sample/#kiara.operations.sample.SampleValueModuleConfig","text":"Source code in kiara/operations/sample.py class SampleValueModuleConfig ( ModuleTypeConfigSchema ): sample_type : str = Field ( description = \"The sample method.\" )","title":"SampleValueModuleConfig"},{"location":"reference/kiara/operations/sample/#kiara.operations.sample.SampleValueModuleConfig.sample_type","text":"The sample method.","title":"sample_type"},{"location":"reference/kiara/operations/sample/#kiara.operations.sample.SampleValueOperationType","text":"Operation type for sampling data of different types. This is useful to reduce the size of some datasets in previews and test-runs, while adjusting parameters and the like. Operations of this type can implement very simple or complex ways to take samples of the data they are fed. The most important ones are sampling operations relating to tables and arrays, but it might also make sense to sample texts, image-sets and so on. Operations of this type take the value to be sampled as input, as well as a sample size (integer), which can be, for example, a number of rows, or percentage of the whole dataset, depending on sample type. Modules that implement sampling should inherit from SampleValueModule , and will get auto-registered with operation ids following this template: <VALUE_TYPE>.sample.<SAMPLE_TYPE_NAME> , where SAMPLE_TYPE_NAME is a descriptive name what will be sampled, or how sampling will be done. Source code in kiara/operations/sample.py class SampleValueOperationType ( OperationType ): \"\"\"Operation type for sampling data of different types. This is useful to reduce the size of some datasets in previews and test-runs, while adjusting parameters and the like. Operations of this type can implement very simple or complex ways to take samples of the data they are fed. The most important ones are sampling operations relating to tables and arrays, but it might also make sense to sample texts, image-sets and so on. Operations of this type take the value to be sampled as input, as well as a sample size (integer), which can be, for example, a number of rows, or percentage of the whole dataset, depending on sample type. Modules that implement sampling should inherit from [SampleValueModule](https://dharpa.org/kiara/latest/api_reference/kiara.operations.sample/#kiara.operations.sample.SampleValueModule), and will get auto-registered with operation ids following this template: `<VALUE_TYPE>.sample.<SAMPLE_TYPE_NAME>`, where `SAMPLE_TYPE_NAME` is a descriptive name what will be sampled, or how sampling will be done. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , SampleValueModule ) def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): sample_op_module_cls : typing . Type [ SampleValueModule ] = op . module_cls # type: ignore source_type = sample_op_module_cls . get_value_type () if source_type == value_type : target_type = op . module_config [ \"sample_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to sample ' { source_type } ' using { target_type } \" ) result [ target_type ] = op return result","title":"SampleValueOperationType"},{"location":"reference/kiara/operations/sample/#kiara.operations.sample.SampleValueOperationType.get_operations_for_value_type","text":"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. Source code in kiara/operations/sample.py def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): sample_op_module_cls : typing . Type [ SampleValueModule ] = op . module_cls # type: ignore source_type = sample_op_module_cls . get_value_type () if source_type == value_type : target_type = op . module_config [ \"sample_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to sample ' { source_type } ' using { target_type } \" ) result [ target_type ] = op return result","title":"get_operations_for_value_type()"},{"location":"reference/kiara/operations/serialize/","text":"SerializeValueModule ( KiaraModule ) \u00b6 Base class for 'serialize' operations. Source code in kiara/operations/serialize.py class SerializeValueModule ( KiaraModule ): \"\"\"Base class for 'serialize' operations.\"\"\" _config_cls = SerializeValueModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring serialization operation for source type ' { value_type } ': type not available\" ) return {} for serialization_type in cls . get_supported_serialization_types (): mod_conf = { \"value_type\" : value_type , \"serialization_type\" : serialization_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Serialize value of type ' { value_type } ' to ' { serialization_type } '.\" , } key = f \"serialize. { value_type } .as. { serialization_type } \" if key in all_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_profiles [ key ] = op_config return all_profiles @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : pass @classmethod def get_supported_serialization_types ( cls ) -> typing . Iterable [ str ]: serialize_types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"to_\" ) and callable ( attr ): serialize_types . append ( attr_name [ 3 :]) return serialize_types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) # serialization_type: str = self.get_config_value(\"serialization_type\") return { \"value_item\" : { \"type\" : value_type , \"doc\" : f \"The ' { value_type } ' value to be serialized.\" , } } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) return { \"value_info\" : { \"type\" : \"value_info\" , \"doc\" : \"Information about the (original) serialized value (can be used to re-constitute the value, incl. its original id).\" , }, \"deserialize_config\" : { \"type\" : \"deserialize_config\" , \"doc\" : f \"The config to use to deserialize the value of type ' { value_type } '.\" , }, } def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_type : str = self . get_config_value ( \"value_type\" ) value_obj = inputs . get_value_obj ( \"value_item\" ) serialization_type = self . get_config_value ( \"serialization_type\" ) if value_type != value_obj . type_name : raise KiaraProcessingException ( f \"Invalid type ( { value_obj . type_name } ) of source value: expected ' { value_type } '.\" ) if not hasattr ( self , f \"to_ { serialization_type } \" ): # this can never happen, I think raise Exception ( f \"Module ' { self . _module_type_id } ' can't serialize ' { value_type } ' to ' { serialization_type } ': missing method 'to_ { serialization_type } '. This is a bug.\" # type: ignore ) func = getattr ( self , f \"to_ { serialization_type } \" ) serialized = func ( value_obj ) if isinstance ( serialized , typing . Mapping ): serialized = DeserializeConfig ( ** serialized ) if not isinstance ( serialized , DeserializeConfig ): raise KiaraProcessingException ( f \"Invalid serialization result type: { type ( serialized ) } \" ) outputs . set_values ( deserialize_config = serialized , value_info = value_obj . get_info () ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/serialize.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) # serialization_type: str = self.get_config_value(\"serialization_type\") return { \"value_item\" : { \"type\" : value_type , \"doc\" : f \"The ' { value_type } ' value to be serialized.\" , } } create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/serialize.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) return { \"value_info\" : { \"type\" : \"value_info\" , \"doc\" : \"Information about the (original) serialized value (can be used to re-constitute the value, incl. its original id).\" , }, \"deserialize_config\" : { \"type\" : \"deserialize_config\" , \"doc\" : f \"The config to use to deserialize the value of type ' { value_type } '.\" , }, } retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/serialize.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring serialization operation for source type ' { value_type } ': type not available\" ) return {} for serialization_type in cls . get_supported_serialization_types (): mod_conf = { \"value_type\" : value_type , \"serialization_type\" : serialization_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Serialize value of type ' { value_type } ' to ' { serialization_type } '.\" , } key = f \"serialize. { value_type } .as. { serialization_type } \" if key in all_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_profiles [ key ] = op_config return all_profiles SerializeValueModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/serialize.py class SerializeValueModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the source value.\" ) serialization_type : str = Field ( description = \"The type of the converted value.\" ) serialization_type : str pydantic-field required \u00b6 The type of the converted value. value_type : str pydantic-field required \u00b6 The type of the source value. SerializeValueOperationType ( OperationType ) \u00b6 Operations that serialize data into formats that can be used for data exchange. NOT USED YET Source code in kiara/operations/serialize.py class SerializeValueOperationType ( OperationType ): \"\"\"Operations that serialize data into formats that can be used for data exchange. NOT USED YET \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , SerializeValueModule ) def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): source_type = op . module_config [ \"value_type\" ] if source_type == value_type : target_type = op . module_config [ \"serialization_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to serialize ' { source_type } ' to { target_type } \" ) result [ target_type ] = op return result get_operations_for_value_type ( self , value_type ) \u00b6 Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. Source code in kiara/operations/serialize.py def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): source_type = op . module_config [ \"value_type\" ] if source_type == value_type : target_type = op . module_config [ \"serialization_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to serialize ' { source_type } ' to { target_type } \" ) result [ target_type ] = op return result","title":"serialize"},{"location":"reference/kiara/operations/serialize/#kiara.operations.serialize.SerializeValueModule","text":"Base class for 'serialize' operations. Source code in kiara/operations/serialize.py class SerializeValueModule ( KiaraModule ): \"\"\"Base class for 'serialize' operations.\"\"\" _config_cls = SerializeValueModuleConfig @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring serialization operation for source type ' { value_type } ': type not available\" ) return {} for serialization_type in cls . get_supported_serialization_types (): mod_conf = { \"value_type\" : value_type , \"serialization_type\" : serialization_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Serialize value of type ' { value_type } ' to ' { serialization_type } '.\" , } key = f \"serialize. { value_type } .as. { serialization_type } \" if key in all_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_profiles [ key ] = op_config return all_profiles @classmethod @abc . abstractmethod def get_value_type ( cls ) -> str : pass @classmethod def get_supported_serialization_types ( cls ) -> typing . Iterable [ str ]: serialize_types = [] for attr_name , attr in cls . __dict__ . items (): if attr_name . startswith ( \"to_\" ) and callable ( attr ): serialize_types . append ( attr_name [ 3 :]) return serialize_types def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) # serialization_type: str = self.get_config_value(\"serialization_type\") return { \"value_item\" : { \"type\" : value_type , \"doc\" : f \"The ' { value_type } ' value to be serialized.\" , } } def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) return { \"value_info\" : { \"type\" : \"value_info\" , \"doc\" : \"Information about the (original) serialized value (can be used to re-constitute the value, incl. its original id).\" , }, \"deserialize_config\" : { \"type\" : \"deserialize_config\" , \"doc\" : f \"The config to use to deserialize the value of type ' { value_type } '.\" , }, } def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_type : str = self . get_config_value ( \"value_type\" ) value_obj = inputs . get_value_obj ( \"value_item\" ) serialization_type = self . get_config_value ( \"serialization_type\" ) if value_type != value_obj . type_name : raise KiaraProcessingException ( f \"Invalid type ( { value_obj . type_name } ) of source value: expected ' { value_type } '.\" ) if not hasattr ( self , f \"to_ { serialization_type } \" ): # this can never happen, I think raise Exception ( f \"Module ' { self . _module_type_id } ' can't serialize ' { value_type } ' to ' { serialization_type } ': missing method 'to_ { serialization_type } '. This is a bug.\" # type: ignore ) func = getattr ( self , f \"to_ { serialization_type } \" ) serialized = func ( value_obj ) if isinstance ( serialized , typing . Mapping ): serialized = DeserializeConfig ( ** serialized ) if not isinstance ( serialized , DeserializeConfig ): raise KiaraProcessingException ( f \"Invalid serialization result type: { type ( serialized ) } \" ) outputs . set_values ( deserialize_config = serialized , value_info = value_obj . get_info () )","title":"SerializeValueModule"},{"location":"reference/kiara/operations/serialize/#kiara.operations.serialize.SerializeValueModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/serialize.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) # serialization_type: str = self.get_config_value(\"serialization_type\") return { \"value_item\" : { \"type\" : value_type , \"doc\" : f \"The ' { value_type } ' value to be serialized.\" , } }","title":"create_input_schema()"},{"location":"reference/kiara/operations/serialize/#kiara.operations.serialize.SerializeValueModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/serialize.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: value_type : str = self . get_config_value ( \"value_type\" ) return { \"value_info\" : { \"type\" : \"value_info\" , \"doc\" : \"Information about the (original) serialized value (can be used to re-constitute the value, incl. its original id).\" , }, \"deserialize_config\" : { \"type\" : \"deserialize_config\" , \"doc\" : f \"The config to use to deserialize the value of type ' { value_type } '.\" , }, }","title":"create_output_schema()"},{"location":"reference/kiara/operations/serialize/#kiara.operations.serialize.SerializeValueModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/serialize.py @classmethod def retrieve_module_profiles ( cls , kiara : \"Kiara\" ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} value_type : str = cls . get_value_type () if value_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring serialization operation for source type ' { value_type } ': type not available\" ) return {} for serialization_type in cls . get_supported_serialization_types (): mod_conf = { \"value_type\" : value_type , \"serialization_type\" : serialization_type , } op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : mod_conf , \"doc\" : f \"Serialize value of type ' { value_type } ' to ' { serialization_type } '.\" , } key = f \"serialize. { value_type } .as. { serialization_type } \" if key in all_profiles . keys (): raise Exception ( f \"Duplicate profile key: { key } \" ) all_profiles [ key ] = op_config return all_profiles","title":"retrieve_module_profiles()"},{"location":"reference/kiara/operations/serialize/#kiara.operations.serialize.SerializeValueModuleConfig","text":"Source code in kiara/operations/serialize.py class SerializeValueModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the source value.\" ) serialization_type : str = Field ( description = \"The type of the converted value.\" )","title":"SerializeValueModuleConfig"},{"location":"reference/kiara/operations/serialize/#kiara.operations.serialize.SerializeValueModuleConfig.serialization_type","text":"The type of the converted value.","title":"serialization_type"},{"location":"reference/kiara/operations/serialize/#kiara.operations.serialize.SerializeValueModuleConfig.value_type","text":"The type of the source value.","title":"value_type"},{"location":"reference/kiara/operations/serialize/#kiara.operations.serialize.SerializeValueOperationType","text":"Operations that serialize data into formats that can be used for data exchange. NOT USED YET Source code in kiara/operations/serialize.py class SerializeValueOperationType ( OperationType ): \"\"\"Operations that serialize data into formats that can be used for data exchange. NOT USED YET \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , SerializeValueModule ) def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): source_type = op . module_config [ \"value_type\" ] if source_type == value_type : target_type = op . module_config [ \"serialization_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to serialize ' { source_type } ' to { target_type } \" ) result [ target_type ] = op return result","title":"SerializeValueOperationType"},{"location":"reference/kiara/operations/serialize/#kiara.operations.serialize.SerializeValueOperationType.get_operations_for_value_type","text":"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. Source code in kiara/operations/serialize.py def get_operations_for_value_type ( self , value_type : str ) -> typing . Dict [ str , Operation ]: \"\"\"Find all operations that serialize the specified type. The result dict uses the serialization type as key, and the operation itself as value. \"\"\" result : typing . Dict [ str , Operation ] = {} for o_id , op in self . operations . items (): source_type = op . module_config [ \"value_type\" ] if source_type == value_type : target_type = op . module_config [ \"serialization_type\" ] if target_type in result . keys (): raise Exception ( f \"Multiple operations to serialize ' { source_type } ' to { target_type } \" ) result [ target_type ] = op return result","title":"get_operations_for_value_type()"},{"location":"reference/kiara/operations/store_value/","text":"StoreOperationType ( OperationType ) \u00b6 Store a value into a local data store. This is a special operation type, that is used internally by the [LocalDataStore](http://dharpa.org/kiara/latest/api_reference/kiara.data.registry.store/#kiara.data.registry.store.LocalDataStore] data registry implementation. For each value type that should be supported by the persistent kiara data store, there must be an implementation of the StoreValueTypeModule class, which handles the actual persisting on disk. In most cases, end users won't need to interact with this type of operation. Source code in kiara/operations/store_value.py class StoreOperationType ( OperationType ): \"\"\"Store a value into a local data store. This is a special operation type, that is used internally by the [LocalDataStore](http://dharpa.org/kiara/latest/api_reference/kiara.data.registry.store/#kiara.data.registry.store.LocalDataStore] data registry implementation. For each value type that should be supported by the persistent *kiara* data store, there must be an implementation of the [StoreValueTypeModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.store_value/#kiara.operations.store_value.StoreValueTypeModule) class, which handles the actual persisting on disk. In most cases, end users won't need to interact with this type of operation. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , StoreValueTypeModule ) def get_store_operation_for_type ( self , value_type : str ) -> Operation : result = [] for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] == value_type : result . append ( op_config ) if not result : raise Exception ( f \"No 'store_value' operation for type ' { value_type } ' registered.\" ) elif len ( result ) != 1 : pass for r in result : print ( r . json ( indent = 2 )) raise Exception ( f \"Multiple 'store_value' operations for type ' { value_type } ' registered.\" ) return result [ 0 ] StoreValueModuleConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 Source code in kiara/operations/store_value.py class StoreValueModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to save.\" ) value_type : str pydantic-field required \u00b6 The type of the value to save. StoreValueTypeModule ( KiaraModule ) \u00b6 Store a specific value type. This is used internally. Source code in kiara/operations/store_value.py class StoreValueTypeModule ( KiaraModule ): \"\"\"Store a specific value type. This is used internally. \"\"\" _config_cls = StoreValueModuleConfig @classmethod def get_supported_value_types ( cls ) -> typing . Set [ str ]: _types = cls . retrieve_supported_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) @classmethod @abc . abstractmethod def retrieve_supported_types ( cls ) -> typing . Union [ str , typing . Iterable [ str ]]: pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} all_types_and_subtypes = set () for sup_type in cls . get_supported_value_types (): if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring save operation for type ' { sup_type } ': type not available\" ) continue all_types_and_subtypes . add ( sup_type ) sub_types = kiara . type_mgmt . get_sub_types ( sup_type ) all_types_and_subtypes . update ( sub_types ) for sup_type in all_types_and_subtypes : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : sup_type }, \"doc\" : f \"Store a value of type ' { sup_type } '.\" , } all_metadata_profiles [ f \"store. { sup_type } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" inputs : typing . Mapping [ str , typing . Any ] = { \"value_id\" : { \"type\" : \"string\" , \"doc\" : \"The id to use when saving the value.\" , }, field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The base path to save the value to.\" , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : \"The original or cloned (if applicable) value that was saved.\" , }, \"load_config\" : { \"type\" : \"load_config\" , \"doc\" : \"The configuration to use with kiara to load the saved value.\" , }, } return outputs @abc . abstractmethod def store_value ( self , value : Value , base_path : str ) -> typing . Union [ typing . Tuple [ typing . Dict [ str , typing . Any ], typing . Any ], typing . Dict [ str , typing . Any ], ]: \"\"\"Save the value, and return the load config needed to load it again.\"\"\" def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_id : str = inputs . get_value_data ( \"value_id\" ) if not value_id : raise KiaraProcessingException ( \"No value id provided.\" ) field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" value_obj : Value = inputs . get_value_obj ( field_name ) base_path : str = inputs . get_value_data ( \"base_path\" ) result = self . store_value ( value = value_obj , base_path = base_path ) if isinstance ( result , typing . Mapping ): load_config = result result_value = value_obj elif isinstance ( result , tuple ): load_config = result [ 0 ] if result [ 1 ]: result_value = result [ 1 ] else : result_value = value_obj else : raise KiaraProcessingException ( f \"Invalid result type for 'store_value' method in class ' { self . __class__ . __name__ } '. This is a bug.\" ) load_config [ \"value_id\" ] = value_id lc = LoadConfig ( ** load_config ) if lc . base_path_input_name and lc . base_path_input_name not in lc . inputs . keys (): raise KiaraProcessingException ( f \"Invalid load config: base path ' { lc . base_path_input_name } ' not part of inputs.\" ) outputs . set_values ( metadata = None , lineage = None , ** { \"load_config\" : lc , field_name : result_value } ) create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/store_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" inputs : typing . Mapping [ str , typing . Any ] = { \"value_id\" : { \"type\" : \"string\" , \"doc\" : \"The id to use when saving the value.\" , }, field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The base path to save the value to.\" , }, } return inputs create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/store_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : \"The original or cloned (if applicable) value that was saved.\" , }, \"load_config\" : { \"type\" : \"load_config\" , \"doc\" : \"The configuration to use with kiara to load the saved value.\" , }, } return outputs retrieve_module_profiles ( kiara ) classmethod \u00b6 Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/store_value.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} all_types_and_subtypes = set () for sup_type in cls . get_supported_value_types (): if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring save operation for type ' { sup_type } ': type not available\" ) continue all_types_and_subtypes . add ( sup_type ) sub_types = kiara . type_mgmt . get_sub_types ( sup_type ) all_types_and_subtypes . update ( sub_types ) for sup_type in all_types_and_subtypes : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : sup_type }, \"doc\" : f \"Store a value of type ' { sup_type } '.\" , } all_metadata_profiles [ f \"store. { sup_type } \" ] = op_config return all_metadata_profiles store_value ( self , value , base_path ) \u00b6 Save the value, and return the load config needed to load it again. Source code in kiara/operations/store_value.py @abc . abstractmethod def store_value ( self , value : Value , base_path : str ) -> typing . Union [ typing . Tuple [ typing . Dict [ str , typing . Any ], typing . Any ], typing . Dict [ str , typing . Any ], ]: \"\"\"Save the value, and return the load config needed to load it again.\"\"\"","title":"store_value"},{"location":"reference/kiara/operations/store_value/#kiara.operations.store_value.StoreOperationType","text":"Store a value into a local data store. This is a special operation type, that is used internally by the [LocalDataStore](http://dharpa.org/kiara/latest/api_reference/kiara.data.registry.store/#kiara.data.registry.store.LocalDataStore] data registry implementation. For each value type that should be supported by the persistent kiara data store, there must be an implementation of the StoreValueTypeModule class, which handles the actual persisting on disk. In most cases, end users won't need to interact with this type of operation. Source code in kiara/operations/store_value.py class StoreOperationType ( OperationType ): \"\"\"Store a value into a local data store. This is a special operation type, that is used internally by the [LocalDataStore](http://dharpa.org/kiara/latest/api_reference/kiara.data.registry.store/#kiara.data.registry.store.LocalDataStore] data registry implementation. For each value type that should be supported by the persistent *kiara* data store, there must be an implementation of the [StoreValueTypeModule](http://dharpa.org/kiara/latest/api_reference/kiara.operations.store_value/#kiara.operations.store_value.StoreValueTypeModule) class, which handles the actual persisting on disk. In most cases, end users won't need to interact with this type of operation. \"\"\" def is_matching_operation ( self , op_config : Operation ) -> bool : return issubclass ( op_config . module_cls , StoreValueTypeModule ) def get_store_operation_for_type ( self , value_type : str ) -> Operation : result = [] for op_config in self . operations . values (): if op_config . module_config [ \"value_type\" ] == value_type : result . append ( op_config ) if not result : raise Exception ( f \"No 'store_value' operation for type ' { value_type } ' registered.\" ) elif len ( result ) != 1 : pass for r in result : print ( r . json ( indent = 2 )) raise Exception ( f \"Multiple 'store_value' operations for type ' { value_type } ' registered.\" ) return result [ 0 ]","title":"StoreOperationType"},{"location":"reference/kiara/operations/store_value/#kiara.operations.store_value.StoreValueModuleConfig","text":"Source code in kiara/operations/store_value.py class StoreValueModuleConfig ( ModuleTypeConfigSchema ): value_type : str = Field ( description = \"The type of the value to save.\" )","title":"StoreValueModuleConfig"},{"location":"reference/kiara/operations/store_value/#kiara.operations.store_value.StoreValueModuleConfig.value_type","text":"The type of the value to save.","title":"value_type"},{"location":"reference/kiara/operations/store_value/#kiara.operations.store_value.StoreValueTypeModule","text":"Store a specific value type. This is used internally. Source code in kiara/operations/store_value.py class StoreValueTypeModule ( KiaraModule ): \"\"\"Store a specific value type. This is used internally. \"\"\" _config_cls = StoreValueModuleConfig @classmethod def get_supported_value_types ( cls ) -> typing . Set [ str ]: _types = cls . retrieve_supported_types () if isinstance ( _types , str ): _types = [ _types ] return set ( _types ) @classmethod @abc . abstractmethod def retrieve_supported_types ( cls ) -> typing . Union [ str , typing . Iterable [ str ]]: pass @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} all_types_and_subtypes = set () for sup_type in cls . get_supported_value_types (): if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring save operation for type ' { sup_type } ': type not available\" ) continue all_types_and_subtypes . add ( sup_type ) sub_types = kiara . type_mgmt . get_sub_types ( sup_type ) all_types_and_subtypes . update ( sub_types ) for sup_type in all_types_and_subtypes : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : sup_type }, \"doc\" : f \"Store a value of type ' { sup_type } '.\" , } all_metadata_profiles [ f \"store. { sup_type } \" ] = op_config return all_metadata_profiles def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" inputs : typing . Mapping [ str , typing . Any ] = { \"value_id\" : { \"type\" : \"string\" , \"doc\" : \"The id to use when saving the value.\" , }, field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The base path to save the value to.\" , }, } return inputs def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : \"The original or cloned (if applicable) value that was saved.\" , }, \"load_config\" : { \"type\" : \"load_config\" , \"doc\" : \"The configuration to use with kiara to load the saved value.\" , }, } return outputs @abc . abstractmethod def store_value ( self , value : Value , base_path : str ) -> typing . Union [ typing . Tuple [ typing . Dict [ str , typing . Any ], typing . Any ], typing . Dict [ str , typing . Any ], ]: \"\"\"Save the value, and return the load config needed to load it again.\"\"\" def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : value_id : str = inputs . get_value_data ( \"value_id\" ) if not value_id : raise KiaraProcessingException ( \"No value id provided.\" ) field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" value_obj : Value = inputs . get_value_obj ( field_name ) base_path : str = inputs . get_value_data ( \"base_path\" ) result = self . store_value ( value = value_obj , base_path = base_path ) if isinstance ( result , typing . Mapping ): load_config = result result_value = value_obj elif isinstance ( result , tuple ): load_config = result [ 0 ] if result [ 1 ]: result_value = result [ 1 ] else : result_value = value_obj else : raise KiaraProcessingException ( f \"Invalid result type for 'store_value' method in class ' { self . __class__ . __name__ } '. This is a bug.\" ) load_config [ \"value_id\" ] = value_id lc = LoadConfig ( ** load_config ) if lc . base_path_input_name and lc . base_path_input_name not in lc . inputs . keys (): raise KiaraProcessingException ( f \"Invalid load config: base path ' { lc . base_path_input_name } ' not part of inputs.\" ) outputs . set_values ( metadata = None , lineage = None , ** { \"load_config\" : lc , field_name : result_value } )","title":"StoreValueTypeModule"},{"location":"reference/kiara/operations/store_value/#kiara.operations.store_value.StoreValueTypeModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/store_value.py def create_input_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" inputs : typing . Mapping [ str , typing . Any ] = { \"value_id\" : { \"type\" : \"string\" , \"doc\" : \"The id to use when saving the value.\" , }, field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : f \"A value of type ' { self . get_config_value ( 'value_type' ) } '.\" , }, \"base_path\" : { \"type\" : \"string\" , \"doc\" : \"The base path to save the value to.\" , }, } return inputs","title":"create_input_schema()"},{"location":"reference/kiara/operations/store_value/#kiara.operations.store_value.StoreValueTypeModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/operations/store_value.py def create_output_schema ( self , ) -> typing . Mapping [ str , typing . Union [ ValueSchema , typing . Mapping [ str , typing . Any ]] ]: field_name = self . get_config_value ( \"value_type\" ) if field_name == \"any\" : field_name = \"value_item\" outputs : typing . Mapping [ str , typing . Any ] = { field_name : { \"type\" : self . get_config_value ( \"value_type\" ), \"doc\" : \"The original or cloned (if applicable) value that was saved.\" , }, \"load_config\" : { \"type\" : \"load_config\" , \"doc\" : \"The configuration to use with kiara to load the saved value.\" , }, } return outputs","title":"create_output_schema()"},{"location":"reference/kiara/operations/store_value/#kiara.operations.store_value.StoreValueTypeModule.retrieve_module_profiles","text":"Retrieve a collection of profiles (pre-set module configs) for this kiara module type. This is used to automatically create generally useful operations (incl. their ids). Source code in kiara/operations/store_value.py @classmethod def retrieve_module_profiles ( cls , kiara : Kiara ) -> typing . Mapping [ str , typing . Union [ typing . Mapping [ str , typing . Any ], Operation ]]: all_metadata_profiles : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]] ] = {} all_types_and_subtypes = set () for sup_type in cls . get_supported_value_types (): if sup_type not in kiara . type_mgmt . value_type_names : log_message ( f \"Ignoring save operation for type ' { sup_type } ': type not available\" ) continue all_types_and_subtypes . add ( sup_type ) sub_types = kiara . type_mgmt . get_sub_types ( sup_type ) all_types_and_subtypes . update ( sub_types ) for sup_type in all_types_and_subtypes : op_config = { \"module_type\" : cls . _module_type_id , # type: ignore \"module_config\" : { \"value_type\" : sup_type }, \"doc\" : f \"Store a value of type ' { sup_type } '.\" , } all_metadata_profiles [ f \"store. { sup_type } \" ] = op_config return all_metadata_profiles","title":"retrieve_module_profiles()"},{"location":"reference/kiara/operations/store_value/#kiara.operations.store_value.StoreValueTypeModule.store_value","text":"Save the value, and return the load config needed to load it again. Source code in kiara/operations/store_value.py @abc . abstractmethod def store_value ( self , value : Value , base_path : str ) -> typing . Union [ typing . Tuple [ typing . Dict [ str , typing . Any ], typing . Any ], typing . Dict [ str , typing . Any ], ]: \"\"\"Save the value, and return the load config needed to load it again.\"\"\"","title":"store_value()"},{"location":"reference/kiara/pipeline/__init__/","text":"PipelineValueInfo ( BaseModel ) pydantic-model \u00b6 Convenience wrapper to make the PipelineState json/dict export prettier. Source code in kiara/pipeline/__init__.py class PipelineValueInfo ( BaseModel ): \"\"\"Convenience wrapper to make the [PipelineState][kiara.pipeline.pipeline.PipelineState] json/dict export prettier.\"\"\" @classmethod def from_value_obj ( cls , value : Value , ensure_metadata : bool = False ): if ensure_metadata : value . get_metadata () return PipelineValueInfo ( id = value . id , value_schema = value . value_schema , is_valid = value . item_is_valid (), is_set = value . is_set , status = value . item_status (), # value_metadata=value.value_metadata, # last_update=value.last_update, # value_hash=value.value_hash, # is_streaming=value.is_streaming, metadata = value . metadata , ) class Config : extra = Extra . forbid allow_mutation = False id : str = Field ( description = \"A unique id for this value.\" ) is_valid : bool = Field ( description = \"Whether the value is set and valid.\" , default = False ) status : str = Field ( description = \"The value status string\" ) is_set : bool = Field ( description = \"Whether the value is set.\" ) value_schema : ValueSchema = Field ( description = \"The schema of this value.\" ) # is_constant: bool = Field( # description=\"Whether this value is a constant.\", default=False # ) # value_metadata: ValueMetadata = Field( # description=\"The metadata of the value itself (not the actual data).\" # ) # last_update: datetime = Field( # default=None, description=\"The time the last update to this value happened.\" # ) # value_hash: typing.Union[ValueHashMarker, int] = Field( # description=\"The hash of the current value.\" # ) # is_streaming: bool = Field( # default=False, # description=\"Whether the value is currently streamed into this object.\", # ) metadata : typing . Dict [ str , typing . Any ] = Field ( description = \"Metadata relating to the actual data (size, no. of rows, etc. -- depending on data type).\" , default_factory = dict , ) id : str pydantic-field required \u00b6 A unique id for this value. is_set : bool pydantic-field required \u00b6 Whether the value is set. is_valid : bool pydantic-field \u00b6 Whether the value is set and valid. metadata : Dict [ str , Any ] pydantic-field \u00b6 Metadata relating to the actual data (size, no. of rows, etc. -- depending on data type). status : str pydantic-field required \u00b6 The value status string value_schema : ValueSchema pydantic-field required \u00b6 The schema of this value. PipelineValuesInfo ( BaseModel ) pydantic-model \u00b6 Convenience wrapper to make the PipelineState json/dict export prettier. This is basically just a simplified version of the ValueSet class that is using pydantic, in order to make it easy to export to json. Source code in kiara/pipeline/__init__.py class PipelineValuesInfo ( BaseModel ): \"\"\"Convenience wrapper to make the [PipelineState][kiara.pipeline.pipeline.PipelineState] json/dict export prettier. This is basically just a simplified version of the [ValueSet][kiara.data.values.ValueSet] class that is using pydantic, in order to make it easy to export to json. \"\"\" @classmethod def from_value_set ( cls , value_set : ValueSet , ensure_metadata : bool = False ): values : typing . Dict [ str , PipelineValueInfo ] = {} for k in value_set . get_all_field_names (): v = value_set . get_value_obj ( k , ensure_metadata = ensure_metadata ) values [ k ] = PipelineValueInfo . from_value_obj ( v , ensure_metadata = ensure_metadata ) return PipelineValuesInfo ( values = values ) values : typing . Dict [ str , PipelineValueInfo ] = Field ( description = \"Field names are keys, and the data as values.\" ) class Config : use_enum_values = True values : Dict [ str , kiara . pipeline . PipelineValueInfo ] pydantic-field required \u00b6 Field names are keys, and the data as values. StepStatus ( Enum ) \u00b6 Enum to describe the state of a workflow. Source code in kiara/pipeline/__init__.py class StepStatus ( Enum ): \"\"\"Enum to describe the state of a workflow.\"\"\" STALE = \"stale\" INPUTS_READY = \"inputs_ready\" RESULTS_INCOMING = \"processing\" RESULTS_READY = \"results_ready\" StepValueAddress ( BaseModel ) pydantic-model \u00b6 Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure. Source code in kiara/pipeline/__init__.py class StepValueAddress ( BaseModel ): \"\"\"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.\"\"\" class Config : extra = Extra . forbid step_id : str = Field ( description = \"The id of a step within a pipeline.\" ) value_name : str = Field ( description = \"The name of the value (output name or pipeline input name).\" ) sub_value : typing . Optional [ typing . Dict [ str , typing . Any ]] = Field ( default = None , description = \"A reference to a subitem of a value (e.g. column, list item)\" , ) @property def alias ( self ): \"\"\"An alias string for this address (in the form ``[step_id].[value_name]``).\"\"\" return generate_step_alias ( self . step_id , self . value_name ) def __eq__ ( self , other ): if not isinstance ( other , StepValueAddress ): return False return ( self . step_id , self . value_name , self . sub_value ) == ( other . step_id , other . value_name , other . sub_value , ) def __hash__ ( self ): return hash (( self . step_id , self . value_name , self . sub_value )) def __repr__ ( self ): if self . sub_value : sub_value = f \" sub_value= { self . sub_value } \" else : sub_value = \"\" return f \"StepValueAddres(step_id= { self . step_id } , value_name= { self . value_name }{ sub_value } )\" def __str__ ( self ): return self . __repr__ () alias property readonly \u00b6 An alias string for this address (in the form [step_id].[value_name] ). step_id : str pydantic-field required \u00b6 The id of a step within a pipeline. sub_value : Dict [ str , Any ] pydantic-field \u00b6 A reference to a subitem of a value (e.g. column, list item) value_name : str pydantic-field required \u00b6 The name of the value (output name or pipeline input name). __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/pipeline/__init__.py def __eq__ ( self , other ): if not isinstance ( other , StepValueAddress ): return False return ( self . step_id , self . value_name , self . sub_value ) == ( other . step_id , other . value_name , other . sub_value , ) __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/pipeline/__init__.py def __hash__ ( self ): return hash (( self . step_id , self . value_name , self . sub_value )) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/pipeline/__init__.py def __repr__ ( self ): if self . sub_value : sub_value = f \" sub_value= { self . sub_value } \" else : sub_value = \"\" return f \"StepValueAddres(step_id= { self . step_id } , value_name= { self . value_name }{ sub_value } )\" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/pipeline/__init__.py def __str__ ( self ): return self . __repr__ () config \u00b6 PipelineConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 A class to hold the configuration for a PipelineModule . If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify auto , in which case Kiara will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a nand logic gate (in Python): and_step = PipelineStepConfig ( module_type = \"and\" , step_id = \"and\" ) not_step = PipelineStepConfig ( module_type = \"not\" , step_id = \"not\" , input_links = { \"a\" : [ \"and.y\" ]} nand_p_conf = PipelineConfig ( doc = \"Returns 'False' if both inputs are 'True'.\" , steps = [ and_step , not_step ], input_aliases = { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, output_aliases = { \"not__y\" : \"y\" }} Or, the same thing in json: { \"module_type_name\" : \"nand\" , \"doc\" : \"Returns 'False' if both inputs are 'True'.\" , \"steps\" : [ { \"module_type\" : \"and\" , \"step_id\" : \"and\" }, { \"module_type\" : \"not\" , \"step_id\" : \"not\" , \"input_links\" : { \"a\" : \"and.y\" } } ], \"input_aliases\" : { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, \"output_aliases\" : { \"not__y\" : \"y\" } } Source code in kiara/pipeline/config.py class PipelineConfig ( ModuleTypeConfigSchema ): \"\"\"A class to hold the configuration for a [PipelineModule][kiara.pipeline.module.PipelineModule]. If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify ``auto``, in which case *Kiara* will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a ``nand`` logic gate (in Python): ``` python and_step = PipelineStepConfig(module_type=\"and\", step_id=\"and\") not_step = PipelineStepConfig(module_type=\"not\", step_id=\"not\", input_links={\"a\": [\"and.y\"]} nand_p_conf = PipelineConfig(doc=\"Returns 'False' if both inputs are 'True'.\", steps=[and_step, not_step], input_aliases={ \"and__a\": \"a\", \"and__b\": \"b\" }, output_aliases={ \"not__y\": \"y\" }} ``` Or, the same thing in json: ``` json { \"module_type_name\": \"nand\", \"doc\": \"Returns 'False' if both inputs are 'True'.\", \"steps\": [ { \"module_type\": \"and\", \"step_id\": \"and\" }, { \"module_type\": \"not\", \"step_id\": \"not\", \"input_links\": { \"a\": \"and.y\" } } ], \"input_aliases\": { \"and__a\": \"a\", \"and__b\": \"b\" }, \"output_aliases\": { \"not__y\": \"y\" } } ``` \"\"\" # @classmethod # def from_file(cls, path: typing.Union[str, Path]): # # content = get_data_from_file(path) # return PipelineConfig(**content) @classmethod def create_pipeline_config ( cls , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineConfig\" : \"\"\"Create a PipelineModule instance. The main 'config' argument here can be either: - a string: in which case it needs to be (in that order): - a module id - an operation id - a path to a local file - a [ModuleConfig][kiara.module_config.ModuleConfig] object - a dict (to create a `ModuleInstsanceConfig` from Arguments: kiara: the kiara context config: the 'main' config object module_config: in case the 'main' config object was a module id, this argument is used to instantiate the module kiara: the kiara context (will use default context instance if not provided) \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () module_config_obj = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) if not module_config_obj . module_type == \"pipeline\" : raise Exception ( f \"Not a valid pipeline configuration: { config } \" ) # TODO: this is a bit round-about, to create a module config first, but it probably doesn't matter pipeline_config_data = module_config_obj . module_config module : PipelineConfig = PipelineConfig ( ** pipeline_config_data ) return module class Config : extra = Extra . allow validate_assignment = True steps : typing . List [ PipelineStepConfig ] = Field ( default_factory = list , description = \"A list of steps/modules of this pipeline, and their connections.\" , ) input_aliases : typing . Union [ str , typing . Dict [ str , str ]] = Field ( default_factory = dict , description = \"A map of input aliases, with the calculated (<step_id>__<input_name> -- double underscore!) name as key, and a string (the resulting workflow input alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.\" , ) output_aliases : typing . Union [ str , typing . Dict [ str , str ]] = Field ( default_factory = dict , description = \"A map of output aliases, with the calculated (<step_id>__<output_name> -- double underscore!) name as key, and a string (the resulting workflow output alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.\" , ) documentation : str = Field ( default = \"-- n/a --\" , description = \"Documentation about what the pipeline does.\" ) context : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"Metadata for this workflow.\" ) @validator ( \"steps\" , pre = True ) def _validate_steps ( cls , v ): steps = [] for step in v : if isinstance ( step , PipelineStepConfig ): steps . append ( step ) elif isinstance ( step , typing . Mapping ): steps . append ( PipelineStepConfig ( ** step )) else : raise TypeError ( step ) return steps def create_pipeline_structure ( self , kiara : typing . Optional [ \"Kiara\" ] = None ) -> \"PipelineStructure\" : from kiara import Kiara , PipelineStructure if kiara is None : kiara = Kiara . instance () ps = PipelineStructure ( config = self , kiara = kiara , ) return ps def create_pipeline ( self , controller : typing . Optional [ \"PipelineController\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): # if parent_id is None: # parent_id = DEFAULT_PIPELINE_PARENT_ID structure = self . create_pipeline_structure ( kiara = kiara ) from kiara import Pipeline pipeline = Pipeline ( structure = structure , controller = controller , ) return pipeline def create_pipeline_module ( self , module_id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineModule\" : if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () from kiara.pipeline.module import PipelineModule module = PipelineModule ( id = module_id , parent_id = parent_id , module_config = self , kiara = kiara , ) return module # def __rich_console__( # self, console: Console, options: ConsoleOptions # ) -> RenderResult: # # table = Table(show_header=False, box=box.SIMPLE) context : Dict [ str , Any ] pydantic-field \u00b6 Metadata for this workflow. documentation : str pydantic-field \u00b6 Documentation about what the pipeline does. input_aliases : Union [ str , Dict [ str , str ]] pydantic-field \u00b6 A map of input aliases, with the calculated ( __ -- double underscore!) name as key, and a string (the resulting workflow input alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible. output_aliases : Union [ str , Dict [ str , str ]] pydantic-field \u00b6 A map of output aliases, with the calculated ( __ -- double underscore!) name as key, and a string (the resulting workflow output alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible. steps : List [ kiara . pipeline . config . PipelineStepConfig ] pydantic-field \u00b6 A list of steps/modules of this pipeline, and their connections. create_pipeline_config ( config , module_config = None , kiara = None ) classmethod \u00b6 Create a PipelineModule instance. The main 'config' argument here can be either: a string: in which case it needs to be (in that order): a module id an operation id a path to a local file a ModuleConfig object a dict (to create a ModuleInstsanceConfig from Parameters: Name Type Description Default kiara Optional[Kiara] the kiara context None config Union[kiara.module_config.ModuleConfig, Mapping[str, Any], str] the 'main' config object required module_config Optional[Mapping[str, Any]] in case the 'main' config object was a module id, this argument is used to instantiate the module None kiara Optional[Kiara] the kiara context (will use default context instance if not provided) None Source code in kiara/pipeline/config.py @classmethod def create_pipeline_config ( cls , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineConfig\" : \"\"\"Create a PipelineModule instance. The main 'config' argument here can be either: - a string: in which case it needs to be (in that order): - a module id - an operation id - a path to a local file - a [ModuleConfig][kiara.module_config.ModuleConfig] object - a dict (to create a `ModuleInstsanceConfig` from Arguments: kiara: the kiara context config: the 'main' config object module_config: in case the 'main' config object was a module id, this argument is used to instantiate the module kiara: the kiara context (will use default context instance if not provided) \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () module_config_obj = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) if not module_config_obj . module_type == \"pipeline\" : raise Exception ( f \"Not a valid pipeline configuration: { config } \" ) # TODO: this is a bit round-about, to create a module config first, but it probably doesn't matter pipeline_config_data = module_config_obj . module_config module : PipelineConfig = PipelineConfig ( ** pipeline_config_data ) return module PipelineStepConfig ( ModuleConfig ) pydantic-model \u00b6 A class to hold the configuration of one module within a PipelineModule . Source code in kiara/pipeline/config.py class PipelineStepConfig ( ModuleConfig ): \"\"\"A class to hold the configuration of one module within a [PipelineModule][kiara.pipeline.module.PipelineModule].\"\"\" class Config : extra = Extra . forbid validate_assignment = True step_id : str = Field ( description = \"The id of the step.\" ) input_links : typing . Dict [ str , typing . List [ StepValueAddress ]] = Field ( default_factory = dict , description = \"The map with the name of an input link as key, and the connected module output name(s) as value.\" , ) @root_validator ( pre = True ) def create_step_id ( cls , values ): if \"module_type\" not in values : raise ValueError ( \"No 'module_type' specified.\" ) if \"step_id\" not in values or not values [ \"step_id\" ]: values [ \"step_id\" ] = slugify ( values [ \"module_type\" ]) return values @validator ( \"step_id\" ) def ensure_valid_id ( cls , v ): # TODO: check with regex if \".\" in v or \" \" in v : raise ValueError ( f \"Step id can't contain special characters or whitespaces: { v } \" ) return v @validator ( \"module_config\" , pre = True ) def ensure_dict ( cls , v ): if v is None : v = {} return v @validator ( \"input_links\" , pre = True ) def ensure_input_links_valid ( cls , v ): if v is None : v = {} result = {} for input_name , output in v . items (): input_links = ensure_step_value_addresses ( default_field_name = input_name , link = output ) result [ input_name ] = input_links return result input_links : Dict [ str , List [ kiara . pipeline . StepValueAddress ]] pydantic-field \u00b6 The map with the name of an input link as key, and the connected module output name(s) as value. step_id : str pydantic-field required \u00b6 The id of the step. StepDesc ( BaseModel ) pydantic-model \u00b6 Details of a single PipelineStep (which lives within a Pipeline Source code in kiara/pipeline/config.py class StepDesc ( BaseModel ): \"\"\"Details of a single [PipelineStep][kiara.pipeline.structure.PipelineStep] (which lives within a [Pipeline][kiara.pipeline.pipeline.Pipeline]\"\"\" class Config : allow_mutation = False extra = Extra . forbid step : PipelineStep = Field ( description = \"Attributes of the step itself.\" ) processing_stage : int = Field ( description = \"The processing stage of this step within a Pipeline.\" ) input_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"\"\"A map that explains what elements connect to this steps inputs. A connection could either be a Pipeline input (indicated by the ``__pipeline__`` token), or another steps output. Example: ``` json input_connections: { \"a\": [\"__pipeline__.a\"], \"b\": [\"step_one.a\"] } ``` \"\"\" ) output_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"A map that explains what elemnts connect to this steps outputs. A connection could be either a Pipeline output, or another steps input.\" ) required : bool = Field ( description = \"Whether this step is always required, or potentially could be skipped in case some inputs are not available.\" ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield f \"[b]Step: { self . step . step_id } [ \\b ]\" input_connections : Dict [ str , List [ str ]] pydantic-field required \u00b6 A map that explains what elements connect to this steps inputs. A connection could either be a Pipeline input (indicated by the __pipeline__ token), or another steps output. Examples: i n pu t _co nne c t io ns : { \"a\" : [ \"__pipeline__.a\" ], \"b\" : [ \"step_one.a\" ] } output_connections : Dict [ str , List [ str ]] pydantic-field required \u00b6 A map that explains what elemnts connect to this steps outputs. A connection could be either a Pipeline output, or another steps input. processing_stage : int pydantic-field required \u00b6 The processing stage of this step within a Pipeline. required : bool pydantic-field required \u00b6 Whether this step is always required, or potentially could be skipped in case some inputs are not available. step : PipelineStep pydantic-field required \u00b6 Attributes of the step itself. controller special \u00b6 PipelineController ( PipelineListener ) \u00b6 An object that controls how a Pipeline should react to events related to it's inputs/outputs. This is the base for the central controller class that needs to be implemented by a Kiara frontend. The default implementation that is used if no PipelineController is provided in a Pipeline constructor is the BatchController , which basically waits until all required inputs are set, and then processes all pipeline steps in one go (in the right order). The pipeline object to control can be set either in the constructor, or via the set_pipeline method. But only once, every subsequent attempt to set a pipeline will raise an Exception. If you want to implement your own controller, you have to override at least one of the (empty) event hook methods: pipeline_inputs_changed pipeline_outputs_changed step_inputs_changed step_outputs_changed Parameters: Name Type Description Default pipeline Pipeline the pipeline object to control None Source code in kiara/pipeline/controller/__init__.py class PipelineController ( PipelineListener ): \"\"\"An object that controls how a [Pipeline][kiara.pipeline.pipeline.Pipeline] should react to events related to it's inputs/outputs. This is the base for the central controller class that needs to be implemented by a *Kiara* frontend. The default implementation that is used if no ``PipelineController`` is provided in a [Pipeline][kiara.pipeline.pipeline.Pipeline] constructor is the [BatchController][kiara.pipeline.controller.BatchController], which basically waits until all required inputs are set, and then processes all pipeline steps in one go (in the right order). The pipeline object to control can be set either in the constructor, or via the ``set_pipeline`` method. But only once, every subsequent attempt to set a pipeline will raise an Exception. If you want to implement your own controller, you have to override at least one of the (empty) event hook methods: - [``pipeline_inputs_changed``][kiara.pipeline.controller.PipelineController.pipeline_inputs_changed] - [``pipeline_outputs_changed``][kiara.pipeline.controller.PipelineController.pipeline_outputs_changed] - [``step_inputs_changed``][kiara.pipeline.controller.PipelineController.step_inputs_changed] - [``step_outputs_changed``][kiara.pipeline.controller.PipelineController.step_outputs_changed] Arguments: pipeline (Pipeline): the pipeline object to control \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : Kiara = kiara self . _pipeline : typing . Optional [ Pipeline ] = None if processor is None : from kiara.processing.synchronous import SynchronousProcessor processor = SynchronousProcessor ( kiara = kiara ) self . _processor : ModuleProcessor = processor self . _job_ids : typing . Dict [ str , str ] = {} \"\"\"A map of the last or current job ids per step_id.\"\"\" if pipeline is not None : self . set_pipeline ( pipeline ) @property def pipeline ( self ) -> \"Pipeline\" : \"\"\"Return the pipeline this controller, well, ...controls...\"\"\" if self . _pipeline is None : raise Exception ( \"Pipeline not set yet.\" ) return self . _pipeline @property def pipeline_status ( self ) -> \"StepStatus\" : return self . pipeline . status def set_pipeline ( self , pipeline : \"Pipeline\" ): \"\"\"Set the pipeline object for this controller. Only one pipeline can be set, once. Arguments: pipeline: the pipeline object \"\"\" if self . _pipeline is not None : raise Exception ( \"Pipeline already set.\" ) self . _pipeline = pipeline @property def processing_stages ( self ) -> typing . List [ typing . List [ str ]]: \"\"\"Return the processing stage order of the pipeline. Returns: a list of lists of step ids \"\"\" return self . pipeline . structure . processing_stages def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the step object for the provided id. Arguments: step_id: the step id Returns: the step object \"\"\" return self . pipeline . get_step ( step_id ) def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the inputs object for the pipeline.\"\"\" return self . pipeline . get_step_inputs ( step_id ) def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the outputs object for the pipeline.\"\"\" return self . pipeline . get_step_outputs ( step_id ) def get_step_input ( self , step_id : str , input_name : str ) -> Value : \"\"\"Get the (current) input value for a specified step and input field name.\"\"\" item = self . get_step_inputs ( step_id ) . get_value_obj ( input_name ) assert item is not None return item def get_step_output ( self , step_id : str , output_name : str ) -> Value : \"\"\"Get the (current) output value for a specified step and output field name.\"\"\" item = self . get_step_outputs ( step_id ) . get_value_obj ( output_name ) assert item is not None return item def get_current_pipeline_state ( self ) -> \"PipelineState\" : \"\"\"Return a description of the current pipeline state. This methods creates a new [PipelineState][kiara.pipeline.pipeline.PipelineState] object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: an object outlining the current pipeline state \"\"\" return self . pipeline . get_current_state () @property def pipeline_inputs ( self ) -> ValueSet : \"\"\"Return the inputs object for this pipeline.\"\"\" return self . pipeline . _pipeline_inputs @pipeline_inputs . setter def pipeline_inputs ( self , inputs : typing . Mapping [ str , typing . Any ]) -> None : \"\"\"Set one, several or all inputs for this pipeline.\"\"\" self . set_pipeline_inputs ( ** inputs ) @property def pipeline_outputs ( self ) -> ValueSet : \"\"\"Return the (current) pipeline outputs object for this pipeline.\"\"\" return self . pipeline . _pipeline_outputs def wait_for_jobs ( self , * job_ids : str , sync_outputs : bool = True ): self . _processor . wait_for ( * job_ids , sync_outputs = sync_outputs ) def can_be_processed ( self , step_id : str ) -> bool : \"\"\"Check whether the step with the provided id is ready to be processed.\"\"\" result = True step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): result = False break return result def check_inputs_status ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]]: \"\"\"Check the status of all stages/steps, and whether they are ready to be processed. The result dictionary uses stage numbers as keys, and a secondary dictionary as values which in turn uses the step_id as key, and a dict with details (keys: 'ready', 'invalid_inputs', 'required') as values. \"\"\" result : typing . Dict [ int , typing . Dict [ str , typing . Dict [ str , typing . Any ]]] = {} for idx , stage in enumerate ( self . processing_stages , start = 1 ): for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], } else : result . setdefault ( idx , {})[ step_id ] = { \"ready\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], \"required\" : True , } else : if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : True , \"invalid_inputs\" : [], } return result def can_be_skipped ( self , step_id : str ) -> bool : \"\"\"Check whether the processing of a step can be skipped.\"\"\" result = True step = self . get_step ( step_id ) if step . required : result = self . can_be_processed ( step_id ) return result def invalid_inputs ( self , step_id : str ) -> typing . List [ str ]: invalid = [] step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): invalid . append ( input_name ) return invalid def process_step ( self , step_id : str , raise_exception : bool = False , wait : bool = False ) -> str : \"\"\"Kick off processing for the step with the provided id. Arguments: step_id: the id of the step that should be started \"\"\" step_inputs = self . get_step_inputs ( step_id ) # if the inputs are not valid, ignore this step if not step_inputs . items_are_valid (): status = step_inputs . check_invalid () assert status is not None raise Exception ( f \"Can't execute step ' { step_id } ', invalid inputs: { ', ' . join ( status . keys ()) } \" ) # get the output 'holder' objects, which we'll need to pass to the module step_outputs = self . get_step_outputs ( step_id ) # get the module object that holds the code that will do the processing step = self . get_step ( step_id ) job_id = self . _processor . start ( pipeline_id = self . pipeline . id , pipeline_name = self . pipeline . title , step_id = step_id , module = step . module , inputs = step_inputs , outputs = step_outputs , ) self . _job_ids [ step_id ] = job_id if wait : self . wait_for_jobs ( job_id , sync_outputs = True ) return job_id def get_job_details ( self , step_or_job_id : str ) -> typing . Optional [ Job ]: \"\"\"Returns job details for a job id, or in case a step_id was provided, the last execution of this step.\"\"\" if self . _job_ids . get ( step_or_job_id , None ) is None : return self . _processor . get_job_details ( step_or_job_id ) else : return self . _processor . get_job_details ( self . _job_ids [ step_or_job_id ]) def step_is_ready ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id is ready to be processed. A ``True`` result means that all input fields are currently set with valid values. Arguments: step_id: the id of the step to check Returns: whether the step is ready (``True``) or not (``False``) \"\"\" return self . get_step_inputs ( step_id ) . items_are_valid () def step_is_finished ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id has been processed successfully. A ``True`` result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Arguments: step_id: the id of the step to check Returns: whether the step result is valid (``True``) or not (``False``) \"\"\" return self . get_step_outputs ( step_id ) . items_are_valid () def pipeline_is_ready ( self ) -> bool : \"\"\"Return whether the pipeline is ready to be processed. A ``True`` result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: whether the pipeline can be processed as a whole (``True``) or not (``False``) \"\"\" return self . pipeline . inputs . items_are_valid () def pipeline_is_finished ( self ) -> bool : \"\"\"Return whether the pipeline has been processed successfully. A ``True`` result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: whether the pipeline was processed successfully (``True``) or not (``False``) \"\"\" return self . pipeline . outputs . items_are_valid () def set_pipeline_inputs ( self , ** inputs : typing . Any ): \"\"\"Set one, several or all inputs for this pipeline. Arguments: **inputs: the input values to set \"\"\" _inputs = self . _pipeline_input_hook ( ** inputs ) self . pipeline_inputs . set_values ( ** _inputs ) def _pipeline_input_hook ( self , ** inputs : typing . Any ): \"\"\"Hook before setting input. Can be implemented by child controller classes, to prevent, transform, validate or queue inputs. \"\"\" log . debug ( f \"Inputs for pipeline ' { self . pipeline . id } ' set: { inputs } \" ) return inputs def process_pipeline ( self ): \"\"\"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Returns: \"\"\" raise NotImplementedError () pipeline : Pipeline property readonly \u00b6 Return the pipeline this controller, well, ...controls... pipeline_inputs : ValueSet property writable \u00b6 Return the inputs object for this pipeline. pipeline_outputs : ValueSet property readonly \u00b6 Return the (current) pipeline outputs object for this pipeline. processing_stages : List [ List [ str ]] property readonly \u00b6 Return the processing stage order of the pipeline. Returns: Type Description List[List[str]] a list of lists of step ids can_be_processed ( self , step_id ) \u00b6 Check whether the step with the provided id is ready to be processed. Source code in kiara/pipeline/controller/__init__.py def can_be_processed ( self , step_id : str ) -> bool : \"\"\"Check whether the step with the provided id is ready to be processed.\"\"\" result = True step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): result = False break return result can_be_skipped ( self , step_id ) \u00b6 Check whether the processing of a step can be skipped. Source code in kiara/pipeline/controller/__init__.py def can_be_skipped ( self , step_id : str ) -> bool : \"\"\"Check whether the processing of a step can be skipped.\"\"\" result = True step = self . get_step ( step_id ) if step . required : result = self . can_be_processed ( step_id ) return result check_inputs_status ( self ) \u00b6 Check the status of all stages/steps, and whether they are ready to be processed. The result dictionary uses stage numbers as keys, and a secondary dictionary as values which in turn uses the step_id as key, and a dict with details (keys: 'ready', 'invalid_inputs', 'required') as values. Source code in kiara/pipeline/controller/__init__.py def check_inputs_status ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]]: \"\"\"Check the status of all stages/steps, and whether they are ready to be processed. The result dictionary uses stage numbers as keys, and a secondary dictionary as values which in turn uses the step_id as key, and a dict with details (keys: 'ready', 'invalid_inputs', 'required') as values. \"\"\" result : typing . Dict [ int , typing . Dict [ str , typing . Dict [ str , typing . Any ]]] = {} for idx , stage in enumerate ( self . processing_stages , start = 1 ): for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], } else : result . setdefault ( idx , {})[ step_id ] = { \"ready\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], \"required\" : True , } else : if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : True , \"invalid_inputs\" : [], } return result get_current_pipeline_state ( self ) \u00b6 Return a description of the current pipeline state. This methods creates a new PipelineState object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: Type Description PipelineState an object outlining the current pipeline state Source code in kiara/pipeline/controller/__init__.py def get_current_pipeline_state ( self ) -> \"PipelineState\" : \"\"\"Return a description of the current pipeline state. This methods creates a new [PipelineState][kiara.pipeline.pipeline.PipelineState] object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: an object outlining the current pipeline state \"\"\" return self . pipeline . get_current_state () get_job_details ( self , step_or_job_id ) \u00b6 Returns job details for a job id, or in case a step_id was provided, the last execution of this step. Source code in kiara/pipeline/controller/__init__.py def get_job_details ( self , step_or_job_id : str ) -> typing . Optional [ Job ]: \"\"\"Returns job details for a job id, or in case a step_id was provided, the last execution of this step.\"\"\" if self . _job_ids . get ( step_or_job_id , None ) is None : return self . _processor . get_job_details ( step_or_job_id ) else : return self . _processor . get_job_details ( self . _job_ids [ step_or_job_id ]) get_step ( self , step_id ) \u00b6 Return the step object for the provided id. Parameters: Name Type Description Default step_id str the step id required Returns: Type Description PipelineStep the step object Source code in kiara/pipeline/controller/__init__.py def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the step object for the provided id. Arguments: step_id: the step id Returns: the step object \"\"\" return self . pipeline . get_step ( step_id ) get_step_input ( self , step_id , input_name ) \u00b6 Get the (current) input value for a specified step and input field name. Source code in kiara/pipeline/controller/__init__.py def get_step_input ( self , step_id : str , input_name : str ) -> Value : \"\"\"Get the (current) input value for a specified step and input field name.\"\"\" item = self . get_step_inputs ( step_id ) . get_value_obj ( input_name ) assert item is not None return item get_step_inputs ( self , step_id ) \u00b6 Return the inputs object for the pipeline. Source code in kiara/pipeline/controller/__init__.py def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the inputs object for the pipeline.\"\"\" return self . pipeline . get_step_inputs ( step_id ) get_step_output ( self , step_id , output_name ) \u00b6 Get the (current) output value for a specified step and output field name. Source code in kiara/pipeline/controller/__init__.py def get_step_output ( self , step_id : str , output_name : str ) -> Value : \"\"\"Get the (current) output value for a specified step and output field name.\"\"\" item = self . get_step_outputs ( step_id ) . get_value_obj ( output_name ) assert item is not None return item get_step_outputs ( self , step_id ) \u00b6 Return the outputs object for the pipeline. Source code in kiara/pipeline/controller/__init__.py def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the outputs object for the pipeline.\"\"\" return self . pipeline . get_step_outputs ( step_id ) pipeline_is_finished ( self ) \u00b6 Return whether the pipeline has been processed successfully. A True result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: Type Description bool whether the pipeline was processed successfully ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def pipeline_is_finished ( self ) -> bool : \"\"\"Return whether the pipeline has been processed successfully. A ``True`` result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: whether the pipeline was processed successfully (``True``) or not (``False``) \"\"\" return self . pipeline . outputs . items_are_valid () pipeline_is_ready ( self ) \u00b6 Return whether the pipeline is ready to be processed. A True result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: Type Description bool whether the pipeline can be processed as a whole ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def pipeline_is_ready ( self ) -> bool : \"\"\"Return whether the pipeline is ready to be processed. A ``True`` result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: whether the pipeline can be processed as a whole (``True``) or not (``False``) \"\"\" return self . pipeline . inputs . items_are_valid () process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/__init__.py def process_pipeline ( self ): \"\"\"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Returns: \"\"\" raise NotImplementedError () process_step ( self , step_id , raise_exception = False , wait = False ) \u00b6 Kick off processing for the step with the provided id. Parameters: Name Type Description Default step_id str the id of the step that should be started required Source code in kiara/pipeline/controller/__init__.py def process_step ( self , step_id : str , raise_exception : bool = False , wait : bool = False ) -> str : \"\"\"Kick off processing for the step with the provided id. Arguments: step_id: the id of the step that should be started \"\"\" step_inputs = self . get_step_inputs ( step_id ) # if the inputs are not valid, ignore this step if not step_inputs . items_are_valid (): status = step_inputs . check_invalid () assert status is not None raise Exception ( f \"Can't execute step ' { step_id } ', invalid inputs: { ', ' . join ( status . keys ()) } \" ) # get the output 'holder' objects, which we'll need to pass to the module step_outputs = self . get_step_outputs ( step_id ) # get the module object that holds the code that will do the processing step = self . get_step ( step_id ) job_id = self . _processor . start ( pipeline_id = self . pipeline . id , pipeline_name = self . pipeline . title , step_id = step_id , module = step . module , inputs = step_inputs , outputs = step_outputs , ) self . _job_ids [ step_id ] = job_id if wait : self . wait_for_jobs ( job_id , sync_outputs = True ) return job_id set_pipeline ( self , pipeline ) \u00b6 Set the pipeline object for this controller. Only one pipeline can be set, once. Parameters: Name Type Description Default pipeline Pipeline the pipeline object required Source code in kiara/pipeline/controller/__init__.py def set_pipeline ( self , pipeline : \"Pipeline\" ): \"\"\"Set the pipeline object for this controller. Only one pipeline can be set, once. Arguments: pipeline: the pipeline object \"\"\" if self . _pipeline is not None : raise Exception ( \"Pipeline already set.\" ) self . _pipeline = pipeline set_pipeline_inputs ( self , ** inputs ) \u00b6 Set one, several or all inputs for this pipeline. Parameters: Name Type Description Default **inputs Any the input values to set {} Source code in kiara/pipeline/controller/__init__.py def set_pipeline_inputs ( self , ** inputs : typing . Any ): \"\"\"Set one, several or all inputs for this pipeline. Arguments: **inputs: the input values to set \"\"\" _inputs = self . _pipeline_input_hook ( ** inputs ) self . pipeline_inputs . set_values ( ** _inputs ) step_is_finished ( self , step_id ) \u00b6 Return whether the step with the provided id has been processed successfully. A True result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Parameters: Name Type Description Default step_id str the id of the step to check required Returns: Type Description bool whether the step result is valid ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def step_is_finished ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id has been processed successfully. A ``True`` result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Arguments: step_id: the id of the step to check Returns: whether the step result is valid (``True``) or not (``False``) \"\"\" return self . get_step_outputs ( step_id ) . items_are_valid () step_is_ready ( self , step_id ) \u00b6 Return whether the step with the provided id is ready to be processed. A True result means that all input fields are currently set with valid values. Parameters: Name Type Description Default step_id str the id of the step to check required Returns: Type Description bool whether the step is ready ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def step_is_ready ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id is ready to be processed. A ``True`` result means that all input fields are currently set with valid values. Arguments: step_id: the id of the step to check Returns: whether the step is ready (``True``) or not (``False``) \"\"\" return self . get_step_inputs ( step_id ) . items_are_valid () batch \u00b6 BatchController ( PipelineController ) \u00b6 A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline Optional[Pipeline] the pipeline to control None auto_process bool whether to automatically start processing the pipeline as soon as the input set is valid True Source code in kiara/pipeline/controller/batch.py class BatchController ( PipelineController ): \"\"\"A [PipelineController][kiara.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively. This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Arguments: pipeline: the pipeline to control auto_process: whether to automatically start processing the pipeline as soon as the input set is valid \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , auto_process : bool = True , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _auto_process : bool = auto_process self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor , kiara = kiara ) @property def auto_process ( self ) -> bool : return self . _auto_process @auto_process . setter def auto_process ( self , auto_process : bool ): self . _auto_process = auto_process def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_exc () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) # for j_id in job_ids: # job = self._processor.get_job_details(j_id) # assert job is not None # if job.error: # print(job.error) finally : self . _is_running = False def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return if self . _auto_process : self . process_pipeline () def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_exc () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) # for j_id in job_ids: # job = self._processor.get_job_details(j_id) # assert job is not None # if job.error: # print(job.error) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return if self . _auto_process : self . process_pipeline () BatchControllerManual ( PipelineController ) \u00b6 A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline Optional[Pipeline] the pipeline to control None auto_process whether to automatically start processing the pipeline as soon as the input set is valid required Source code in kiara/pipeline/controller/batch.py class BatchControllerManual ( PipelineController ): \"\"\"A [PipelineController][kiara.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively. This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Arguments: pipeline: the pipeline to control auto_process: whether to automatically start processing the pipeline as soon as the input set is valid \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _finished_until : typing . Optional [ int ] = None self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor , kiara = kiara ) @property def last_finished_stage ( self ) -> int : if self . _finished_until is None : return 0 else : return self . _finished_until def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return def pipeline_inputs_changed ( self , event : \"PipelineInputEvent\" ): self . _finished_until = None # print(\"============================\") # min_stage_to_clear = sys.maxsize # for inp in event.updated_pipeline_inputs: # stage = self.pipeline.get_stage_for_pipeline_input(inp) # if stage < min_stage_to_clear: # min_stage_to_clear = stage # # for stage, steps in self.pipeline.get_steps_by_stage().items(): # if stage < min_stage_to_clear: # continue # for step_id, step in steps.items(): # step_inputs = self.get_step_inputs(step_id) # empty = { k: None for k in step_inputs.keys() } # step_inputs.set_values(**empty) # step_outputs = self.get_step_outputs(step_id) # empty = { k: None for k in step_outputs.keys() } # step_outputs.set_values(**empty) def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False def process_stage ( self , stage_nr : int ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Union [ None , str , Exception ]]]: if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True result : typing . Dict [ int , typing . Dict [ str , typing . Union [ None , str , Exception ]] ] = {} try : for idx , stage in enumerate ( self . processing_stages ): current_stage = idx + 1 if current_stage > stage_nr : break if self . _finished_until is not None and idx <= self . _finished_until : continue job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( current_stage , {})[ step_id ] = None continue else : exc = Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) result . setdefault ( current_stage , {})[ step_id ] = exc try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) result . setdefault ( current_stage , {})[ step_id ] = job_id except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) result . setdefault ( current_stage , {})[ step_id ] = e self . _processor . wait_for ( * job_ids ) # for job_id in job_ids: # job = self.get_job_details(job_id) # dbg(job.dict()) self . _finished_until = idx finally : self . _is_running = False print ( \"BATCH FINISHED\" ) return result pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/pipeline/controller/batch.py def pipeline_inputs_changed ( self , event : \"PipelineInputEvent\" ): self . _finished_until = None # print(\"============================\") # min_stage_to_clear = sys.maxsize # for inp in event.updated_pipeline_inputs: # stage = self.pipeline.get_stage_for_pipeline_input(inp) # if stage < min_stage_to_clear: # min_stage_to_clear = stage # # for stage, steps in self.pipeline.get_steps_by_stage().items(): # if stage < min_stage_to_clear: # continue # for step_id, step in steps.items(): # step_inputs = self.get_step_inputs(step_id) # empty = { k: None for k in step_inputs.keys() } # step_inputs.set_values(**empty) # step_outputs = self.get_step_outputs(step_id) # empty = { k: None for k in step_outputs.keys() } # step_outputs.set_values(**empty) pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return listeners \u00b6 PipelineListener ( ABC ) \u00b6 Source code in kiara/pipeline/listeners.py class PipelineListener ( abc . ABC ): def step_inputs_changed ( self , event : StepInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Arguments: event: the step input event \"\"\" def step_outputs_changed ( self , event : StepOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Arguments: event: the step output event \"\"\" def pipeline_inputs_changed ( self , event : PipelineInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. !!! note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the ``step_inputs_changed`` method if you want to. This behaviour might change in the future. Arguments: event: the pipeline input event \"\"\" def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Arguments: event: the pipeline output event \"\"\" pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/pipeline/listeners.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. !!! note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the ``step_inputs_changed`` method if you want to. This behaviour might change in the future. Arguments: event: the pipeline input event \"\"\" pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/listeners.py def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Arguments: event: the pipeline output event \"\"\" step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/listeners.py def step_inputs_changed ( self , event : StepInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Arguments: event: the step input event \"\"\" step_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Parameters: Name Type Description Default event StepOutputEvent the step output event required Source code in kiara/pipeline/listeners.py def step_outputs_changed ( self , event : StepOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Arguments: event: the step output event \"\"\" module \u00b6 PipelineModule ( KiaraModule ) \u00b6 A KiaraModule that contains a collection of interconnected other modules. Source code in kiara/pipeline/module.py class PipelineModule ( KiaraModule [ PipelineConfig ]): \"\"\"A [KiaraModule][kiara.module.KiaraModule] that contains a collection of interconnected other modules.\"\"\" _config_cls : typing . Type [ PipelineConfig ] = PipelineConfig # type: ignore _module_type_id = \"pipeline\" @classmethod def is_pipeline ( cls ) -> bool : return True def __init__ ( self , id : typing . Optional [ str ], parent_id : typing . Optional [ str ] = None , module_config : typing . Union [ None , PipelineConfig , typing . Mapping [ str , typing . Any ] ] = None , # controller: typing.Union[ # None, PipelineController, str, typing.Type[PipelineController] # ] = None, kiara : typing . Optional [ \"Kiara\" ] = None , ): # if controller is not None and not isinstance(controller, PipelineController): # raise NotImplementedError() # if controller is None: super () . __init__ ( id = id , parent_id = parent_id , module_config = module_config , kiara = kiara , ) self . _pipeline_structure : PipelineStructure = self . _create_structure () assert not self . _config . constants self . _config . constants = dict ( self . _pipeline_structure . constants ) @property def structure ( self ) -> PipelineStructure : \"\"\"The ``PipelineStructure`` of this module.\"\"\" return self . _pipeline_structure def _create_structure ( self ) -> PipelineStructure : pipeline_structure = PipelineStructure ( config = self . config , kiara = self . _kiara ) return pipeline_structure def create_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_input_schema def create_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_output_schema def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : from kiara import Pipeline # controller = BatchController(auto_process=False, kiara=self._kiara) pipeline = Pipeline ( structure = self . structure ) pipeline . inputs . set_values ( ** inputs ) if not pipeline . inputs . items_are_valid (): raise KiaraProcessingException ( f \"Can't start processing of { self . _module_type_id } pipeline: one or several inputs missing or invalid.\" ) # type: ignore if not pipeline . status == StepStatus . RESULTS_READY : # TODO: error details raise KiaraProcessingException ( f \"Error when running pipeline of type ' { self . _module_type_id } '.\" ) # type: ignore outputs . set_values ( ** pipeline . outputs ) structure : PipelineStructure property readonly \u00b6 The PipelineStructure of this module. create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/pipeline/module.py def create_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_input_schema create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/pipeline/module.py def create_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_output_schema is_pipeline () classmethod \u00b6 Check whether this module type is a pipeline, or not. Source code in kiara/pipeline/module.py @classmethod def is_pipeline ( cls ) -> bool : return True pipeline \u00b6 Pipeline \u00b6 An instance of a PipelineStructure that holds state for all of the inputs/outputs of the steps within. Source code in kiara/pipeline/pipeline.py class Pipeline ( object ): \"\"\"An instance of a [PipelineStructure][kiara.pipeline.structure.PipelineStructure] that holds state for all of the inputs/outputs of the steps within.\"\"\" def __init__ ( self , structure : PipelineStructure , # constants: typing.Optional[typing.Mapping[str, typing.Any]] = None, controller : typing . Optional [ PipelineController ] = None , title : typing . Optional [ str ] = None , ): self . _id : str = str ( uuid . uuid4 ()) if title is None : title = self . _id self . _title : str = title self . _structure : PipelineStructure = structure self . _pipeline_inputs : SlottedValueSet = None # type: ignore self . _pipeline_outputs : SlottedValueSet = None # type: ignore self . _step_inputs : typing . Mapping [ str , ValueSet ] = None # type: ignore self . _step_outputs : typing . Mapping [ str , ValueSet ] = None # type: ignore self . _value_refs : typing . Mapping [ ValueSlot , typing . Iterable [ ValueRef ]] = None # type: ignore self . _status : StepStatus = StepStatus . STALE self . _steps_by_stage : typing . Optional [ typing . Dict [ int , typing . Dict [ str , PipelineStep ]] ] = None self . _inputs_by_stage : typing . Optional [ typing . Dict [ int , typing . List [ str ]] ] = None self . _outputs_by_stage : typing . Optional [ typing . Dict [ int , typing . List [ str ]] ] = None self . _kiara : \"Kiara\" = self . _structure . _kiara self . _data_registry : DataRegistry = self . _kiara . data_registry self . _init_values () if controller is None : controller = BatchController ( self , kiara = self . _kiara ) else : controller . set_pipeline ( self ) self . _controller : PipelineController = controller self . _listeners : typing . List [ PipelineListener ] = [] self . _update_status () def __eq__ ( self , other ): if not isinstance ( other , Pipeline ): return False return self . _id == other . _id def __hash__ ( self ): return hash ( self . _id ) @property def id ( self ) -> str : return self . _id @property def title ( self ) -> str : return self . _title @property def structure ( self ) -> PipelineStructure : return self . _structure @property def controller ( self ) -> PipelineController : if self . _controller is None : raise Exception ( \"No controller set (yet).\" ) return self . _controller @property def inputs ( self ) -> SlottedValueSet : \"\"\"All (pipeline) input values of this pipeline.\"\"\" return self . _pipeline_inputs @property def outputs ( self ) -> SlottedValueSet : \"\"\"All (pipeline) output values of this pipeline.\"\"\" return self . _pipeline_outputs # def set_pipeline_inputs(self, **inputs: typing.Any): # self._controller.set_pipeline_inputs(**inputs) @property def step_ids ( self ) -> typing . Iterable [ str ]: \"\"\"Return all ids of the steps of this pipeline.\"\"\" return self . _structure . step_ids def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the object representing a step in this workflow, identified by the step id.\"\"\" return self . _structure . get_step ( step_id ) def get_steps_by_stage ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , PipelineStep ]]: \"\"\"Return a all pipeline steps, ordered by stage they belong to.\"\"\" if self . _steps_by_stage is not None : return self . _steps_by_stage result : typing . Dict [ int , typing . Dict [ str , PipelineStep ]] = {} for step_id in self . step_ids : step = self . get_step ( step_id ) stage = step . processing_stage assert stage is not None result . setdefault ( stage , {})[ step_id ] = step self . _steps_by_stage = result return self . _steps_by_stage def get_pipeline_inputs_by_stage ( self ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by stage they are first required.\"\"\" if self . _inputs_by_stage is not None : return self . _inputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . inputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepInputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _inputs_by_stage = result return self . _inputs_by_stage def get_pipeline_outputs_by_stage ( self , ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by the stage that needs to be executed before they are available.\"\"\" if self . _outputs_by_stage is not None : return self . _outputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . outputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepOutputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _outputs_by_stage = result return self . _outputs_by_stage def get_pipeline_inputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline inputs that are required for a stage to be processed. The result of this method does not include inputs that were required earlier already. \"\"\" return self . get_pipeline_inputs_by_stage () . get ( stage , []) def get_stage_for_pipeline_input ( self , input_name : str ) -> int : for stage , input_names in self . get_pipeline_inputs_by_stage () . items (): if input_name in input_names : return stage raise Exception ( f \"No input name ' { input_name } '. Available inputs: { ', ' . join ( self . inputs . keys ()) } \" ) def stage_for_pipeline_output ( self , output_name : str ) -> int : for stage , output_names in self . get_pipeline_outputs_by_stage () . items (): if output_name in output_names : return stage raise Exception ( f \"No output name ' { output_name } '. Available outputs: { ', ' . join ( self . outputs . keys ()) } \" ) def get_pipeline_outputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline outputs that are first available after the specified stage completed processing.\"\"\" return self . get_pipeline_outputs_by_stage () . get ( stage , []) def get_pipeline_inputs_for_step ( self , step_id : str ) -> typing . List [ str ]: result = [] for field_name , value_slot in self . inputs . _value_slots . items (): refs = self . _value_refs [ value_slot ] for ref in refs : if not isinstance ( ref , PipelineInputRef ): continue for ci in ref . connected_inputs : if ci . step_id == step_id and ref . value_name not in result : result . append ( ref . value_name ) return result def get_pipeline_outputs_for_step ( self , step_id : str ) -> typing . List [ str ]: result = [] for field_name , value_slot in self . outputs . _value_slots . items (): refs = self . _value_refs [ value_slot ] for ref in refs : if not isinstance ( ref , PipelineOutputRef ): continue if ( ref . connected_output . step_id == step_id and ref . value_name not in result ): result . append ( ref . value_name ) return result def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all inputs for a step id (incl. inputs that are not pipeline inputs but connected to other modules output).\"\"\" return self . _step_inputs [ step_id ] def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all outputs for a step id (incl. outputs that are not pipeline outputs).\"\"\" return self . _step_outputs [ step_id ] def add_listener ( self , listener : PipelineListener ) -> None : \"\"\"Add a listener taht gets notified on any internal pipeline input/output events.\"\"\" self . _listeners . append ( listener ) @property def status ( self ) -> StepStatus : \"\"\"Return the current status of this pipeline.\"\"\" return self . _state def _update_status ( self ): \"\"\"Make sure internal state variable is up to date.\"\"\" if self . inputs is None : new_state = StepStatus . STALE elif not self . inputs . items_are_valid (): new_state = StepStatus . STALE elif not self . outputs . items_are_valid (): new_state = StepStatus . INPUTS_READY else : new_state = StepStatus . RESULTS_READY self . _state = new_state def _init_values ( self ): \"\"\"Initialize this object. This should only be called once. Basically, this goes through all the inputs and outputs of all steps, and 'allocates' a PipelineValueInfo object for each of them. In case where output/input or pipeline-input/input points are connected, only one value item is allocated, since those refer to the same value. \"\"\" pipeline_inputs : typing . Dict [ str , ValueSlot ] = {} pipeline_outputs : typing . Dict [ str , ValueSlot ] = {} all_step_inputs : typing . Dict [ str , typing . Dict [ str , ValueSlot ]] = {} all_step_outputs : typing . Dict [ str , typing . Dict [ str , ValueSlot ]] = {} value_refs : typing . Dict [ ValueSlot , typing . List [ ValueRef ]] = {} # create the value objects that are associated with step outputs # all pipeline outputs are created here too, since the only place # those can be associated are step outputs for step_id , step_details in self . _structure . steps_details . items (): step_outputs : typing . Mapping [ str , StepOutputRef ] = step_details [ \"outputs\" ] for output_name , output_point in step_outputs . items (): init_output_value_item = self . _data_registry . register_data ( value_schema = output_point . value_schema ) output_value_slot = self . _data_registry . register_alias ( value_or_schema = init_output_value_item , callbacks = [ self ] ) value_refs . setdefault ( output_value_slot , []) . append ( output_point ) all_step_outputs . setdefault ( step_id , {})[ output_name ] = output_value_slot # not all step outputs necessarily need to be connected to a pipeline output if output_point . pipeline_output : pipeline_outputs [ output_point . pipeline_output ] = output_value_slot po = self . _structure . pipeline_outputs [ output_point . pipeline_output ] value_refs . setdefault ( output_value_slot , []) . append ( po ) # create the value objects that are associated with step inputs for step_id , step_details in self . _structure . steps_details . items (): step_inputs : typing . Mapping [ str , StepInputRef ] = step_details [ \"inputs\" ] for input_name , input_point in step_inputs . items (): # if this step input gets fed from a pipeline_input (meaning user input in most cases), # we need to create a DataValue for that pipeline input # vm = ValueMetadata( # origin=f\"{self.id}.steps.{step_id}.inputs.{input_point.value_name}\" # ) if input_point . connected_pipeline_input : connected_pipeline_input_name = input_point . connected_pipeline_input pipeline_input_field : PipelineInputRef = ( self . _structure . pipeline_inputs [ connected_pipeline_input_name ] ) pipeline_input_slot : ValueSlot = pipeline_inputs . get ( connected_pipeline_input_name , None ) if pipeline_input_slot is None : # if the pipeline input wasn't created by another step input before, # we need to take care of it here if pipeline_input_field . is_constant : init_value = self . structure . constants [ pipeline_input_field . value_name ] else : init_value = self . structure . defaults . get ( pipeline_input_field . value_name , SpecialValue . NOT_SET ) init_pipeline_input_value = self . _data_registry . register_data ( value_data = init_value , value_schema = pipeline_input_field . value_schema , ) # TODO: check whether it's a constant? pipeline_input_slot = self . _data_registry . register_alias ( value_or_schema = init_pipeline_input_value , callbacks = [ self ] ) value_refs . setdefault ( pipeline_input_slot , []) . append ( pipeline_input_field ) pipeline_inputs [ connected_pipeline_input_name ] = pipeline_input_slot all_step_inputs . setdefault ( step_id , {})[ input_name ] = pipeline_input_slot value_refs . setdefault ( pipeline_input_slot , []) . append ( input_point ) elif input_point . connected_outputs : for co in input_point . connected_outputs : if len ( input_point . connected_outputs ) == 1 and not co . sub_value : # this means the input is the same value as the connected output output_value : ValueSlot = all_step_outputs [ co . step_id ][ co . value_name ] all_step_inputs . setdefault ( input_point . step_id , {})[ input_point . value_name ] = output_value value_refs . setdefault ( output_value , []) . append ( input_point ) else : print ( input_point . connected_outputs ) raise NotImplementedError () # sub_value = co.sub_value # linked_values = {} # for co in input_point.connected_outputs: # output_value = all_step_outputs[co.step_id][co.value_name] # sub_value = co.sub_value # if len(input_point.connected_outputs) > 1 and not sub_value: # raise NotImplementedError() # sub_value = {\"config\": co.step_id} # if sub_value is not None: # raise NotImplementedError # # linked_values[output_value.id] = sub_value # # step_input = self._data_registry.register_linked_value( # parent_id=self.id, # linked_values=linked_values, # value_schema=input_point.value_schema, # value_refs=input_point, # ) # self._data_registry.register_callback( # self.values_updated, step_input # ) # all_step_inputs.setdefault(input_point.step_id, {})[ # input_point.value_name # ] = step_input else : raise Exception ( f \"Invalid value point type for this location: { input_point } \" ) if not pipeline_inputs : raise Exception ( f \"Can't init pipeline ' { self . title } ': no pipeline inputs\" ) self . _pipeline_inputs = SlottedValueSet ( items = pipeline_inputs , read_only = False , title = f \"Inputs for pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) if not pipeline_outputs : raise Exception ( f \"Can't init pipeline ' { self . title } ': no pipeline outputs\" ) self . _pipeline_outputs = SlottedValueSet ( items = pipeline_outputs , read_only = True , title = f \"Outputs for pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) self . _step_inputs = {} for step_id , inputs in all_step_inputs . items (): self . _step_inputs [ step_id ] = SlottedValueSet ( items = inputs , read_only = True , title = f \"Inputs for step ' { step_id } ' of pipeline ' { self . title } \" , kiara = self . _kiara , registry = self . _data_registry , ) self . _step_outputs = {} for step_id , outputs in all_step_outputs . items (): self . _step_outputs [ step_id ] = SlottedValueSet ( read_only = False , items = outputs , title = f \"Outputs for step ' { step_id } ' of pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) self . _value_refs = value_refs self . _steps_by_stage = None self . _inputs_by_stage = None def values_updated ( self , * items : ValueSlot ) -> None : updated_inputs : typing . Dict [ str , typing . List [ str ]] = {} updated_outputs : typing . Dict [ str , typing . List [ str ]] = {} updated_pipeline_inputs : typing . List [ str ] = [] updated_pipeline_outputs : typing . List [ str ] = [] # print(\"===================================================\") # for item in items: # print(item) # print(\"===================================================\") self . _update_status () if self . _value_refs is None : # means init is not finished yet return for item in items : # TODO: multiple value fields, also check pipeline id references = self . _value_refs . get ( item , None ) assert references for p in references : if isinstance ( p , StepInputRef ): updated_inputs . setdefault ( p . step_id , []) . append ( p . value_name ) elif isinstance ( p , StepOutputRef ): updated_outputs . setdefault ( p . step_id , []) . append ( p . value_name ) elif isinstance ( p , PipelineInputRef ): updated_pipeline_inputs . append ( p . value_name ) elif isinstance ( p , PipelineOutputRef ): updated_pipeline_outputs . append ( p . value_name ) else : raise TypeError ( f \"Can't update, invalid type: { type ( p ) } \" ) # print('========================================') # print('---') # print(\"Upaded pipeline input\") # print(updated_pipeline_inputs) # print('---') # print(\"Upaded step inputs\") # print(updated_inputs) # print('---') # print(\"Upaded step outputs\") # print(updated_outputs) # print('---') # print(\"Upaded pipeline outputs\") # print(updated_pipeline_outputs) if updated_pipeline_inputs : event_pi = PipelineInputEvent ( pipeline_id = self . id , updated_pipeline_inputs = updated_pipeline_inputs , ) self . _controller . pipeline_inputs_changed ( event_pi ) self . _notify_pipeline_listeners ( event_pi ) if updated_outputs : event_so = StepOutputEvent ( pipeline_id = self . id , updated_step_outputs = updated_outputs , ) self . _controller . step_outputs_changed ( event_so ) self . _notify_pipeline_listeners ( event_so ) if updated_inputs : event_si = StepInputEvent ( pipeline_id = self . id , updated_step_inputs = updated_inputs , ) self . _controller . step_inputs_changed ( event_si ) self . _notify_pipeline_listeners ( event_si ) if updated_pipeline_outputs : event_po = PipelineOutputEvent ( pipeline_id = self . id , updated_pipeline_outputs = updated_pipeline_outputs , ) self . _controller . pipeline_outputs_changed ( event_po ) self . _notify_pipeline_listeners ( event_po ) def _notify_pipeline_listeners ( self , event : StepEvent ): for listener in self . _listeners : if event . type == \"step_input\" : # type: ignore listener . step_inputs_changed ( event ) # type: ignore elif event . type == \"step_output\" : # type: ignore listener . step_outputs_changed ( event ) # type: ignore elif event . type == \"pipeline_input\" : # type: ignore listener . pipeline_inputs_changed ( event ) # type: ignore elif event . type == \"pipeline_output\" : # type: ignore listener . pipeline_outputs_changed ( event ) # type: ignore else : raise Exception ( f \"Unsupported type: { event . type } \" ) # type: ignore def get_current_state ( self ) -> \"PipelineState\" : step_inputs = {} step_states = {} for k , v in self . _step_inputs . items (): step_inputs [ k ] = PipelineValuesInfo . from_value_set ( v ) if v . items_are_valid (): step_states [ k ] = StepStatus . INPUTS_READY else : step_states [ k ] = StepStatus . STALE step_outputs = {} for k , v in self . _step_outputs . items (): step_outputs [ k ] = PipelineValuesInfo . from_value_set ( v ) if v . items_are_valid (): step_states [ k ] = StepStatus . RESULTS_READY from kiara.info.pipelines import PipelineState state = PipelineState ( structure = self . structure . to_details (), pipeline_inputs = self . _pipeline_inputs . to_details (), pipeline_outputs = self . _pipeline_outputs . to_details (), step_states = step_states , step_inputs = step_inputs , step_outputs = step_outputs , status = self . status , ) return state def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield self . get_current_state () inputs : SlottedValueSet property readonly \u00b6 All (pipeline) input values of this pipeline. outputs : SlottedValueSet property readonly \u00b6 All (pipeline) output values of this pipeline. status : StepStatus property readonly \u00b6 Return the current status of this pipeline. step_ids : Iterable [ str ] property readonly \u00b6 Return all ids of the steps of this pipeline. add_listener ( self , listener ) \u00b6 Add a listener taht gets notified on any internal pipeline input/output events. Source code in kiara/pipeline/pipeline.py def add_listener ( self , listener : PipelineListener ) -> None : \"\"\"Add a listener taht gets notified on any internal pipeline input/output events.\"\"\" self . _listeners . append ( listener ) get_pipeline_inputs_by_stage ( self ) \u00b6 Return a list of pipeline input names, ordered by stage they are first required. Source code in kiara/pipeline/pipeline.py def get_pipeline_inputs_by_stage ( self ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by stage they are first required.\"\"\" if self . _inputs_by_stage is not None : return self . _inputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . inputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepInputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _inputs_by_stage = result return self . _inputs_by_stage get_pipeline_inputs_for_stage ( self , stage ) \u00b6 Return a list of pipeline inputs that are required for a stage to be processed. The result of this method does not include inputs that were required earlier already. Source code in kiara/pipeline/pipeline.py def get_pipeline_inputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline inputs that are required for a stage to be processed. The result of this method does not include inputs that were required earlier already. \"\"\" return self . get_pipeline_inputs_by_stage () . get ( stage , []) get_pipeline_outputs_by_stage ( self ) \u00b6 Return a list of pipeline input names, ordered by the stage that needs to be executed before they are available. Source code in kiara/pipeline/pipeline.py def get_pipeline_outputs_by_stage ( self , ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by the stage that needs to be executed before they are available.\"\"\" if self . _outputs_by_stage is not None : return self . _outputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . outputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepOutputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _outputs_by_stage = result return self . _outputs_by_stage get_pipeline_outputs_for_stage ( self , stage ) \u00b6 Return a list of pipeline outputs that are first available after the specified stage completed processing. Source code in kiara/pipeline/pipeline.py def get_pipeline_outputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline outputs that are first available after the specified stage completed processing.\"\"\" return self . get_pipeline_outputs_by_stage () . get ( stage , []) get_step ( self , step_id ) \u00b6 Return the object representing a step in this workflow, identified by the step id. Source code in kiara/pipeline/pipeline.py def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the object representing a step in this workflow, identified by the step id.\"\"\" return self . _structure . get_step ( step_id ) get_step_inputs ( self , step_id ) \u00b6 Return all inputs for a step id (incl. inputs that are not pipeline inputs but connected to other modules output). Source code in kiara/pipeline/pipeline.py def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all inputs for a step id (incl. inputs that are not pipeline inputs but connected to other modules output).\"\"\" return self . _step_inputs [ step_id ] get_step_outputs ( self , step_id ) \u00b6 Return all outputs for a step id (incl. outputs that are not pipeline outputs). Source code in kiara/pipeline/pipeline.py def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all outputs for a step id (incl. outputs that are not pipeline outputs).\"\"\" return self . _step_outputs [ step_id ] get_steps_by_stage ( self ) \u00b6 Return a all pipeline steps, ordered by stage they belong to. Source code in kiara/pipeline/pipeline.py def get_steps_by_stage ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , PipelineStep ]]: \"\"\"Return a all pipeline steps, ordered by stage they belong to.\"\"\" if self . _steps_by_stage is not None : return self . _steps_by_stage result : typing . Dict [ int , typing . Dict [ str , PipelineStep ]] = {} for step_id in self . step_ids : step = self . get_step ( step_id ) stage = step . processing_stage assert stage is not None result . setdefault ( stage , {})[ step_id ] = step self . _steps_by_stage = result return self . _steps_by_stage structure \u00b6 PipelineStep ( BaseModel ) pydantic-model \u00b6 A step within a pipeline-structure, includes information about it's connection(s) and other metadata. Source code in kiara/pipeline/structure.py class PipelineStep ( BaseModel ): \"\"\"A step within a pipeline-structure, includes information about it's connection(s) and other metadata.\"\"\" class Config : validate_assignment = True extra = Extra . forbid @classmethod def create_steps ( cls , * steps : \"PipelineStepConfig\" , kiara : \"Kiara\" ) -> typing . List [ \"PipelineStep\" ]: result : typing . List [ PipelineStep ] = [] if kiara is None : from kiara.module import Kiara kiara = Kiara . instance () for step in steps : _s = PipelineStep ( step_id = step . step_id , module_type = step . module_type , module_config = copy . deepcopy ( step . module_config ), input_links = copy . deepcopy ( step . input_links ), _kiara = kiara , # type: ignore ) result . append ( _s ) return result _module : typing . Optional [ \"KiaraModule\" ] = PrivateAttr ( default = None ) @validator ( \"step_id\" ) def _validate_step_id ( cls , v ): assert isinstance ( v , str ) if \".\" in v : raise ValueError ( \"Step ids can't contain '.' characters.\" ) return v step_id : str module_type : str = Field ( description = \"The module type.\" ) module_config : typing . Mapping [ str , typing . Any ] = Field ( description = \"The module config.\" , default_factory = dict ) required : bool = Field ( description = \"Whether this step is required within the workflow. \\n\\n In some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required.\" , default = True , ) processing_stage : typing . Optional [ int ] = Field ( default = None , description = \"The stage number this step is executed within the pipeline.\" , ) input_links : typing . Mapping [ str , typing . List [ StepValueAddress ]] = Field ( description = \"The links that connect to inputs of the module.\" , default_factory = list , ) _kiara : typing . Optional [ \"Kiara\" ] = PrivateAttr ( default = None ) _id : str = PrivateAttr () def __init__ ( self , ** data ): # type: ignore self . _id = str ( uuid . uuid4 ()) kiara = data . pop ( \"_kiara\" , None ) if kiara is None : from kiara import Kiara kiara = Kiara . instance () super () . __init__ ( ** data ) self . _kiara : \"Kiara\" = kiara @property def kiara ( self ): return self . _kiara @property def module ( self ) -> \"KiaraModule\" : if self . _module is None : try : if ( self . module_type in self . kiara . operation_mgmt . profiles . keys () and not self . module_config ): op = self . kiara . operation_mgmt . profiles [ self . module_type ] self . _module = op . module else : self . _module = self . kiara . create_module ( id = self . step_id , module_type = self . module_type , module_config = self . module_config , ) except Exception as e : raise Exception ( f \"Can't assemble pipeline step ' { self . step_id } ': { e } \" ) return self . _module def __eq__ ( self , other ): if not isinstance ( other , PipelineStep ): return False return self . _id == other . _id # # TODO: also check whether _kiara obj is equal? # eq = (self.step_id, self.parent_id, self.module, self.processing_stage,) == ( # other.step_id, # other.parent_id, # other.module, # other.processing_stage, # ) # # if not eq: # return False # # hs = DeepHash(self.input_links) # ho = DeepHash(other.input_links) # # return hs[self.input_links] == ho[other.input_links] def __hash__ ( self ): return hash ( self . _id ) # # TODO: also include _kiara obj? # # TODO: figure out whether that can be made to work without deephash # hs = DeepHash(self.input_links) # return hash( # ( # self.step_id, # self.parent_id, # self.module, # self.processing_stage, # hs[self.input_links], # ) # ) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (step_id= { self . step_id } module_type= { self . module_type } processing_stage= { self . processing_stage } )\" def __str__ ( self ): return f \"step: { self . step_id } (module: { self . module_type } )\" input_links : Mapping [ str , List [ kiara . pipeline . StepValueAddress ]] pydantic-field \u00b6 The links that connect to inputs of the module. module_config : Mapping [ str , Any ] pydantic-field \u00b6 The module config. module_type : str pydantic-field required \u00b6 The module type. processing_stage : int pydantic-field \u00b6 The stage number this step is executed within the pipeline. required : bool pydantic-field \u00b6 Whether this step is required within the workflow. In some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/pipeline/structure.py def __eq__ ( self , other ): if not isinstance ( other , PipelineStep ): return False return self . _id == other . _id # # TODO: also check whether _kiara obj is equal? # eq = (self.step_id, self.parent_id, self.module, self.processing_stage,) == ( # other.step_id, # other.parent_id, # other.module, # other.processing_stage, # ) # # if not eq: # return False # # hs = DeepHash(self.input_links) # ho = DeepHash(other.input_links) # # return hs[self.input_links] == ho[other.input_links] __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/pipeline/structure.py def __hash__ ( self ): return hash ( self . _id ) # # TODO: also include _kiara obj? # # TODO: figure out whether that can be made to work without deephash # hs = DeepHash(self.input_links) # return hash( # ( # self.step_id, # self.parent_id, # self.module, # self.processing_stage, # hs[self.input_links], # ) # ) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/pipeline/structure.py def __repr__ ( self ): return f \" { self . __class__ . __name__ } (step_id= { self . step_id } module_type= { self . module_type } processing_stage= { self . processing_stage } )\" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/pipeline/structure.py def __str__ ( self ): return f \"step: { self . step_id } (module: { self . module_type } )\" PipelineStructure \u00b6 An object that holds one or several steps, and describes the connections between them. Source code in kiara/pipeline/structure.py class PipelineStructure ( object ): \"\"\"An object that holds one or several steps, and describes the connections between them.\"\"\" def __init__ ( self , config : \"PipelineConfig\" , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _structure_config : \"PipelineConfig\" = config steps = self . _structure_config . steps input_aliases = self . _structure_config . input_aliases output_aliases = self . _structure_config . output_aliases if not steps : raise Exception ( \"No steps provided.\" ) if kiara is None : kiara = Kiara . instance () self . _kiara : Kiara = kiara self . _steps : typing . List [ PipelineStep ] = PipelineStep . create_steps ( * steps , kiara = self . _kiara ) # self._pipeline_id: str = parent_id if input_aliases is None : input_aliases = {} if isinstance ( input_aliases , str ): if input_aliases not in ALLOWED_INPUT_ALIAS_MARKERS : raise Exception ( f \"Can't create pipeline, invalid value ' { input_aliases } ' for 'input_aliases'. Either specify a dict, or use one of: { ', ' . join ( ALLOWED_INPUT_ALIAS_MARKERS ) } \" ) input_aliases = calculate_shortest_field_aliases ( self . _steps , input_aliases , \"inputs\" ) if isinstance ( output_aliases , str ): if output_aliases not in ALLOWED_INPUT_ALIAS_MARKERS : raise Exception ( f \"Can't create pipeline, invalid value ' { output_aliases } ' for 'output_aliases'. Either specify a dict, or use one of: { ', ' . join ( ALLOWED_INPUT_ALIAS_MARKERS ) } \" ) output_aliases = calculate_shortest_field_aliases ( self . _steps , output_aliases , \"outputs\" ) self . _input_aliases : typing . Dict [ str , str ] = dict ( input_aliases ) # type: ignore if output_aliases is None : output_aliases = {} self . _output_aliases : typing . Dict [ str , str ] = dict ( output_aliases ) # type: ignore # this is hardcoded for now self . _add_all_workflow_outputs : bool = False self . _constants : typing . Dict [ str , typing . Any ] = None # type: ignore self . _defaults : typing . Dict [ str , typing . Any ] = None # type: ignore self . _execution_graph : nx . DiGraph = None # type: ignore self . _data_flow_graph : nx . DiGraph = None # type: ignore self . _data_flow_graph_simple : nx . DiGraph = None # type: ignore self . _processing_stages : typing . List [ typing . List [ str ]] = None # type: ignore self . _steps_details : typing . Dict [ str , typing . Any ] = None # type: ignore \"\"\"Holds details about the (current) processing steps contained in this workflow.\"\"\" # @property # def pipeline_id(self) -> str: # return self._pipeline_id @property def structure_config ( self ) -> \"PipelineConfig\" : return self . _structure_config @property def steps ( self ) -> typing . Iterable [ PipelineStep ]: return self . _steps @property def modules ( self ) -> typing . Iterable [ \"KiaraModule\" ]: return ( s . module for s in self . steps ) @property def steps_details ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _steps_details is None : self . _process_steps () return self . _steps_details @property def step_ids ( self ) -> typing . Iterable [ str ]: if self . _steps_details is None : self . _process_steps () return self . _steps_details . keys () @property def constants ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _constants is None : self . _process_steps () return self . _constants @property def defaults ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _defaults is None : self . _process_steps () return self . _defaults def get_step ( self , step_id : str ) -> PipelineStep : d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"step\" ] def get_step_inputs ( self , step_id : str ) -> typing . Mapping [ str , StepInputRef ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"inputs\" ] def get_step_outputs ( self , step_id : str ) -> typing . Mapping [ str , StepOutputRef ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"outputs\" ] def get_step_details ( self , step_id : str ) -> typing . Mapping [ str , typing . Any ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d @property def execution_graph ( self ) -> nx . DiGraph : if self . _execution_graph is None : self . _process_steps () return self . _execution_graph @property def data_flow_graph ( self ) -> nx . DiGraph : if self . _data_flow_graph is None : self . _process_steps () return self . _data_flow_graph @property def data_flow_graph_simple ( self ) -> nx . DiGraph : if self . _data_flow_graph_simple is None : self . _process_steps () return self . _data_flow_graph_simple @property def processing_stages ( self ) -> typing . List [ typing . List [ str ]]: if self . _steps_details is None : self . _process_steps () return self . _processing_stages @lru_cache () def _get_node_of_type ( self , node_type : str ): if self . _steps_details is None : self . _process_steps () return [ node for node , attr in self . _data_flow_graph . nodes ( data = True ) if attr [ \"type\" ] == node_type ] @property def steps_inputs ( self ) -> typing . Dict [ str , StepInputRef ]: return { node . alias : node for node in self . _get_node_of_type ( node_type = StepInputRef . __name__ ) } @property def steps_outputs ( self ) -> typing . Dict [ str , StepOutputRef ]: return { node . alias : node for node in self . _get_node_of_type ( node_type = StepOutputRef . __name__ ) } @property def pipeline_inputs ( self ) -> typing . Dict [ str , PipelineInputRef ]: return { node . value_name : node for node in self . _get_node_of_type ( node_type = PipelineInputRef . __name__ ) } @property def pipeline_outputs ( self ) -> typing . Dict [ str , PipelineOutputRef ]: return { node . value_name : node for node in self . _get_node_of_type ( node_type = PipelineOutputRef . __name__ ) } @property def pipeline_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return { input_name : w_in . value_schema for input_name , w_in in self . pipeline_inputs . items () } @property def pipeline_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return { output_name : w_out . value_schema for output_name , w_out in self . pipeline_outputs . items () } def _process_steps ( self ): \"\"\"The core method of this class, it connects all the processing modules, their inputs and outputs.\"\"\" steps_details : typing . Dict [ str , typing . Any ] = {} execution_graph = nx . DiGraph () execution_graph . add_node ( \"__root__\" ) data_flow_graph = nx . DiGraph () data_flow_graph_simple = nx . DiGraph () processing_stages = [] constants = {} structure_defaults = {} # temp variable, to hold all outputs outputs : typing . Dict [ str , StepOutputRef ] = {} # process all pipeline and step outputs first _temp_steps_map : typing . Dict [ str , PipelineStep ] = {} pipeline_outputs : typing . Dict [ str , PipelineOutputRef ] = {} for step in self . _steps : _temp_steps_map [ step . step_id ] = step if step . step_id in steps_details . keys (): raise Exception ( f \"Can't process steps: duplicate step_id ' { step . step_id } '\" ) steps_details [ step . step_id ] = { \"step\" : step , \"outputs\" : {}, \"inputs\" : {}, } data_flow_graph . add_node ( step , type = \"step\" ) # go through all the module outputs, create points for them and connect them to pipeline outputs for output_name , schema in step . module . output_schemas . items (): step_output = StepOutputRef ( value_name = output_name , value_schema = schema , step_id = step . step_id , ) steps_details [ step . step_id ][ \"outputs\" ][ output_name ] = step_output step_alias = generate_step_alias ( step . step_id , output_name ) outputs [ step_alias ] = step_output step_output_name = generate_pipeline_endpoint_name ( step_id = step . step_id , value_name = output_name ) if self . _output_aliases : if step_output_name in self . _output_aliases . keys (): step_output_name = self . _output_aliases [ step_output_name ] else : if not self . _add_all_workflow_outputs : # this output is not interesting for the workflow step_output_name = None if step_output_name : step_output_address = StepValueAddress ( step_id = step . step_id , value_name = output_name ) pipeline_output = PipelineOutputRef ( value_name = step_output_name , connected_output = step_output_address , value_schema = schema , ) pipeline_outputs [ step_output_name ] = pipeline_output step_output . pipeline_output = pipeline_output . value_name data_flow_graph . add_node ( pipeline_output , type = PipelineOutputRef . __name__ ) data_flow_graph . add_edge ( step_output , pipeline_output ) data_flow_graph_simple . add_node ( pipeline_output , type = PipelineOutputRef . __name__ ) data_flow_graph_simple . add_edge ( step , pipeline_output ) data_flow_graph . add_node ( step_output , type = StepOutputRef . __name__ ) data_flow_graph . add_edge ( step , step_output ) # now process inputs, and connect them to the appropriate output/pipeline-input points existing_pipeline_input_points : typing . Dict [ str , PipelineInputRef ] = {} for step in self . _steps : other_step_dependency : typing . Set = set () # go through all the inputs of a module, create input points and connect them to either # other module outputs, or pipeline inputs (which need to be created) module_constants : typing . Mapping [ str , typing . Any ] = step . module . get_config_value ( \"constants\" ) for input_name , schema in step . module . input_schemas . items (): matching_input_links : typing . List [ StepValueAddress ] = [] is_constant = input_name in module_constants . keys () for value_name , input_links in step . input_links . items (): if value_name == input_name : for input_link in input_links : if input_link in matching_input_links : raise Exception ( f \"Duplicate input link: { input_link } \" ) matching_input_links . append ( input_link ) if matching_input_links : # this means we connect to other steps output connected_output_points : typing . List [ StepOutputRef ] = [] connected_outputs : typing . List [ StepValueAddress ] = [] for input_link in matching_input_links : output_id = generate_step_alias ( input_link . step_id , input_link . value_name ) if output_id not in outputs . keys (): raise Exception ( f \"Can't connect input ' { input_name } ' for step ' { step . step_id } ': no output ' { output_id } ' available. Available output names: { ', ' . join ( outputs . keys ()) } \" ) connected_output_points . append ( outputs [ output_id ]) connected_outputs . append ( input_link ) other_step_dependency . add ( input_link . step_id ) step_input_point = StepInputRef ( step_id = step . step_id , value_name = input_name , value_schema = schema , is_constant = is_constant , connected_pipeline_input = None , connected_outputs = connected_outputs , ) for op in connected_output_points : op . connected_inputs . append ( step_input_point . address ) data_flow_graph . add_edge ( op , step_input_point ) data_flow_graph_simple . add_edge ( _temp_steps_map [ op . step_id ], step_input_point ) # TODO: name edge data_flow_graph_simple . add_edge ( step_input_point , step ) # TODO: name edge else : # this means we connect to pipeline input pipeline_input_name = generate_pipeline_endpoint_name ( step_id = step . step_id , value_name = input_name ) # check whether this input has an alias associated with it if self . _input_aliases : if pipeline_input_name in self . _input_aliases . keys (): # this means we use the pipeline alias pipeline_input_name = self . _input_aliases [ pipeline_input_name ] if pipeline_input_name in existing_pipeline_input_points . keys (): # we already created a pipeline input with this name # TODO: check whether schema fits connected_pipeline_input = existing_pipeline_input_points [ pipeline_input_name ] assert connected_pipeline_input . is_constant == is_constant else : # we need to create the pipeline input connected_pipeline_input = PipelineInputRef ( value_name = pipeline_input_name , value_schema = schema , is_constant = is_constant , ) existing_pipeline_input_points [ pipeline_input_name ] = connected_pipeline_input data_flow_graph . add_node ( connected_pipeline_input , type = PipelineInputRef . __name__ ) data_flow_graph_simple . add_node ( connected_pipeline_input , type = PipelineInputRef . __name__ ) if is_constant : constants [ pipeline_input_name ] = step . module . get_config_value ( \"constants\" )[ input_name ] default_val = step . module . get_config_value ( \"defaults\" ) . get ( input_name , None ) if is_constant and default_val is not None : raise Exception ( f \"Module config invalid for step ' { step . step_id } ': both default value and constant provided for input ' { input_name } '.\" ) elif default_val is not None : structure_defaults [ pipeline_input_name ] = default_val step_input_point = StepInputRef ( step_id = step . step_id , value_name = input_name , value_schema = schema , connected_pipeline_input = connected_pipeline_input . value_name , connected_outputs = None , ) connected_pipeline_input . connected_inputs . append ( step_input_point . address ) data_flow_graph . add_edge ( connected_pipeline_input , step_input_point ) data_flow_graph_simple . add_edge ( connected_pipeline_input , step ) data_flow_graph . add_node ( step_input_point , type = StepInputRef . __name__ ) steps_details [ step . step_id ][ \"inputs\" ][ input_name ] = step_input_point data_flow_graph . add_edge ( step_input_point , step ) if other_step_dependency : for module_id in other_step_dependency : execution_graph . add_edge ( module_id , step . step_id ) else : execution_graph . add_edge ( \"__root__\" , step . step_id ) # calculate execution order path_lengths : typing . Dict [ str , int ] = {} for step in self . _steps : step_id = step . step_id paths = list ( nx . all_simple_paths ( execution_graph , \"__root__\" , step_id )) max_steps = max ( paths , key = lambda x : len ( x )) path_lengths [ step_id ] = len ( max_steps ) - 1 max_length = max ( path_lengths . values ()) for i in range ( 1 , max_length + 1 ): stage : typing . List [ str ] = [ m for m , length in path_lengths . items () if length == i ] processing_stages . append ( stage ) for _step_id in stage : steps_details [ _step_id ][ \"processing_stage\" ] = i steps_details [ _step_id ][ \"step\" ] . processing_stage = i self . _constants = constants self . _defaults = structure_defaults self . _steps_details = steps_details self . _execution_graph = execution_graph self . _data_flow_graph = data_flow_graph self . _data_flow_graph_simple = data_flow_graph_simple self . _processing_stages = processing_stages self . _get_node_of_type . cache_clear () # calculating which steps are always required to execute to compute one of the required pipeline outputs. # this is done because in some cases it's possible that some steps can be skipped to execute if they # don't have a valid input set, because the inputs downstream they are connecting to are 'non-required' # optional_steps = [] last_stage = self . _processing_stages [ - 1 ] step_nodes : typing . List [ PipelineStep ] = [ node for node in self . _data_flow_graph_simple . nodes if isinstance ( node , PipelineStep ) ] all_required_inputs = [] for step_id in last_stage : step = self . get_step ( step_id ) step_nodes . remove ( step ) for k , s_inp in self . get_step_inputs ( step_id ) . items (): if not s_inp . value_schema . is_required (): continue all_required_inputs . append ( s_inp ) for pipeline_input in self . pipeline_inputs . values (): for last_step_input in all_required_inputs : try : path = nx . shortest_path ( self . _data_flow_graph_simple , pipeline_input , last_step_input ) for p in path : if p in step_nodes : step_nodes . remove ( p ) except ( NetworkXNoPath , NodeNotFound ): pass # print(\"NO PATH\") # print(f\"{pipeline_input} -> {last_step_input}\") for s in step_nodes : s . required = False for input_name , inp in self . pipeline_inputs . items (): steps = set () for ci in inp . connected_inputs : steps . add ( ci . step_id ) optional = True for step_id in steps : step = self . get_step ( step_id ) if step . required : optional = False break if optional : inp . value_schema . optional = True def extend ( self , other : typing . Union [ \"Pipeline\" , \"PipelineStructure\" , \"PipelineConfig\" , typing . Mapping [ str , typing . Any ], ], input_links : typing . Optional [ typing . Mapping [ str , typing . Iterable [ StepValueAddress ]] ] = None , ) -> \"PipelineStructure\" : return extend_pipeline ( self , other ) def to_details ( self ) -> \"PipelineStructureDesc\" : from kiara.info.pipelines import PipelineStructureDesc return PipelineStructureDesc . create_pipeline_structure_desc ( pipeline = self ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : d = self . to_details () yield d values \u00b6 PipelineInputRef ( ValueRef ) pydantic-model \u00b6 An input to a pipeline. Source code in kiara/pipeline/values.py class PipelineInputRef ( ValueRef ): \"\"\"An input to a pipeline.\"\"\" connected_inputs : typing . List [ StepValueAddress ] = Field ( description = \"The step inputs that are connected to this pipeline input\" , default_factory = list , ) is_constant : bool = Field ( \"Whether this input is a constant and can't be changed by the user.\" ) @property def alias ( self ) -> str : return generate_step_alias ( PIPELINE_PARENT_MARKER , self . value_name ) connected_inputs : List [ kiara . pipeline . StepValueAddress ] pydantic-field \u00b6 The step inputs that are connected to this pipeline input PipelineOutputRef ( ValueRef ) pydantic-model \u00b6 An output to a pipeline. Source code in kiara/pipeline/values.py class PipelineOutputRef ( ValueRef ): \"\"\"An output to a pipeline.\"\"\" connected_output : StepValueAddress = Field ( description = \"Connected step outputs.\" ) @property def alias ( self ) -> str : return generate_step_alias ( PIPELINE_PARENT_MARKER , self . value_name ) connected_output : StepValueAddress pydantic-field required \u00b6 Connected step outputs. StepInputRef ( ValueRef ) pydantic-model \u00b6 An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both. Source code in kiara/pipeline/values.py class StepInputRef ( ValueRef ): \"\"\"An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both. \"\"\" step_id : str = Field ( description = \"The step id.\" ) connected_outputs : typing . Optional [ typing . List [ StepValueAddress ]] = Field ( default = None , description = \"A potential connected list of one or several module outputs.\" , ) connected_pipeline_input : typing . Optional [ str ] = Field ( default = None , description = \"A potential pipeline input.\" ) is_constant : bool = Field ( \"Whether this input is a constant and can't be changed by the user.\" ) @root_validator ( pre = True ) def ensure_single_connected_item ( cls , values ): if values . get ( \"connected_outputs\" , None ) and values . get ( \"connected_pipeline_input\" ): raise ValueError ( \"Multiple connected items, only one allowed.\" ) return values @property def alias ( self ) -> str : return generate_step_alias ( self . step_id , self . value_name ) @property def address ( self ) -> StepValueAddress : return StepValueAddress ( step_id = self . step_id , value_name = self . value_name ) def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . step_id } . { self . value_name } ( { self . value_schema . type } )\" connected_outputs : List [ kiara . pipeline . StepValueAddress ] pydantic-field \u00b6 A potential connected list of one or several module outputs. connected_pipeline_input : str pydantic-field \u00b6 A potential pipeline input. step_id : str pydantic-field required \u00b6 The step id. StepOutputRef ( ValueRef ) pydantic-model \u00b6 An output to a step. Source code in kiara/pipeline/values.py class StepOutputRef ( ValueRef ): \"\"\"An output to a step.\"\"\" class Config : allow_mutation = True step_id : str = Field ( description = \"The step id.\" ) pipeline_output : typing . Optional [ str ] = Field ( description = \"The connected pipeline output.\" ) connected_inputs : typing . List [ StepValueAddress ] = Field ( description = \"The step inputs that are connected to this step output\" , default_factory = list , ) @property def alias ( self ) -> str : return generate_step_alias ( self . step_id , self . value_name ) @property def address ( self ) -> StepValueAddress : return StepValueAddress ( step_id = self . step_id , value_name = self . value_name ) def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . step_id } . { self . value_name } ( { self . value_schema . type } )\" connected_inputs : List [ kiara . pipeline . StepValueAddress ] pydantic-field \u00b6 The step inputs that are connected to this step output pipeline_output : str pydantic-field \u00b6 The connected pipeline output. step_id : str pydantic-field required \u00b6 The step id. ValueRef ( BaseModel ) pydantic-model \u00b6 An object that holds information about the location of a value within a pipeline (or other structure). Basically, a ValueRef helps the containing object where in its structure the value belongs (for example so it can update dependent other values). A ValueRef object (obviously) does not contain the value itself. There are four different ValueRef type that are relevant for pipelines: kiara.pipeline.values.StepInputRef : an input to a step kiara.pipeline.values.StepOutputRef : an output of a step kiara.pipeline.values.PipelineInputRef : an input to a pipeline kiara.pipeline.values.PipelineOutputRef : an output for a pipeline Several ValueRef objects can target the same value, for example a step output and a connected step input would reference the same Value (in most cases).. Source code in kiara/pipeline/values.py class ValueRef ( BaseModel ): \"\"\"An object that holds information about the location of a value within a pipeline (or other structure). Basically, a `ValueRef` helps the containing object where in its structure the value belongs (for example so it can update dependent other values). A `ValueRef` object (obviously) does not contain the value itself. There are four different ValueRef type that are relevant for pipelines: - [kiara.pipeline.values.StepInputRef][]: an input to a step - [kiara.pipeline.values.StepOutputRef][]: an output of a step - [kiara.pipeline.values.PipelineInputRef][]: an input to a pipeline - [kiara.pipeline.values.PipelineOutputRef][]: an output for a pipeline Several `ValueRef` objects can target the same value, for example a step output and a connected step input would reference the same `Value` (in most cases).. \"\"\" class Config : allow_mutation = True extra = Extra . forbid _id : uuid . UUID = PrivateAttr ( default_factory = uuid . uuid4 ) value_name : str value_schema : ValueSchema # pipeline_id: str def __eq__ ( self , other ): if not isinstance ( other , self . __class__ ): return False return self . _id == other . _id def __hash__ ( self ): return hash ( self . _id ) def __repr__ ( self ): step_id = \"\" if hasattr ( self , \"step_id\" ): step_id = f \" step_id=' { self . step_id } '\" return f \" { self . __class__ . __name__ } (value_name=' { self . value_name } ' { step_id } )\" def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . value_name } ( { self . value_schema . type } )\" __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/pipeline/values.py def __eq__ ( self , other ): if not isinstance ( other , self . __class__ ): return False return self . _id == other . _id __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/pipeline/values.py def __hash__ ( self ): return hash ( self . _id ) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/pipeline/values.py def __repr__ ( self ): step_id = \"\" if hasattr ( self , \"step_id\" ): step_id = f \" step_id=' { self . step_id } '\" return f \" { self . __class__ . __name__ } (value_name=' { self . value_name } ' { step_id } )\" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/pipeline/values.py def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . value_name } ( { self . value_schema . type } )\"","title":"pipeline"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.PipelineValueInfo","text":"Convenience wrapper to make the PipelineState json/dict export prettier. Source code in kiara/pipeline/__init__.py class PipelineValueInfo ( BaseModel ): \"\"\"Convenience wrapper to make the [PipelineState][kiara.pipeline.pipeline.PipelineState] json/dict export prettier.\"\"\" @classmethod def from_value_obj ( cls , value : Value , ensure_metadata : bool = False ): if ensure_metadata : value . get_metadata () return PipelineValueInfo ( id = value . id , value_schema = value . value_schema , is_valid = value . item_is_valid (), is_set = value . is_set , status = value . item_status (), # value_metadata=value.value_metadata, # last_update=value.last_update, # value_hash=value.value_hash, # is_streaming=value.is_streaming, metadata = value . metadata , ) class Config : extra = Extra . forbid allow_mutation = False id : str = Field ( description = \"A unique id for this value.\" ) is_valid : bool = Field ( description = \"Whether the value is set and valid.\" , default = False ) status : str = Field ( description = \"The value status string\" ) is_set : bool = Field ( description = \"Whether the value is set.\" ) value_schema : ValueSchema = Field ( description = \"The schema of this value.\" ) # is_constant: bool = Field( # description=\"Whether this value is a constant.\", default=False # ) # value_metadata: ValueMetadata = Field( # description=\"The metadata of the value itself (not the actual data).\" # ) # last_update: datetime = Field( # default=None, description=\"The time the last update to this value happened.\" # ) # value_hash: typing.Union[ValueHashMarker, int] = Field( # description=\"The hash of the current value.\" # ) # is_streaming: bool = Field( # default=False, # description=\"Whether the value is currently streamed into this object.\", # ) metadata : typing . Dict [ str , typing . Any ] = Field ( description = \"Metadata relating to the actual data (size, no. of rows, etc. -- depending on data type).\" , default_factory = dict , )","title":"PipelineValueInfo"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.PipelineValueInfo.id","text":"A unique id for this value.","title":"id"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.PipelineValueInfo.is_set","text":"Whether the value is set.","title":"is_set"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.PipelineValueInfo.is_valid","text":"Whether the value is set and valid.","title":"is_valid"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.PipelineValueInfo.metadata","text":"Metadata relating to the actual data (size, no. of rows, etc. -- depending on data type).","title":"metadata"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.PipelineValueInfo.status","text":"The value status string","title":"status"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.PipelineValueInfo.value_schema","text":"The schema of this value.","title":"value_schema"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.PipelineValuesInfo","text":"Convenience wrapper to make the PipelineState json/dict export prettier. This is basically just a simplified version of the ValueSet class that is using pydantic, in order to make it easy to export to json. Source code in kiara/pipeline/__init__.py class PipelineValuesInfo ( BaseModel ): \"\"\"Convenience wrapper to make the [PipelineState][kiara.pipeline.pipeline.PipelineState] json/dict export prettier. This is basically just a simplified version of the [ValueSet][kiara.data.values.ValueSet] class that is using pydantic, in order to make it easy to export to json. \"\"\" @classmethod def from_value_set ( cls , value_set : ValueSet , ensure_metadata : bool = False ): values : typing . Dict [ str , PipelineValueInfo ] = {} for k in value_set . get_all_field_names (): v = value_set . get_value_obj ( k , ensure_metadata = ensure_metadata ) values [ k ] = PipelineValueInfo . from_value_obj ( v , ensure_metadata = ensure_metadata ) return PipelineValuesInfo ( values = values ) values : typing . Dict [ str , PipelineValueInfo ] = Field ( description = \"Field names are keys, and the data as values.\" ) class Config : use_enum_values = True","title":"PipelineValuesInfo"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.PipelineValuesInfo.values","text":"Field names are keys, and the data as values.","title":"values"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.StepStatus","text":"Enum to describe the state of a workflow. Source code in kiara/pipeline/__init__.py class StepStatus ( Enum ): \"\"\"Enum to describe the state of a workflow.\"\"\" STALE = \"stale\" INPUTS_READY = \"inputs_ready\" RESULTS_INCOMING = \"processing\" RESULTS_READY = \"results_ready\"","title":"StepStatus"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.StepValueAddress","text":"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure. Source code in kiara/pipeline/__init__.py class StepValueAddress ( BaseModel ): \"\"\"Small model to describe the address of a value of a step, within a Pipeline/PipelineStructure.\"\"\" class Config : extra = Extra . forbid step_id : str = Field ( description = \"The id of a step within a pipeline.\" ) value_name : str = Field ( description = \"The name of the value (output name or pipeline input name).\" ) sub_value : typing . Optional [ typing . Dict [ str , typing . Any ]] = Field ( default = None , description = \"A reference to a subitem of a value (e.g. column, list item)\" , ) @property def alias ( self ): \"\"\"An alias string for this address (in the form ``[step_id].[value_name]``).\"\"\" return generate_step_alias ( self . step_id , self . value_name ) def __eq__ ( self , other ): if not isinstance ( other , StepValueAddress ): return False return ( self . step_id , self . value_name , self . sub_value ) == ( other . step_id , other . value_name , other . sub_value , ) def __hash__ ( self ): return hash (( self . step_id , self . value_name , self . sub_value )) def __repr__ ( self ): if self . sub_value : sub_value = f \" sub_value= { self . sub_value } \" else : sub_value = \"\" return f \"StepValueAddres(step_id= { self . step_id } , value_name= { self . value_name }{ sub_value } )\" def __str__ ( self ): return self . __repr__ ()","title":"StepValueAddress"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.StepValueAddress.alias","text":"An alias string for this address (in the form [step_id].[value_name] ).","title":"alias"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.StepValueAddress.step_id","text":"The id of a step within a pipeline.","title":"step_id"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.StepValueAddress.sub_value","text":"A reference to a subitem of a value (e.g. column, list item)","title":"sub_value"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.StepValueAddress.value_name","text":"The name of the value (output name or pipeline input name).","title":"value_name"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.StepValueAddress.__eq__","text":"Return self==value. Source code in kiara/pipeline/__init__.py def __eq__ ( self , other ): if not isinstance ( other , StepValueAddress ): return False return ( self . step_id , self . value_name , self . sub_value ) == ( other . step_id , other . value_name , other . sub_value , )","title":"__eq__()"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.StepValueAddress.__hash__","text":"Return hash(self). Source code in kiara/pipeline/__init__.py def __hash__ ( self ): return hash (( self . step_id , self . value_name , self . sub_value ))","title":"__hash__()"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.StepValueAddress.__repr__","text":"Return repr(self). Source code in kiara/pipeline/__init__.py def __repr__ ( self ): if self . sub_value : sub_value = f \" sub_value= { self . sub_value } \" else : sub_value = \"\" return f \"StepValueAddres(step_id= { self . step_id } , value_name= { self . value_name }{ sub_value } )\"","title":"__repr__()"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.StepValueAddress.__str__","text":"Return str(self). Source code in kiara/pipeline/__init__.py def __str__ ( self ): return self . __repr__ ()","title":"__str__()"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.config","text":"","title":"config"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.config.PipelineConfig","text":"A class to hold the configuration for a PipelineModule . If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify auto , in which case Kiara will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a nand logic gate (in Python): and_step = PipelineStepConfig ( module_type = \"and\" , step_id = \"and\" ) not_step = PipelineStepConfig ( module_type = \"not\" , step_id = \"not\" , input_links = { \"a\" : [ \"and.y\" ]} nand_p_conf = PipelineConfig ( doc = \"Returns 'False' if both inputs are 'True'.\" , steps = [ and_step , not_step ], input_aliases = { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, output_aliases = { \"not__y\" : \"y\" }} Or, the same thing in json: { \"module_type_name\" : \"nand\" , \"doc\" : \"Returns 'False' if both inputs are 'True'.\" , \"steps\" : [ { \"module_type\" : \"and\" , \"step_id\" : \"and\" }, { \"module_type\" : \"not\" , \"step_id\" : \"not\" , \"input_links\" : { \"a\" : \"and.y\" } } ], \"input_aliases\" : { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, \"output_aliases\" : { \"not__y\" : \"y\" } } Source code in kiara/pipeline/config.py class PipelineConfig ( ModuleTypeConfigSchema ): \"\"\"A class to hold the configuration for a [PipelineModule][kiara.pipeline.module.PipelineModule]. If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify ``auto``, in which case *Kiara* will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a ``nand`` logic gate (in Python): ``` python and_step = PipelineStepConfig(module_type=\"and\", step_id=\"and\") not_step = PipelineStepConfig(module_type=\"not\", step_id=\"not\", input_links={\"a\": [\"and.y\"]} nand_p_conf = PipelineConfig(doc=\"Returns 'False' if both inputs are 'True'.\", steps=[and_step, not_step], input_aliases={ \"and__a\": \"a\", \"and__b\": \"b\" }, output_aliases={ \"not__y\": \"y\" }} ``` Or, the same thing in json: ``` json { \"module_type_name\": \"nand\", \"doc\": \"Returns 'False' if both inputs are 'True'.\", \"steps\": [ { \"module_type\": \"and\", \"step_id\": \"and\" }, { \"module_type\": \"not\", \"step_id\": \"not\", \"input_links\": { \"a\": \"and.y\" } } ], \"input_aliases\": { \"and__a\": \"a\", \"and__b\": \"b\" }, \"output_aliases\": { \"not__y\": \"y\" } } ``` \"\"\" # @classmethod # def from_file(cls, path: typing.Union[str, Path]): # # content = get_data_from_file(path) # return PipelineConfig(**content) @classmethod def create_pipeline_config ( cls , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineConfig\" : \"\"\"Create a PipelineModule instance. The main 'config' argument here can be either: - a string: in which case it needs to be (in that order): - a module id - an operation id - a path to a local file - a [ModuleConfig][kiara.module_config.ModuleConfig] object - a dict (to create a `ModuleInstsanceConfig` from Arguments: kiara: the kiara context config: the 'main' config object module_config: in case the 'main' config object was a module id, this argument is used to instantiate the module kiara: the kiara context (will use default context instance if not provided) \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () module_config_obj = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) if not module_config_obj . module_type == \"pipeline\" : raise Exception ( f \"Not a valid pipeline configuration: { config } \" ) # TODO: this is a bit round-about, to create a module config first, but it probably doesn't matter pipeline_config_data = module_config_obj . module_config module : PipelineConfig = PipelineConfig ( ** pipeline_config_data ) return module class Config : extra = Extra . allow validate_assignment = True steps : typing . List [ PipelineStepConfig ] = Field ( default_factory = list , description = \"A list of steps/modules of this pipeline, and their connections.\" , ) input_aliases : typing . Union [ str , typing . Dict [ str , str ]] = Field ( default_factory = dict , description = \"A map of input aliases, with the calculated (<step_id>__<input_name> -- double underscore!) name as key, and a string (the resulting workflow input alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.\" , ) output_aliases : typing . Union [ str , typing . Dict [ str , str ]] = Field ( default_factory = dict , description = \"A map of output aliases, with the calculated (<step_id>__<output_name> -- double underscore!) name as key, and a string (the resulting workflow output alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.\" , ) documentation : str = Field ( default = \"-- n/a --\" , description = \"Documentation about what the pipeline does.\" ) context : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"Metadata for this workflow.\" ) @validator ( \"steps\" , pre = True ) def _validate_steps ( cls , v ): steps = [] for step in v : if isinstance ( step , PipelineStepConfig ): steps . append ( step ) elif isinstance ( step , typing . Mapping ): steps . append ( PipelineStepConfig ( ** step )) else : raise TypeError ( step ) return steps def create_pipeline_structure ( self , kiara : typing . Optional [ \"Kiara\" ] = None ) -> \"PipelineStructure\" : from kiara import Kiara , PipelineStructure if kiara is None : kiara = Kiara . instance () ps = PipelineStructure ( config = self , kiara = kiara , ) return ps def create_pipeline ( self , controller : typing . Optional [ \"PipelineController\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): # if parent_id is None: # parent_id = DEFAULT_PIPELINE_PARENT_ID structure = self . create_pipeline_structure ( kiara = kiara ) from kiara import Pipeline pipeline = Pipeline ( structure = structure , controller = controller , ) return pipeline def create_pipeline_module ( self , module_id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineModule\" : if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () from kiara.pipeline.module import PipelineModule module = PipelineModule ( id = module_id , parent_id = parent_id , module_config = self , kiara = kiara , ) return module # def __rich_console__( # self, console: Console, options: ConsoleOptions # ) -> RenderResult: # # table = Table(show_header=False, box=box.SIMPLE)","title":"PipelineConfig"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.config.PipelineStepConfig","text":"A class to hold the configuration of one module within a PipelineModule . Source code in kiara/pipeline/config.py class PipelineStepConfig ( ModuleConfig ): \"\"\"A class to hold the configuration of one module within a [PipelineModule][kiara.pipeline.module.PipelineModule].\"\"\" class Config : extra = Extra . forbid validate_assignment = True step_id : str = Field ( description = \"The id of the step.\" ) input_links : typing . Dict [ str , typing . List [ StepValueAddress ]] = Field ( default_factory = dict , description = \"The map with the name of an input link as key, and the connected module output name(s) as value.\" , ) @root_validator ( pre = True ) def create_step_id ( cls , values ): if \"module_type\" not in values : raise ValueError ( \"No 'module_type' specified.\" ) if \"step_id\" not in values or not values [ \"step_id\" ]: values [ \"step_id\" ] = slugify ( values [ \"module_type\" ]) return values @validator ( \"step_id\" ) def ensure_valid_id ( cls , v ): # TODO: check with regex if \".\" in v or \" \" in v : raise ValueError ( f \"Step id can't contain special characters or whitespaces: { v } \" ) return v @validator ( \"module_config\" , pre = True ) def ensure_dict ( cls , v ): if v is None : v = {} return v @validator ( \"input_links\" , pre = True ) def ensure_input_links_valid ( cls , v ): if v is None : v = {} result = {} for input_name , output in v . items (): input_links = ensure_step_value_addresses ( default_field_name = input_name , link = output ) result [ input_name ] = input_links return result","title":"PipelineStepConfig"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.config.StepDesc","text":"Details of a single PipelineStep (which lives within a Pipeline Source code in kiara/pipeline/config.py class StepDesc ( BaseModel ): \"\"\"Details of a single [PipelineStep][kiara.pipeline.structure.PipelineStep] (which lives within a [Pipeline][kiara.pipeline.pipeline.Pipeline]\"\"\" class Config : allow_mutation = False extra = Extra . forbid step : PipelineStep = Field ( description = \"Attributes of the step itself.\" ) processing_stage : int = Field ( description = \"The processing stage of this step within a Pipeline.\" ) input_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"\"\"A map that explains what elements connect to this steps inputs. A connection could either be a Pipeline input (indicated by the ``__pipeline__`` token), or another steps output. Example: ``` json input_connections: { \"a\": [\"__pipeline__.a\"], \"b\": [\"step_one.a\"] } ``` \"\"\" ) output_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"A map that explains what elemnts connect to this steps outputs. A connection could be either a Pipeline output, or another steps input.\" ) required : bool = Field ( description = \"Whether this step is always required, or potentially could be skipped in case some inputs are not available.\" ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield f \"[b]Step: { self . step . step_id } [ \\b ]\"","title":"StepDesc"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.controller","text":"","title":"controller"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.controller.PipelineController","text":"An object that controls how a Pipeline should react to events related to it's inputs/outputs. This is the base for the central controller class that needs to be implemented by a Kiara frontend. The default implementation that is used if no PipelineController is provided in a Pipeline constructor is the BatchController , which basically waits until all required inputs are set, and then processes all pipeline steps in one go (in the right order). The pipeline object to control can be set either in the constructor, or via the set_pipeline method. But only once, every subsequent attempt to set a pipeline will raise an Exception. If you want to implement your own controller, you have to override at least one of the (empty) event hook methods: pipeline_inputs_changed pipeline_outputs_changed step_inputs_changed step_outputs_changed Parameters: Name Type Description Default pipeline Pipeline the pipeline object to control None Source code in kiara/pipeline/controller/__init__.py class PipelineController ( PipelineListener ): \"\"\"An object that controls how a [Pipeline][kiara.pipeline.pipeline.Pipeline] should react to events related to it's inputs/outputs. This is the base for the central controller class that needs to be implemented by a *Kiara* frontend. The default implementation that is used if no ``PipelineController`` is provided in a [Pipeline][kiara.pipeline.pipeline.Pipeline] constructor is the [BatchController][kiara.pipeline.controller.BatchController], which basically waits until all required inputs are set, and then processes all pipeline steps in one go (in the right order). The pipeline object to control can be set either in the constructor, or via the ``set_pipeline`` method. But only once, every subsequent attempt to set a pipeline will raise an Exception. If you want to implement your own controller, you have to override at least one of the (empty) event hook methods: - [``pipeline_inputs_changed``][kiara.pipeline.controller.PipelineController.pipeline_inputs_changed] - [``pipeline_outputs_changed``][kiara.pipeline.controller.PipelineController.pipeline_outputs_changed] - [``step_inputs_changed``][kiara.pipeline.controller.PipelineController.step_inputs_changed] - [``step_outputs_changed``][kiara.pipeline.controller.PipelineController.step_outputs_changed] Arguments: pipeline (Pipeline): the pipeline object to control \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : Kiara = kiara self . _pipeline : typing . Optional [ Pipeline ] = None if processor is None : from kiara.processing.synchronous import SynchronousProcessor processor = SynchronousProcessor ( kiara = kiara ) self . _processor : ModuleProcessor = processor self . _job_ids : typing . Dict [ str , str ] = {} \"\"\"A map of the last or current job ids per step_id.\"\"\" if pipeline is not None : self . set_pipeline ( pipeline ) @property def pipeline ( self ) -> \"Pipeline\" : \"\"\"Return the pipeline this controller, well, ...controls...\"\"\" if self . _pipeline is None : raise Exception ( \"Pipeline not set yet.\" ) return self . _pipeline @property def pipeline_status ( self ) -> \"StepStatus\" : return self . pipeline . status def set_pipeline ( self , pipeline : \"Pipeline\" ): \"\"\"Set the pipeline object for this controller. Only one pipeline can be set, once. Arguments: pipeline: the pipeline object \"\"\" if self . _pipeline is not None : raise Exception ( \"Pipeline already set.\" ) self . _pipeline = pipeline @property def processing_stages ( self ) -> typing . List [ typing . List [ str ]]: \"\"\"Return the processing stage order of the pipeline. Returns: a list of lists of step ids \"\"\" return self . pipeline . structure . processing_stages def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the step object for the provided id. Arguments: step_id: the step id Returns: the step object \"\"\" return self . pipeline . get_step ( step_id ) def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the inputs object for the pipeline.\"\"\" return self . pipeline . get_step_inputs ( step_id ) def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the outputs object for the pipeline.\"\"\" return self . pipeline . get_step_outputs ( step_id ) def get_step_input ( self , step_id : str , input_name : str ) -> Value : \"\"\"Get the (current) input value for a specified step and input field name.\"\"\" item = self . get_step_inputs ( step_id ) . get_value_obj ( input_name ) assert item is not None return item def get_step_output ( self , step_id : str , output_name : str ) -> Value : \"\"\"Get the (current) output value for a specified step and output field name.\"\"\" item = self . get_step_outputs ( step_id ) . get_value_obj ( output_name ) assert item is not None return item def get_current_pipeline_state ( self ) -> \"PipelineState\" : \"\"\"Return a description of the current pipeline state. This methods creates a new [PipelineState][kiara.pipeline.pipeline.PipelineState] object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: an object outlining the current pipeline state \"\"\" return self . pipeline . get_current_state () @property def pipeline_inputs ( self ) -> ValueSet : \"\"\"Return the inputs object for this pipeline.\"\"\" return self . pipeline . _pipeline_inputs @pipeline_inputs . setter def pipeline_inputs ( self , inputs : typing . Mapping [ str , typing . Any ]) -> None : \"\"\"Set one, several or all inputs for this pipeline.\"\"\" self . set_pipeline_inputs ( ** inputs ) @property def pipeline_outputs ( self ) -> ValueSet : \"\"\"Return the (current) pipeline outputs object for this pipeline.\"\"\" return self . pipeline . _pipeline_outputs def wait_for_jobs ( self , * job_ids : str , sync_outputs : bool = True ): self . _processor . wait_for ( * job_ids , sync_outputs = sync_outputs ) def can_be_processed ( self , step_id : str ) -> bool : \"\"\"Check whether the step with the provided id is ready to be processed.\"\"\" result = True step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): result = False break return result def check_inputs_status ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]]: \"\"\"Check the status of all stages/steps, and whether they are ready to be processed. The result dictionary uses stage numbers as keys, and a secondary dictionary as values which in turn uses the step_id as key, and a dict with details (keys: 'ready', 'invalid_inputs', 'required') as values. \"\"\" result : typing . Dict [ int , typing . Dict [ str , typing . Dict [ str , typing . Any ]]] = {} for idx , stage in enumerate ( self . processing_stages , start = 1 ): for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], } else : result . setdefault ( idx , {})[ step_id ] = { \"ready\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], \"required\" : True , } else : if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : True , \"invalid_inputs\" : [], } return result def can_be_skipped ( self , step_id : str ) -> bool : \"\"\"Check whether the processing of a step can be skipped.\"\"\" result = True step = self . get_step ( step_id ) if step . required : result = self . can_be_processed ( step_id ) return result def invalid_inputs ( self , step_id : str ) -> typing . List [ str ]: invalid = [] step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): invalid . append ( input_name ) return invalid def process_step ( self , step_id : str , raise_exception : bool = False , wait : bool = False ) -> str : \"\"\"Kick off processing for the step with the provided id. Arguments: step_id: the id of the step that should be started \"\"\" step_inputs = self . get_step_inputs ( step_id ) # if the inputs are not valid, ignore this step if not step_inputs . items_are_valid (): status = step_inputs . check_invalid () assert status is not None raise Exception ( f \"Can't execute step ' { step_id } ', invalid inputs: { ', ' . join ( status . keys ()) } \" ) # get the output 'holder' objects, which we'll need to pass to the module step_outputs = self . get_step_outputs ( step_id ) # get the module object that holds the code that will do the processing step = self . get_step ( step_id ) job_id = self . _processor . start ( pipeline_id = self . pipeline . id , pipeline_name = self . pipeline . title , step_id = step_id , module = step . module , inputs = step_inputs , outputs = step_outputs , ) self . _job_ids [ step_id ] = job_id if wait : self . wait_for_jobs ( job_id , sync_outputs = True ) return job_id def get_job_details ( self , step_or_job_id : str ) -> typing . Optional [ Job ]: \"\"\"Returns job details for a job id, or in case a step_id was provided, the last execution of this step.\"\"\" if self . _job_ids . get ( step_or_job_id , None ) is None : return self . _processor . get_job_details ( step_or_job_id ) else : return self . _processor . get_job_details ( self . _job_ids [ step_or_job_id ]) def step_is_ready ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id is ready to be processed. A ``True`` result means that all input fields are currently set with valid values. Arguments: step_id: the id of the step to check Returns: whether the step is ready (``True``) or not (``False``) \"\"\" return self . get_step_inputs ( step_id ) . items_are_valid () def step_is_finished ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id has been processed successfully. A ``True`` result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Arguments: step_id: the id of the step to check Returns: whether the step result is valid (``True``) or not (``False``) \"\"\" return self . get_step_outputs ( step_id ) . items_are_valid () def pipeline_is_ready ( self ) -> bool : \"\"\"Return whether the pipeline is ready to be processed. A ``True`` result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: whether the pipeline can be processed as a whole (``True``) or not (``False``) \"\"\" return self . pipeline . inputs . items_are_valid () def pipeline_is_finished ( self ) -> bool : \"\"\"Return whether the pipeline has been processed successfully. A ``True`` result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: whether the pipeline was processed successfully (``True``) or not (``False``) \"\"\" return self . pipeline . outputs . items_are_valid () def set_pipeline_inputs ( self , ** inputs : typing . Any ): \"\"\"Set one, several or all inputs for this pipeline. Arguments: **inputs: the input values to set \"\"\" _inputs = self . _pipeline_input_hook ( ** inputs ) self . pipeline_inputs . set_values ( ** _inputs ) def _pipeline_input_hook ( self , ** inputs : typing . Any ): \"\"\"Hook before setting input. Can be implemented by child controller classes, to prevent, transform, validate or queue inputs. \"\"\" log . debug ( f \"Inputs for pipeline ' { self . pipeline . id } ' set: { inputs } \" ) return inputs def process_pipeline ( self ): \"\"\"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Returns: \"\"\" raise NotImplementedError ()","title":"PipelineController"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.controller.batch","text":"","title":"batch"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.listeners","text":"","title":"listeners"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.listeners.PipelineListener","text":"Source code in kiara/pipeline/listeners.py class PipelineListener ( abc . ABC ): def step_inputs_changed ( self , event : StepInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Arguments: event: the step input event \"\"\" def step_outputs_changed ( self , event : StepOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Arguments: event: the step output event \"\"\" def pipeline_inputs_changed ( self , event : PipelineInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. !!! note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the ``step_inputs_changed`` method if you want to. This behaviour might change in the future. Arguments: event: the pipeline input event \"\"\" def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Arguments: event: the pipeline output event \"\"\"","title":"PipelineListener"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.module","text":"","title":"module"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.module.PipelineModule","text":"A KiaraModule that contains a collection of interconnected other modules. Source code in kiara/pipeline/module.py class PipelineModule ( KiaraModule [ PipelineConfig ]): \"\"\"A [KiaraModule][kiara.module.KiaraModule] that contains a collection of interconnected other modules.\"\"\" _config_cls : typing . Type [ PipelineConfig ] = PipelineConfig # type: ignore _module_type_id = \"pipeline\" @classmethod def is_pipeline ( cls ) -> bool : return True def __init__ ( self , id : typing . Optional [ str ], parent_id : typing . Optional [ str ] = None , module_config : typing . Union [ None , PipelineConfig , typing . Mapping [ str , typing . Any ] ] = None , # controller: typing.Union[ # None, PipelineController, str, typing.Type[PipelineController] # ] = None, kiara : typing . Optional [ \"Kiara\" ] = None , ): # if controller is not None and not isinstance(controller, PipelineController): # raise NotImplementedError() # if controller is None: super () . __init__ ( id = id , parent_id = parent_id , module_config = module_config , kiara = kiara , ) self . _pipeline_structure : PipelineStructure = self . _create_structure () assert not self . _config . constants self . _config . constants = dict ( self . _pipeline_structure . constants ) @property def structure ( self ) -> PipelineStructure : \"\"\"The ``PipelineStructure`` of this module.\"\"\" return self . _pipeline_structure def _create_structure ( self ) -> PipelineStructure : pipeline_structure = PipelineStructure ( config = self . config , kiara = self . _kiara ) return pipeline_structure def create_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_input_schema def create_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_output_schema def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : from kiara import Pipeline # controller = BatchController(auto_process=False, kiara=self._kiara) pipeline = Pipeline ( structure = self . structure ) pipeline . inputs . set_values ( ** inputs ) if not pipeline . inputs . items_are_valid (): raise KiaraProcessingException ( f \"Can't start processing of { self . _module_type_id } pipeline: one or several inputs missing or invalid.\" ) # type: ignore if not pipeline . status == StepStatus . RESULTS_READY : # TODO: error details raise KiaraProcessingException ( f \"Error when running pipeline of type ' { self . _module_type_id } '.\" ) # type: ignore outputs . set_values ( ** pipeline . outputs )","title":"PipelineModule"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.pipeline","text":"","title":"pipeline"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.pipeline.Pipeline","text":"An instance of a PipelineStructure that holds state for all of the inputs/outputs of the steps within. Source code in kiara/pipeline/pipeline.py class Pipeline ( object ): \"\"\"An instance of a [PipelineStructure][kiara.pipeline.structure.PipelineStructure] that holds state for all of the inputs/outputs of the steps within.\"\"\" def __init__ ( self , structure : PipelineStructure , # constants: typing.Optional[typing.Mapping[str, typing.Any]] = None, controller : typing . Optional [ PipelineController ] = None , title : typing . Optional [ str ] = None , ): self . _id : str = str ( uuid . uuid4 ()) if title is None : title = self . _id self . _title : str = title self . _structure : PipelineStructure = structure self . _pipeline_inputs : SlottedValueSet = None # type: ignore self . _pipeline_outputs : SlottedValueSet = None # type: ignore self . _step_inputs : typing . Mapping [ str , ValueSet ] = None # type: ignore self . _step_outputs : typing . Mapping [ str , ValueSet ] = None # type: ignore self . _value_refs : typing . Mapping [ ValueSlot , typing . Iterable [ ValueRef ]] = None # type: ignore self . _status : StepStatus = StepStatus . STALE self . _steps_by_stage : typing . Optional [ typing . Dict [ int , typing . Dict [ str , PipelineStep ]] ] = None self . _inputs_by_stage : typing . Optional [ typing . Dict [ int , typing . List [ str ]] ] = None self . _outputs_by_stage : typing . Optional [ typing . Dict [ int , typing . List [ str ]] ] = None self . _kiara : \"Kiara\" = self . _structure . _kiara self . _data_registry : DataRegistry = self . _kiara . data_registry self . _init_values () if controller is None : controller = BatchController ( self , kiara = self . _kiara ) else : controller . set_pipeline ( self ) self . _controller : PipelineController = controller self . _listeners : typing . List [ PipelineListener ] = [] self . _update_status () def __eq__ ( self , other ): if not isinstance ( other , Pipeline ): return False return self . _id == other . _id def __hash__ ( self ): return hash ( self . _id ) @property def id ( self ) -> str : return self . _id @property def title ( self ) -> str : return self . _title @property def structure ( self ) -> PipelineStructure : return self . _structure @property def controller ( self ) -> PipelineController : if self . _controller is None : raise Exception ( \"No controller set (yet).\" ) return self . _controller @property def inputs ( self ) -> SlottedValueSet : \"\"\"All (pipeline) input values of this pipeline.\"\"\" return self . _pipeline_inputs @property def outputs ( self ) -> SlottedValueSet : \"\"\"All (pipeline) output values of this pipeline.\"\"\" return self . _pipeline_outputs # def set_pipeline_inputs(self, **inputs: typing.Any): # self._controller.set_pipeline_inputs(**inputs) @property def step_ids ( self ) -> typing . Iterable [ str ]: \"\"\"Return all ids of the steps of this pipeline.\"\"\" return self . _structure . step_ids def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the object representing a step in this workflow, identified by the step id.\"\"\" return self . _structure . get_step ( step_id ) def get_steps_by_stage ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , PipelineStep ]]: \"\"\"Return a all pipeline steps, ordered by stage they belong to.\"\"\" if self . _steps_by_stage is not None : return self . _steps_by_stage result : typing . Dict [ int , typing . Dict [ str , PipelineStep ]] = {} for step_id in self . step_ids : step = self . get_step ( step_id ) stage = step . processing_stage assert stage is not None result . setdefault ( stage , {})[ step_id ] = step self . _steps_by_stage = result return self . _steps_by_stage def get_pipeline_inputs_by_stage ( self ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by stage they are first required.\"\"\" if self . _inputs_by_stage is not None : return self . _inputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . inputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepInputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _inputs_by_stage = result return self . _inputs_by_stage def get_pipeline_outputs_by_stage ( self , ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by the stage that needs to be executed before they are available.\"\"\" if self . _outputs_by_stage is not None : return self . _outputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . outputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepOutputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _outputs_by_stage = result return self . _outputs_by_stage def get_pipeline_inputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline inputs that are required for a stage to be processed. The result of this method does not include inputs that were required earlier already. \"\"\" return self . get_pipeline_inputs_by_stage () . get ( stage , []) def get_stage_for_pipeline_input ( self , input_name : str ) -> int : for stage , input_names in self . get_pipeline_inputs_by_stage () . items (): if input_name in input_names : return stage raise Exception ( f \"No input name ' { input_name } '. Available inputs: { ', ' . join ( self . inputs . keys ()) } \" ) def stage_for_pipeline_output ( self , output_name : str ) -> int : for stage , output_names in self . get_pipeline_outputs_by_stage () . items (): if output_name in output_names : return stage raise Exception ( f \"No output name ' { output_name } '. Available outputs: { ', ' . join ( self . outputs . keys ()) } \" ) def get_pipeline_outputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline outputs that are first available after the specified stage completed processing.\"\"\" return self . get_pipeline_outputs_by_stage () . get ( stage , []) def get_pipeline_inputs_for_step ( self , step_id : str ) -> typing . List [ str ]: result = [] for field_name , value_slot in self . inputs . _value_slots . items (): refs = self . _value_refs [ value_slot ] for ref in refs : if not isinstance ( ref , PipelineInputRef ): continue for ci in ref . connected_inputs : if ci . step_id == step_id and ref . value_name not in result : result . append ( ref . value_name ) return result def get_pipeline_outputs_for_step ( self , step_id : str ) -> typing . List [ str ]: result = [] for field_name , value_slot in self . outputs . _value_slots . items (): refs = self . _value_refs [ value_slot ] for ref in refs : if not isinstance ( ref , PipelineOutputRef ): continue if ( ref . connected_output . step_id == step_id and ref . value_name not in result ): result . append ( ref . value_name ) return result def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all inputs for a step id (incl. inputs that are not pipeline inputs but connected to other modules output).\"\"\" return self . _step_inputs [ step_id ] def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all outputs for a step id (incl. outputs that are not pipeline outputs).\"\"\" return self . _step_outputs [ step_id ] def add_listener ( self , listener : PipelineListener ) -> None : \"\"\"Add a listener taht gets notified on any internal pipeline input/output events.\"\"\" self . _listeners . append ( listener ) @property def status ( self ) -> StepStatus : \"\"\"Return the current status of this pipeline.\"\"\" return self . _state def _update_status ( self ): \"\"\"Make sure internal state variable is up to date.\"\"\" if self . inputs is None : new_state = StepStatus . STALE elif not self . inputs . items_are_valid (): new_state = StepStatus . STALE elif not self . outputs . items_are_valid (): new_state = StepStatus . INPUTS_READY else : new_state = StepStatus . RESULTS_READY self . _state = new_state def _init_values ( self ): \"\"\"Initialize this object. This should only be called once. Basically, this goes through all the inputs and outputs of all steps, and 'allocates' a PipelineValueInfo object for each of them. In case where output/input or pipeline-input/input points are connected, only one value item is allocated, since those refer to the same value. \"\"\" pipeline_inputs : typing . Dict [ str , ValueSlot ] = {} pipeline_outputs : typing . Dict [ str , ValueSlot ] = {} all_step_inputs : typing . Dict [ str , typing . Dict [ str , ValueSlot ]] = {} all_step_outputs : typing . Dict [ str , typing . Dict [ str , ValueSlot ]] = {} value_refs : typing . Dict [ ValueSlot , typing . List [ ValueRef ]] = {} # create the value objects that are associated with step outputs # all pipeline outputs are created here too, since the only place # those can be associated are step outputs for step_id , step_details in self . _structure . steps_details . items (): step_outputs : typing . Mapping [ str , StepOutputRef ] = step_details [ \"outputs\" ] for output_name , output_point in step_outputs . items (): init_output_value_item = self . _data_registry . register_data ( value_schema = output_point . value_schema ) output_value_slot = self . _data_registry . register_alias ( value_or_schema = init_output_value_item , callbacks = [ self ] ) value_refs . setdefault ( output_value_slot , []) . append ( output_point ) all_step_outputs . setdefault ( step_id , {})[ output_name ] = output_value_slot # not all step outputs necessarily need to be connected to a pipeline output if output_point . pipeline_output : pipeline_outputs [ output_point . pipeline_output ] = output_value_slot po = self . _structure . pipeline_outputs [ output_point . pipeline_output ] value_refs . setdefault ( output_value_slot , []) . append ( po ) # create the value objects that are associated with step inputs for step_id , step_details in self . _structure . steps_details . items (): step_inputs : typing . Mapping [ str , StepInputRef ] = step_details [ \"inputs\" ] for input_name , input_point in step_inputs . items (): # if this step input gets fed from a pipeline_input (meaning user input in most cases), # we need to create a DataValue for that pipeline input # vm = ValueMetadata( # origin=f\"{self.id}.steps.{step_id}.inputs.{input_point.value_name}\" # ) if input_point . connected_pipeline_input : connected_pipeline_input_name = input_point . connected_pipeline_input pipeline_input_field : PipelineInputRef = ( self . _structure . pipeline_inputs [ connected_pipeline_input_name ] ) pipeline_input_slot : ValueSlot = pipeline_inputs . get ( connected_pipeline_input_name , None ) if pipeline_input_slot is None : # if the pipeline input wasn't created by another step input before, # we need to take care of it here if pipeline_input_field . is_constant : init_value = self . structure . constants [ pipeline_input_field . value_name ] else : init_value = self . structure . defaults . get ( pipeline_input_field . value_name , SpecialValue . NOT_SET ) init_pipeline_input_value = self . _data_registry . register_data ( value_data = init_value , value_schema = pipeline_input_field . value_schema , ) # TODO: check whether it's a constant? pipeline_input_slot = self . _data_registry . register_alias ( value_or_schema = init_pipeline_input_value , callbacks = [ self ] ) value_refs . setdefault ( pipeline_input_slot , []) . append ( pipeline_input_field ) pipeline_inputs [ connected_pipeline_input_name ] = pipeline_input_slot all_step_inputs . setdefault ( step_id , {})[ input_name ] = pipeline_input_slot value_refs . setdefault ( pipeline_input_slot , []) . append ( input_point ) elif input_point . connected_outputs : for co in input_point . connected_outputs : if len ( input_point . connected_outputs ) == 1 and not co . sub_value : # this means the input is the same value as the connected output output_value : ValueSlot = all_step_outputs [ co . step_id ][ co . value_name ] all_step_inputs . setdefault ( input_point . step_id , {})[ input_point . value_name ] = output_value value_refs . setdefault ( output_value , []) . append ( input_point ) else : print ( input_point . connected_outputs ) raise NotImplementedError () # sub_value = co.sub_value # linked_values = {} # for co in input_point.connected_outputs: # output_value = all_step_outputs[co.step_id][co.value_name] # sub_value = co.sub_value # if len(input_point.connected_outputs) > 1 and not sub_value: # raise NotImplementedError() # sub_value = {\"config\": co.step_id} # if sub_value is not None: # raise NotImplementedError # # linked_values[output_value.id] = sub_value # # step_input = self._data_registry.register_linked_value( # parent_id=self.id, # linked_values=linked_values, # value_schema=input_point.value_schema, # value_refs=input_point, # ) # self._data_registry.register_callback( # self.values_updated, step_input # ) # all_step_inputs.setdefault(input_point.step_id, {})[ # input_point.value_name # ] = step_input else : raise Exception ( f \"Invalid value point type for this location: { input_point } \" ) if not pipeline_inputs : raise Exception ( f \"Can't init pipeline ' { self . title } ': no pipeline inputs\" ) self . _pipeline_inputs = SlottedValueSet ( items = pipeline_inputs , read_only = False , title = f \"Inputs for pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) if not pipeline_outputs : raise Exception ( f \"Can't init pipeline ' { self . title } ': no pipeline outputs\" ) self . _pipeline_outputs = SlottedValueSet ( items = pipeline_outputs , read_only = True , title = f \"Outputs for pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) self . _step_inputs = {} for step_id , inputs in all_step_inputs . items (): self . _step_inputs [ step_id ] = SlottedValueSet ( items = inputs , read_only = True , title = f \"Inputs for step ' { step_id } ' of pipeline ' { self . title } \" , kiara = self . _kiara , registry = self . _data_registry , ) self . _step_outputs = {} for step_id , outputs in all_step_outputs . items (): self . _step_outputs [ step_id ] = SlottedValueSet ( read_only = False , items = outputs , title = f \"Outputs for step ' { step_id } ' of pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) self . _value_refs = value_refs self . _steps_by_stage = None self . _inputs_by_stage = None def values_updated ( self , * items : ValueSlot ) -> None : updated_inputs : typing . Dict [ str , typing . List [ str ]] = {} updated_outputs : typing . Dict [ str , typing . List [ str ]] = {} updated_pipeline_inputs : typing . List [ str ] = [] updated_pipeline_outputs : typing . List [ str ] = [] # print(\"===================================================\") # for item in items: # print(item) # print(\"===================================================\") self . _update_status () if self . _value_refs is None : # means init is not finished yet return for item in items : # TODO: multiple value fields, also check pipeline id references = self . _value_refs . get ( item , None ) assert references for p in references : if isinstance ( p , StepInputRef ): updated_inputs . setdefault ( p . step_id , []) . append ( p . value_name ) elif isinstance ( p , StepOutputRef ): updated_outputs . setdefault ( p . step_id , []) . append ( p . value_name ) elif isinstance ( p , PipelineInputRef ): updated_pipeline_inputs . append ( p . value_name ) elif isinstance ( p , PipelineOutputRef ): updated_pipeline_outputs . append ( p . value_name ) else : raise TypeError ( f \"Can't update, invalid type: { type ( p ) } \" ) # print('========================================') # print('---') # print(\"Upaded pipeline input\") # print(updated_pipeline_inputs) # print('---') # print(\"Upaded step inputs\") # print(updated_inputs) # print('---') # print(\"Upaded step outputs\") # print(updated_outputs) # print('---') # print(\"Upaded pipeline outputs\") # print(updated_pipeline_outputs) if updated_pipeline_inputs : event_pi = PipelineInputEvent ( pipeline_id = self . id , updated_pipeline_inputs = updated_pipeline_inputs , ) self . _controller . pipeline_inputs_changed ( event_pi ) self . _notify_pipeline_listeners ( event_pi ) if updated_outputs : event_so = StepOutputEvent ( pipeline_id = self . id , updated_step_outputs = updated_outputs , ) self . _controller . step_outputs_changed ( event_so ) self . _notify_pipeline_listeners ( event_so ) if updated_inputs : event_si = StepInputEvent ( pipeline_id = self . id , updated_step_inputs = updated_inputs , ) self . _controller . step_inputs_changed ( event_si ) self . _notify_pipeline_listeners ( event_si ) if updated_pipeline_outputs : event_po = PipelineOutputEvent ( pipeline_id = self . id , updated_pipeline_outputs = updated_pipeline_outputs , ) self . _controller . pipeline_outputs_changed ( event_po ) self . _notify_pipeline_listeners ( event_po ) def _notify_pipeline_listeners ( self , event : StepEvent ): for listener in self . _listeners : if event . type == \"step_input\" : # type: ignore listener . step_inputs_changed ( event ) # type: ignore elif event . type == \"step_output\" : # type: ignore listener . step_outputs_changed ( event ) # type: ignore elif event . type == \"pipeline_input\" : # type: ignore listener . pipeline_inputs_changed ( event ) # type: ignore elif event . type == \"pipeline_output\" : # type: ignore listener . pipeline_outputs_changed ( event ) # type: ignore else : raise Exception ( f \"Unsupported type: { event . type } \" ) # type: ignore def get_current_state ( self ) -> \"PipelineState\" : step_inputs = {} step_states = {} for k , v in self . _step_inputs . items (): step_inputs [ k ] = PipelineValuesInfo . from_value_set ( v ) if v . items_are_valid (): step_states [ k ] = StepStatus . INPUTS_READY else : step_states [ k ] = StepStatus . STALE step_outputs = {} for k , v in self . _step_outputs . items (): step_outputs [ k ] = PipelineValuesInfo . from_value_set ( v ) if v . items_are_valid (): step_states [ k ] = StepStatus . RESULTS_READY from kiara.info.pipelines import PipelineState state = PipelineState ( structure = self . structure . to_details (), pipeline_inputs = self . _pipeline_inputs . to_details (), pipeline_outputs = self . _pipeline_outputs . to_details (), step_states = step_states , step_inputs = step_inputs , step_outputs = step_outputs , status = self . status , ) return state def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield self . get_current_state ()","title":"Pipeline"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.structure","text":"","title":"structure"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.structure.PipelineStep","text":"A step within a pipeline-structure, includes information about it's connection(s) and other metadata. Source code in kiara/pipeline/structure.py class PipelineStep ( BaseModel ): \"\"\"A step within a pipeline-structure, includes information about it's connection(s) and other metadata.\"\"\" class Config : validate_assignment = True extra = Extra . forbid @classmethod def create_steps ( cls , * steps : \"PipelineStepConfig\" , kiara : \"Kiara\" ) -> typing . List [ \"PipelineStep\" ]: result : typing . List [ PipelineStep ] = [] if kiara is None : from kiara.module import Kiara kiara = Kiara . instance () for step in steps : _s = PipelineStep ( step_id = step . step_id , module_type = step . module_type , module_config = copy . deepcopy ( step . module_config ), input_links = copy . deepcopy ( step . input_links ), _kiara = kiara , # type: ignore ) result . append ( _s ) return result _module : typing . Optional [ \"KiaraModule\" ] = PrivateAttr ( default = None ) @validator ( \"step_id\" ) def _validate_step_id ( cls , v ): assert isinstance ( v , str ) if \".\" in v : raise ValueError ( \"Step ids can't contain '.' characters.\" ) return v step_id : str module_type : str = Field ( description = \"The module type.\" ) module_config : typing . Mapping [ str , typing . Any ] = Field ( description = \"The module config.\" , default_factory = dict ) required : bool = Field ( description = \"Whether this step is required within the workflow. \\n\\n In some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required.\" , default = True , ) processing_stage : typing . Optional [ int ] = Field ( default = None , description = \"The stage number this step is executed within the pipeline.\" , ) input_links : typing . Mapping [ str , typing . List [ StepValueAddress ]] = Field ( description = \"The links that connect to inputs of the module.\" , default_factory = list , ) _kiara : typing . Optional [ \"Kiara\" ] = PrivateAttr ( default = None ) _id : str = PrivateAttr () def __init__ ( self , ** data ): # type: ignore self . _id = str ( uuid . uuid4 ()) kiara = data . pop ( \"_kiara\" , None ) if kiara is None : from kiara import Kiara kiara = Kiara . instance () super () . __init__ ( ** data ) self . _kiara : \"Kiara\" = kiara @property def kiara ( self ): return self . _kiara @property def module ( self ) -> \"KiaraModule\" : if self . _module is None : try : if ( self . module_type in self . kiara . operation_mgmt . profiles . keys () and not self . module_config ): op = self . kiara . operation_mgmt . profiles [ self . module_type ] self . _module = op . module else : self . _module = self . kiara . create_module ( id = self . step_id , module_type = self . module_type , module_config = self . module_config , ) except Exception as e : raise Exception ( f \"Can't assemble pipeline step ' { self . step_id } ': { e } \" ) return self . _module def __eq__ ( self , other ): if not isinstance ( other , PipelineStep ): return False return self . _id == other . _id # # TODO: also check whether _kiara obj is equal? # eq = (self.step_id, self.parent_id, self.module, self.processing_stage,) == ( # other.step_id, # other.parent_id, # other.module, # other.processing_stage, # ) # # if not eq: # return False # # hs = DeepHash(self.input_links) # ho = DeepHash(other.input_links) # # return hs[self.input_links] == ho[other.input_links] def __hash__ ( self ): return hash ( self . _id ) # # TODO: also include _kiara obj? # # TODO: figure out whether that can be made to work without deephash # hs = DeepHash(self.input_links) # return hash( # ( # self.step_id, # self.parent_id, # self.module, # self.processing_stage, # hs[self.input_links], # ) # ) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (step_id= { self . step_id } module_type= { self . module_type } processing_stage= { self . processing_stage } )\" def __str__ ( self ): return f \"step: { self . step_id } (module: { self . module_type } )\"","title":"PipelineStep"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.structure.PipelineStructure","text":"An object that holds one or several steps, and describes the connections between them. Source code in kiara/pipeline/structure.py class PipelineStructure ( object ): \"\"\"An object that holds one or several steps, and describes the connections between them.\"\"\" def __init__ ( self , config : \"PipelineConfig\" , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _structure_config : \"PipelineConfig\" = config steps = self . _structure_config . steps input_aliases = self . _structure_config . input_aliases output_aliases = self . _structure_config . output_aliases if not steps : raise Exception ( \"No steps provided.\" ) if kiara is None : kiara = Kiara . instance () self . _kiara : Kiara = kiara self . _steps : typing . List [ PipelineStep ] = PipelineStep . create_steps ( * steps , kiara = self . _kiara ) # self._pipeline_id: str = parent_id if input_aliases is None : input_aliases = {} if isinstance ( input_aliases , str ): if input_aliases not in ALLOWED_INPUT_ALIAS_MARKERS : raise Exception ( f \"Can't create pipeline, invalid value ' { input_aliases } ' for 'input_aliases'. Either specify a dict, or use one of: { ', ' . join ( ALLOWED_INPUT_ALIAS_MARKERS ) } \" ) input_aliases = calculate_shortest_field_aliases ( self . _steps , input_aliases , \"inputs\" ) if isinstance ( output_aliases , str ): if output_aliases not in ALLOWED_INPUT_ALIAS_MARKERS : raise Exception ( f \"Can't create pipeline, invalid value ' { output_aliases } ' for 'output_aliases'. Either specify a dict, or use one of: { ', ' . join ( ALLOWED_INPUT_ALIAS_MARKERS ) } \" ) output_aliases = calculate_shortest_field_aliases ( self . _steps , output_aliases , \"outputs\" ) self . _input_aliases : typing . Dict [ str , str ] = dict ( input_aliases ) # type: ignore if output_aliases is None : output_aliases = {} self . _output_aliases : typing . Dict [ str , str ] = dict ( output_aliases ) # type: ignore # this is hardcoded for now self . _add_all_workflow_outputs : bool = False self . _constants : typing . Dict [ str , typing . Any ] = None # type: ignore self . _defaults : typing . Dict [ str , typing . Any ] = None # type: ignore self . _execution_graph : nx . DiGraph = None # type: ignore self . _data_flow_graph : nx . DiGraph = None # type: ignore self . _data_flow_graph_simple : nx . DiGraph = None # type: ignore self . _processing_stages : typing . List [ typing . List [ str ]] = None # type: ignore self . _steps_details : typing . Dict [ str , typing . Any ] = None # type: ignore \"\"\"Holds details about the (current) processing steps contained in this workflow.\"\"\" # @property # def pipeline_id(self) -> str: # return self._pipeline_id @property def structure_config ( self ) -> \"PipelineConfig\" : return self . _structure_config @property def steps ( self ) -> typing . Iterable [ PipelineStep ]: return self . _steps @property def modules ( self ) -> typing . Iterable [ \"KiaraModule\" ]: return ( s . module for s in self . steps ) @property def steps_details ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _steps_details is None : self . _process_steps () return self . _steps_details @property def step_ids ( self ) -> typing . Iterable [ str ]: if self . _steps_details is None : self . _process_steps () return self . _steps_details . keys () @property def constants ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _constants is None : self . _process_steps () return self . _constants @property def defaults ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _defaults is None : self . _process_steps () return self . _defaults def get_step ( self , step_id : str ) -> PipelineStep : d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"step\" ] def get_step_inputs ( self , step_id : str ) -> typing . Mapping [ str , StepInputRef ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"inputs\" ] def get_step_outputs ( self , step_id : str ) -> typing . Mapping [ str , StepOutputRef ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"outputs\" ] def get_step_details ( self , step_id : str ) -> typing . Mapping [ str , typing . Any ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d @property def execution_graph ( self ) -> nx . DiGraph : if self . _execution_graph is None : self . _process_steps () return self . _execution_graph @property def data_flow_graph ( self ) -> nx . DiGraph : if self . _data_flow_graph is None : self . _process_steps () return self . _data_flow_graph @property def data_flow_graph_simple ( self ) -> nx . DiGraph : if self . _data_flow_graph_simple is None : self . _process_steps () return self . _data_flow_graph_simple @property def processing_stages ( self ) -> typing . List [ typing . List [ str ]]: if self . _steps_details is None : self . _process_steps () return self . _processing_stages @lru_cache () def _get_node_of_type ( self , node_type : str ): if self . _steps_details is None : self . _process_steps () return [ node for node , attr in self . _data_flow_graph . nodes ( data = True ) if attr [ \"type\" ] == node_type ] @property def steps_inputs ( self ) -> typing . Dict [ str , StepInputRef ]: return { node . alias : node for node in self . _get_node_of_type ( node_type = StepInputRef . __name__ ) } @property def steps_outputs ( self ) -> typing . Dict [ str , StepOutputRef ]: return { node . alias : node for node in self . _get_node_of_type ( node_type = StepOutputRef . __name__ ) } @property def pipeline_inputs ( self ) -> typing . Dict [ str , PipelineInputRef ]: return { node . value_name : node for node in self . _get_node_of_type ( node_type = PipelineInputRef . __name__ ) } @property def pipeline_outputs ( self ) -> typing . Dict [ str , PipelineOutputRef ]: return { node . value_name : node for node in self . _get_node_of_type ( node_type = PipelineOutputRef . __name__ ) } @property def pipeline_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return { input_name : w_in . value_schema for input_name , w_in in self . pipeline_inputs . items () } @property def pipeline_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return { output_name : w_out . value_schema for output_name , w_out in self . pipeline_outputs . items () } def _process_steps ( self ): \"\"\"The core method of this class, it connects all the processing modules, their inputs and outputs.\"\"\" steps_details : typing . Dict [ str , typing . Any ] = {} execution_graph = nx . DiGraph () execution_graph . add_node ( \"__root__\" ) data_flow_graph = nx . DiGraph () data_flow_graph_simple = nx . DiGraph () processing_stages = [] constants = {} structure_defaults = {} # temp variable, to hold all outputs outputs : typing . Dict [ str , StepOutputRef ] = {} # process all pipeline and step outputs first _temp_steps_map : typing . Dict [ str , PipelineStep ] = {} pipeline_outputs : typing . Dict [ str , PipelineOutputRef ] = {} for step in self . _steps : _temp_steps_map [ step . step_id ] = step if step . step_id in steps_details . keys (): raise Exception ( f \"Can't process steps: duplicate step_id ' { step . step_id } '\" ) steps_details [ step . step_id ] = { \"step\" : step , \"outputs\" : {}, \"inputs\" : {}, } data_flow_graph . add_node ( step , type = \"step\" ) # go through all the module outputs, create points for them and connect them to pipeline outputs for output_name , schema in step . module . output_schemas . items (): step_output = StepOutputRef ( value_name = output_name , value_schema = schema , step_id = step . step_id , ) steps_details [ step . step_id ][ \"outputs\" ][ output_name ] = step_output step_alias = generate_step_alias ( step . step_id , output_name ) outputs [ step_alias ] = step_output step_output_name = generate_pipeline_endpoint_name ( step_id = step . step_id , value_name = output_name ) if self . _output_aliases : if step_output_name in self . _output_aliases . keys (): step_output_name = self . _output_aliases [ step_output_name ] else : if not self . _add_all_workflow_outputs : # this output is not interesting for the workflow step_output_name = None if step_output_name : step_output_address = StepValueAddress ( step_id = step . step_id , value_name = output_name ) pipeline_output = PipelineOutputRef ( value_name = step_output_name , connected_output = step_output_address , value_schema = schema , ) pipeline_outputs [ step_output_name ] = pipeline_output step_output . pipeline_output = pipeline_output . value_name data_flow_graph . add_node ( pipeline_output , type = PipelineOutputRef . __name__ ) data_flow_graph . add_edge ( step_output , pipeline_output ) data_flow_graph_simple . add_node ( pipeline_output , type = PipelineOutputRef . __name__ ) data_flow_graph_simple . add_edge ( step , pipeline_output ) data_flow_graph . add_node ( step_output , type = StepOutputRef . __name__ ) data_flow_graph . add_edge ( step , step_output ) # now process inputs, and connect them to the appropriate output/pipeline-input points existing_pipeline_input_points : typing . Dict [ str , PipelineInputRef ] = {} for step in self . _steps : other_step_dependency : typing . Set = set () # go through all the inputs of a module, create input points and connect them to either # other module outputs, or pipeline inputs (which need to be created) module_constants : typing . Mapping [ str , typing . Any ] = step . module . get_config_value ( \"constants\" ) for input_name , schema in step . module . input_schemas . items (): matching_input_links : typing . List [ StepValueAddress ] = [] is_constant = input_name in module_constants . keys () for value_name , input_links in step . input_links . items (): if value_name == input_name : for input_link in input_links : if input_link in matching_input_links : raise Exception ( f \"Duplicate input link: { input_link } \" ) matching_input_links . append ( input_link ) if matching_input_links : # this means we connect to other steps output connected_output_points : typing . List [ StepOutputRef ] = [] connected_outputs : typing . List [ StepValueAddress ] = [] for input_link in matching_input_links : output_id = generate_step_alias ( input_link . step_id , input_link . value_name ) if output_id not in outputs . keys (): raise Exception ( f \"Can't connect input ' { input_name } ' for step ' { step . step_id } ': no output ' { output_id } ' available. Available output names: { ', ' . join ( outputs . keys ()) } \" ) connected_output_points . append ( outputs [ output_id ]) connected_outputs . append ( input_link ) other_step_dependency . add ( input_link . step_id ) step_input_point = StepInputRef ( step_id = step . step_id , value_name = input_name , value_schema = schema , is_constant = is_constant , connected_pipeline_input = None , connected_outputs = connected_outputs , ) for op in connected_output_points : op . connected_inputs . append ( step_input_point . address ) data_flow_graph . add_edge ( op , step_input_point ) data_flow_graph_simple . add_edge ( _temp_steps_map [ op . step_id ], step_input_point ) # TODO: name edge data_flow_graph_simple . add_edge ( step_input_point , step ) # TODO: name edge else : # this means we connect to pipeline input pipeline_input_name = generate_pipeline_endpoint_name ( step_id = step . step_id , value_name = input_name ) # check whether this input has an alias associated with it if self . _input_aliases : if pipeline_input_name in self . _input_aliases . keys (): # this means we use the pipeline alias pipeline_input_name = self . _input_aliases [ pipeline_input_name ] if pipeline_input_name in existing_pipeline_input_points . keys (): # we already created a pipeline input with this name # TODO: check whether schema fits connected_pipeline_input = existing_pipeline_input_points [ pipeline_input_name ] assert connected_pipeline_input . is_constant == is_constant else : # we need to create the pipeline input connected_pipeline_input = PipelineInputRef ( value_name = pipeline_input_name , value_schema = schema , is_constant = is_constant , ) existing_pipeline_input_points [ pipeline_input_name ] = connected_pipeline_input data_flow_graph . add_node ( connected_pipeline_input , type = PipelineInputRef . __name__ ) data_flow_graph_simple . add_node ( connected_pipeline_input , type = PipelineInputRef . __name__ ) if is_constant : constants [ pipeline_input_name ] = step . module . get_config_value ( \"constants\" )[ input_name ] default_val = step . module . get_config_value ( \"defaults\" ) . get ( input_name , None ) if is_constant and default_val is not None : raise Exception ( f \"Module config invalid for step ' { step . step_id } ': both default value and constant provided for input ' { input_name } '.\" ) elif default_val is not None : structure_defaults [ pipeline_input_name ] = default_val step_input_point = StepInputRef ( step_id = step . step_id , value_name = input_name , value_schema = schema , connected_pipeline_input = connected_pipeline_input . value_name , connected_outputs = None , ) connected_pipeline_input . connected_inputs . append ( step_input_point . address ) data_flow_graph . add_edge ( connected_pipeline_input , step_input_point ) data_flow_graph_simple . add_edge ( connected_pipeline_input , step ) data_flow_graph . add_node ( step_input_point , type = StepInputRef . __name__ ) steps_details [ step . step_id ][ \"inputs\" ][ input_name ] = step_input_point data_flow_graph . add_edge ( step_input_point , step ) if other_step_dependency : for module_id in other_step_dependency : execution_graph . add_edge ( module_id , step . step_id ) else : execution_graph . add_edge ( \"__root__\" , step . step_id ) # calculate execution order path_lengths : typing . Dict [ str , int ] = {} for step in self . _steps : step_id = step . step_id paths = list ( nx . all_simple_paths ( execution_graph , \"__root__\" , step_id )) max_steps = max ( paths , key = lambda x : len ( x )) path_lengths [ step_id ] = len ( max_steps ) - 1 max_length = max ( path_lengths . values ()) for i in range ( 1 , max_length + 1 ): stage : typing . List [ str ] = [ m for m , length in path_lengths . items () if length == i ] processing_stages . append ( stage ) for _step_id in stage : steps_details [ _step_id ][ \"processing_stage\" ] = i steps_details [ _step_id ][ \"step\" ] . processing_stage = i self . _constants = constants self . _defaults = structure_defaults self . _steps_details = steps_details self . _execution_graph = execution_graph self . _data_flow_graph = data_flow_graph self . _data_flow_graph_simple = data_flow_graph_simple self . _processing_stages = processing_stages self . _get_node_of_type . cache_clear () # calculating which steps are always required to execute to compute one of the required pipeline outputs. # this is done because in some cases it's possible that some steps can be skipped to execute if they # don't have a valid input set, because the inputs downstream they are connecting to are 'non-required' # optional_steps = [] last_stage = self . _processing_stages [ - 1 ] step_nodes : typing . List [ PipelineStep ] = [ node for node in self . _data_flow_graph_simple . nodes if isinstance ( node , PipelineStep ) ] all_required_inputs = [] for step_id in last_stage : step = self . get_step ( step_id ) step_nodes . remove ( step ) for k , s_inp in self . get_step_inputs ( step_id ) . items (): if not s_inp . value_schema . is_required (): continue all_required_inputs . append ( s_inp ) for pipeline_input in self . pipeline_inputs . values (): for last_step_input in all_required_inputs : try : path = nx . shortest_path ( self . _data_flow_graph_simple , pipeline_input , last_step_input ) for p in path : if p in step_nodes : step_nodes . remove ( p ) except ( NetworkXNoPath , NodeNotFound ): pass # print(\"NO PATH\") # print(f\"{pipeline_input} -> {last_step_input}\") for s in step_nodes : s . required = False for input_name , inp in self . pipeline_inputs . items (): steps = set () for ci in inp . connected_inputs : steps . add ( ci . step_id ) optional = True for step_id in steps : step = self . get_step ( step_id ) if step . required : optional = False break if optional : inp . value_schema . optional = True def extend ( self , other : typing . Union [ \"Pipeline\" , \"PipelineStructure\" , \"PipelineConfig\" , typing . Mapping [ str , typing . Any ], ], input_links : typing . Optional [ typing . Mapping [ str , typing . Iterable [ StepValueAddress ]] ] = None , ) -> \"PipelineStructure\" : return extend_pipeline ( self , other ) def to_details ( self ) -> \"PipelineStructureDesc\" : from kiara.info.pipelines import PipelineStructureDesc return PipelineStructureDesc . create_pipeline_structure_desc ( pipeline = self ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : d = self . to_details () yield d","title":"PipelineStructure"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.values","text":"","title":"values"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.values.PipelineInputRef","text":"An input to a pipeline. Source code in kiara/pipeline/values.py class PipelineInputRef ( ValueRef ): \"\"\"An input to a pipeline.\"\"\" connected_inputs : typing . List [ StepValueAddress ] = Field ( description = \"The step inputs that are connected to this pipeline input\" , default_factory = list , ) is_constant : bool = Field ( \"Whether this input is a constant and can't be changed by the user.\" ) @property def alias ( self ) -> str : return generate_step_alias ( PIPELINE_PARENT_MARKER , self . value_name )","title":"PipelineInputRef"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.values.PipelineOutputRef","text":"An output to a pipeline. Source code in kiara/pipeline/values.py class PipelineOutputRef ( ValueRef ): \"\"\"An output to a pipeline.\"\"\" connected_output : StepValueAddress = Field ( description = \"Connected step outputs.\" ) @property def alias ( self ) -> str : return generate_step_alias ( PIPELINE_PARENT_MARKER , self . value_name )","title":"PipelineOutputRef"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.values.StepInputRef","text":"An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both. Source code in kiara/pipeline/values.py class StepInputRef ( ValueRef ): \"\"\"An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both. \"\"\" step_id : str = Field ( description = \"The step id.\" ) connected_outputs : typing . Optional [ typing . List [ StepValueAddress ]] = Field ( default = None , description = \"A potential connected list of one or several module outputs.\" , ) connected_pipeline_input : typing . Optional [ str ] = Field ( default = None , description = \"A potential pipeline input.\" ) is_constant : bool = Field ( \"Whether this input is a constant and can't be changed by the user.\" ) @root_validator ( pre = True ) def ensure_single_connected_item ( cls , values ): if values . get ( \"connected_outputs\" , None ) and values . get ( \"connected_pipeline_input\" ): raise ValueError ( \"Multiple connected items, only one allowed.\" ) return values @property def alias ( self ) -> str : return generate_step_alias ( self . step_id , self . value_name ) @property def address ( self ) -> StepValueAddress : return StepValueAddress ( step_id = self . step_id , value_name = self . value_name ) def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . step_id } . { self . value_name } ( { self . value_schema . type } )\"","title":"StepInputRef"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.values.StepOutputRef","text":"An output to a step. Source code in kiara/pipeline/values.py class StepOutputRef ( ValueRef ): \"\"\"An output to a step.\"\"\" class Config : allow_mutation = True step_id : str = Field ( description = \"The step id.\" ) pipeline_output : typing . Optional [ str ] = Field ( description = \"The connected pipeline output.\" ) connected_inputs : typing . List [ StepValueAddress ] = Field ( description = \"The step inputs that are connected to this step output\" , default_factory = list , ) @property def alias ( self ) -> str : return generate_step_alias ( self . step_id , self . value_name ) @property def address ( self ) -> StepValueAddress : return StepValueAddress ( step_id = self . step_id , value_name = self . value_name ) def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . step_id } . { self . value_name } ( { self . value_schema . type } )\"","title":"StepOutputRef"},{"location":"reference/kiara/pipeline/__init__/#kiara.pipeline.values.ValueRef","text":"An object that holds information about the location of a value within a pipeline (or other structure). Basically, a ValueRef helps the containing object where in its structure the value belongs (for example so it can update dependent other values). A ValueRef object (obviously) does not contain the value itself. There are four different ValueRef type that are relevant for pipelines: kiara.pipeline.values.StepInputRef : an input to a step kiara.pipeline.values.StepOutputRef : an output of a step kiara.pipeline.values.PipelineInputRef : an input to a pipeline kiara.pipeline.values.PipelineOutputRef : an output for a pipeline Several ValueRef objects can target the same value, for example a step output and a connected step input would reference the same Value (in most cases).. Source code in kiara/pipeline/values.py class ValueRef ( BaseModel ): \"\"\"An object that holds information about the location of a value within a pipeline (or other structure). Basically, a `ValueRef` helps the containing object where in its structure the value belongs (for example so it can update dependent other values). A `ValueRef` object (obviously) does not contain the value itself. There are four different ValueRef type that are relevant for pipelines: - [kiara.pipeline.values.StepInputRef][]: an input to a step - [kiara.pipeline.values.StepOutputRef][]: an output of a step - [kiara.pipeline.values.PipelineInputRef][]: an input to a pipeline - [kiara.pipeline.values.PipelineOutputRef][]: an output for a pipeline Several `ValueRef` objects can target the same value, for example a step output and a connected step input would reference the same `Value` (in most cases).. \"\"\" class Config : allow_mutation = True extra = Extra . forbid _id : uuid . UUID = PrivateAttr ( default_factory = uuid . uuid4 ) value_name : str value_schema : ValueSchema # pipeline_id: str def __eq__ ( self , other ): if not isinstance ( other , self . __class__ ): return False return self . _id == other . _id def __hash__ ( self ): return hash ( self . _id ) def __repr__ ( self ): step_id = \"\" if hasattr ( self , \"step_id\" ): step_id = f \" step_id=' { self . step_id } '\" return f \" { self . __class__ . __name__ } (value_name=' { self . value_name } ' { step_id } )\" def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . value_name } ( { self . value_schema . type } )\"","title":"ValueRef"},{"location":"reference/kiara/pipeline/config/","text":"PipelineConfig ( ModuleTypeConfigSchema ) pydantic-model \u00b6 A class to hold the configuration for a PipelineModule . If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify auto , in which case Kiara will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a nand logic gate (in Python): and_step = PipelineStepConfig ( module_type = \"and\" , step_id = \"and\" ) not_step = PipelineStepConfig ( module_type = \"not\" , step_id = \"not\" , input_links = { \"a\" : [ \"and.y\" ]} nand_p_conf = PipelineConfig ( doc = \"Returns 'False' if both inputs are 'True'.\" , steps = [ and_step , not_step ], input_aliases = { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, output_aliases = { \"not__y\" : \"y\" }} Or, the same thing in json: { \"module_type_name\" : \"nand\" , \"doc\" : \"Returns 'False' if both inputs are 'True'.\" , \"steps\" : [ { \"module_type\" : \"and\" , \"step_id\" : \"and\" }, { \"module_type\" : \"not\" , \"step_id\" : \"not\" , \"input_links\" : { \"a\" : \"and.y\" } } ], \"input_aliases\" : { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, \"output_aliases\" : { \"not__y\" : \"y\" } } Source code in kiara/pipeline/config.py class PipelineConfig ( ModuleTypeConfigSchema ): \"\"\"A class to hold the configuration for a [PipelineModule][kiara.pipeline.module.PipelineModule]. If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify ``auto``, in which case *Kiara* will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a ``nand`` logic gate (in Python): ``` python and_step = PipelineStepConfig(module_type=\"and\", step_id=\"and\") not_step = PipelineStepConfig(module_type=\"not\", step_id=\"not\", input_links={\"a\": [\"and.y\"]} nand_p_conf = PipelineConfig(doc=\"Returns 'False' if both inputs are 'True'.\", steps=[and_step, not_step], input_aliases={ \"and__a\": \"a\", \"and__b\": \"b\" }, output_aliases={ \"not__y\": \"y\" }} ``` Or, the same thing in json: ``` json { \"module_type_name\": \"nand\", \"doc\": \"Returns 'False' if both inputs are 'True'.\", \"steps\": [ { \"module_type\": \"and\", \"step_id\": \"and\" }, { \"module_type\": \"not\", \"step_id\": \"not\", \"input_links\": { \"a\": \"and.y\" } } ], \"input_aliases\": { \"and__a\": \"a\", \"and__b\": \"b\" }, \"output_aliases\": { \"not__y\": \"y\" } } ``` \"\"\" # @classmethod # def from_file(cls, path: typing.Union[str, Path]): # # content = get_data_from_file(path) # return PipelineConfig(**content) @classmethod def create_pipeline_config ( cls , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineConfig\" : \"\"\"Create a PipelineModule instance. The main 'config' argument here can be either: - a string: in which case it needs to be (in that order): - a module id - an operation id - a path to a local file - a [ModuleConfig][kiara.module_config.ModuleConfig] object - a dict (to create a `ModuleInstsanceConfig` from Arguments: kiara: the kiara context config: the 'main' config object module_config: in case the 'main' config object was a module id, this argument is used to instantiate the module kiara: the kiara context (will use default context instance if not provided) \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () module_config_obj = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) if not module_config_obj . module_type == \"pipeline\" : raise Exception ( f \"Not a valid pipeline configuration: { config } \" ) # TODO: this is a bit round-about, to create a module config first, but it probably doesn't matter pipeline_config_data = module_config_obj . module_config module : PipelineConfig = PipelineConfig ( ** pipeline_config_data ) return module class Config : extra = Extra . allow validate_assignment = True steps : typing . List [ PipelineStepConfig ] = Field ( default_factory = list , description = \"A list of steps/modules of this pipeline, and their connections.\" , ) input_aliases : typing . Union [ str , typing . Dict [ str , str ]] = Field ( default_factory = dict , description = \"A map of input aliases, with the calculated (<step_id>__<input_name> -- double underscore!) name as key, and a string (the resulting workflow input alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.\" , ) output_aliases : typing . Union [ str , typing . Dict [ str , str ]] = Field ( default_factory = dict , description = \"A map of output aliases, with the calculated (<step_id>__<output_name> -- double underscore!) name as key, and a string (the resulting workflow output alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.\" , ) documentation : str = Field ( default = \"-- n/a --\" , description = \"Documentation about what the pipeline does.\" ) context : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"Metadata for this workflow.\" ) @validator ( \"steps\" , pre = True ) def _validate_steps ( cls , v ): steps = [] for step in v : if isinstance ( step , PipelineStepConfig ): steps . append ( step ) elif isinstance ( step , typing . Mapping ): steps . append ( PipelineStepConfig ( ** step )) else : raise TypeError ( step ) return steps def create_pipeline_structure ( self , kiara : typing . Optional [ \"Kiara\" ] = None ) -> \"PipelineStructure\" : from kiara import Kiara , PipelineStructure if kiara is None : kiara = Kiara . instance () ps = PipelineStructure ( config = self , kiara = kiara , ) return ps def create_pipeline ( self , controller : typing . Optional [ \"PipelineController\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): # if parent_id is None: # parent_id = DEFAULT_PIPELINE_PARENT_ID structure = self . create_pipeline_structure ( kiara = kiara ) from kiara import Pipeline pipeline = Pipeline ( structure = structure , controller = controller , ) return pipeline def create_pipeline_module ( self , module_id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineModule\" : if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () from kiara.pipeline.module import PipelineModule module = PipelineModule ( id = module_id , parent_id = parent_id , module_config = self , kiara = kiara , ) return module # def __rich_console__( # self, console: Console, options: ConsoleOptions # ) -> RenderResult: # # table = Table(show_header=False, box=box.SIMPLE) context : Dict [ str , Any ] pydantic-field \u00b6 Metadata for this workflow. documentation : str pydantic-field \u00b6 Documentation about what the pipeline does. input_aliases : Union [ str , Dict [ str , str ]] pydantic-field \u00b6 A map of input aliases, with the calculated ( __ -- double underscore!) name as key, and a string (the resulting workflow input alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible. output_aliases : Union [ str , Dict [ str , str ]] pydantic-field \u00b6 A map of output aliases, with the calculated ( __ -- double underscore!) name as key, and a string (the resulting workflow output alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible. steps : List [ kiara . pipeline . config . PipelineStepConfig ] pydantic-field \u00b6 A list of steps/modules of this pipeline, and their connections. create_pipeline_config ( config , module_config = None , kiara = None ) classmethod \u00b6 Create a PipelineModule instance. The main 'config' argument here can be either: a string: in which case it needs to be (in that order): a module id an operation id a path to a local file a ModuleConfig object a dict (to create a ModuleInstsanceConfig from Parameters: Name Type Description Default kiara Optional[Kiara] the kiara context None config Union[kiara.module_config.ModuleConfig, Mapping[str, Any], str] the 'main' config object required module_config Optional[Mapping[str, Any]] in case the 'main' config object was a module id, this argument is used to instantiate the module None kiara Optional[Kiara] the kiara context (will use default context instance if not provided) None Source code in kiara/pipeline/config.py @classmethod def create_pipeline_config ( cls , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineConfig\" : \"\"\"Create a PipelineModule instance. The main 'config' argument here can be either: - a string: in which case it needs to be (in that order): - a module id - an operation id - a path to a local file - a [ModuleConfig][kiara.module_config.ModuleConfig] object - a dict (to create a `ModuleInstsanceConfig` from Arguments: kiara: the kiara context config: the 'main' config object module_config: in case the 'main' config object was a module id, this argument is used to instantiate the module kiara: the kiara context (will use default context instance if not provided) \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () module_config_obj = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) if not module_config_obj . module_type == \"pipeline\" : raise Exception ( f \"Not a valid pipeline configuration: { config } \" ) # TODO: this is a bit round-about, to create a module config first, but it probably doesn't matter pipeline_config_data = module_config_obj . module_config module : PipelineConfig = PipelineConfig ( ** pipeline_config_data ) return module PipelineStepConfig ( ModuleConfig ) pydantic-model \u00b6 A class to hold the configuration of one module within a PipelineModule . Source code in kiara/pipeline/config.py class PipelineStepConfig ( ModuleConfig ): \"\"\"A class to hold the configuration of one module within a [PipelineModule][kiara.pipeline.module.PipelineModule].\"\"\" class Config : extra = Extra . forbid validate_assignment = True step_id : str = Field ( description = \"The id of the step.\" ) input_links : typing . Dict [ str , typing . List [ StepValueAddress ]] = Field ( default_factory = dict , description = \"The map with the name of an input link as key, and the connected module output name(s) as value.\" , ) @root_validator ( pre = True ) def create_step_id ( cls , values ): if \"module_type\" not in values : raise ValueError ( \"No 'module_type' specified.\" ) if \"step_id\" not in values or not values [ \"step_id\" ]: values [ \"step_id\" ] = slugify ( values [ \"module_type\" ]) return values @validator ( \"step_id\" ) def ensure_valid_id ( cls , v ): # TODO: check with regex if \".\" in v or \" \" in v : raise ValueError ( f \"Step id can't contain special characters or whitespaces: { v } \" ) return v @validator ( \"module_config\" , pre = True ) def ensure_dict ( cls , v ): if v is None : v = {} return v @validator ( \"input_links\" , pre = True ) def ensure_input_links_valid ( cls , v ): if v is None : v = {} result = {} for input_name , output in v . items (): input_links = ensure_step_value_addresses ( default_field_name = input_name , link = output ) result [ input_name ] = input_links return result input_links : Dict [ str , List [ kiara . pipeline . StepValueAddress ]] pydantic-field \u00b6 The map with the name of an input link as key, and the connected module output name(s) as value. step_id : str pydantic-field required \u00b6 The id of the step. StepDesc ( BaseModel ) pydantic-model \u00b6 Details of a single PipelineStep (which lives within a Pipeline Source code in kiara/pipeline/config.py class StepDesc ( BaseModel ): \"\"\"Details of a single [PipelineStep][kiara.pipeline.structure.PipelineStep] (which lives within a [Pipeline][kiara.pipeline.pipeline.Pipeline]\"\"\" class Config : allow_mutation = False extra = Extra . forbid step : PipelineStep = Field ( description = \"Attributes of the step itself.\" ) processing_stage : int = Field ( description = \"The processing stage of this step within a Pipeline.\" ) input_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"\"\"A map that explains what elements connect to this steps inputs. A connection could either be a Pipeline input (indicated by the ``__pipeline__`` token), or another steps output. Example: ``` json input_connections: { \"a\": [\"__pipeline__.a\"], \"b\": [\"step_one.a\"] } ``` \"\"\" ) output_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"A map that explains what elemnts connect to this steps outputs. A connection could be either a Pipeline output, or another steps input.\" ) required : bool = Field ( description = \"Whether this step is always required, or potentially could be skipped in case some inputs are not available.\" ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield f \"[b]Step: { self . step . step_id } [ \\b ]\" input_connections : Dict [ str , List [ str ]] pydantic-field required \u00b6 A map that explains what elements connect to this steps inputs. A connection could either be a Pipeline input (indicated by the __pipeline__ token), or another steps output. Examples: i n pu t _co nne c t io ns : { \"a\" : [ \"__pipeline__.a\" ], \"b\" : [ \"step_one.a\" ] } output_connections : Dict [ str , List [ str ]] pydantic-field required \u00b6 A map that explains what elemnts connect to this steps outputs. A connection could be either a Pipeline output, or another steps input. processing_stage : int pydantic-field required \u00b6 The processing stage of this step within a Pipeline. required : bool pydantic-field required \u00b6 Whether this step is always required, or potentially could be skipped in case some inputs are not available. step : PipelineStep pydantic-field required \u00b6 Attributes of the step itself.","title":"config"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.PipelineConfig","text":"A class to hold the configuration for a PipelineModule . If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify auto , in which case Kiara will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a nand logic gate (in Python): and_step = PipelineStepConfig ( module_type = \"and\" , step_id = \"and\" ) not_step = PipelineStepConfig ( module_type = \"not\" , step_id = \"not\" , input_links = { \"a\" : [ \"and.y\" ]} nand_p_conf = PipelineConfig ( doc = \"Returns 'False' if both inputs are 'True'.\" , steps = [ and_step , not_step ], input_aliases = { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, output_aliases = { \"not__y\" : \"y\" }} Or, the same thing in json: { \"module_type_name\" : \"nand\" , \"doc\" : \"Returns 'False' if both inputs are 'True'.\" , \"steps\" : [ { \"module_type\" : \"and\" , \"step_id\" : \"and\" }, { \"module_type\" : \"not\" , \"step_id\" : \"not\" , \"input_links\" : { \"a\" : \"and.y\" } } ], \"input_aliases\" : { \"and__a\" : \"a\" , \"and__b\" : \"b\" }, \"output_aliases\" : { \"not__y\" : \"y\" } } Source code in kiara/pipeline/config.py class PipelineConfig ( ModuleTypeConfigSchema ): \"\"\"A class to hold the configuration for a [PipelineModule][kiara.pipeline.module.PipelineModule]. If you want to control the pipeline input and output names, you need to have to provide a map that uses the autogenerated field name ([step_id]__[field_name] -- 2 underscores!!) as key, and the desired field name as value. The reason that schema for the autogenerated field names exist is that it's hard to ensure the uniqueness of each field; some steps can have the same input field names, but will need different input values. In some cases, some inputs of different steps need the same input. Those sorts of things. So, to make sure that we always use the right values, I chose to implement a conservative default approach, accepting that in some cases the user will be prompted for duplicate inputs for the same value. To remedy that, the pipeline creator has the option to manually specify a mapping to rename some or all of the input/output fields. Further, because in a lot of cases there won't be any overlapping fields, the creator can specify ``auto``, in which case *Kiara* will automatically create a mapping that tries to map autogenerated field names to the shortest possible names for each case. Examples: Configuration for a pipeline module that functions as a ``nand`` logic gate (in Python): ``` python and_step = PipelineStepConfig(module_type=\"and\", step_id=\"and\") not_step = PipelineStepConfig(module_type=\"not\", step_id=\"not\", input_links={\"a\": [\"and.y\"]} nand_p_conf = PipelineConfig(doc=\"Returns 'False' if both inputs are 'True'.\", steps=[and_step, not_step], input_aliases={ \"and__a\": \"a\", \"and__b\": \"b\" }, output_aliases={ \"not__y\": \"y\" }} ``` Or, the same thing in json: ``` json { \"module_type_name\": \"nand\", \"doc\": \"Returns 'False' if both inputs are 'True'.\", \"steps\": [ { \"module_type\": \"and\", \"step_id\": \"and\" }, { \"module_type\": \"not\", \"step_id\": \"not\", \"input_links\": { \"a\": \"and.y\" } } ], \"input_aliases\": { \"and__a\": \"a\", \"and__b\": \"b\" }, \"output_aliases\": { \"not__y\": \"y\" } } ``` \"\"\" # @classmethod # def from_file(cls, path: typing.Union[str, Path]): # # content = get_data_from_file(path) # return PipelineConfig(**content) @classmethod def create_pipeline_config ( cls , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineConfig\" : \"\"\"Create a PipelineModule instance. The main 'config' argument here can be either: - a string: in which case it needs to be (in that order): - a module id - an operation id - a path to a local file - a [ModuleConfig][kiara.module_config.ModuleConfig] object - a dict (to create a `ModuleInstsanceConfig` from Arguments: kiara: the kiara context config: the 'main' config object module_config: in case the 'main' config object was a module id, this argument is used to instantiate the module kiara: the kiara context (will use default context instance if not provided) \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () module_config_obj = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) if not module_config_obj . module_type == \"pipeline\" : raise Exception ( f \"Not a valid pipeline configuration: { config } \" ) # TODO: this is a bit round-about, to create a module config first, but it probably doesn't matter pipeline_config_data = module_config_obj . module_config module : PipelineConfig = PipelineConfig ( ** pipeline_config_data ) return module class Config : extra = Extra . allow validate_assignment = True steps : typing . List [ PipelineStepConfig ] = Field ( default_factory = list , description = \"A list of steps/modules of this pipeline, and their connections.\" , ) input_aliases : typing . Union [ str , typing . Dict [ str , str ]] = Field ( default_factory = dict , description = \"A map of input aliases, with the calculated (<step_id>__<input_name> -- double underscore!) name as key, and a string (the resulting workflow input alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.\" , ) output_aliases : typing . Union [ str , typing . Dict [ str , str ]] = Field ( default_factory = dict , description = \"A map of output aliases, with the calculated (<step_id>__<output_name> -- double underscore!) name as key, and a string (the resulting workflow output alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.\" , ) documentation : str = Field ( default = \"-- n/a --\" , description = \"Documentation about what the pipeline does.\" ) context : typing . Dict [ str , typing . Any ] = Field ( default_factory = dict , description = \"Metadata for this workflow.\" ) @validator ( \"steps\" , pre = True ) def _validate_steps ( cls , v ): steps = [] for step in v : if isinstance ( step , PipelineStepConfig ): steps . append ( step ) elif isinstance ( step , typing . Mapping ): steps . append ( PipelineStepConfig ( ** step )) else : raise TypeError ( step ) return steps def create_pipeline_structure ( self , kiara : typing . Optional [ \"Kiara\" ] = None ) -> \"PipelineStructure\" : from kiara import Kiara , PipelineStructure if kiara is None : kiara = Kiara . instance () ps = PipelineStructure ( config = self , kiara = kiara , ) return ps def create_pipeline ( self , controller : typing . Optional [ \"PipelineController\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): # if parent_id is None: # parent_id = DEFAULT_PIPELINE_PARENT_ID structure = self . create_pipeline_structure ( kiara = kiara ) from kiara import Pipeline pipeline = Pipeline ( structure = structure , controller = controller , ) return pipeline def create_pipeline_module ( self , module_id : typing . Optional [ str ] = None , parent_id : typing . Optional [ str ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineModule\" : if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () from kiara.pipeline.module import PipelineModule module = PipelineModule ( id = module_id , parent_id = parent_id , module_config = self , kiara = kiara , ) return module # def __rich_console__( # self, console: Console, options: ConsoleOptions # ) -> RenderResult: # # table = Table(show_header=False, box=box.SIMPLE)","title":"PipelineConfig"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.PipelineConfig.context","text":"Metadata for this workflow.","title":"context"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.PipelineConfig.documentation","text":"Documentation about what the pipeline does.","title":"documentation"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.PipelineConfig.input_aliases","text":"A map of input aliases, with the calculated ( __ -- double underscore!) name as key, and a string (the resulting workflow input alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.","title":"input_aliases"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.PipelineConfig.output_aliases","text":"A map of output aliases, with the calculated ( __ -- double underscore!) name as key, and a string (the resulting workflow output alias) as value. Check the documentation for the config class for which marker strings can be used to automatically create this map if possible.","title":"output_aliases"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.PipelineConfig.steps","text":"A list of steps/modules of this pipeline, and their connections.","title":"steps"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.PipelineConfig.create_pipeline_config","text":"Create a PipelineModule instance. The main 'config' argument here can be either: a string: in which case it needs to be (in that order): a module id an operation id a path to a local file a ModuleConfig object a dict (to create a ModuleInstsanceConfig from Parameters: Name Type Description Default kiara Optional[Kiara] the kiara context None config Union[kiara.module_config.ModuleConfig, Mapping[str, Any], str] the 'main' config object required module_config Optional[Mapping[str, Any]] in case the 'main' config object was a module id, this argument is used to instantiate the module None kiara Optional[Kiara] the kiara context (will use default context instance if not provided) None Source code in kiara/pipeline/config.py @classmethod def create_pipeline_config ( cls , config : typing . Union [ ModuleConfig , typing . Mapping [ str , typing . Any ], str ], module_config : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"PipelineConfig\" : \"\"\"Create a PipelineModule instance. The main 'config' argument here can be either: - a string: in which case it needs to be (in that order): - a module id - an operation id - a path to a local file - a [ModuleConfig][kiara.module_config.ModuleConfig] object - a dict (to create a `ModuleInstsanceConfig` from Arguments: kiara: the kiara context config: the 'main' config object module_config: in case the 'main' config object was a module id, this argument is used to instantiate the module kiara: the kiara context (will use default context instance if not provided) \"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () module_config_obj = ModuleConfig . create_module_config ( config = config , module_config = module_config , kiara = kiara ) if not module_config_obj . module_type == \"pipeline\" : raise Exception ( f \"Not a valid pipeline configuration: { config } \" ) # TODO: this is a bit round-about, to create a module config first, but it probably doesn't matter pipeline_config_data = module_config_obj . module_config module : PipelineConfig = PipelineConfig ( ** pipeline_config_data ) return module","title":"create_pipeline_config()"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.PipelineStepConfig","text":"A class to hold the configuration of one module within a PipelineModule . Source code in kiara/pipeline/config.py class PipelineStepConfig ( ModuleConfig ): \"\"\"A class to hold the configuration of one module within a [PipelineModule][kiara.pipeline.module.PipelineModule].\"\"\" class Config : extra = Extra . forbid validate_assignment = True step_id : str = Field ( description = \"The id of the step.\" ) input_links : typing . Dict [ str , typing . List [ StepValueAddress ]] = Field ( default_factory = dict , description = \"The map with the name of an input link as key, and the connected module output name(s) as value.\" , ) @root_validator ( pre = True ) def create_step_id ( cls , values ): if \"module_type\" not in values : raise ValueError ( \"No 'module_type' specified.\" ) if \"step_id\" not in values or not values [ \"step_id\" ]: values [ \"step_id\" ] = slugify ( values [ \"module_type\" ]) return values @validator ( \"step_id\" ) def ensure_valid_id ( cls , v ): # TODO: check with regex if \".\" in v or \" \" in v : raise ValueError ( f \"Step id can't contain special characters or whitespaces: { v } \" ) return v @validator ( \"module_config\" , pre = True ) def ensure_dict ( cls , v ): if v is None : v = {} return v @validator ( \"input_links\" , pre = True ) def ensure_input_links_valid ( cls , v ): if v is None : v = {} result = {} for input_name , output in v . items (): input_links = ensure_step_value_addresses ( default_field_name = input_name , link = output ) result [ input_name ] = input_links return result","title":"PipelineStepConfig"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.PipelineStepConfig.input_links","text":"The map with the name of an input link as key, and the connected module output name(s) as value.","title":"input_links"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.PipelineStepConfig.step_id","text":"The id of the step.","title":"step_id"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.StepDesc","text":"Details of a single PipelineStep (which lives within a Pipeline Source code in kiara/pipeline/config.py class StepDesc ( BaseModel ): \"\"\"Details of a single [PipelineStep][kiara.pipeline.structure.PipelineStep] (which lives within a [Pipeline][kiara.pipeline.pipeline.Pipeline]\"\"\" class Config : allow_mutation = False extra = Extra . forbid step : PipelineStep = Field ( description = \"Attributes of the step itself.\" ) processing_stage : int = Field ( description = \"The processing stage of this step within a Pipeline.\" ) input_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"\"\"A map that explains what elements connect to this steps inputs. A connection could either be a Pipeline input (indicated by the ``__pipeline__`` token), or another steps output. Example: ``` json input_connections: { \"a\": [\"__pipeline__.a\"], \"b\": [\"step_one.a\"] } ``` \"\"\" ) output_connections : typing . Dict [ str , typing . List [ str ]] = Field ( description = \"A map that explains what elemnts connect to this steps outputs. A connection could be either a Pipeline output, or another steps input.\" ) required : bool = Field ( description = \"Whether this step is always required, or potentially could be skipped in case some inputs are not available.\" ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield f \"[b]Step: { self . step . step_id } [ \\b ]\"","title":"StepDesc"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.StepDesc.input_connections","text":"A map that explains what elements connect to this steps inputs. A connection could either be a Pipeline input (indicated by the __pipeline__ token), or another steps output. Examples: i n pu t _co nne c t io ns : { \"a\" : [ \"__pipeline__.a\" ], \"b\" : [ \"step_one.a\" ] }","title":"input_connections"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.StepDesc.output_connections","text":"A map that explains what elemnts connect to this steps outputs. A connection could be either a Pipeline output, or another steps input.","title":"output_connections"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.StepDesc.processing_stage","text":"The processing stage of this step within a Pipeline.","title":"processing_stage"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.StepDesc.required","text":"Whether this step is always required, or potentially could be skipped in case some inputs are not available.","title":"required"},{"location":"reference/kiara/pipeline/config/#kiara.pipeline.config.StepDesc.step","text":"Attributes of the step itself.","title":"step"},{"location":"reference/kiara/pipeline/listeners/","text":"PipelineListener ( ABC ) \u00b6 Source code in kiara/pipeline/listeners.py class PipelineListener ( abc . ABC ): def step_inputs_changed ( self , event : StepInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Arguments: event: the step input event \"\"\" def step_outputs_changed ( self , event : StepOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Arguments: event: the step output event \"\"\" def pipeline_inputs_changed ( self , event : PipelineInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. !!! note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the ``step_inputs_changed`` method if you want to. This behaviour might change in the future. Arguments: event: the pipeline input event \"\"\" def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Arguments: event: the pipeline output event \"\"\" pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/pipeline/listeners.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. !!! note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the ``step_inputs_changed`` method if you want to. This behaviour might change in the future. Arguments: event: the pipeline input event \"\"\" pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/listeners.py def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Arguments: event: the pipeline output event \"\"\" step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/listeners.py def step_inputs_changed ( self , event : StepInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Arguments: event: the step input event \"\"\" step_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Parameters: Name Type Description Default event StepOutputEvent the step output event required Source code in kiara/pipeline/listeners.py def step_outputs_changed ( self , event : StepOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Arguments: event: the step output event \"\"\"","title":"listeners"},{"location":"reference/kiara/pipeline/listeners/#kiara.pipeline.listeners.PipelineListener","text":"Source code in kiara/pipeline/listeners.py class PipelineListener ( abc . ABC ): def step_inputs_changed ( self , event : StepInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Arguments: event: the step input event \"\"\" def step_outputs_changed ( self , event : StepOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Arguments: event: the step output event \"\"\" def pipeline_inputs_changed ( self , event : PipelineInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. !!! note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the ``step_inputs_changed`` method if you want to. This behaviour might change in the future. Arguments: event: the pipeline input event \"\"\" def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Arguments: event: the pipeline output event \"\"\"","title":"PipelineListener"},{"location":"reference/kiara/pipeline/listeners/#kiara.pipeline.listeners.PipelineListener.pipeline_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/pipeline/listeners.py def pipeline_inputs_changed ( self , event : PipelineInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. !!! note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the ``step_inputs_changed`` method if you want to. This behaviour might change in the future. Arguments: event: the pipeline input event \"\"\"","title":"pipeline_inputs_changed()"},{"location":"reference/kiara/pipeline/listeners/#kiara.pipeline.listeners.PipelineListener.pipeline_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/listeners.py def pipeline_outputs_changed ( self , event : PipelineOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Arguments: event: the pipeline output event \"\"\"","title":"pipeline_outputs_changed()"},{"location":"reference/kiara/pipeline/listeners/#kiara.pipeline.listeners.PipelineListener.step_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/listeners.py def step_inputs_changed ( self , event : StepInputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Arguments: event: the step input event \"\"\"","title":"step_inputs_changed()"},{"location":"reference/kiara/pipeline/listeners/#kiara.pipeline.listeners.PipelineListener.step_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Parameters: Name Type Description Default event StepOutputEvent the step output event required Source code in kiara/pipeline/listeners.py def step_outputs_changed ( self , event : StepOutputEvent ): \"\"\"Method to override if the implementing controller needs to react to events where one or several step outputs have changed. Arguments: event: the step output event \"\"\"","title":"step_outputs_changed()"},{"location":"reference/kiara/pipeline/module/","text":"PipelineModule ( KiaraModule ) \u00b6 A KiaraModule that contains a collection of interconnected other modules. Source code in kiara/pipeline/module.py class PipelineModule ( KiaraModule [ PipelineConfig ]): \"\"\"A [KiaraModule][kiara.module.KiaraModule] that contains a collection of interconnected other modules.\"\"\" _config_cls : typing . Type [ PipelineConfig ] = PipelineConfig # type: ignore _module_type_id = \"pipeline\" @classmethod def is_pipeline ( cls ) -> bool : return True def __init__ ( self , id : typing . Optional [ str ], parent_id : typing . Optional [ str ] = None , module_config : typing . Union [ None , PipelineConfig , typing . Mapping [ str , typing . Any ] ] = None , # controller: typing.Union[ # None, PipelineController, str, typing.Type[PipelineController] # ] = None, kiara : typing . Optional [ \"Kiara\" ] = None , ): # if controller is not None and not isinstance(controller, PipelineController): # raise NotImplementedError() # if controller is None: super () . __init__ ( id = id , parent_id = parent_id , module_config = module_config , kiara = kiara , ) self . _pipeline_structure : PipelineStructure = self . _create_structure () assert not self . _config . constants self . _config . constants = dict ( self . _pipeline_structure . constants ) @property def structure ( self ) -> PipelineStructure : \"\"\"The ``PipelineStructure`` of this module.\"\"\" return self . _pipeline_structure def _create_structure ( self ) -> PipelineStructure : pipeline_structure = PipelineStructure ( config = self . config , kiara = self . _kiara ) return pipeline_structure def create_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_input_schema def create_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_output_schema def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : from kiara import Pipeline # controller = BatchController(auto_process=False, kiara=self._kiara) pipeline = Pipeline ( structure = self . structure ) pipeline . inputs . set_values ( ** inputs ) if not pipeline . inputs . items_are_valid (): raise KiaraProcessingException ( f \"Can't start processing of { self . _module_type_id } pipeline: one or several inputs missing or invalid.\" ) # type: ignore if not pipeline . status == StepStatus . RESULTS_READY : # TODO: error details raise KiaraProcessingException ( f \"Error when running pipeline of type ' { self . _module_type_id } '.\" ) # type: ignore outputs . set_values ( ** pipeline . outputs ) structure : PipelineStructure property readonly \u00b6 The PipelineStructure of this module. create_input_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/pipeline/module.py def create_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_input_schema create_output_schema ( self ) \u00b6 Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/pipeline/module.py def create_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_output_schema is_pipeline () classmethod \u00b6 Check whether this module type is a pipeline, or not. Source code in kiara/pipeline/module.py @classmethod def is_pipeline ( cls ) -> bool : return True","title":"module"},{"location":"reference/kiara/pipeline/module/#kiara.pipeline.module.PipelineModule","text":"A KiaraModule that contains a collection of interconnected other modules. Source code in kiara/pipeline/module.py class PipelineModule ( KiaraModule [ PipelineConfig ]): \"\"\"A [KiaraModule][kiara.module.KiaraModule] that contains a collection of interconnected other modules.\"\"\" _config_cls : typing . Type [ PipelineConfig ] = PipelineConfig # type: ignore _module_type_id = \"pipeline\" @classmethod def is_pipeline ( cls ) -> bool : return True def __init__ ( self , id : typing . Optional [ str ], parent_id : typing . Optional [ str ] = None , module_config : typing . Union [ None , PipelineConfig , typing . Mapping [ str , typing . Any ] ] = None , # controller: typing.Union[ # None, PipelineController, str, typing.Type[PipelineController] # ] = None, kiara : typing . Optional [ \"Kiara\" ] = None , ): # if controller is not None and not isinstance(controller, PipelineController): # raise NotImplementedError() # if controller is None: super () . __init__ ( id = id , parent_id = parent_id , module_config = module_config , kiara = kiara , ) self . _pipeline_structure : PipelineStructure = self . _create_structure () assert not self . _config . constants self . _config . constants = dict ( self . _pipeline_structure . constants ) @property def structure ( self ) -> PipelineStructure : \"\"\"The ``PipelineStructure`` of this module.\"\"\" return self . _pipeline_structure def _create_structure ( self ) -> PipelineStructure : pipeline_structure = PipelineStructure ( config = self . config , kiara = self . _kiara ) return pipeline_structure def create_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_input_schema def create_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_output_schema def process ( self , inputs : ValueSet , outputs : ValueSet ) -> None : from kiara import Pipeline # controller = BatchController(auto_process=False, kiara=self._kiara) pipeline = Pipeline ( structure = self . structure ) pipeline . inputs . set_values ( ** inputs ) if not pipeline . inputs . items_are_valid (): raise KiaraProcessingException ( f \"Can't start processing of { self . _module_type_id } pipeline: one or several inputs missing or invalid.\" ) # type: ignore if not pipeline . status == StepStatus . RESULTS_READY : # TODO: error details raise KiaraProcessingException ( f \"Error when running pipeline of type ' { self . _module_type_id } '.\" ) # type: ignore outputs . set_values ( ** pipeline . outputs )","title":"PipelineModule"},{"location":"reference/kiara/pipeline/module/#kiara.pipeline.module.PipelineModule.structure","text":"The PipelineStructure of this module.","title":"structure"},{"location":"reference/kiara/pipeline/module/#kiara.pipeline.module.PipelineModule.create_input_schema","text":"Abstract method to implement by child classes, returns a description of the input schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[input_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this input]\", \"optional*': [boolean whether this input is optional or required (defaults to 'False')] \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/pipeline/module.py def create_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_input_schema","title":"create_input_schema()"},{"location":"reference/kiara/pipeline/module/#kiara.pipeline.module.PipelineModule.create_output_schema","text":"Abstract method to implement by child classes, returns a description of the output schema of this module. If returning a dictionary of dictionaries, the format of the return value is as follows (items with '*' are optional): { \"[output_field_name]: { \"type\": \"[value_type]\", \"doc*\": \"[a description of this output]\" \"[other_input_field_name]: { \"type: ... ... } Source code in kiara/pipeline/module.py def create_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return self . structure . pipeline_output_schema","title":"create_output_schema()"},{"location":"reference/kiara/pipeline/module/#kiara.pipeline.module.PipelineModule.is_pipeline","text":"Check whether this module type is a pipeline, or not. Source code in kiara/pipeline/module.py @classmethod def is_pipeline ( cls ) -> bool : return True","title":"is_pipeline()"},{"location":"reference/kiara/pipeline/pipeline/","text":"Pipeline \u00b6 An instance of a PipelineStructure that holds state for all of the inputs/outputs of the steps within. Source code in kiara/pipeline/pipeline.py class Pipeline ( object ): \"\"\"An instance of a [PipelineStructure][kiara.pipeline.structure.PipelineStructure] that holds state for all of the inputs/outputs of the steps within.\"\"\" def __init__ ( self , structure : PipelineStructure , # constants: typing.Optional[typing.Mapping[str, typing.Any]] = None, controller : typing . Optional [ PipelineController ] = None , title : typing . Optional [ str ] = None , ): self . _id : str = str ( uuid . uuid4 ()) if title is None : title = self . _id self . _title : str = title self . _structure : PipelineStructure = structure self . _pipeline_inputs : SlottedValueSet = None # type: ignore self . _pipeline_outputs : SlottedValueSet = None # type: ignore self . _step_inputs : typing . Mapping [ str , ValueSet ] = None # type: ignore self . _step_outputs : typing . Mapping [ str , ValueSet ] = None # type: ignore self . _value_refs : typing . Mapping [ ValueSlot , typing . Iterable [ ValueRef ]] = None # type: ignore self . _status : StepStatus = StepStatus . STALE self . _steps_by_stage : typing . Optional [ typing . Dict [ int , typing . Dict [ str , PipelineStep ]] ] = None self . _inputs_by_stage : typing . Optional [ typing . Dict [ int , typing . List [ str ]] ] = None self . _outputs_by_stage : typing . Optional [ typing . Dict [ int , typing . List [ str ]] ] = None self . _kiara : \"Kiara\" = self . _structure . _kiara self . _data_registry : DataRegistry = self . _kiara . data_registry self . _init_values () if controller is None : controller = BatchController ( self , kiara = self . _kiara ) else : controller . set_pipeline ( self ) self . _controller : PipelineController = controller self . _listeners : typing . List [ PipelineListener ] = [] self . _update_status () def __eq__ ( self , other ): if not isinstance ( other , Pipeline ): return False return self . _id == other . _id def __hash__ ( self ): return hash ( self . _id ) @property def id ( self ) -> str : return self . _id @property def title ( self ) -> str : return self . _title @property def structure ( self ) -> PipelineStructure : return self . _structure @property def controller ( self ) -> PipelineController : if self . _controller is None : raise Exception ( \"No controller set (yet).\" ) return self . _controller @property def inputs ( self ) -> SlottedValueSet : \"\"\"All (pipeline) input values of this pipeline.\"\"\" return self . _pipeline_inputs @property def outputs ( self ) -> SlottedValueSet : \"\"\"All (pipeline) output values of this pipeline.\"\"\" return self . _pipeline_outputs # def set_pipeline_inputs(self, **inputs: typing.Any): # self._controller.set_pipeline_inputs(**inputs) @property def step_ids ( self ) -> typing . Iterable [ str ]: \"\"\"Return all ids of the steps of this pipeline.\"\"\" return self . _structure . step_ids def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the object representing a step in this workflow, identified by the step id.\"\"\" return self . _structure . get_step ( step_id ) def get_steps_by_stage ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , PipelineStep ]]: \"\"\"Return a all pipeline steps, ordered by stage they belong to.\"\"\" if self . _steps_by_stage is not None : return self . _steps_by_stage result : typing . Dict [ int , typing . Dict [ str , PipelineStep ]] = {} for step_id in self . step_ids : step = self . get_step ( step_id ) stage = step . processing_stage assert stage is not None result . setdefault ( stage , {})[ step_id ] = step self . _steps_by_stage = result return self . _steps_by_stage def get_pipeline_inputs_by_stage ( self ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by stage they are first required.\"\"\" if self . _inputs_by_stage is not None : return self . _inputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . inputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepInputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _inputs_by_stage = result return self . _inputs_by_stage def get_pipeline_outputs_by_stage ( self , ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by the stage that needs to be executed before they are available.\"\"\" if self . _outputs_by_stage is not None : return self . _outputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . outputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepOutputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _outputs_by_stage = result return self . _outputs_by_stage def get_pipeline_inputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline inputs that are required for a stage to be processed. The result of this method does not include inputs that were required earlier already. \"\"\" return self . get_pipeline_inputs_by_stage () . get ( stage , []) def get_stage_for_pipeline_input ( self , input_name : str ) -> int : for stage , input_names in self . get_pipeline_inputs_by_stage () . items (): if input_name in input_names : return stage raise Exception ( f \"No input name ' { input_name } '. Available inputs: { ', ' . join ( self . inputs . keys ()) } \" ) def stage_for_pipeline_output ( self , output_name : str ) -> int : for stage , output_names in self . get_pipeline_outputs_by_stage () . items (): if output_name in output_names : return stage raise Exception ( f \"No output name ' { output_name } '. Available outputs: { ', ' . join ( self . outputs . keys ()) } \" ) def get_pipeline_outputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline outputs that are first available after the specified stage completed processing.\"\"\" return self . get_pipeline_outputs_by_stage () . get ( stage , []) def get_pipeline_inputs_for_step ( self , step_id : str ) -> typing . List [ str ]: result = [] for field_name , value_slot in self . inputs . _value_slots . items (): refs = self . _value_refs [ value_slot ] for ref in refs : if not isinstance ( ref , PipelineInputRef ): continue for ci in ref . connected_inputs : if ci . step_id == step_id and ref . value_name not in result : result . append ( ref . value_name ) return result def get_pipeline_outputs_for_step ( self , step_id : str ) -> typing . List [ str ]: result = [] for field_name , value_slot in self . outputs . _value_slots . items (): refs = self . _value_refs [ value_slot ] for ref in refs : if not isinstance ( ref , PipelineOutputRef ): continue if ( ref . connected_output . step_id == step_id and ref . value_name not in result ): result . append ( ref . value_name ) return result def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all inputs for a step id (incl. inputs that are not pipeline inputs but connected to other modules output).\"\"\" return self . _step_inputs [ step_id ] def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all outputs for a step id (incl. outputs that are not pipeline outputs).\"\"\" return self . _step_outputs [ step_id ] def add_listener ( self , listener : PipelineListener ) -> None : \"\"\"Add a listener taht gets notified on any internal pipeline input/output events.\"\"\" self . _listeners . append ( listener ) @property def status ( self ) -> StepStatus : \"\"\"Return the current status of this pipeline.\"\"\" return self . _state def _update_status ( self ): \"\"\"Make sure internal state variable is up to date.\"\"\" if self . inputs is None : new_state = StepStatus . STALE elif not self . inputs . items_are_valid (): new_state = StepStatus . STALE elif not self . outputs . items_are_valid (): new_state = StepStatus . INPUTS_READY else : new_state = StepStatus . RESULTS_READY self . _state = new_state def _init_values ( self ): \"\"\"Initialize this object. This should only be called once. Basically, this goes through all the inputs and outputs of all steps, and 'allocates' a PipelineValueInfo object for each of them. In case where output/input or pipeline-input/input points are connected, only one value item is allocated, since those refer to the same value. \"\"\" pipeline_inputs : typing . Dict [ str , ValueSlot ] = {} pipeline_outputs : typing . Dict [ str , ValueSlot ] = {} all_step_inputs : typing . Dict [ str , typing . Dict [ str , ValueSlot ]] = {} all_step_outputs : typing . Dict [ str , typing . Dict [ str , ValueSlot ]] = {} value_refs : typing . Dict [ ValueSlot , typing . List [ ValueRef ]] = {} # create the value objects that are associated with step outputs # all pipeline outputs are created here too, since the only place # those can be associated are step outputs for step_id , step_details in self . _structure . steps_details . items (): step_outputs : typing . Mapping [ str , StepOutputRef ] = step_details [ \"outputs\" ] for output_name , output_point in step_outputs . items (): init_output_value_item = self . _data_registry . register_data ( value_schema = output_point . value_schema ) output_value_slot = self . _data_registry . register_alias ( value_or_schema = init_output_value_item , callbacks = [ self ] ) value_refs . setdefault ( output_value_slot , []) . append ( output_point ) all_step_outputs . setdefault ( step_id , {})[ output_name ] = output_value_slot # not all step outputs necessarily need to be connected to a pipeline output if output_point . pipeline_output : pipeline_outputs [ output_point . pipeline_output ] = output_value_slot po = self . _structure . pipeline_outputs [ output_point . pipeline_output ] value_refs . setdefault ( output_value_slot , []) . append ( po ) # create the value objects that are associated with step inputs for step_id , step_details in self . _structure . steps_details . items (): step_inputs : typing . Mapping [ str , StepInputRef ] = step_details [ \"inputs\" ] for input_name , input_point in step_inputs . items (): # if this step input gets fed from a pipeline_input (meaning user input in most cases), # we need to create a DataValue for that pipeline input # vm = ValueMetadata( # origin=f\"{self.id}.steps.{step_id}.inputs.{input_point.value_name}\" # ) if input_point . connected_pipeline_input : connected_pipeline_input_name = input_point . connected_pipeline_input pipeline_input_field : PipelineInputRef = ( self . _structure . pipeline_inputs [ connected_pipeline_input_name ] ) pipeline_input_slot : ValueSlot = pipeline_inputs . get ( connected_pipeline_input_name , None ) if pipeline_input_slot is None : # if the pipeline input wasn't created by another step input before, # we need to take care of it here if pipeline_input_field . is_constant : init_value = self . structure . constants [ pipeline_input_field . value_name ] else : init_value = self . structure . defaults . get ( pipeline_input_field . value_name , SpecialValue . NOT_SET ) init_pipeline_input_value = self . _data_registry . register_data ( value_data = init_value , value_schema = pipeline_input_field . value_schema , ) # TODO: check whether it's a constant? pipeline_input_slot = self . _data_registry . register_alias ( value_or_schema = init_pipeline_input_value , callbacks = [ self ] ) value_refs . setdefault ( pipeline_input_slot , []) . append ( pipeline_input_field ) pipeline_inputs [ connected_pipeline_input_name ] = pipeline_input_slot all_step_inputs . setdefault ( step_id , {})[ input_name ] = pipeline_input_slot value_refs . setdefault ( pipeline_input_slot , []) . append ( input_point ) elif input_point . connected_outputs : for co in input_point . connected_outputs : if len ( input_point . connected_outputs ) == 1 and not co . sub_value : # this means the input is the same value as the connected output output_value : ValueSlot = all_step_outputs [ co . step_id ][ co . value_name ] all_step_inputs . setdefault ( input_point . step_id , {})[ input_point . value_name ] = output_value value_refs . setdefault ( output_value , []) . append ( input_point ) else : print ( input_point . connected_outputs ) raise NotImplementedError () # sub_value = co.sub_value # linked_values = {} # for co in input_point.connected_outputs: # output_value = all_step_outputs[co.step_id][co.value_name] # sub_value = co.sub_value # if len(input_point.connected_outputs) > 1 and not sub_value: # raise NotImplementedError() # sub_value = {\"config\": co.step_id} # if sub_value is not None: # raise NotImplementedError # # linked_values[output_value.id] = sub_value # # step_input = self._data_registry.register_linked_value( # parent_id=self.id, # linked_values=linked_values, # value_schema=input_point.value_schema, # value_refs=input_point, # ) # self._data_registry.register_callback( # self.values_updated, step_input # ) # all_step_inputs.setdefault(input_point.step_id, {})[ # input_point.value_name # ] = step_input else : raise Exception ( f \"Invalid value point type for this location: { input_point } \" ) if not pipeline_inputs : raise Exception ( f \"Can't init pipeline ' { self . title } ': no pipeline inputs\" ) self . _pipeline_inputs = SlottedValueSet ( items = pipeline_inputs , read_only = False , title = f \"Inputs for pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) if not pipeline_outputs : raise Exception ( f \"Can't init pipeline ' { self . title } ': no pipeline outputs\" ) self . _pipeline_outputs = SlottedValueSet ( items = pipeline_outputs , read_only = True , title = f \"Outputs for pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) self . _step_inputs = {} for step_id , inputs in all_step_inputs . items (): self . _step_inputs [ step_id ] = SlottedValueSet ( items = inputs , read_only = True , title = f \"Inputs for step ' { step_id } ' of pipeline ' { self . title } \" , kiara = self . _kiara , registry = self . _data_registry , ) self . _step_outputs = {} for step_id , outputs in all_step_outputs . items (): self . _step_outputs [ step_id ] = SlottedValueSet ( read_only = False , items = outputs , title = f \"Outputs for step ' { step_id } ' of pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) self . _value_refs = value_refs self . _steps_by_stage = None self . _inputs_by_stage = None def values_updated ( self , * items : ValueSlot ) -> None : updated_inputs : typing . Dict [ str , typing . List [ str ]] = {} updated_outputs : typing . Dict [ str , typing . List [ str ]] = {} updated_pipeline_inputs : typing . List [ str ] = [] updated_pipeline_outputs : typing . List [ str ] = [] # print(\"===================================================\") # for item in items: # print(item) # print(\"===================================================\") self . _update_status () if self . _value_refs is None : # means init is not finished yet return for item in items : # TODO: multiple value fields, also check pipeline id references = self . _value_refs . get ( item , None ) assert references for p in references : if isinstance ( p , StepInputRef ): updated_inputs . setdefault ( p . step_id , []) . append ( p . value_name ) elif isinstance ( p , StepOutputRef ): updated_outputs . setdefault ( p . step_id , []) . append ( p . value_name ) elif isinstance ( p , PipelineInputRef ): updated_pipeline_inputs . append ( p . value_name ) elif isinstance ( p , PipelineOutputRef ): updated_pipeline_outputs . append ( p . value_name ) else : raise TypeError ( f \"Can't update, invalid type: { type ( p ) } \" ) # print('========================================') # print('---') # print(\"Upaded pipeline input\") # print(updated_pipeline_inputs) # print('---') # print(\"Upaded step inputs\") # print(updated_inputs) # print('---') # print(\"Upaded step outputs\") # print(updated_outputs) # print('---') # print(\"Upaded pipeline outputs\") # print(updated_pipeline_outputs) if updated_pipeline_inputs : event_pi = PipelineInputEvent ( pipeline_id = self . id , updated_pipeline_inputs = updated_pipeline_inputs , ) self . _controller . pipeline_inputs_changed ( event_pi ) self . _notify_pipeline_listeners ( event_pi ) if updated_outputs : event_so = StepOutputEvent ( pipeline_id = self . id , updated_step_outputs = updated_outputs , ) self . _controller . step_outputs_changed ( event_so ) self . _notify_pipeline_listeners ( event_so ) if updated_inputs : event_si = StepInputEvent ( pipeline_id = self . id , updated_step_inputs = updated_inputs , ) self . _controller . step_inputs_changed ( event_si ) self . _notify_pipeline_listeners ( event_si ) if updated_pipeline_outputs : event_po = PipelineOutputEvent ( pipeline_id = self . id , updated_pipeline_outputs = updated_pipeline_outputs , ) self . _controller . pipeline_outputs_changed ( event_po ) self . _notify_pipeline_listeners ( event_po ) def _notify_pipeline_listeners ( self , event : StepEvent ): for listener in self . _listeners : if event . type == \"step_input\" : # type: ignore listener . step_inputs_changed ( event ) # type: ignore elif event . type == \"step_output\" : # type: ignore listener . step_outputs_changed ( event ) # type: ignore elif event . type == \"pipeline_input\" : # type: ignore listener . pipeline_inputs_changed ( event ) # type: ignore elif event . type == \"pipeline_output\" : # type: ignore listener . pipeline_outputs_changed ( event ) # type: ignore else : raise Exception ( f \"Unsupported type: { event . type } \" ) # type: ignore def get_current_state ( self ) -> \"PipelineState\" : step_inputs = {} step_states = {} for k , v in self . _step_inputs . items (): step_inputs [ k ] = PipelineValuesInfo . from_value_set ( v ) if v . items_are_valid (): step_states [ k ] = StepStatus . INPUTS_READY else : step_states [ k ] = StepStatus . STALE step_outputs = {} for k , v in self . _step_outputs . items (): step_outputs [ k ] = PipelineValuesInfo . from_value_set ( v ) if v . items_are_valid (): step_states [ k ] = StepStatus . RESULTS_READY from kiara.info.pipelines import PipelineState state = PipelineState ( structure = self . structure . to_details (), pipeline_inputs = self . _pipeline_inputs . to_details (), pipeline_outputs = self . _pipeline_outputs . to_details (), step_states = step_states , step_inputs = step_inputs , step_outputs = step_outputs , status = self . status , ) return state def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield self . get_current_state () inputs : SlottedValueSet property readonly \u00b6 All (pipeline) input values of this pipeline. outputs : SlottedValueSet property readonly \u00b6 All (pipeline) output values of this pipeline. status : StepStatus property readonly \u00b6 Return the current status of this pipeline. step_ids : Iterable [ str ] property readonly \u00b6 Return all ids of the steps of this pipeline. add_listener ( self , listener ) \u00b6 Add a listener taht gets notified on any internal pipeline input/output events. Source code in kiara/pipeline/pipeline.py def add_listener ( self , listener : PipelineListener ) -> None : \"\"\"Add a listener taht gets notified on any internal pipeline input/output events.\"\"\" self . _listeners . append ( listener ) get_pipeline_inputs_by_stage ( self ) \u00b6 Return a list of pipeline input names, ordered by stage they are first required. Source code in kiara/pipeline/pipeline.py def get_pipeline_inputs_by_stage ( self ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by stage they are first required.\"\"\" if self . _inputs_by_stage is not None : return self . _inputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . inputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepInputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _inputs_by_stage = result return self . _inputs_by_stage get_pipeline_inputs_for_stage ( self , stage ) \u00b6 Return a list of pipeline inputs that are required for a stage to be processed. The result of this method does not include inputs that were required earlier already. Source code in kiara/pipeline/pipeline.py def get_pipeline_inputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline inputs that are required for a stage to be processed. The result of this method does not include inputs that were required earlier already. \"\"\" return self . get_pipeline_inputs_by_stage () . get ( stage , []) get_pipeline_outputs_by_stage ( self ) \u00b6 Return a list of pipeline input names, ordered by the stage that needs to be executed before they are available. Source code in kiara/pipeline/pipeline.py def get_pipeline_outputs_by_stage ( self , ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by the stage that needs to be executed before they are available.\"\"\" if self . _outputs_by_stage is not None : return self . _outputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . outputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepOutputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _outputs_by_stage = result return self . _outputs_by_stage get_pipeline_outputs_for_stage ( self , stage ) \u00b6 Return a list of pipeline outputs that are first available after the specified stage completed processing. Source code in kiara/pipeline/pipeline.py def get_pipeline_outputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline outputs that are first available after the specified stage completed processing.\"\"\" return self . get_pipeline_outputs_by_stage () . get ( stage , []) get_step ( self , step_id ) \u00b6 Return the object representing a step in this workflow, identified by the step id. Source code in kiara/pipeline/pipeline.py def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the object representing a step in this workflow, identified by the step id.\"\"\" return self . _structure . get_step ( step_id ) get_step_inputs ( self , step_id ) \u00b6 Return all inputs for a step id (incl. inputs that are not pipeline inputs but connected to other modules output). Source code in kiara/pipeline/pipeline.py def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all inputs for a step id (incl. inputs that are not pipeline inputs but connected to other modules output).\"\"\" return self . _step_inputs [ step_id ] get_step_outputs ( self , step_id ) \u00b6 Return all outputs for a step id (incl. outputs that are not pipeline outputs). Source code in kiara/pipeline/pipeline.py def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all outputs for a step id (incl. outputs that are not pipeline outputs).\"\"\" return self . _step_outputs [ step_id ] get_steps_by_stage ( self ) \u00b6 Return a all pipeline steps, ordered by stage they belong to. Source code in kiara/pipeline/pipeline.py def get_steps_by_stage ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , PipelineStep ]]: \"\"\"Return a all pipeline steps, ordered by stage they belong to.\"\"\" if self . _steps_by_stage is not None : return self . _steps_by_stage result : typing . Dict [ int , typing . Dict [ str , PipelineStep ]] = {} for step_id in self . step_ids : step = self . get_step ( step_id ) stage = step . processing_stage assert stage is not None result . setdefault ( stage , {})[ step_id ] = step self . _steps_by_stage = result return self . _steps_by_stage","title":"pipeline"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline","text":"An instance of a PipelineStructure that holds state for all of the inputs/outputs of the steps within. Source code in kiara/pipeline/pipeline.py class Pipeline ( object ): \"\"\"An instance of a [PipelineStructure][kiara.pipeline.structure.PipelineStructure] that holds state for all of the inputs/outputs of the steps within.\"\"\" def __init__ ( self , structure : PipelineStructure , # constants: typing.Optional[typing.Mapping[str, typing.Any]] = None, controller : typing . Optional [ PipelineController ] = None , title : typing . Optional [ str ] = None , ): self . _id : str = str ( uuid . uuid4 ()) if title is None : title = self . _id self . _title : str = title self . _structure : PipelineStructure = structure self . _pipeline_inputs : SlottedValueSet = None # type: ignore self . _pipeline_outputs : SlottedValueSet = None # type: ignore self . _step_inputs : typing . Mapping [ str , ValueSet ] = None # type: ignore self . _step_outputs : typing . Mapping [ str , ValueSet ] = None # type: ignore self . _value_refs : typing . Mapping [ ValueSlot , typing . Iterable [ ValueRef ]] = None # type: ignore self . _status : StepStatus = StepStatus . STALE self . _steps_by_stage : typing . Optional [ typing . Dict [ int , typing . Dict [ str , PipelineStep ]] ] = None self . _inputs_by_stage : typing . Optional [ typing . Dict [ int , typing . List [ str ]] ] = None self . _outputs_by_stage : typing . Optional [ typing . Dict [ int , typing . List [ str ]] ] = None self . _kiara : \"Kiara\" = self . _structure . _kiara self . _data_registry : DataRegistry = self . _kiara . data_registry self . _init_values () if controller is None : controller = BatchController ( self , kiara = self . _kiara ) else : controller . set_pipeline ( self ) self . _controller : PipelineController = controller self . _listeners : typing . List [ PipelineListener ] = [] self . _update_status () def __eq__ ( self , other ): if not isinstance ( other , Pipeline ): return False return self . _id == other . _id def __hash__ ( self ): return hash ( self . _id ) @property def id ( self ) -> str : return self . _id @property def title ( self ) -> str : return self . _title @property def structure ( self ) -> PipelineStructure : return self . _structure @property def controller ( self ) -> PipelineController : if self . _controller is None : raise Exception ( \"No controller set (yet).\" ) return self . _controller @property def inputs ( self ) -> SlottedValueSet : \"\"\"All (pipeline) input values of this pipeline.\"\"\" return self . _pipeline_inputs @property def outputs ( self ) -> SlottedValueSet : \"\"\"All (pipeline) output values of this pipeline.\"\"\" return self . _pipeline_outputs # def set_pipeline_inputs(self, **inputs: typing.Any): # self._controller.set_pipeline_inputs(**inputs) @property def step_ids ( self ) -> typing . Iterable [ str ]: \"\"\"Return all ids of the steps of this pipeline.\"\"\" return self . _structure . step_ids def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the object representing a step in this workflow, identified by the step id.\"\"\" return self . _structure . get_step ( step_id ) def get_steps_by_stage ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , PipelineStep ]]: \"\"\"Return a all pipeline steps, ordered by stage they belong to.\"\"\" if self . _steps_by_stage is not None : return self . _steps_by_stage result : typing . Dict [ int , typing . Dict [ str , PipelineStep ]] = {} for step_id in self . step_ids : step = self . get_step ( step_id ) stage = step . processing_stage assert stage is not None result . setdefault ( stage , {})[ step_id ] = step self . _steps_by_stage = result return self . _steps_by_stage def get_pipeline_inputs_by_stage ( self ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by stage they are first required.\"\"\" if self . _inputs_by_stage is not None : return self . _inputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . inputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepInputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _inputs_by_stage = result return self . _inputs_by_stage def get_pipeline_outputs_by_stage ( self , ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by the stage that needs to be executed before they are available.\"\"\" if self . _outputs_by_stage is not None : return self . _outputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . outputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepOutputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _outputs_by_stage = result return self . _outputs_by_stage def get_pipeline_inputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline inputs that are required for a stage to be processed. The result of this method does not include inputs that were required earlier already. \"\"\" return self . get_pipeline_inputs_by_stage () . get ( stage , []) def get_stage_for_pipeline_input ( self , input_name : str ) -> int : for stage , input_names in self . get_pipeline_inputs_by_stage () . items (): if input_name in input_names : return stage raise Exception ( f \"No input name ' { input_name } '. Available inputs: { ', ' . join ( self . inputs . keys ()) } \" ) def stage_for_pipeline_output ( self , output_name : str ) -> int : for stage , output_names in self . get_pipeline_outputs_by_stage () . items (): if output_name in output_names : return stage raise Exception ( f \"No output name ' { output_name } '. Available outputs: { ', ' . join ( self . outputs . keys ()) } \" ) def get_pipeline_outputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline outputs that are first available after the specified stage completed processing.\"\"\" return self . get_pipeline_outputs_by_stage () . get ( stage , []) def get_pipeline_inputs_for_step ( self , step_id : str ) -> typing . List [ str ]: result = [] for field_name , value_slot in self . inputs . _value_slots . items (): refs = self . _value_refs [ value_slot ] for ref in refs : if not isinstance ( ref , PipelineInputRef ): continue for ci in ref . connected_inputs : if ci . step_id == step_id and ref . value_name not in result : result . append ( ref . value_name ) return result def get_pipeline_outputs_for_step ( self , step_id : str ) -> typing . List [ str ]: result = [] for field_name , value_slot in self . outputs . _value_slots . items (): refs = self . _value_refs [ value_slot ] for ref in refs : if not isinstance ( ref , PipelineOutputRef ): continue if ( ref . connected_output . step_id == step_id and ref . value_name not in result ): result . append ( ref . value_name ) return result def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all inputs for a step id (incl. inputs that are not pipeline inputs but connected to other modules output).\"\"\" return self . _step_inputs [ step_id ] def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all outputs for a step id (incl. outputs that are not pipeline outputs).\"\"\" return self . _step_outputs [ step_id ] def add_listener ( self , listener : PipelineListener ) -> None : \"\"\"Add a listener taht gets notified on any internal pipeline input/output events.\"\"\" self . _listeners . append ( listener ) @property def status ( self ) -> StepStatus : \"\"\"Return the current status of this pipeline.\"\"\" return self . _state def _update_status ( self ): \"\"\"Make sure internal state variable is up to date.\"\"\" if self . inputs is None : new_state = StepStatus . STALE elif not self . inputs . items_are_valid (): new_state = StepStatus . STALE elif not self . outputs . items_are_valid (): new_state = StepStatus . INPUTS_READY else : new_state = StepStatus . RESULTS_READY self . _state = new_state def _init_values ( self ): \"\"\"Initialize this object. This should only be called once. Basically, this goes through all the inputs and outputs of all steps, and 'allocates' a PipelineValueInfo object for each of them. In case where output/input or pipeline-input/input points are connected, only one value item is allocated, since those refer to the same value. \"\"\" pipeline_inputs : typing . Dict [ str , ValueSlot ] = {} pipeline_outputs : typing . Dict [ str , ValueSlot ] = {} all_step_inputs : typing . Dict [ str , typing . Dict [ str , ValueSlot ]] = {} all_step_outputs : typing . Dict [ str , typing . Dict [ str , ValueSlot ]] = {} value_refs : typing . Dict [ ValueSlot , typing . List [ ValueRef ]] = {} # create the value objects that are associated with step outputs # all pipeline outputs are created here too, since the only place # those can be associated are step outputs for step_id , step_details in self . _structure . steps_details . items (): step_outputs : typing . Mapping [ str , StepOutputRef ] = step_details [ \"outputs\" ] for output_name , output_point in step_outputs . items (): init_output_value_item = self . _data_registry . register_data ( value_schema = output_point . value_schema ) output_value_slot = self . _data_registry . register_alias ( value_or_schema = init_output_value_item , callbacks = [ self ] ) value_refs . setdefault ( output_value_slot , []) . append ( output_point ) all_step_outputs . setdefault ( step_id , {})[ output_name ] = output_value_slot # not all step outputs necessarily need to be connected to a pipeline output if output_point . pipeline_output : pipeline_outputs [ output_point . pipeline_output ] = output_value_slot po = self . _structure . pipeline_outputs [ output_point . pipeline_output ] value_refs . setdefault ( output_value_slot , []) . append ( po ) # create the value objects that are associated with step inputs for step_id , step_details in self . _structure . steps_details . items (): step_inputs : typing . Mapping [ str , StepInputRef ] = step_details [ \"inputs\" ] for input_name , input_point in step_inputs . items (): # if this step input gets fed from a pipeline_input (meaning user input in most cases), # we need to create a DataValue for that pipeline input # vm = ValueMetadata( # origin=f\"{self.id}.steps.{step_id}.inputs.{input_point.value_name}\" # ) if input_point . connected_pipeline_input : connected_pipeline_input_name = input_point . connected_pipeline_input pipeline_input_field : PipelineInputRef = ( self . _structure . pipeline_inputs [ connected_pipeline_input_name ] ) pipeline_input_slot : ValueSlot = pipeline_inputs . get ( connected_pipeline_input_name , None ) if pipeline_input_slot is None : # if the pipeline input wasn't created by another step input before, # we need to take care of it here if pipeline_input_field . is_constant : init_value = self . structure . constants [ pipeline_input_field . value_name ] else : init_value = self . structure . defaults . get ( pipeline_input_field . value_name , SpecialValue . NOT_SET ) init_pipeline_input_value = self . _data_registry . register_data ( value_data = init_value , value_schema = pipeline_input_field . value_schema , ) # TODO: check whether it's a constant? pipeline_input_slot = self . _data_registry . register_alias ( value_or_schema = init_pipeline_input_value , callbacks = [ self ] ) value_refs . setdefault ( pipeline_input_slot , []) . append ( pipeline_input_field ) pipeline_inputs [ connected_pipeline_input_name ] = pipeline_input_slot all_step_inputs . setdefault ( step_id , {})[ input_name ] = pipeline_input_slot value_refs . setdefault ( pipeline_input_slot , []) . append ( input_point ) elif input_point . connected_outputs : for co in input_point . connected_outputs : if len ( input_point . connected_outputs ) == 1 and not co . sub_value : # this means the input is the same value as the connected output output_value : ValueSlot = all_step_outputs [ co . step_id ][ co . value_name ] all_step_inputs . setdefault ( input_point . step_id , {})[ input_point . value_name ] = output_value value_refs . setdefault ( output_value , []) . append ( input_point ) else : print ( input_point . connected_outputs ) raise NotImplementedError () # sub_value = co.sub_value # linked_values = {} # for co in input_point.connected_outputs: # output_value = all_step_outputs[co.step_id][co.value_name] # sub_value = co.sub_value # if len(input_point.connected_outputs) > 1 and not sub_value: # raise NotImplementedError() # sub_value = {\"config\": co.step_id} # if sub_value is not None: # raise NotImplementedError # # linked_values[output_value.id] = sub_value # # step_input = self._data_registry.register_linked_value( # parent_id=self.id, # linked_values=linked_values, # value_schema=input_point.value_schema, # value_refs=input_point, # ) # self._data_registry.register_callback( # self.values_updated, step_input # ) # all_step_inputs.setdefault(input_point.step_id, {})[ # input_point.value_name # ] = step_input else : raise Exception ( f \"Invalid value point type for this location: { input_point } \" ) if not pipeline_inputs : raise Exception ( f \"Can't init pipeline ' { self . title } ': no pipeline inputs\" ) self . _pipeline_inputs = SlottedValueSet ( items = pipeline_inputs , read_only = False , title = f \"Inputs for pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) if not pipeline_outputs : raise Exception ( f \"Can't init pipeline ' { self . title } ': no pipeline outputs\" ) self . _pipeline_outputs = SlottedValueSet ( items = pipeline_outputs , read_only = True , title = f \"Outputs for pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) self . _step_inputs = {} for step_id , inputs in all_step_inputs . items (): self . _step_inputs [ step_id ] = SlottedValueSet ( items = inputs , read_only = True , title = f \"Inputs for step ' { step_id } ' of pipeline ' { self . title } \" , kiara = self . _kiara , registry = self . _data_registry , ) self . _step_outputs = {} for step_id , outputs in all_step_outputs . items (): self . _step_outputs [ step_id ] = SlottedValueSet ( read_only = False , items = outputs , title = f \"Outputs for step ' { step_id } ' of pipeline ' { self . title } '\" , kiara = self . _kiara , registry = self . _data_registry , ) self . _value_refs = value_refs self . _steps_by_stage = None self . _inputs_by_stage = None def values_updated ( self , * items : ValueSlot ) -> None : updated_inputs : typing . Dict [ str , typing . List [ str ]] = {} updated_outputs : typing . Dict [ str , typing . List [ str ]] = {} updated_pipeline_inputs : typing . List [ str ] = [] updated_pipeline_outputs : typing . List [ str ] = [] # print(\"===================================================\") # for item in items: # print(item) # print(\"===================================================\") self . _update_status () if self . _value_refs is None : # means init is not finished yet return for item in items : # TODO: multiple value fields, also check pipeline id references = self . _value_refs . get ( item , None ) assert references for p in references : if isinstance ( p , StepInputRef ): updated_inputs . setdefault ( p . step_id , []) . append ( p . value_name ) elif isinstance ( p , StepOutputRef ): updated_outputs . setdefault ( p . step_id , []) . append ( p . value_name ) elif isinstance ( p , PipelineInputRef ): updated_pipeline_inputs . append ( p . value_name ) elif isinstance ( p , PipelineOutputRef ): updated_pipeline_outputs . append ( p . value_name ) else : raise TypeError ( f \"Can't update, invalid type: { type ( p ) } \" ) # print('========================================') # print('---') # print(\"Upaded pipeline input\") # print(updated_pipeline_inputs) # print('---') # print(\"Upaded step inputs\") # print(updated_inputs) # print('---') # print(\"Upaded step outputs\") # print(updated_outputs) # print('---') # print(\"Upaded pipeline outputs\") # print(updated_pipeline_outputs) if updated_pipeline_inputs : event_pi = PipelineInputEvent ( pipeline_id = self . id , updated_pipeline_inputs = updated_pipeline_inputs , ) self . _controller . pipeline_inputs_changed ( event_pi ) self . _notify_pipeline_listeners ( event_pi ) if updated_outputs : event_so = StepOutputEvent ( pipeline_id = self . id , updated_step_outputs = updated_outputs , ) self . _controller . step_outputs_changed ( event_so ) self . _notify_pipeline_listeners ( event_so ) if updated_inputs : event_si = StepInputEvent ( pipeline_id = self . id , updated_step_inputs = updated_inputs , ) self . _controller . step_inputs_changed ( event_si ) self . _notify_pipeline_listeners ( event_si ) if updated_pipeline_outputs : event_po = PipelineOutputEvent ( pipeline_id = self . id , updated_pipeline_outputs = updated_pipeline_outputs , ) self . _controller . pipeline_outputs_changed ( event_po ) self . _notify_pipeline_listeners ( event_po ) def _notify_pipeline_listeners ( self , event : StepEvent ): for listener in self . _listeners : if event . type == \"step_input\" : # type: ignore listener . step_inputs_changed ( event ) # type: ignore elif event . type == \"step_output\" : # type: ignore listener . step_outputs_changed ( event ) # type: ignore elif event . type == \"pipeline_input\" : # type: ignore listener . pipeline_inputs_changed ( event ) # type: ignore elif event . type == \"pipeline_output\" : # type: ignore listener . pipeline_outputs_changed ( event ) # type: ignore else : raise Exception ( f \"Unsupported type: { event . type } \" ) # type: ignore def get_current_state ( self ) -> \"PipelineState\" : step_inputs = {} step_states = {} for k , v in self . _step_inputs . items (): step_inputs [ k ] = PipelineValuesInfo . from_value_set ( v ) if v . items_are_valid (): step_states [ k ] = StepStatus . INPUTS_READY else : step_states [ k ] = StepStatus . STALE step_outputs = {} for k , v in self . _step_outputs . items (): step_outputs [ k ] = PipelineValuesInfo . from_value_set ( v ) if v . items_are_valid (): step_states [ k ] = StepStatus . RESULTS_READY from kiara.info.pipelines import PipelineState state = PipelineState ( structure = self . structure . to_details (), pipeline_inputs = self . _pipeline_inputs . to_details (), pipeline_outputs = self . _pipeline_outputs . to_details (), step_states = step_states , step_inputs = step_inputs , step_outputs = step_outputs , status = self . status , ) return state def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield self . get_current_state ()","title":"Pipeline"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline.inputs","text":"All (pipeline) input values of this pipeline.","title":"inputs"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline.outputs","text":"All (pipeline) output values of this pipeline.","title":"outputs"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline.status","text":"Return the current status of this pipeline.","title":"status"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline.step_ids","text":"Return all ids of the steps of this pipeline.","title":"step_ids"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline.add_listener","text":"Add a listener taht gets notified on any internal pipeline input/output events. Source code in kiara/pipeline/pipeline.py def add_listener ( self , listener : PipelineListener ) -> None : \"\"\"Add a listener taht gets notified on any internal pipeline input/output events.\"\"\" self . _listeners . append ( listener )","title":"add_listener()"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline.get_pipeline_inputs_by_stage","text":"Return a list of pipeline input names, ordered by stage they are first required. Source code in kiara/pipeline/pipeline.py def get_pipeline_inputs_by_stage ( self ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by stage they are first required.\"\"\" if self . _inputs_by_stage is not None : return self . _inputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . inputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepInputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _inputs_by_stage = result return self . _inputs_by_stage","title":"get_pipeline_inputs_by_stage()"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline.get_pipeline_inputs_for_stage","text":"Return a list of pipeline inputs that are required for a stage to be processed. The result of this method does not include inputs that were required earlier already. Source code in kiara/pipeline/pipeline.py def get_pipeline_inputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline inputs that are required for a stage to be processed. The result of this method does not include inputs that were required earlier already. \"\"\" return self . get_pipeline_inputs_by_stage () . get ( stage , [])","title":"get_pipeline_inputs_for_stage()"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline.get_pipeline_outputs_by_stage","text":"Return a list of pipeline input names, ordered by the stage that needs to be executed before they are available. Source code in kiara/pipeline/pipeline.py def get_pipeline_outputs_by_stage ( self , ) -> typing . Mapping [ int , typing . Iterable [ str ]]: \"\"\"Return a list of pipeline input names, ordered by the stage that needs to be executed before they are available.\"\"\" if self . _outputs_by_stage is not None : return self . _outputs_by_stage result : typing . Dict [ int , typing . List [ str ]] = {} for k , v in self . outputs . _value_slots . items (): # type: ignore refs = self . _value_refs [ v ] min_stage = sys . maxsize for ref in refs : if not isinstance ( ref , StepOutputRef ): continue step = self . get_step ( ref . step_id ) stage = step . processing_stage assert stage is not None if stage < min_stage : min_stage = stage # type: ignore result . setdefault ( min_stage , []) . append ( k ) self . _outputs_by_stage = result return self . _outputs_by_stage","title":"get_pipeline_outputs_by_stage()"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline.get_pipeline_outputs_for_stage","text":"Return a list of pipeline outputs that are first available after the specified stage completed processing. Source code in kiara/pipeline/pipeline.py def get_pipeline_outputs_for_stage ( self , stage : int ) -> typing . Iterable [ str ]: \"\"\"Return a list of pipeline outputs that are first available after the specified stage completed processing.\"\"\" return self . get_pipeline_outputs_by_stage () . get ( stage , [])","title":"get_pipeline_outputs_for_stage()"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline.get_step","text":"Return the object representing a step in this workflow, identified by the step id. Source code in kiara/pipeline/pipeline.py def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the object representing a step in this workflow, identified by the step id.\"\"\" return self . _structure . get_step ( step_id )","title":"get_step()"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline.get_step_inputs","text":"Return all inputs for a step id (incl. inputs that are not pipeline inputs but connected to other modules output). Source code in kiara/pipeline/pipeline.py def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all inputs for a step id (incl. inputs that are not pipeline inputs but connected to other modules output).\"\"\" return self . _step_inputs [ step_id ]","title":"get_step_inputs()"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline.get_step_outputs","text":"Return all outputs for a step id (incl. outputs that are not pipeline outputs). Source code in kiara/pipeline/pipeline.py def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return all outputs for a step id (incl. outputs that are not pipeline outputs).\"\"\" return self . _step_outputs [ step_id ]","title":"get_step_outputs()"},{"location":"reference/kiara/pipeline/pipeline/#kiara.pipeline.pipeline.Pipeline.get_steps_by_stage","text":"Return a all pipeline steps, ordered by stage they belong to. Source code in kiara/pipeline/pipeline.py def get_steps_by_stage ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , PipelineStep ]]: \"\"\"Return a all pipeline steps, ordered by stage they belong to.\"\"\" if self . _steps_by_stage is not None : return self . _steps_by_stage result : typing . Dict [ int , typing . Dict [ str , PipelineStep ]] = {} for step_id in self . step_ids : step = self . get_step ( step_id ) stage = step . processing_stage assert stage is not None result . setdefault ( stage , {})[ step_id ] = step self . _steps_by_stage = result return self . _steps_by_stage","title":"get_steps_by_stage()"},{"location":"reference/kiara/pipeline/structure/","text":"PipelineStep ( BaseModel ) pydantic-model \u00b6 A step within a pipeline-structure, includes information about it's connection(s) and other metadata. Source code in kiara/pipeline/structure.py class PipelineStep ( BaseModel ): \"\"\"A step within a pipeline-structure, includes information about it's connection(s) and other metadata.\"\"\" class Config : validate_assignment = True extra = Extra . forbid @classmethod def create_steps ( cls , * steps : \"PipelineStepConfig\" , kiara : \"Kiara\" ) -> typing . List [ \"PipelineStep\" ]: result : typing . List [ PipelineStep ] = [] if kiara is None : from kiara.module import Kiara kiara = Kiara . instance () for step in steps : _s = PipelineStep ( step_id = step . step_id , module_type = step . module_type , module_config = copy . deepcopy ( step . module_config ), input_links = copy . deepcopy ( step . input_links ), _kiara = kiara , # type: ignore ) result . append ( _s ) return result _module : typing . Optional [ \"KiaraModule\" ] = PrivateAttr ( default = None ) @validator ( \"step_id\" ) def _validate_step_id ( cls , v ): assert isinstance ( v , str ) if \".\" in v : raise ValueError ( \"Step ids can't contain '.' characters.\" ) return v step_id : str module_type : str = Field ( description = \"The module type.\" ) module_config : typing . Mapping [ str , typing . Any ] = Field ( description = \"The module config.\" , default_factory = dict ) required : bool = Field ( description = \"Whether this step is required within the workflow. \\n\\n In some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required.\" , default = True , ) processing_stage : typing . Optional [ int ] = Field ( default = None , description = \"The stage number this step is executed within the pipeline.\" , ) input_links : typing . Mapping [ str , typing . List [ StepValueAddress ]] = Field ( description = \"The links that connect to inputs of the module.\" , default_factory = list , ) _kiara : typing . Optional [ \"Kiara\" ] = PrivateAttr ( default = None ) _id : str = PrivateAttr () def __init__ ( self , ** data ): # type: ignore self . _id = str ( uuid . uuid4 ()) kiara = data . pop ( \"_kiara\" , None ) if kiara is None : from kiara import Kiara kiara = Kiara . instance () super () . __init__ ( ** data ) self . _kiara : \"Kiara\" = kiara @property def kiara ( self ): return self . _kiara @property def module ( self ) -> \"KiaraModule\" : if self . _module is None : try : if ( self . module_type in self . kiara . operation_mgmt . profiles . keys () and not self . module_config ): op = self . kiara . operation_mgmt . profiles [ self . module_type ] self . _module = op . module else : self . _module = self . kiara . create_module ( id = self . step_id , module_type = self . module_type , module_config = self . module_config , ) except Exception as e : raise Exception ( f \"Can't assemble pipeline step ' { self . step_id } ': { e } \" ) return self . _module def __eq__ ( self , other ): if not isinstance ( other , PipelineStep ): return False return self . _id == other . _id # # TODO: also check whether _kiara obj is equal? # eq = (self.step_id, self.parent_id, self.module, self.processing_stage,) == ( # other.step_id, # other.parent_id, # other.module, # other.processing_stage, # ) # # if not eq: # return False # # hs = DeepHash(self.input_links) # ho = DeepHash(other.input_links) # # return hs[self.input_links] == ho[other.input_links] def __hash__ ( self ): return hash ( self . _id ) # # TODO: also include _kiara obj? # # TODO: figure out whether that can be made to work without deephash # hs = DeepHash(self.input_links) # return hash( # ( # self.step_id, # self.parent_id, # self.module, # self.processing_stage, # hs[self.input_links], # ) # ) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (step_id= { self . step_id } module_type= { self . module_type } processing_stage= { self . processing_stage } )\" def __str__ ( self ): return f \"step: { self . step_id } (module: { self . module_type } )\" input_links : Mapping [ str , List [ kiara . pipeline . StepValueAddress ]] pydantic-field \u00b6 The links that connect to inputs of the module. module_config : Mapping [ str , Any ] pydantic-field \u00b6 The module config. module_type : str pydantic-field required \u00b6 The module type. processing_stage : int pydantic-field \u00b6 The stage number this step is executed within the pipeline. required : bool pydantic-field \u00b6 Whether this step is required within the workflow. In some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required. __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/pipeline/structure.py def __eq__ ( self , other ): if not isinstance ( other , PipelineStep ): return False return self . _id == other . _id # # TODO: also check whether _kiara obj is equal? # eq = (self.step_id, self.parent_id, self.module, self.processing_stage,) == ( # other.step_id, # other.parent_id, # other.module, # other.processing_stage, # ) # # if not eq: # return False # # hs = DeepHash(self.input_links) # ho = DeepHash(other.input_links) # # return hs[self.input_links] == ho[other.input_links] __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/pipeline/structure.py def __hash__ ( self ): return hash ( self . _id ) # # TODO: also include _kiara obj? # # TODO: figure out whether that can be made to work without deephash # hs = DeepHash(self.input_links) # return hash( # ( # self.step_id, # self.parent_id, # self.module, # self.processing_stage, # hs[self.input_links], # ) # ) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/pipeline/structure.py def __repr__ ( self ): return f \" { self . __class__ . __name__ } (step_id= { self . step_id } module_type= { self . module_type } processing_stage= { self . processing_stage } )\" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/pipeline/structure.py def __str__ ( self ): return f \"step: { self . step_id } (module: { self . module_type } )\" PipelineStructure \u00b6 An object that holds one or several steps, and describes the connections between them. Source code in kiara/pipeline/structure.py class PipelineStructure ( object ): \"\"\"An object that holds one or several steps, and describes the connections between them.\"\"\" def __init__ ( self , config : \"PipelineConfig\" , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _structure_config : \"PipelineConfig\" = config steps = self . _structure_config . steps input_aliases = self . _structure_config . input_aliases output_aliases = self . _structure_config . output_aliases if not steps : raise Exception ( \"No steps provided.\" ) if kiara is None : kiara = Kiara . instance () self . _kiara : Kiara = kiara self . _steps : typing . List [ PipelineStep ] = PipelineStep . create_steps ( * steps , kiara = self . _kiara ) # self._pipeline_id: str = parent_id if input_aliases is None : input_aliases = {} if isinstance ( input_aliases , str ): if input_aliases not in ALLOWED_INPUT_ALIAS_MARKERS : raise Exception ( f \"Can't create pipeline, invalid value ' { input_aliases } ' for 'input_aliases'. Either specify a dict, or use one of: { ', ' . join ( ALLOWED_INPUT_ALIAS_MARKERS ) } \" ) input_aliases = calculate_shortest_field_aliases ( self . _steps , input_aliases , \"inputs\" ) if isinstance ( output_aliases , str ): if output_aliases not in ALLOWED_INPUT_ALIAS_MARKERS : raise Exception ( f \"Can't create pipeline, invalid value ' { output_aliases } ' for 'output_aliases'. Either specify a dict, or use one of: { ', ' . join ( ALLOWED_INPUT_ALIAS_MARKERS ) } \" ) output_aliases = calculate_shortest_field_aliases ( self . _steps , output_aliases , \"outputs\" ) self . _input_aliases : typing . Dict [ str , str ] = dict ( input_aliases ) # type: ignore if output_aliases is None : output_aliases = {} self . _output_aliases : typing . Dict [ str , str ] = dict ( output_aliases ) # type: ignore # this is hardcoded for now self . _add_all_workflow_outputs : bool = False self . _constants : typing . Dict [ str , typing . Any ] = None # type: ignore self . _defaults : typing . Dict [ str , typing . Any ] = None # type: ignore self . _execution_graph : nx . DiGraph = None # type: ignore self . _data_flow_graph : nx . DiGraph = None # type: ignore self . _data_flow_graph_simple : nx . DiGraph = None # type: ignore self . _processing_stages : typing . List [ typing . List [ str ]] = None # type: ignore self . _steps_details : typing . Dict [ str , typing . Any ] = None # type: ignore \"\"\"Holds details about the (current) processing steps contained in this workflow.\"\"\" # @property # def pipeline_id(self) -> str: # return self._pipeline_id @property def structure_config ( self ) -> \"PipelineConfig\" : return self . _structure_config @property def steps ( self ) -> typing . Iterable [ PipelineStep ]: return self . _steps @property def modules ( self ) -> typing . Iterable [ \"KiaraModule\" ]: return ( s . module for s in self . steps ) @property def steps_details ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _steps_details is None : self . _process_steps () return self . _steps_details @property def step_ids ( self ) -> typing . Iterable [ str ]: if self . _steps_details is None : self . _process_steps () return self . _steps_details . keys () @property def constants ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _constants is None : self . _process_steps () return self . _constants @property def defaults ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _defaults is None : self . _process_steps () return self . _defaults def get_step ( self , step_id : str ) -> PipelineStep : d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"step\" ] def get_step_inputs ( self , step_id : str ) -> typing . Mapping [ str , StepInputRef ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"inputs\" ] def get_step_outputs ( self , step_id : str ) -> typing . Mapping [ str , StepOutputRef ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"outputs\" ] def get_step_details ( self , step_id : str ) -> typing . Mapping [ str , typing . Any ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d @property def execution_graph ( self ) -> nx . DiGraph : if self . _execution_graph is None : self . _process_steps () return self . _execution_graph @property def data_flow_graph ( self ) -> nx . DiGraph : if self . _data_flow_graph is None : self . _process_steps () return self . _data_flow_graph @property def data_flow_graph_simple ( self ) -> nx . DiGraph : if self . _data_flow_graph_simple is None : self . _process_steps () return self . _data_flow_graph_simple @property def processing_stages ( self ) -> typing . List [ typing . List [ str ]]: if self . _steps_details is None : self . _process_steps () return self . _processing_stages @lru_cache () def _get_node_of_type ( self , node_type : str ): if self . _steps_details is None : self . _process_steps () return [ node for node , attr in self . _data_flow_graph . nodes ( data = True ) if attr [ \"type\" ] == node_type ] @property def steps_inputs ( self ) -> typing . Dict [ str , StepInputRef ]: return { node . alias : node for node in self . _get_node_of_type ( node_type = StepInputRef . __name__ ) } @property def steps_outputs ( self ) -> typing . Dict [ str , StepOutputRef ]: return { node . alias : node for node in self . _get_node_of_type ( node_type = StepOutputRef . __name__ ) } @property def pipeline_inputs ( self ) -> typing . Dict [ str , PipelineInputRef ]: return { node . value_name : node for node in self . _get_node_of_type ( node_type = PipelineInputRef . __name__ ) } @property def pipeline_outputs ( self ) -> typing . Dict [ str , PipelineOutputRef ]: return { node . value_name : node for node in self . _get_node_of_type ( node_type = PipelineOutputRef . __name__ ) } @property def pipeline_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return { input_name : w_in . value_schema for input_name , w_in in self . pipeline_inputs . items () } @property def pipeline_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return { output_name : w_out . value_schema for output_name , w_out in self . pipeline_outputs . items () } def _process_steps ( self ): \"\"\"The core method of this class, it connects all the processing modules, their inputs and outputs.\"\"\" steps_details : typing . Dict [ str , typing . Any ] = {} execution_graph = nx . DiGraph () execution_graph . add_node ( \"__root__\" ) data_flow_graph = nx . DiGraph () data_flow_graph_simple = nx . DiGraph () processing_stages = [] constants = {} structure_defaults = {} # temp variable, to hold all outputs outputs : typing . Dict [ str , StepOutputRef ] = {} # process all pipeline and step outputs first _temp_steps_map : typing . Dict [ str , PipelineStep ] = {} pipeline_outputs : typing . Dict [ str , PipelineOutputRef ] = {} for step in self . _steps : _temp_steps_map [ step . step_id ] = step if step . step_id in steps_details . keys (): raise Exception ( f \"Can't process steps: duplicate step_id ' { step . step_id } '\" ) steps_details [ step . step_id ] = { \"step\" : step , \"outputs\" : {}, \"inputs\" : {}, } data_flow_graph . add_node ( step , type = \"step\" ) # go through all the module outputs, create points for them and connect them to pipeline outputs for output_name , schema in step . module . output_schemas . items (): step_output = StepOutputRef ( value_name = output_name , value_schema = schema , step_id = step . step_id , ) steps_details [ step . step_id ][ \"outputs\" ][ output_name ] = step_output step_alias = generate_step_alias ( step . step_id , output_name ) outputs [ step_alias ] = step_output step_output_name = generate_pipeline_endpoint_name ( step_id = step . step_id , value_name = output_name ) if self . _output_aliases : if step_output_name in self . _output_aliases . keys (): step_output_name = self . _output_aliases [ step_output_name ] else : if not self . _add_all_workflow_outputs : # this output is not interesting for the workflow step_output_name = None if step_output_name : step_output_address = StepValueAddress ( step_id = step . step_id , value_name = output_name ) pipeline_output = PipelineOutputRef ( value_name = step_output_name , connected_output = step_output_address , value_schema = schema , ) pipeline_outputs [ step_output_name ] = pipeline_output step_output . pipeline_output = pipeline_output . value_name data_flow_graph . add_node ( pipeline_output , type = PipelineOutputRef . __name__ ) data_flow_graph . add_edge ( step_output , pipeline_output ) data_flow_graph_simple . add_node ( pipeline_output , type = PipelineOutputRef . __name__ ) data_flow_graph_simple . add_edge ( step , pipeline_output ) data_flow_graph . add_node ( step_output , type = StepOutputRef . __name__ ) data_flow_graph . add_edge ( step , step_output ) # now process inputs, and connect them to the appropriate output/pipeline-input points existing_pipeline_input_points : typing . Dict [ str , PipelineInputRef ] = {} for step in self . _steps : other_step_dependency : typing . Set = set () # go through all the inputs of a module, create input points and connect them to either # other module outputs, or pipeline inputs (which need to be created) module_constants : typing . Mapping [ str , typing . Any ] = step . module . get_config_value ( \"constants\" ) for input_name , schema in step . module . input_schemas . items (): matching_input_links : typing . List [ StepValueAddress ] = [] is_constant = input_name in module_constants . keys () for value_name , input_links in step . input_links . items (): if value_name == input_name : for input_link in input_links : if input_link in matching_input_links : raise Exception ( f \"Duplicate input link: { input_link } \" ) matching_input_links . append ( input_link ) if matching_input_links : # this means we connect to other steps output connected_output_points : typing . List [ StepOutputRef ] = [] connected_outputs : typing . List [ StepValueAddress ] = [] for input_link in matching_input_links : output_id = generate_step_alias ( input_link . step_id , input_link . value_name ) if output_id not in outputs . keys (): raise Exception ( f \"Can't connect input ' { input_name } ' for step ' { step . step_id } ': no output ' { output_id } ' available. Available output names: { ', ' . join ( outputs . keys ()) } \" ) connected_output_points . append ( outputs [ output_id ]) connected_outputs . append ( input_link ) other_step_dependency . add ( input_link . step_id ) step_input_point = StepInputRef ( step_id = step . step_id , value_name = input_name , value_schema = schema , is_constant = is_constant , connected_pipeline_input = None , connected_outputs = connected_outputs , ) for op in connected_output_points : op . connected_inputs . append ( step_input_point . address ) data_flow_graph . add_edge ( op , step_input_point ) data_flow_graph_simple . add_edge ( _temp_steps_map [ op . step_id ], step_input_point ) # TODO: name edge data_flow_graph_simple . add_edge ( step_input_point , step ) # TODO: name edge else : # this means we connect to pipeline input pipeline_input_name = generate_pipeline_endpoint_name ( step_id = step . step_id , value_name = input_name ) # check whether this input has an alias associated with it if self . _input_aliases : if pipeline_input_name in self . _input_aliases . keys (): # this means we use the pipeline alias pipeline_input_name = self . _input_aliases [ pipeline_input_name ] if pipeline_input_name in existing_pipeline_input_points . keys (): # we already created a pipeline input with this name # TODO: check whether schema fits connected_pipeline_input = existing_pipeline_input_points [ pipeline_input_name ] assert connected_pipeline_input . is_constant == is_constant else : # we need to create the pipeline input connected_pipeline_input = PipelineInputRef ( value_name = pipeline_input_name , value_schema = schema , is_constant = is_constant , ) existing_pipeline_input_points [ pipeline_input_name ] = connected_pipeline_input data_flow_graph . add_node ( connected_pipeline_input , type = PipelineInputRef . __name__ ) data_flow_graph_simple . add_node ( connected_pipeline_input , type = PipelineInputRef . __name__ ) if is_constant : constants [ pipeline_input_name ] = step . module . get_config_value ( \"constants\" )[ input_name ] default_val = step . module . get_config_value ( \"defaults\" ) . get ( input_name , None ) if is_constant and default_val is not None : raise Exception ( f \"Module config invalid for step ' { step . step_id } ': both default value and constant provided for input ' { input_name } '.\" ) elif default_val is not None : structure_defaults [ pipeline_input_name ] = default_val step_input_point = StepInputRef ( step_id = step . step_id , value_name = input_name , value_schema = schema , connected_pipeline_input = connected_pipeline_input . value_name , connected_outputs = None , ) connected_pipeline_input . connected_inputs . append ( step_input_point . address ) data_flow_graph . add_edge ( connected_pipeline_input , step_input_point ) data_flow_graph_simple . add_edge ( connected_pipeline_input , step ) data_flow_graph . add_node ( step_input_point , type = StepInputRef . __name__ ) steps_details [ step . step_id ][ \"inputs\" ][ input_name ] = step_input_point data_flow_graph . add_edge ( step_input_point , step ) if other_step_dependency : for module_id in other_step_dependency : execution_graph . add_edge ( module_id , step . step_id ) else : execution_graph . add_edge ( \"__root__\" , step . step_id ) # calculate execution order path_lengths : typing . Dict [ str , int ] = {} for step in self . _steps : step_id = step . step_id paths = list ( nx . all_simple_paths ( execution_graph , \"__root__\" , step_id )) max_steps = max ( paths , key = lambda x : len ( x )) path_lengths [ step_id ] = len ( max_steps ) - 1 max_length = max ( path_lengths . values ()) for i in range ( 1 , max_length + 1 ): stage : typing . List [ str ] = [ m for m , length in path_lengths . items () if length == i ] processing_stages . append ( stage ) for _step_id in stage : steps_details [ _step_id ][ \"processing_stage\" ] = i steps_details [ _step_id ][ \"step\" ] . processing_stage = i self . _constants = constants self . _defaults = structure_defaults self . _steps_details = steps_details self . _execution_graph = execution_graph self . _data_flow_graph = data_flow_graph self . _data_flow_graph_simple = data_flow_graph_simple self . _processing_stages = processing_stages self . _get_node_of_type . cache_clear () # calculating which steps are always required to execute to compute one of the required pipeline outputs. # this is done because in some cases it's possible that some steps can be skipped to execute if they # don't have a valid input set, because the inputs downstream they are connecting to are 'non-required' # optional_steps = [] last_stage = self . _processing_stages [ - 1 ] step_nodes : typing . List [ PipelineStep ] = [ node for node in self . _data_flow_graph_simple . nodes if isinstance ( node , PipelineStep ) ] all_required_inputs = [] for step_id in last_stage : step = self . get_step ( step_id ) step_nodes . remove ( step ) for k , s_inp in self . get_step_inputs ( step_id ) . items (): if not s_inp . value_schema . is_required (): continue all_required_inputs . append ( s_inp ) for pipeline_input in self . pipeline_inputs . values (): for last_step_input in all_required_inputs : try : path = nx . shortest_path ( self . _data_flow_graph_simple , pipeline_input , last_step_input ) for p in path : if p in step_nodes : step_nodes . remove ( p ) except ( NetworkXNoPath , NodeNotFound ): pass # print(\"NO PATH\") # print(f\"{pipeline_input} -> {last_step_input}\") for s in step_nodes : s . required = False for input_name , inp in self . pipeline_inputs . items (): steps = set () for ci in inp . connected_inputs : steps . add ( ci . step_id ) optional = True for step_id in steps : step = self . get_step ( step_id ) if step . required : optional = False break if optional : inp . value_schema . optional = True def extend ( self , other : typing . Union [ \"Pipeline\" , \"PipelineStructure\" , \"PipelineConfig\" , typing . Mapping [ str , typing . Any ], ], input_links : typing . Optional [ typing . Mapping [ str , typing . Iterable [ StepValueAddress ]] ] = None , ) -> \"PipelineStructure\" : return extend_pipeline ( self , other ) def to_details ( self ) -> \"PipelineStructureDesc\" : from kiara.info.pipelines import PipelineStructureDesc return PipelineStructureDesc . create_pipeline_structure_desc ( pipeline = self ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : d = self . to_details () yield d","title":"structure"},{"location":"reference/kiara/pipeline/structure/#kiara.pipeline.structure.PipelineStep","text":"A step within a pipeline-structure, includes information about it's connection(s) and other metadata. Source code in kiara/pipeline/structure.py class PipelineStep ( BaseModel ): \"\"\"A step within a pipeline-structure, includes information about it's connection(s) and other metadata.\"\"\" class Config : validate_assignment = True extra = Extra . forbid @classmethod def create_steps ( cls , * steps : \"PipelineStepConfig\" , kiara : \"Kiara\" ) -> typing . List [ \"PipelineStep\" ]: result : typing . List [ PipelineStep ] = [] if kiara is None : from kiara.module import Kiara kiara = Kiara . instance () for step in steps : _s = PipelineStep ( step_id = step . step_id , module_type = step . module_type , module_config = copy . deepcopy ( step . module_config ), input_links = copy . deepcopy ( step . input_links ), _kiara = kiara , # type: ignore ) result . append ( _s ) return result _module : typing . Optional [ \"KiaraModule\" ] = PrivateAttr ( default = None ) @validator ( \"step_id\" ) def _validate_step_id ( cls , v ): assert isinstance ( v , str ) if \".\" in v : raise ValueError ( \"Step ids can't contain '.' characters.\" ) return v step_id : str module_type : str = Field ( description = \"The module type.\" ) module_config : typing . Mapping [ str , typing . Any ] = Field ( description = \"The module config.\" , default_factory = dict ) required : bool = Field ( description = \"Whether this step is required within the workflow. \\n\\n In some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required.\" , default = True , ) processing_stage : typing . Optional [ int ] = Field ( default = None , description = \"The stage number this step is executed within the pipeline.\" , ) input_links : typing . Mapping [ str , typing . List [ StepValueAddress ]] = Field ( description = \"The links that connect to inputs of the module.\" , default_factory = list , ) _kiara : typing . Optional [ \"Kiara\" ] = PrivateAttr ( default = None ) _id : str = PrivateAttr () def __init__ ( self , ** data ): # type: ignore self . _id = str ( uuid . uuid4 ()) kiara = data . pop ( \"_kiara\" , None ) if kiara is None : from kiara import Kiara kiara = Kiara . instance () super () . __init__ ( ** data ) self . _kiara : \"Kiara\" = kiara @property def kiara ( self ): return self . _kiara @property def module ( self ) -> \"KiaraModule\" : if self . _module is None : try : if ( self . module_type in self . kiara . operation_mgmt . profiles . keys () and not self . module_config ): op = self . kiara . operation_mgmt . profiles [ self . module_type ] self . _module = op . module else : self . _module = self . kiara . create_module ( id = self . step_id , module_type = self . module_type , module_config = self . module_config , ) except Exception as e : raise Exception ( f \"Can't assemble pipeline step ' { self . step_id } ': { e } \" ) return self . _module def __eq__ ( self , other ): if not isinstance ( other , PipelineStep ): return False return self . _id == other . _id # # TODO: also check whether _kiara obj is equal? # eq = (self.step_id, self.parent_id, self.module, self.processing_stage,) == ( # other.step_id, # other.parent_id, # other.module, # other.processing_stage, # ) # # if not eq: # return False # # hs = DeepHash(self.input_links) # ho = DeepHash(other.input_links) # # return hs[self.input_links] == ho[other.input_links] def __hash__ ( self ): return hash ( self . _id ) # # TODO: also include _kiara obj? # # TODO: figure out whether that can be made to work without deephash # hs = DeepHash(self.input_links) # return hash( # ( # self.step_id, # self.parent_id, # self.module, # self.processing_stage, # hs[self.input_links], # ) # ) def __repr__ ( self ): return f \" { self . __class__ . __name__ } (step_id= { self . step_id } module_type= { self . module_type } processing_stage= { self . processing_stage } )\" def __str__ ( self ): return f \"step: { self . step_id } (module: { self . module_type } )\"","title":"PipelineStep"},{"location":"reference/kiara/pipeline/structure/#kiara.pipeline.structure.PipelineStep.input_links","text":"The links that connect to inputs of the module.","title":"input_links"},{"location":"reference/kiara/pipeline/structure/#kiara.pipeline.structure.PipelineStep.module_config","text":"The module config.","title":"module_config"},{"location":"reference/kiara/pipeline/structure/#kiara.pipeline.structure.PipelineStep.module_type","text":"The module type.","title":"module_type"},{"location":"reference/kiara/pipeline/structure/#kiara.pipeline.structure.PipelineStep.processing_stage","text":"The stage number this step is executed within the pipeline.","title":"processing_stage"},{"location":"reference/kiara/pipeline/structure/#kiara.pipeline.structure.PipelineStep.required","text":"Whether this step is required within the workflow. In some cases, when none of the pipeline outputs have a required input that connects to a step, then it is not necessary for this step to have been executed, even if it is placed before a step in the execution hierarchy. This also means that the pipeline inputs that are connected to this step might not be required.","title":"required"},{"location":"reference/kiara/pipeline/structure/#kiara.pipeline.structure.PipelineStep.__eq__","text":"Return self==value. Source code in kiara/pipeline/structure.py def __eq__ ( self , other ): if not isinstance ( other , PipelineStep ): return False return self . _id == other . _id # # TODO: also check whether _kiara obj is equal? # eq = (self.step_id, self.parent_id, self.module, self.processing_stage,) == ( # other.step_id, # other.parent_id, # other.module, # other.processing_stage, # ) # # if not eq: # return False # # hs = DeepHash(self.input_links) # ho = DeepHash(other.input_links) # # return hs[self.input_links] == ho[other.input_links]","title":"__eq__()"},{"location":"reference/kiara/pipeline/structure/#kiara.pipeline.structure.PipelineStep.__hash__","text":"Return hash(self). Source code in kiara/pipeline/structure.py def __hash__ ( self ): return hash ( self . _id ) # # TODO: also include _kiara obj? # # TODO: figure out whether that can be made to work without deephash # hs = DeepHash(self.input_links) # return hash( # ( # self.step_id, # self.parent_id, # self.module, # self.processing_stage, # hs[self.input_links], # ) # )","title":"__hash__()"},{"location":"reference/kiara/pipeline/structure/#kiara.pipeline.structure.PipelineStep.__repr__","text":"Return repr(self). Source code in kiara/pipeline/structure.py def __repr__ ( self ): return f \" { self . __class__ . __name__ } (step_id= { self . step_id } module_type= { self . module_type } processing_stage= { self . processing_stage } )\"","title":"__repr__()"},{"location":"reference/kiara/pipeline/structure/#kiara.pipeline.structure.PipelineStep.__str__","text":"Return str(self). Source code in kiara/pipeline/structure.py def __str__ ( self ): return f \"step: { self . step_id } (module: { self . module_type } )\"","title":"__str__()"},{"location":"reference/kiara/pipeline/structure/#kiara.pipeline.structure.PipelineStructure","text":"An object that holds one or several steps, and describes the connections between them. Source code in kiara/pipeline/structure.py class PipelineStructure ( object ): \"\"\"An object that holds one or several steps, and describes the connections between them.\"\"\" def __init__ ( self , config : \"PipelineConfig\" , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _structure_config : \"PipelineConfig\" = config steps = self . _structure_config . steps input_aliases = self . _structure_config . input_aliases output_aliases = self . _structure_config . output_aliases if not steps : raise Exception ( \"No steps provided.\" ) if kiara is None : kiara = Kiara . instance () self . _kiara : Kiara = kiara self . _steps : typing . List [ PipelineStep ] = PipelineStep . create_steps ( * steps , kiara = self . _kiara ) # self._pipeline_id: str = parent_id if input_aliases is None : input_aliases = {} if isinstance ( input_aliases , str ): if input_aliases not in ALLOWED_INPUT_ALIAS_MARKERS : raise Exception ( f \"Can't create pipeline, invalid value ' { input_aliases } ' for 'input_aliases'. Either specify a dict, or use one of: { ', ' . join ( ALLOWED_INPUT_ALIAS_MARKERS ) } \" ) input_aliases = calculate_shortest_field_aliases ( self . _steps , input_aliases , \"inputs\" ) if isinstance ( output_aliases , str ): if output_aliases not in ALLOWED_INPUT_ALIAS_MARKERS : raise Exception ( f \"Can't create pipeline, invalid value ' { output_aliases } ' for 'output_aliases'. Either specify a dict, or use one of: { ', ' . join ( ALLOWED_INPUT_ALIAS_MARKERS ) } \" ) output_aliases = calculate_shortest_field_aliases ( self . _steps , output_aliases , \"outputs\" ) self . _input_aliases : typing . Dict [ str , str ] = dict ( input_aliases ) # type: ignore if output_aliases is None : output_aliases = {} self . _output_aliases : typing . Dict [ str , str ] = dict ( output_aliases ) # type: ignore # this is hardcoded for now self . _add_all_workflow_outputs : bool = False self . _constants : typing . Dict [ str , typing . Any ] = None # type: ignore self . _defaults : typing . Dict [ str , typing . Any ] = None # type: ignore self . _execution_graph : nx . DiGraph = None # type: ignore self . _data_flow_graph : nx . DiGraph = None # type: ignore self . _data_flow_graph_simple : nx . DiGraph = None # type: ignore self . _processing_stages : typing . List [ typing . List [ str ]] = None # type: ignore self . _steps_details : typing . Dict [ str , typing . Any ] = None # type: ignore \"\"\"Holds details about the (current) processing steps contained in this workflow.\"\"\" # @property # def pipeline_id(self) -> str: # return self._pipeline_id @property def structure_config ( self ) -> \"PipelineConfig\" : return self . _structure_config @property def steps ( self ) -> typing . Iterable [ PipelineStep ]: return self . _steps @property def modules ( self ) -> typing . Iterable [ \"KiaraModule\" ]: return ( s . module for s in self . steps ) @property def steps_details ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _steps_details is None : self . _process_steps () return self . _steps_details @property def step_ids ( self ) -> typing . Iterable [ str ]: if self . _steps_details is None : self . _process_steps () return self . _steps_details . keys () @property def constants ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _constants is None : self . _process_steps () return self . _constants @property def defaults ( self ) -> typing . Mapping [ str , typing . Any ]: if self . _defaults is None : self . _process_steps () return self . _defaults def get_step ( self , step_id : str ) -> PipelineStep : d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"step\" ] def get_step_inputs ( self , step_id : str ) -> typing . Mapping [ str , StepInputRef ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"inputs\" ] def get_step_outputs ( self , step_id : str ) -> typing . Mapping [ str , StepOutputRef ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d [ \"outputs\" ] def get_step_details ( self , step_id : str ) -> typing . Mapping [ str , typing . Any ]: d = self . steps_details . get ( step_id , None ) if d is None : raise Exception ( f \"No module with id: { step_id } \" ) return d @property def execution_graph ( self ) -> nx . DiGraph : if self . _execution_graph is None : self . _process_steps () return self . _execution_graph @property def data_flow_graph ( self ) -> nx . DiGraph : if self . _data_flow_graph is None : self . _process_steps () return self . _data_flow_graph @property def data_flow_graph_simple ( self ) -> nx . DiGraph : if self . _data_flow_graph_simple is None : self . _process_steps () return self . _data_flow_graph_simple @property def processing_stages ( self ) -> typing . List [ typing . List [ str ]]: if self . _steps_details is None : self . _process_steps () return self . _processing_stages @lru_cache () def _get_node_of_type ( self , node_type : str ): if self . _steps_details is None : self . _process_steps () return [ node for node , attr in self . _data_flow_graph . nodes ( data = True ) if attr [ \"type\" ] == node_type ] @property def steps_inputs ( self ) -> typing . Dict [ str , StepInputRef ]: return { node . alias : node for node in self . _get_node_of_type ( node_type = StepInputRef . __name__ ) } @property def steps_outputs ( self ) -> typing . Dict [ str , StepOutputRef ]: return { node . alias : node for node in self . _get_node_of_type ( node_type = StepOutputRef . __name__ ) } @property def pipeline_inputs ( self ) -> typing . Dict [ str , PipelineInputRef ]: return { node . value_name : node for node in self . _get_node_of_type ( node_type = PipelineInputRef . __name__ ) } @property def pipeline_outputs ( self ) -> typing . Dict [ str , PipelineOutputRef ]: return { node . value_name : node for node in self . _get_node_of_type ( node_type = PipelineOutputRef . __name__ ) } @property def pipeline_input_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return { input_name : w_in . value_schema for input_name , w_in in self . pipeline_inputs . items () } @property def pipeline_output_schema ( self ) -> typing . Mapping [ str , ValueSchema ]: return { output_name : w_out . value_schema for output_name , w_out in self . pipeline_outputs . items () } def _process_steps ( self ): \"\"\"The core method of this class, it connects all the processing modules, their inputs and outputs.\"\"\" steps_details : typing . Dict [ str , typing . Any ] = {} execution_graph = nx . DiGraph () execution_graph . add_node ( \"__root__\" ) data_flow_graph = nx . DiGraph () data_flow_graph_simple = nx . DiGraph () processing_stages = [] constants = {} structure_defaults = {} # temp variable, to hold all outputs outputs : typing . Dict [ str , StepOutputRef ] = {} # process all pipeline and step outputs first _temp_steps_map : typing . Dict [ str , PipelineStep ] = {} pipeline_outputs : typing . Dict [ str , PipelineOutputRef ] = {} for step in self . _steps : _temp_steps_map [ step . step_id ] = step if step . step_id in steps_details . keys (): raise Exception ( f \"Can't process steps: duplicate step_id ' { step . step_id } '\" ) steps_details [ step . step_id ] = { \"step\" : step , \"outputs\" : {}, \"inputs\" : {}, } data_flow_graph . add_node ( step , type = \"step\" ) # go through all the module outputs, create points for them and connect them to pipeline outputs for output_name , schema in step . module . output_schemas . items (): step_output = StepOutputRef ( value_name = output_name , value_schema = schema , step_id = step . step_id , ) steps_details [ step . step_id ][ \"outputs\" ][ output_name ] = step_output step_alias = generate_step_alias ( step . step_id , output_name ) outputs [ step_alias ] = step_output step_output_name = generate_pipeline_endpoint_name ( step_id = step . step_id , value_name = output_name ) if self . _output_aliases : if step_output_name in self . _output_aliases . keys (): step_output_name = self . _output_aliases [ step_output_name ] else : if not self . _add_all_workflow_outputs : # this output is not interesting for the workflow step_output_name = None if step_output_name : step_output_address = StepValueAddress ( step_id = step . step_id , value_name = output_name ) pipeline_output = PipelineOutputRef ( value_name = step_output_name , connected_output = step_output_address , value_schema = schema , ) pipeline_outputs [ step_output_name ] = pipeline_output step_output . pipeline_output = pipeline_output . value_name data_flow_graph . add_node ( pipeline_output , type = PipelineOutputRef . __name__ ) data_flow_graph . add_edge ( step_output , pipeline_output ) data_flow_graph_simple . add_node ( pipeline_output , type = PipelineOutputRef . __name__ ) data_flow_graph_simple . add_edge ( step , pipeline_output ) data_flow_graph . add_node ( step_output , type = StepOutputRef . __name__ ) data_flow_graph . add_edge ( step , step_output ) # now process inputs, and connect them to the appropriate output/pipeline-input points existing_pipeline_input_points : typing . Dict [ str , PipelineInputRef ] = {} for step in self . _steps : other_step_dependency : typing . Set = set () # go through all the inputs of a module, create input points and connect them to either # other module outputs, or pipeline inputs (which need to be created) module_constants : typing . Mapping [ str , typing . Any ] = step . module . get_config_value ( \"constants\" ) for input_name , schema in step . module . input_schemas . items (): matching_input_links : typing . List [ StepValueAddress ] = [] is_constant = input_name in module_constants . keys () for value_name , input_links in step . input_links . items (): if value_name == input_name : for input_link in input_links : if input_link in matching_input_links : raise Exception ( f \"Duplicate input link: { input_link } \" ) matching_input_links . append ( input_link ) if matching_input_links : # this means we connect to other steps output connected_output_points : typing . List [ StepOutputRef ] = [] connected_outputs : typing . List [ StepValueAddress ] = [] for input_link in matching_input_links : output_id = generate_step_alias ( input_link . step_id , input_link . value_name ) if output_id not in outputs . keys (): raise Exception ( f \"Can't connect input ' { input_name } ' for step ' { step . step_id } ': no output ' { output_id } ' available. Available output names: { ', ' . join ( outputs . keys ()) } \" ) connected_output_points . append ( outputs [ output_id ]) connected_outputs . append ( input_link ) other_step_dependency . add ( input_link . step_id ) step_input_point = StepInputRef ( step_id = step . step_id , value_name = input_name , value_schema = schema , is_constant = is_constant , connected_pipeline_input = None , connected_outputs = connected_outputs , ) for op in connected_output_points : op . connected_inputs . append ( step_input_point . address ) data_flow_graph . add_edge ( op , step_input_point ) data_flow_graph_simple . add_edge ( _temp_steps_map [ op . step_id ], step_input_point ) # TODO: name edge data_flow_graph_simple . add_edge ( step_input_point , step ) # TODO: name edge else : # this means we connect to pipeline input pipeline_input_name = generate_pipeline_endpoint_name ( step_id = step . step_id , value_name = input_name ) # check whether this input has an alias associated with it if self . _input_aliases : if pipeline_input_name in self . _input_aliases . keys (): # this means we use the pipeline alias pipeline_input_name = self . _input_aliases [ pipeline_input_name ] if pipeline_input_name in existing_pipeline_input_points . keys (): # we already created a pipeline input with this name # TODO: check whether schema fits connected_pipeline_input = existing_pipeline_input_points [ pipeline_input_name ] assert connected_pipeline_input . is_constant == is_constant else : # we need to create the pipeline input connected_pipeline_input = PipelineInputRef ( value_name = pipeline_input_name , value_schema = schema , is_constant = is_constant , ) existing_pipeline_input_points [ pipeline_input_name ] = connected_pipeline_input data_flow_graph . add_node ( connected_pipeline_input , type = PipelineInputRef . __name__ ) data_flow_graph_simple . add_node ( connected_pipeline_input , type = PipelineInputRef . __name__ ) if is_constant : constants [ pipeline_input_name ] = step . module . get_config_value ( \"constants\" )[ input_name ] default_val = step . module . get_config_value ( \"defaults\" ) . get ( input_name , None ) if is_constant and default_val is not None : raise Exception ( f \"Module config invalid for step ' { step . step_id } ': both default value and constant provided for input ' { input_name } '.\" ) elif default_val is not None : structure_defaults [ pipeline_input_name ] = default_val step_input_point = StepInputRef ( step_id = step . step_id , value_name = input_name , value_schema = schema , connected_pipeline_input = connected_pipeline_input . value_name , connected_outputs = None , ) connected_pipeline_input . connected_inputs . append ( step_input_point . address ) data_flow_graph . add_edge ( connected_pipeline_input , step_input_point ) data_flow_graph_simple . add_edge ( connected_pipeline_input , step ) data_flow_graph . add_node ( step_input_point , type = StepInputRef . __name__ ) steps_details [ step . step_id ][ \"inputs\" ][ input_name ] = step_input_point data_flow_graph . add_edge ( step_input_point , step ) if other_step_dependency : for module_id in other_step_dependency : execution_graph . add_edge ( module_id , step . step_id ) else : execution_graph . add_edge ( \"__root__\" , step . step_id ) # calculate execution order path_lengths : typing . Dict [ str , int ] = {} for step in self . _steps : step_id = step . step_id paths = list ( nx . all_simple_paths ( execution_graph , \"__root__\" , step_id )) max_steps = max ( paths , key = lambda x : len ( x )) path_lengths [ step_id ] = len ( max_steps ) - 1 max_length = max ( path_lengths . values ()) for i in range ( 1 , max_length + 1 ): stage : typing . List [ str ] = [ m for m , length in path_lengths . items () if length == i ] processing_stages . append ( stage ) for _step_id in stage : steps_details [ _step_id ][ \"processing_stage\" ] = i steps_details [ _step_id ][ \"step\" ] . processing_stage = i self . _constants = constants self . _defaults = structure_defaults self . _steps_details = steps_details self . _execution_graph = execution_graph self . _data_flow_graph = data_flow_graph self . _data_flow_graph_simple = data_flow_graph_simple self . _processing_stages = processing_stages self . _get_node_of_type . cache_clear () # calculating which steps are always required to execute to compute one of the required pipeline outputs. # this is done because in some cases it's possible that some steps can be skipped to execute if they # don't have a valid input set, because the inputs downstream they are connecting to are 'non-required' # optional_steps = [] last_stage = self . _processing_stages [ - 1 ] step_nodes : typing . List [ PipelineStep ] = [ node for node in self . _data_flow_graph_simple . nodes if isinstance ( node , PipelineStep ) ] all_required_inputs = [] for step_id in last_stage : step = self . get_step ( step_id ) step_nodes . remove ( step ) for k , s_inp in self . get_step_inputs ( step_id ) . items (): if not s_inp . value_schema . is_required (): continue all_required_inputs . append ( s_inp ) for pipeline_input in self . pipeline_inputs . values (): for last_step_input in all_required_inputs : try : path = nx . shortest_path ( self . _data_flow_graph_simple , pipeline_input , last_step_input ) for p in path : if p in step_nodes : step_nodes . remove ( p ) except ( NetworkXNoPath , NodeNotFound ): pass # print(\"NO PATH\") # print(f\"{pipeline_input} -> {last_step_input}\") for s in step_nodes : s . required = False for input_name , inp in self . pipeline_inputs . items (): steps = set () for ci in inp . connected_inputs : steps . add ( ci . step_id ) optional = True for step_id in steps : step = self . get_step ( step_id ) if step . required : optional = False break if optional : inp . value_schema . optional = True def extend ( self , other : typing . Union [ \"Pipeline\" , \"PipelineStructure\" , \"PipelineConfig\" , typing . Mapping [ str , typing . Any ], ], input_links : typing . Optional [ typing . Mapping [ str , typing . Iterable [ StepValueAddress ]] ] = None , ) -> \"PipelineStructure\" : return extend_pipeline ( self , other ) def to_details ( self ) -> \"PipelineStructureDesc\" : from kiara.info.pipelines import PipelineStructureDesc return PipelineStructureDesc . create_pipeline_structure_desc ( pipeline = self ) def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : d = self . to_details () yield d","title":"PipelineStructure"},{"location":"reference/kiara/pipeline/utils/","text":"","title":"utils"},{"location":"reference/kiara/pipeline/values/","text":"PipelineInputRef ( ValueRef ) pydantic-model \u00b6 An input to a pipeline. Source code in kiara/pipeline/values.py class PipelineInputRef ( ValueRef ): \"\"\"An input to a pipeline.\"\"\" connected_inputs : typing . List [ StepValueAddress ] = Field ( description = \"The step inputs that are connected to this pipeline input\" , default_factory = list , ) is_constant : bool = Field ( \"Whether this input is a constant and can't be changed by the user.\" ) @property def alias ( self ) -> str : return generate_step_alias ( PIPELINE_PARENT_MARKER , self . value_name ) connected_inputs : List [ kiara . pipeline . StepValueAddress ] pydantic-field \u00b6 The step inputs that are connected to this pipeline input PipelineOutputRef ( ValueRef ) pydantic-model \u00b6 An output to a pipeline. Source code in kiara/pipeline/values.py class PipelineOutputRef ( ValueRef ): \"\"\"An output to a pipeline.\"\"\" connected_output : StepValueAddress = Field ( description = \"Connected step outputs.\" ) @property def alias ( self ) -> str : return generate_step_alias ( PIPELINE_PARENT_MARKER , self . value_name ) connected_output : StepValueAddress pydantic-field required \u00b6 Connected step outputs. StepInputRef ( ValueRef ) pydantic-model \u00b6 An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both. Source code in kiara/pipeline/values.py class StepInputRef ( ValueRef ): \"\"\"An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both. \"\"\" step_id : str = Field ( description = \"The step id.\" ) connected_outputs : typing . Optional [ typing . List [ StepValueAddress ]] = Field ( default = None , description = \"A potential connected list of one or several module outputs.\" , ) connected_pipeline_input : typing . Optional [ str ] = Field ( default = None , description = \"A potential pipeline input.\" ) is_constant : bool = Field ( \"Whether this input is a constant and can't be changed by the user.\" ) @root_validator ( pre = True ) def ensure_single_connected_item ( cls , values ): if values . get ( \"connected_outputs\" , None ) and values . get ( \"connected_pipeline_input\" ): raise ValueError ( \"Multiple connected items, only one allowed.\" ) return values @property def alias ( self ) -> str : return generate_step_alias ( self . step_id , self . value_name ) @property def address ( self ) -> StepValueAddress : return StepValueAddress ( step_id = self . step_id , value_name = self . value_name ) def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . step_id } . { self . value_name } ( { self . value_schema . type } )\" connected_outputs : List [ kiara . pipeline . StepValueAddress ] pydantic-field \u00b6 A potential connected list of one or several module outputs. connected_pipeline_input : str pydantic-field \u00b6 A potential pipeline input. step_id : str pydantic-field required \u00b6 The step id. StepOutputRef ( ValueRef ) pydantic-model \u00b6 An output to a step. Source code in kiara/pipeline/values.py class StepOutputRef ( ValueRef ): \"\"\"An output to a step.\"\"\" class Config : allow_mutation = True step_id : str = Field ( description = \"The step id.\" ) pipeline_output : typing . Optional [ str ] = Field ( description = \"The connected pipeline output.\" ) connected_inputs : typing . List [ StepValueAddress ] = Field ( description = \"The step inputs that are connected to this step output\" , default_factory = list , ) @property def alias ( self ) -> str : return generate_step_alias ( self . step_id , self . value_name ) @property def address ( self ) -> StepValueAddress : return StepValueAddress ( step_id = self . step_id , value_name = self . value_name ) def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . step_id } . { self . value_name } ( { self . value_schema . type } )\" connected_inputs : List [ kiara . pipeline . StepValueAddress ] pydantic-field \u00b6 The step inputs that are connected to this step output pipeline_output : str pydantic-field \u00b6 The connected pipeline output. step_id : str pydantic-field required \u00b6 The step id. ValueRef ( BaseModel ) pydantic-model \u00b6 An object that holds information about the location of a value within a pipeline (or other structure). Basically, a ValueRef helps the containing object where in its structure the value belongs (for example so it can update dependent other values). A ValueRef object (obviously) does not contain the value itself. There are four different ValueRef type that are relevant for pipelines: kiara.pipeline.values.StepInputRef : an input to a step kiara.pipeline.values.StepOutputRef : an output of a step kiara.pipeline.values.PipelineInputRef : an input to a pipeline kiara.pipeline.values.PipelineOutputRef : an output for a pipeline Several ValueRef objects can target the same value, for example a step output and a connected step input would reference the same Value (in most cases).. Source code in kiara/pipeline/values.py class ValueRef ( BaseModel ): \"\"\"An object that holds information about the location of a value within a pipeline (or other structure). Basically, a `ValueRef` helps the containing object where in its structure the value belongs (for example so it can update dependent other values). A `ValueRef` object (obviously) does not contain the value itself. There are four different ValueRef type that are relevant for pipelines: - [kiara.pipeline.values.StepInputRef][]: an input to a step - [kiara.pipeline.values.StepOutputRef][]: an output of a step - [kiara.pipeline.values.PipelineInputRef][]: an input to a pipeline - [kiara.pipeline.values.PipelineOutputRef][]: an output for a pipeline Several `ValueRef` objects can target the same value, for example a step output and a connected step input would reference the same `Value` (in most cases).. \"\"\" class Config : allow_mutation = True extra = Extra . forbid _id : uuid . UUID = PrivateAttr ( default_factory = uuid . uuid4 ) value_name : str value_schema : ValueSchema # pipeline_id: str def __eq__ ( self , other ): if not isinstance ( other , self . __class__ ): return False return self . _id == other . _id def __hash__ ( self ): return hash ( self . _id ) def __repr__ ( self ): step_id = \"\" if hasattr ( self , \"step_id\" ): step_id = f \" step_id=' { self . step_id } '\" return f \" { self . __class__ . __name__ } (value_name=' { self . value_name } ' { step_id } )\" def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . value_name } ( { self . value_schema . type } )\" __eq__ ( self , other ) special \u00b6 Return self==value. Source code in kiara/pipeline/values.py def __eq__ ( self , other ): if not isinstance ( other , self . __class__ ): return False return self . _id == other . _id __hash__ ( self ) special \u00b6 Return hash(self). Source code in kiara/pipeline/values.py def __hash__ ( self ): return hash ( self . _id ) __repr__ ( self ) special \u00b6 Return repr(self). Source code in kiara/pipeline/values.py def __repr__ ( self ): step_id = \"\" if hasattr ( self , \"step_id\" ): step_id = f \" step_id=' { self . step_id } '\" return f \" { self . __class__ . __name__ } (value_name=' { self . value_name } ' { step_id } )\" __str__ ( self ) special \u00b6 Return str(self). Source code in kiara/pipeline/values.py def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . value_name } ( { self . value_schema . type } )\"","title":"values"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.PipelineInputRef","text":"An input to a pipeline. Source code in kiara/pipeline/values.py class PipelineInputRef ( ValueRef ): \"\"\"An input to a pipeline.\"\"\" connected_inputs : typing . List [ StepValueAddress ] = Field ( description = \"The step inputs that are connected to this pipeline input\" , default_factory = list , ) is_constant : bool = Field ( \"Whether this input is a constant and can't be changed by the user.\" ) @property def alias ( self ) -> str : return generate_step_alias ( PIPELINE_PARENT_MARKER , self . value_name )","title":"PipelineInputRef"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.PipelineInputRef.connected_inputs","text":"The step inputs that are connected to this pipeline input","title":"connected_inputs"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.PipelineOutputRef","text":"An output to a pipeline. Source code in kiara/pipeline/values.py class PipelineOutputRef ( ValueRef ): \"\"\"An output to a pipeline.\"\"\" connected_output : StepValueAddress = Field ( description = \"Connected step outputs.\" ) @property def alias ( self ) -> str : return generate_step_alias ( PIPELINE_PARENT_MARKER , self . value_name )","title":"PipelineOutputRef"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.PipelineOutputRef.connected_output","text":"Connected step outputs.","title":"connected_output"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.StepInputRef","text":"An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both. Source code in kiara/pipeline/values.py class StepInputRef ( ValueRef ): \"\"\"An input to a step. This object can either have a 'connected_outputs' set, or a 'connected_pipeline_input', not both. \"\"\" step_id : str = Field ( description = \"The step id.\" ) connected_outputs : typing . Optional [ typing . List [ StepValueAddress ]] = Field ( default = None , description = \"A potential connected list of one or several module outputs.\" , ) connected_pipeline_input : typing . Optional [ str ] = Field ( default = None , description = \"A potential pipeline input.\" ) is_constant : bool = Field ( \"Whether this input is a constant and can't be changed by the user.\" ) @root_validator ( pre = True ) def ensure_single_connected_item ( cls , values ): if values . get ( \"connected_outputs\" , None ) and values . get ( \"connected_pipeline_input\" ): raise ValueError ( \"Multiple connected items, only one allowed.\" ) return values @property def alias ( self ) -> str : return generate_step_alias ( self . step_id , self . value_name ) @property def address ( self ) -> StepValueAddress : return StepValueAddress ( step_id = self . step_id , value_name = self . value_name ) def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . step_id } . { self . value_name } ( { self . value_schema . type } )\"","title":"StepInputRef"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.StepInputRef.connected_outputs","text":"A potential connected list of one or several module outputs.","title":"connected_outputs"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.StepInputRef.connected_pipeline_input","text":"A potential pipeline input.","title":"connected_pipeline_input"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.StepInputRef.step_id","text":"The step id.","title":"step_id"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.StepOutputRef","text":"An output to a step. Source code in kiara/pipeline/values.py class StepOutputRef ( ValueRef ): \"\"\"An output to a step.\"\"\" class Config : allow_mutation = True step_id : str = Field ( description = \"The step id.\" ) pipeline_output : typing . Optional [ str ] = Field ( description = \"The connected pipeline output.\" ) connected_inputs : typing . List [ StepValueAddress ] = Field ( description = \"The step inputs that are connected to this step output\" , default_factory = list , ) @property def alias ( self ) -> str : return generate_step_alias ( self . step_id , self . value_name ) @property def address ( self ) -> StepValueAddress : return StepValueAddress ( step_id = self . step_id , value_name = self . value_name ) def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . step_id } . { self . value_name } ( { self . value_schema . type } )\"","title":"StepOutputRef"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.StepOutputRef.connected_inputs","text":"The step inputs that are connected to this step output","title":"connected_inputs"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.StepOutputRef.pipeline_output","text":"The connected pipeline output.","title":"pipeline_output"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.StepOutputRef.step_id","text":"The step id.","title":"step_id"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.ValueRef","text":"An object that holds information about the location of a value within a pipeline (or other structure). Basically, a ValueRef helps the containing object where in its structure the value belongs (for example so it can update dependent other values). A ValueRef object (obviously) does not contain the value itself. There are four different ValueRef type that are relevant for pipelines: kiara.pipeline.values.StepInputRef : an input to a step kiara.pipeline.values.StepOutputRef : an output of a step kiara.pipeline.values.PipelineInputRef : an input to a pipeline kiara.pipeline.values.PipelineOutputRef : an output for a pipeline Several ValueRef objects can target the same value, for example a step output and a connected step input would reference the same Value (in most cases).. Source code in kiara/pipeline/values.py class ValueRef ( BaseModel ): \"\"\"An object that holds information about the location of a value within a pipeline (or other structure). Basically, a `ValueRef` helps the containing object where in its structure the value belongs (for example so it can update dependent other values). A `ValueRef` object (obviously) does not contain the value itself. There are four different ValueRef type that are relevant for pipelines: - [kiara.pipeline.values.StepInputRef][]: an input to a step - [kiara.pipeline.values.StepOutputRef][]: an output of a step - [kiara.pipeline.values.PipelineInputRef][]: an input to a pipeline - [kiara.pipeline.values.PipelineOutputRef][]: an output for a pipeline Several `ValueRef` objects can target the same value, for example a step output and a connected step input would reference the same `Value` (in most cases).. \"\"\" class Config : allow_mutation = True extra = Extra . forbid _id : uuid . UUID = PrivateAttr ( default_factory = uuid . uuid4 ) value_name : str value_schema : ValueSchema # pipeline_id: str def __eq__ ( self , other ): if not isinstance ( other , self . __class__ ): return False return self . _id == other . _id def __hash__ ( self ): return hash ( self . _id ) def __repr__ ( self ): step_id = \"\" if hasattr ( self , \"step_id\" ): step_id = f \" step_id=' { self . step_id } '\" return f \" { self . __class__ . __name__ } (value_name=' { self . value_name } ' { step_id } )\" def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . value_name } ( { self . value_schema . type } )\"","title":"ValueRef"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.ValueRef.__eq__","text":"Return self==value. Source code in kiara/pipeline/values.py def __eq__ ( self , other ): if not isinstance ( other , self . __class__ ): return False return self . _id == other . _id","title":"__eq__()"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.ValueRef.__hash__","text":"Return hash(self). Source code in kiara/pipeline/values.py def __hash__ ( self ): return hash ( self . _id )","title":"__hash__()"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.ValueRef.__repr__","text":"Return repr(self). Source code in kiara/pipeline/values.py def __repr__ ( self ): step_id = \"\" if hasattr ( self , \"step_id\" ): step_id = f \" step_id=' { self . step_id } '\" return f \" { self . __class__ . __name__ } (value_name=' { self . value_name } ' { step_id } )\"","title":"__repr__()"},{"location":"reference/kiara/pipeline/values/#kiara.pipeline.values.ValueRef.__str__","text":"Return str(self). Source code in kiara/pipeline/values.py def __str__ ( self ): name = camel_case_to_snake_case ( self . __class__ . __name__ [ 0 : - 5 ], repl = \" \" ) return f \" { name } : { self . value_name } ( { self . value_schema . type } )\"","title":"__str__()"},{"location":"reference/kiara/pipeline/controller/__init__/","text":"PipelineController ( PipelineListener ) \u00b6 An object that controls how a Pipeline should react to events related to it's inputs/outputs. This is the base for the central controller class that needs to be implemented by a Kiara frontend. The default implementation that is used if no PipelineController is provided in a Pipeline constructor is the BatchController , which basically waits until all required inputs are set, and then processes all pipeline steps in one go (in the right order). The pipeline object to control can be set either in the constructor, or via the set_pipeline method. But only once, every subsequent attempt to set a pipeline will raise an Exception. If you want to implement your own controller, you have to override at least one of the (empty) event hook methods: pipeline_inputs_changed pipeline_outputs_changed step_inputs_changed step_outputs_changed Parameters: Name Type Description Default pipeline Pipeline the pipeline object to control None Source code in kiara/pipeline/controller/__init__.py class PipelineController ( PipelineListener ): \"\"\"An object that controls how a [Pipeline][kiara.pipeline.pipeline.Pipeline] should react to events related to it's inputs/outputs. This is the base for the central controller class that needs to be implemented by a *Kiara* frontend. The default implementation that is used if no ``PipelineController`` is provided in a [Pipeline][kiara.pipeline.pipeline.Pipeline] constructor is the [BatchController][kiara.pipeline.controller.BatchController], which basically waits until all required inputs are set, and then processes all pipeline steps in one go (in the right order). The pipeline object to control can be set either in the constructor, or via the ``set_pipeline`` method. But only once, every subsequent attempt to set a pipeline will raise an Exception. If you want to implement your own controller, you have to override at least one of the (empty) event hook methods: - [``pipeline_inputs_changed``][kiara.pipeline.controller.PipelineController.pipeline_inputs_changed] - [``pipeline_outputs_changed``][kiara.pipeline.controller.PipelineController.pipeline_outputs_changed] - [``step_inputs_changed``][kiara.pipeline.controller.PipelineController.step_inputs_changed] - [``step_outputs_changed``][kiara.pipeline.controller.PipelineController.step_outputs_changed] Arguments: pipeline (Pipeline): the pipeline object to control \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : Kiara = kiara self . _pipeline : typing . Optional [ Pipeline ] = None if processor is None : from kiara.processing.synchronous import SynchronousProcessor processor = SynchronousProcessor ( kiara = kiara ) self . _processor : ModuleProcessor = processor self . _job_ids : typing . Dict [ str , str ] = {} \"\"\"A map of the last or current job ids per step_id.\"\"\" if pipeline is not None : self . set_pipeline ( pipeline ) @property def pipeline ( self ) -> \"Pipeline\" : \"\"\"Return the pipeline this controller, well, ...controls...\"\"\" if self . _pipeline is None : raise Exception ( \"Pipeline not set yet.\" ) return self . _pipeline @property def pipeline_status ( self ) -> \"StepStatus\" : return self . pipeline . status def set_pipeline ( self , pipeline : \"Pipeline\" ): \"\"\"Set the pipeline object for this controller. Only one pipeline can be set, once. Arguments: pipeline: the pipeline object \"\"\" if self . _pipeline is not None : raise Exception ( \"Pipeline already set.\" ) self . _pipeline = pipeline @property def processing_stages ( self ) -> typing . List [ typing . List [ str ]]: \"\"\"Return the processing stage order of the pipeline. Returns: a list of lists of step ids \"\"\" return self . pipeline . structure . processing_stages def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the step object for the provided id. Arguments: step_id: the step id Returns: the step object \"\"\" return self . pipeline . get_step ( step_id ) def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the inputs object for the pipeline.\"\"\" return self . pipeline . get_step_inputs ( step_id ) def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the outputs object for the pipeline.\"\"\" return self . pipeline . get_step_outputs ( step_id ) def get_step_input ( self , step_id : str , input_name : str ) -> Value : \"\"\"Get the (current) input value for a specified step and input field name.\"\"\" item = self . get_step_inputs ( step_id ) . get_value_obj ( input_name ) assert item is not None return item def get_step_output ( self , step_id : str , output_name : str ) -> Value : \"\"\"Get the (current) output value for a specified step and output field name.\"\"\" item = self . get_step_outputs ( step_id ) . get_value_obj ( output_name ) assert item is not None return item def get_current_pipeline_state ( self ) -> \"PipelineState\" : \"\"\"Return a description of the current pipeline state. This methods creates a new [PipelineState][kiara.pipeline.pipeline.PipelineState] object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: an object outlining the current pipeline state \"\"\" return self . pipeline . get_current_state () @property def pipeline_inputs ( self ) -> ValueSet : \"\"\"Return the inputs object for this pipeline.\"\"\" return self . pipeline . _pipeline_inputs @pipeline_inputs . setter def pipeline_inputs ( self , inputs : typing . Mapping [ str , typing . Any ]) -> None : \"\"\"Set one, several or all inputs for this pipeline.\"\"\" self . set_pipeline_inputs ( ** inputs ) @property def pipeline_outputs ( self ) -> ValueSet : \"\"\"Return the (current) pipeline outputs object for this pipeline.\"\"\" return self . pipeline . _pipeline_outputs def wait_for_jobs ( self , * job_ids : str , sync_outputs : bool = True ): self . _processor . wait_for ( * job_ids , sync_outputs = sync_outputs ) def can_be_processed ( self , step_id : str ) -> bool : \"\"\"Check whether the step with the provided id is ready to be processed.\"\"\" result = True step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): result = False break return result def check_inputs_status ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]]: \"\"\"Check the status of all stages/steps, and whether they are ready to be processed. The result dictionary uses stage numbers as keys, and a secondary dictionary as values which in turn uses the step_id as key, and a dict with details (keys: 'ready', 'invalid_inputs', 'required') as values. \"\"\" result : typing . Dict [ int , typing . Dict [ str , typing . Dict [ str , typing . Any ]]] = {} for idx , stage in enumerate ( self . processing_stages , start = 1 ): for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], } else : result . setdefault ( idx , {})[ step_id ] = { \"ready\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], \"required\" : True , } else : if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : True , \"invalid_inputs\" : [], } return result def can_be_skipped ( self , step_id : str ) -> bool : \"\"\"Check whether the processing of a step can be skipped.\"\"\" result = True step = self . get_step ( step_id ) if step . required : result = self . can_be_processed ( step_id ) return result def invalid_inputs ( self , step_id : str ) -> typing . List [ str ]: invalid = [] step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): invalid . append ( input_name ) return invalid def process_step ( self , step_id : str , raise_exception : bool = False , wait : bool = False ) -> str : \"\"\"Kick off processing for the step with the provided id. Arguments: step_id: the id of the step that should be started \"\"\" step_inputs = self . get_step_inputs ( step_id ) # if the inputs are not valid, ignore this step if not step_inputs . items_are_valid (): status = step_inputs . check_invalid () assert status is not None raise Exception ( f \"Can't execute step ' { step_id } ', invalid inputs: { ', ' . join ( status . keys ()) } \" ) # get the output 'holder' objects, which we'll need to pass to the module step_outputs = self . get_step_outputs ( step_id ) # get the module object that holds the code that will do the processing step = self . get_step ( step_id ) job_id = self . _processor . start ( pipeline_id = self . pipeline . id , pipeline_name = self . pipeline . title , step_id = step_id , module = step . module , inputs = step_inputs , outputs = step_outputs , ) self . _job_ids [ step_id ] = job_id if wait : self . wait_for_jobs ( job_id , sync_outputs = True ) return job_id def get_job_details ( self , step_or_job_id : str ) -> typing . Optional [ Job ]: \"\"\"Returns job details for a job id, or in case a step_id was provided, the last execution of this step.\"\"\" if self . _job_ids . get ( step_or_job_id , None ) is None : return self . _processor . get_job_details ( step_or_job_id ) else : return self . _processor . get_job_details ( self . _job_ids [ step_or_job_id ]) def step_is_ready ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id is ready to be processed. A ``True`` result means that all input fields are currently set with valid values. Arguments: step_id: the id of the step to check Returns: whether the step is ready (``True``) or not (``False``) \"\"\" return self . get_step_inputs ( step_id ) . items_are_valid () def step_is_finished ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id has been processed successfully. A ``True`` result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Arguments: step_id: the id of the step to check Returns: whether the step result is valid (``True``) or not (``False``) \"\"\" return self . get_step_outputs ( step_id ) . items_are_valid () def pipeline_is_ready ( self ) -> bool : \"\"\"Return whether the pipeline is ready to be processed. A ``True`` result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: whether the pipeline can be processed as a whole (``True``) or not (``False``) \"\"\" return self . pipeline . inputs . items_are_valid () def pipeline_is_finished ( self ) -> bool : \"\"\"Return whether the pipeline has been processed successfully. A ``True`` result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: whether the pipeline was processed successfully (``True``) or not (``False``) \"\"\" return self . pipeline . outputs . items_are_valid () def set_pipeline_inputs ( self , ** inputs : typing . Any ): \"\"\"Set one, several or all inputs for this pipeline. Arguments: **inputs: the input values to set \"\"\" _inputs = self . _pipeline_input_hook ( ** inputs ) self . pipeline_inputs . set_values ( ** _inputs ) def _pipeline_input_hook ( self , ** inputs : typing . Any ): \"\"\"Hook before setting input. Can be implemented by child controller classes, to prevent, transform, validate or queue inputs. \"\"\" log . debug ( f \"Inputs for pipeline ' { self . pipeline . id } ' set: { inputs } \" ) return inputs def process_pipeline ( self ): \"\"\"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Returns: \"\"\" raise NotImplementedError () pipeline : Pipeline property readonly \u00b6 Return the pipeline this controller, well, ...controls... pipeline_inputs : ValueSet property writable \u00b6 Return the inputs object for this pipeline. pipeline_outputs : ValueSet property readonly \u00b6 Return the (current) pipeline outputs object for this pipeline. processing_stages : List [ List [ str ]] property readonly \u00b6 Return the processing stage order of the pipeline. Returns: Type Description List[List[str]] a list of lists of step ids can_be_processed ( self , step_id ) \u00b6 Check whether the step with the provided id is ready to be processed. Source code in kiara/pipeline/controller/__init__.py def can_be_processed ( self , step_id : str ) -> bool : \"\"\"Check whether the step with the provided id is ready to be processed.\"\"\" result = True step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): result = False break return result can_be_skipped ( self , step_id ) \u00b6 Check whether the processing of a step can be skipped. Source code in kiara/pipeline/controller/__init__.py def can_be_skipped ( self , step_id : str ) -> bool : \"\"\"Check whether the processing of a step can be skipped.\"\"\" result = True step = self . get_step ( step_id ) if step . required : result = self . can_be_processed ( step_id ) return result check_inputs_status ( self ) \u00b6 Check the status of all stages/steps, and whether they are ready to be processed. The result dictionary uses stage numbers as keys, and a secondary dictionary as values which in turn uses the step_id as key, and a dict with details (keys: 'ready', 'invalid_inputs', 'required') as values. Source code in kiara/pipeline/controller/__init__.py def check_inputs_status ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]]: \"\"\"Check the status of all stages/steps, and whether they are ready to be processed. The result dictionary uses stage numbers as keys, and a secondary dictionary as values which in turn uses the step_id as key, and a dict with details (keys: 'ready', 'invalid_inputs', 'required') as values. \"\"\" result : typing . Dict [ int , typing . Dict [ str , typing . Dict [ str , typing . Any ]]] = {} for idx , stage in enumerate ( self . processing_stages , start = 1 ): for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], } else : result . setdefault ( idx , {})[ step_id ] = { \"ready\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], \"required\" : True , } else : if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : True , \"invalid_inputs\" : [], } return result get_current_pipeline_state ( self ) \u00b6 Return a description of the current pipeline state. This methods creates a new PipelineState object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: Type Description PipelineState an object outlining the current pipeline state Source code in kiara/pipeline/controller/__init__.py def get_current_pipeline_state ( self ) -> \"PipelineState\" : \"\"\"Return a description of the current pipeline state. This methods creates a new [PipelineState][kiara.pipeline.pipeline.PipelineState] object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: an object outlining the current pipeline state \"\"\" return self . pipeline . get_current_state () get_job_details ( self , step_or_job_id ) \u00b6 Returns job details for a job id, or in case a step_id was provided, the last execution of this step. Source code in kiara/pipeline/controller/__init__.py def get_job_details ( self , step_or_job_id : str ) -> typing . Optional [ Job ]: \"\"\"Returns job details for a job id, or in case a step_id was provided, the last execution of this step.\"\"\" if self . _job_ids . get ( step_or_job_id , None ) is None : return self . _processor . get_job_details ( step_or_job_id ) else : return self . _processor . get_job_details ( self . _job_ids [ step_or_job_id ]) get_step ( self , step_id ) \u00b6 Return the step object for the provided id. Parameters: Name Type Description Default step_id str the step id required Returns: Type Description PipelineStep the step object Source code in kiara/pipeline/controller/__init__.py def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the step object for the provided id. Arguments: step_id: the step id Returns: the step object \"\"\" return self . pipeline . get_step ( step_id ) get_step_input ( self , step_id , input_name ) \u00b6 Get the (current) input value for a specified step and input field name. Source code in kiara/pipeline/controller/__init__.py def get_step_input ( self , step_id : str , input_name : str ) -> Value : \"\"\"Get the (current) input value for a specified step and input field name.\"\"\" item = self . get_step_inputs ( step_id ) . get_value_obj ( input_name ) assert item is not None return item get_step_inputs ( self , step_id ) \u00b6 Return the inputs object for the pipeline. Source code in kiara/pipeline/controller/__init__.py def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the inputs object for the pipeline.\"\"\" return self . pipeline . get_step_inputs ( step_id ) get_step_output ( self , step_id , output_name ) \u00b6 Get the (current) output value for a specified step and output field name. Source code in kiara/pipeline/controller/__init__.py def get_step_output ( self , step_id : str , output_name : str ) -> Value : \"\"\"Get the (current) output value for a specified step and output field name.\"\"\" item = self . get_step_outputs ( step_id ) . get_value_obj ( output_name ) assert item is not None return item get_step_outputs ( self , step_id ) \u00b6 Return the outputs object for the pipeline. Source code in kiara/pipeline/controller/__init__.py def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the outputs object for the pipeline.\"\"\" return self . pipeline . get_step_outputs ( step_id ) pipeline_is_finished ( self ) \u00b6 Return whether the pipeline has been processed successfully. A True result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: Type Description bool whether the pipeline was processed successfully ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def pipeline_is_finished ( self ) -> bool : \"\"\"Return whether the pipeline has been processed successfully. A ``True`` result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: whether the pipeline was processed successfully (``True``) or not (``False``) \"\"\" return self . pipeline . outputs . items_are_valid () pipeline_is_ready ( self ) \u00b6 Return whether the pipeline is ready to be processed. A True result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: Type Description bool whether the pipeline can be processed as a whole ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def pipeline_is_ready ( self ) -> bool : \"\"\"Return whether the pipeline is ready to be processed. A ``True`` result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: whether the pipeline can be processed as a whole (``True``) or not (``False``) \"\"\" return self . pipeline . inputs . items_are_valid () process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/__init__.py def process_pipeline ( self ): \"\"\"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Returns: \"\"\" raise NotImplementedError () process_step ( self , step_id , raise_exception = False , wait = False ) \u00b6 Kick off processing for the step with the provided id. Parameters: Name Type Description Default step_id str the id of the step that should be started required Source code in kiara/pipeline/controller/__init__.py def process_step ( self , step_id : str , raise_exception : bool = False , wait : bool = False ) -> str : \"\"\"Kick off processing for the step with the provided id. Arguments: step_id: the id of the step that should be started \"\"\" step_inputs = self . get_step_inputs ( step_id ) # if the inputs are not valid, ignore this step if not step_inputs . items_are_valid (): status = step_inputs . check_invalid () assert status is not None raise Exception ( f \"Can't execute step ' { step_id } ', invalid inputs: { ', ' . join ( status . keys ()) } \" ) # get the output 'holder' objects, which we'll need to pass to the module step_outputs = self . get_step_outputs ( step_id ) # get the module object that holds the code that will do the processing step = self . get_step ( step_id ) job_id = self . _processor . start ( pipeline_id = self . pipeline . id , pipeline_name = self . pipeline . title , step_id = step_id , module = step . module , inputs = step_inputs , outputs = step_outputs , ) self . _job_ids [ step_id ] = job_id if wait : self . wait_for_jobs ( job_id , sync_outputs = True ) return job_id set_pipeline ( self , pipeline ) \u00b6 Set the pipeline object for this controller. Only one pipeline can be set, once. Parameters: Name Type Description Default pipeline Pipeline the pipeline object required Source code in kiara/pipeline/controller/__init__.py def set_pipeline ( self , pipeline : \"Pipeline\" ): \"\"\"Set the pipeline object for this controller. Only one pipeline can be set, once. Arguments: pipeline: the pipeline object \"\"\" if self . _pipeline is not None : raise Exception ( \"Pipeline already set.\" ) self . _pipeline = pipeline set_pipeline_inputs ( self , ** inputs ) \u00b6 Set one, several or all inputs for this pipeline. Parameters: Name Type Description Default **inputs Any the input values to set {} Source code in kiara/pipeline/controller/__init__.py def set_pipeline_inputs ( self , ** inputs : typing . Any ): \"\"\"Set one, several or all inputs for this pipeline. Arguments: **inputs: the input values to set \"\"\" _inputs = self . _pipeline_input_hook ( ** inputs ) self . pipeline_inputs . set_values ( ** _inputs ) step_is_finished ( self , step_id ) \u00b6 Return whether the step with the provided id has been processed successfully. A True result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Parameters: Name Type Description Default step_id str the id of the step to check required Returns: Type Description bool whether the step result is valid ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def step_is_finished ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id has been processed successfully. A ``True`` result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Arguments: step_id: the id of the step to check Returns: whether the step result is valid (``True``) or not (``False``) \"\"\" return self . get_step_outputs ( step_id ) . items_are_valid () step_is_ready ( self , step_id ) \u00b6 Return whether the step with the provided id is ready to be processed. A True result means that all input fields are currently set with valid values. Parameters: Name Type Description Default step_id str the id of the step to check required Returns: Type Description bool whether the step is ready ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def step_is_ready ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id is ready to be processed. A ``True`` result means that all input fields are currently set with valid values. Arguments: step_id: the id of the step to check Returns: whether the step is ready (``True``) or not (``False``) \"\"\" return self . get_step_inputs ( step_id ) . items_are_valid () batch \u00b6 BatchController ( PipelineController ) \u00b6 A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline Optional[Pipeline] the pipeline to control None auto_process bool whether to automatically start processing the pipeline as soon as the input set is valid True Source code in kiara/pipeline/controller/batch.py class BatchController ( PipelineController ): \"\"\"A [PipelineController][kiara.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively. This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Arguments: pipeline: the pipeline to control auto_process: whether to automatically start processing the pipeline as soon as the input set is valid \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , auto_process : bool = True , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _auto_process : bool = auto_process self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor , kiara = kiara ) @property def auto_process ( self ) -> bool : return self . _auto_process @auto_process . setter def auto_process ( self , auto_process : bool ): self . _auto_process = auto_process def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_exc () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) # for j_id in job_ids: # job = self._processor.get_job_details(j_id) # assert job is not None # if job.error: # print(job.error) finally : self . _is_running = False def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return if self . _auto_process : self . process_pipeline () def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_exc () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) # for j_id in job_ids: # job = self._processor.get_job_details(j_id) # assert job is not None # if job.error: # print(job.error) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return if self . _auto_process : self . process_pipeline () BatchControllerManual ( PipelineController ) \u00b6 A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline Optional[Pipeline] the pipeline to control None auto_process whether to automatically start processing the pipeline as soon as the input set is valid required Source code in kiara/pipeline/controller/batch.py class BatchControllerManual ( PipelineController ): \"\"\"A [PipelineController][kiara.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively. This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Arguments: pipeline: the pipeline to control auto_process: whether to automatically start processing the pipeline as soon as the input set is valid \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _finished_until : typing . Optional [ int ] = None self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor , kiara = kiara ) @property def last_finished_stage ( self ) -> int : if self . _finished_until is None : return 0 else : return self . _finished_until def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return def pipeline_inputs_changed ( self , event : \"PipelineInputEvent\" ): self . _finished_until = None # print(\"============================\") # min_stage_to_clear = sys.maxsize # for inp in event.updated_pipeline_inputs: # stage = self.pipeline.get_stage_for_pipeline_input(inp) # if stage < min_stage_to_clear: # min_stage_to_clear = stage # # for stage, steps in self.pipeline.get_steps_by_stage().items(): # if stage < min_stage_to_clear: # continue # for step_id, step in steps.items(): # step_inputs = self.get_step_inputs(step_id) # empty = { k: None for k in step_inputs.keys() } # step_inputs.set_values(**empty) # step_outputs = self.get_step_outputs(step_id) # empty = { k: None for k in step_outputs.keys() } # step_outputs.set_values(**empty) def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False def process_stage ( self , stage_nr : int ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Union [ None , str , Exception ]]]: if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True result : typing . Dict [ int , typing . Dict [ str , typing . Union [ None , str , Exception ]] ] = {} try : for idx , stage in enumerate ( self . processing_stages ): current_stage = idx + 1 if current_stage > stage_nr : break if self . _finished_until is not None and idx <= self . _finished_until : continue job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( current_stage , {})[ step_id ] = None continue else : exc = Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) result . setdefault ( current_stage , {})[ step_id ] = exc try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) result . setdefault ( current_stage , {})[ step_id ] = job_id except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) result . setdefault ( current_stage , {})[ step_id ] = e self . _processor . wait_for ( * job_ids ) # for job_id in job_ids: # job = self.get_job_details(job_id) # dbg(job.dict()) self . _finished_until = idx finally : self . _is_running = False print ( \"BATCH FINISHED\" ) return result pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/pipeline/controller/batch.py def pipeline_inputs_changed ( self , event : \"PipelineInputEvent\" ): self . _finished_until = None # print(\"============================\") # min_stage_to_clear = sys.maxsize # for inp in event.updated_pipeline_inputs: # stage = self.pipeline.get_stage_for_pipeline_input(inp) # if stage < min_stage_to_clear: # min_stage_to_clear = stage # # for stage, steps in self.pipeline.get_steps_by_stage().items(): # if stage < min_stage_to_clear: # continue # for step_id, step in steps.items(): # step_inputs = self.get_step_inputs(step_id) # empty = { k: None for k in step_inputs.keys() } # step_inputs.set_values(**empty) # step_outputs = self.get_step_outputs(step_id) # empty = { k: None for k in step_outputs.keys() } # step_outputs.set_values(**empty) pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return","title":"controller"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController","text":"An object that controls how a Pipeline should react to events related to it's inputs/outputs. This is the base for the central controller class that needs to be implemented by a Kiara frontend. The default implementation that is used if no PipelineController is provided in a Pipeline constructor is the BatchController , which basically waits until all required inputs are set, and then processes all pipeline steps in one go (in the right order). The pipeline object to control can be set either in the constructor, or via the set_pipeline method. But only once, every subsequent attempt to set a pipeline will raise an Exception. If you want to implement your own controller, you have to override at least one of the (empty) event hook methods: pipeline_inputs_changed pipeline_outputs_changed step_inputs_changed step_outputs_changed Parameters: Name Type Description Default pipeline Pipeline the pipeline object to control None Source code in kiara/pipeline/controller/__init__.py class PipelineController ( PipelineListener ): \"\"\"An object that controls how a [Pipeline][kiara.pipeline.pipeline.Pipeline] should react to events related to it's inputs/outputs. This is the base for the central controller class that needs to be implemented by a *Kiara* frontend. The default implementation that is used if no ``PipelineController`` is provided in a [Pipeline][kiara.pipeline.pipeline.Pipeline] constructor is the [BatchController][kiara.pipeline.controller.BatchController], which basically waits until all required inputs are set, and then processes all pipeline steps in one go (in the right order). The pipeline object to control can be set either in the constructor, or via the ``set_pipeline`` method. But only once, every subsequent attempt to set a pipeline will raise an Exception. If you want to implement your own controller, you have to override at least one of the (empty) event hook methods: - [``pipeline_inputs_changed``][kiara.pipeline.controller.PipelineController.pipeline_inputs_changed] - [``pipeline_outputs_changed``][kiara.pipeline.controller.PipelineController.pipeline_outputs_changed] - [``step_inputs_changed``][kiara.pipeline.controller.PipelineController.step_inputs_changed] - [``step_outputs_changed``][kiara.pipeline.controller.PipelineController.step_outputs_changed] Arguments: pipeline (Pipeline): the pipeline object to control \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : Kiara = kiara self . _pipeline : typing . Optional [ Pipeline ] = None if processor is None : from kiara.processing.synchronous import SynchronousProcessor processor = SynchronousProcessor ( kiara = kiara ) self . _processor : ModuleProcessor = processor self . _job_ids : typing . Dict [ str , str ] = {} \"\"\"A map of the last or current job ids per step_id.\"\"\" if pipeline is not None : self . set_pipeline ( pipeline ) @property def pipeline ( self ) -> \"Pipeline\" : \"\"\"Return the pipeline this controller, well, ...controls...\"\"\" if self . _pipeline is None : raise Exception ( \"Pipeline not set yet.\" ) return self . _pipeline @property def pipeline_status ( self ) -> \"StepStatus\" : return self . pipeline . status def set_pipeline ( self , pipeline : \"Pipeline\" ): \"\"\"Set the pipeline object for this controller. Only one pipeline can be set, once. Arguments: pipeline: the pipeline object \"\"\" if self . _pipeline is not None : raise Exception ( \"Pipeline already set.\" ) self . _pipeline = pipeline @property def processing_stages ( self ) -> typing . List [ typing . List [ str ]]: \"\"\"Return the processing stage order of the pipeline. Returns: a list of lists of step ids \"\"\" return self . pipeline . structure . processing_stages def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the step object for the provided id. Arguments: step_id: the step id Returns: the step object \"\"\" return self . pipeline . get_step ( step_id ) def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the inputs object for the pipeline.\"\"\" return self . pipeline . get_step_inputs ( step_id ) def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the outputs object for the pipeline.\"\"\" return self . pipeline . get_step_outputs ( step_id ) def get_step_input ( self , step_id : str , input_name : str ) -> Value : \"\"\"Get the (current) input value for a specified step and input field name.\"\"\" item = self . get_step_inputs ( step_id ) . get_value_obj ( input_name ) assert item is not None return item def get_step_output ( self , step_id : str , output_name : str ) -> Value : \"\"\"Get the (current) output value for a specified step and output field name.\"\"\" item = self . get_step_outputs ( step_id ) . get_value_obj ( output_name ) assert item is not None return item def get_current_pipeline_state ( self ) -> \"PipelineState\" : \"\"\"Return a description of the current pipeline state. This methods creates a new [PipelineState][kiara.pipeline.pipeline.PipelineState] object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: an object outlining the current pipeline state \"\"\" return self . pipeline . get_current_state () @property def pipeline_inputs ( self ) -> ValueSet : \"\"\"Return the inputs object for this pipeline.\"\"\" return self . pipeline . _pipeline_inputs @pipeline_inputs . setter def pipeline_inputs ( self , inputs : typing . Mapping [ str , typing . Any ]) -> None : \"\"\"Set one, several or all inputs for this pipeline.\"\"\" self . set_pipeline_inputs ( ** inputs ) @property def pipeline_outputs ( self ) -> ValueSet : \"\"\"Return the (current) pipeline outputs object for this pipeline.\"\"\" return self . pipeline . _pipeline_outputs def wait_for_jobs ( self , * job_ids : str , sync_outputs : bool = True ): self . _processor . wait_for ( * job_ids , sync_outputs = sync_outputs ) def can_be_processed ( self , step_id : str ) -> bool : \"\"\"Check whether the step with the provided id is ready to be processed.\"\"\" result = True step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): result = False break return result def check_inputs_status ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]]: \"\"\"Check the status of all stages/steps, and whether they are ready to be processed. The result dictionary uses stage numbers as keys, and a secondary dictionary as values which in turn uses the step_id as key, and a dict with details (keys: 'ready', 'invalid_inputs', 'required') as values. \"\"\" result : typing . Dict [ int , typing . Dict [ str , typing . Dict [ str , typing . Any ]]] = {} for idx , stage in enumerate ( self . processing_stages , start = 1 ): for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], } else : result . setdefault ( idx , {})[ step_id ] = { \"ready\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], \"required\" : True , } else : if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : True , \"invalid_inputs\" : [], } return result def can_be_skipped ( self , step_id : str ) -> bool : \"\"\"Check whether the processing of a step can be skipped.\"\"\" result = True step = self . get_step ( step_id ) if step . required : result = self . can_be_processed ( step_id ) return result def invalid_inputs ( self , step_id : str ) -> typing . List [ str ]: invalid = [] step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): invalid . append ( input_name ) return invalid def process_step ( self , step_id : str , raise_exception : bool = False , wait : bool = False ) -> str : \"\"\"Kick off processing for the step with the provided id. Arguments: step_id: the id of the step that should be started \"\"\" step_inputs = self . get_step_inputs ( step_id ) # if the inputs are not valid, ignore this step if not step_inputs . items_are_valid (): status = step_inputs . check_invalid () assert status is not None raise Exception ( f \"Can't execute step ' { step_id } ', invalid inputs: { ', ' . join ( status . keys ()) } \" ) # get the output 'holder' objects, which we'll need to pass to the module step_outputs = self . get_step_outputs ( step_id ) # get the module object that holds the code that will do the processing step = self . get_step ( step_id ) job_id = self . _processor . start ( pipeline_id = self . pipeline . id , pipeline_name = self . pipeline . title , step_id = step_id , module = step . module , inputs = step_inputs , outputs = step_outputs , ) self . _job_ids [ step_id ] = job_id if wait : self . wait_for_jobs ( job_id , sync_outputs = True ) return job_id def get_job_details ( self , step_or_job_id : str ) -> typing . Optional [ Job ]: \"\"\"Returns job details for a job id, or in case a step_id was provided, the last execution of this step.\"\"\" if self . _job_ids . get ( step_or_job_id , None ) is None : return self . _processor . get_job_details ( step_or_job_id ) else : return self . _processor . get_job_details ( self . _job_ids [ step_or_job_id ]) def step_is_ready ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id is ready to be processed. A ``True`` result means that all input fields are currently set with valid values. Arguments: step_id: the id of the step to check Returns: whether the step is ready (``True``) or not (``False``) \"\"\" return self . get_step_inputs ( step_id ) . items_are_valid () def step_is_finished ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id has been processed successfully. A ``True`` result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Arguments: step_id: the id of the step to check Returns: whether the step result is valid (``True``) or not (``False``) \"\"\" return self . get_step_outputs ( step_id ) . items_are_valid () def pipeline_is_ready ( self ) -> bool : \"\"\"Return whether the pipeline is ready to be processed. A ``True`` result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: whether the pipeline can be processed as a whole (``True``) or not (``False``) \"\"\" return self . pipeline . inputs . items_are_valid () def pipeline_is_finished ( self ) -> bool : \"\"\"Return whether the pipeline has been processed successfully. A ``True`` result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: whether the pipeline was processed successfully (``True``) or not (``False``) \"\"\" return self . pipeline . outputs . items_are_valid () def set_pipeline_inputs ( self , ** inputs : typing . Any ): \"\"\"Set one, several or all inputs for this pipeline. Arguments: **inputs: the input values to set \"\"\" _inputs = self . _pipeline_input_hook ( ** inputs ) self . pipeline_inputs . set_values ( ** _inputs ) def _pipeline_input_hook ( self , ** inputs : typing . Any ): \"\"\"Hook before setting input. Can be implemented by child controller classes, to prevent, transform, validate or queue inputs. \"\"\" log . debug ( f \"Inputs for pipeline ' { self . pipeline . id } ' set: { inputs } \" ) return inputs def process_pipeline ( self ): \"\"\"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Returns: \"\"\" raise NotImplementedError ()","title":"PipelineController"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.pipeline","text":"Return the pipeline this controller, well, ...controls...","title":"pipeline"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.pipeline_inputs","text":"Return the inputs object for this pipeline.","title":"pipeline_inputs"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.pipeline_outputs","text":"Return the (current) pipeline outputs object for this pipeline.","title":"pipeline_outputs"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.processing_stages","text":"Return the processing stage order of the pipeline. Returns: Type Description List[List[str]] a list of lists of step ids","title":"processing_stages"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.can_be_processed","text":"Check whether the step with the provided id is ready to be processed. Source code in kiara/pipeline/controller/__init__.py def can_be_processed ( self , step_id : str ) -> bool : \"\"\"Check whether the step with the provided id is ready to be processed.\"\"\" result = True step_inputs = self . get_step_inputs ( step_id = step_id ) for input_name in step_inputs . get_all_field_names (): value = step_inputs . get_value_obj ( input_name ) if not value . item_is_valid (): result = False break return result","title":"can_be_processed()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.can_be_skipped","text":"Check whether the processing of a step can be skipped. Source code in kiara/pipeline/controller/__init__.py def can_be_skipped ( self , step_id : str ) -> bool : \"\"\"Check whether the processing of a step can be skipped.\"\"\" result = True step = self . get_step ( step_id ) if step . required : result = self . can_be_processed ( step_id ) return result","title":"can_be_skipped()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.check_inputs_status","text":"Check the status of all stages/steps, and whether they are ready to be processed. The result dictionary uses stage numbers as keys, and a secondary dictionary as values which in turn uses the step_id as key, and a dict with details (keys: 'ready', 'invalid_inputs', 'required') as values. Source code in kiara/pipeline/controller/__init__.py def check_inputs_status ( self , ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Mapping [ str , typing . Any ]]]: \"\"\"Check the status of all stages/steps, and whether they are ready to be processed. The result dictionary uses stage numbers as keys, and a secondary dictionary as values which in turn uses the step_id as key, and a dict with details (keys: 'ready', 'invalid_inputs', 'required') as values. \"\"\" result : typing . Dict [ int , typing . Dict [ str , typing . Dict [ str , typing . Any ]]] = {} for idx , stage in enumerate ( self . processing_stages , start = 1 ): for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], } else : result . setdefault ( idx , {})[ step_id ] = { \"ready\" : False , \"invalid_inputs\" : [ f \" { step_id } . { ii } \" for ii in self . invalid_inputs ( step_id ) ], \"required\" : True , } else : if self . can_be_skipped ( step_id ): result . setdefault ( idx , {})[ step_id ] = { \"ready\" : True , \"required\" : True , \"invalid_inputs\" : [], } return result","title":"check_inputs_status()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.get_current_pipeline_state","text":"Return a description of the current pipeline state. This methods creates a new PipelineState object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: Type Description PipelineState an object outlining the current pipeline state Source code in kiara/pipeline/controller/__init__.py def get_current_pipeline_state ( self ) -> \"PipelineState\" : \"\"\"Return a description of the current pipeline state. This methods creates a new [PipelineState][kiara.pipeline.pipeline.PipelineState] object when called, containing the pipeline structure as well as metadata about pipeline as well as step inputs and outputs. Returns: an object outlining the current pipeline state \"\"\" return self . pipeline . get_current_state ()","title":"get_current_pipeline_state()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.get_job_details","text":"Returns job details for a job id, or in case a step_id was provided, the last execution of this step. Source code in kiara/pipeline/controller/__init__.py def get_job_details ( self , step_or_job_id : str ) -> typing . Optional [ Job ]: \"\"\"Returns job details for a job id, or in case a step_id was provided, the last execution of this step.\"\"\" if self . _job_ids . get ( step_or_job_id , None ) is None : return self . _processor . get_job_details ( step_or_job_id ) else : return self . _processor . get_job_details ( self . _job_ids [ step_or_job_id ])","title":"get_job_details()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.get_step","text":"Return the step object for the provided id. Parameters: Name Type Description Default step_id str the step id required Returns: Type Description PipelineStep the step object Source code in kiara/pipeline/controller/__init__.py def get_step ( self , step_id : str ) -> PipelineStep : \"\"\"Return the step object for the provided id. Arguments: step_id: the step id Returns: the step object \"\"\" return self . pipeline . get_step ( step_id )","title":"get_step()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.get_step_input","text":"Get the (current) input value for a specified step and input field name. Source code in kiara/pipeline/controller/__init__.py def get_step_input ( self , step_id : str , input_name : str ) -> Value : \"\"\"Get the (current) input value for a specified step and input field name.\"\"\" item = self . get_step_inputs ( step_id ) . get_value_obj ( input_name ) assert item is not None return item","title":"get_step_input()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.get_step_inputs","text":"Return the inputs object for the pipeline. Source code in kiara/pipeline/controller/__init__.py def get_step_inputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the inputs object for the pipeline.\"\"\" return self . pipeline . get_step_inputs ( step_id )","title":"get_step_inputs()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.get_step_output","text":"Get the (current) output value for a specified step and output field name. Source code in kiara/pipeline/controller/__init__.py def get_step_output ( self , step_id : str , output_name : str ) -> Value : \"\"\"Get the (current) output value for a specified step and output field name.\"\"\" item = self . get_step_outputs ( step_id ) . get_value_obj ( output_name ) assert item is not None return item","title":"get_step_output()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.get_step_outputs","text":"Return the outputs object for the pipeline. Source code in kiara/pipeline/controller/__init__.py def get_step_outputs ( self , step_id : str ) -> ValueSet : \"\"\"Return the outputs object for the pipeline.\"\"\" return self . pipeline . get_step_outputs ( step_id )","title":"get_step_outputs()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.pipeline_is_finished","text":"Return whether the pipeline has been processed successfully. A True result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: Type Description bool whether the pipeline was processed successfully ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def pipeline_is_finished ( self ) -> bool : \"\"\"Return whether the pipeline has been processed successfully. A ``True`` result means that every step of the pipeline has been processed successfully, and no pipeline input has changed since that happened. Returns: whether the pipeline was processed successfully (``True``) or not (``False``) \"\"\" return self . pipeline . outputs . items_are_valid ()","title":"pipeline_is_finished()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.pipeline_is_ready","text":"Return whether the pipeline is ready to be processed. A True result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: Type Description bool whether the pipeline can be processed as a whole ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def pipeline_is_ready ( self ) -> bool : \"\"\"Return whether the pipeline is ready to be processed. A ``True`` result means that all pipeline inputs are set with valid values, and therefore every step within the pipeline can be processed. Returns: whether the pipeline can be processed as a whole (``True``) or not (``False``) \"\"\" return self . pipeline . inputs . items_are_valid ()","title":"pipeline_is_ready()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.process_pipeline","text":"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/__init__.py def process_pipeline ( self ): \"\"\"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Returns: \"\"\" raise NotImplementedError ()","title":"process_pipeline()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.process_step","text":"Kick off processing for the step with the provided id. Parameters: Name Type Description Default step_id str the id of the step that should be started required Source code in kiara/pipeline/controller/__init__.py def process_step ( self , step_id : str , raise_exception : bool = False , wait : bool = False ) -> str : \"\"\"Kick off processing for the step with the provided id. Arguments: step_id: the id of the step that should be started \"\"\" step_inputs = self . get_step_inputs ( step_id ) # if the inputs are not valid, ignore this step if not step_inputs . items_are_valid (): status = step_inputs . check_invalid () assert status is not None raise Exception ( f \"Can't execute step ' { step_id } ', invalid inputs: { ', ' . join ( status . keys ()) } \" ) # get the output 'holder' objects, which we'll need to pass to the module step_outputs = self . get_step_outputs ( step_id ) # get the module object that holds the code that will do the processing step = self . get_step ( step_id ) job_id = self . _processor . start ( pipeline_id = self . pipeline . id , pipeline_name = self . pipeline . title , step_id = step_id , module = step . module , inputs = step_inputs , outputs = step_outputs , ) self . _job_ids [ step_id ] = job_id if wait : self . wait_for_jobs ( job_id , sync_outputs = True ) return job_id","title":"process_step()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.set_pipeline","text":"Set the pipeline object for this controller. Only one pipeline can be set, once. Parameters: Name Type Description Default pipeline Pipeline the pipeline object required Source code in kiara/pipeline/controller/__init__.py def set_pipeline ( self , pipeline : \"Pipeline\" ): \"\"\"Set the pipeline object for this controller. Only one pipeline can be set, once. Arguments: pipeline: the pipeline object \"\"\" if self . _pipeline is not None : raise Exception ( \"Pipeline already set.\" ) self . _pipeline = pipeline","title":"set_pipeline()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.set_pipeline_inputs","text":"Set one, several or all inputs for this pipeline. Parameters: Name Type Description Default **inputs Any the input values to set {} Source code in kiara/pipeline/controller/__init__.py def set_pipeline_inputs ( self , ** inputs : typing . Any ): \"\"\"Set one, several or all inputs for this pipeline. Arguments: **inputs: the input values to set \"\"\" _inputs = self . _pipeline_input_hook ( ** inputs ) self . pipeline_inputs . set_values ( ** _inputs )","title":"set_pipeline_inputs()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.step_is_finished","text":"Return whether the step with the provided id has been processed successfully. A True result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Parameters: Name Type Description Default step_id str the id of the step to check required Returns: Type Description bool whether the step result is valid ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def step_is_finished ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id has been processed successfully. A ``True`` result means that all output fields are currently set with valid values, and the inputs haven't changed since the last time processing was done. Arguments: step_id: the id of the step to check Returns: whether the step result is valid (``True``) or not (``False``) \"\"\" return self . get_step_outputs ( step_id ) . items_are_valid ()","title":"step_is_finished()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.PipelineController.step_is_ready","text":"Return whether the step with the provided id is ready to be processed. A True result means that all input fields are currently set with valid values. Parameters: Name Type Description Default step_id str the id of the step to check required Returns: Type Description bool whether the step is ready ( True ) or not ( False ) Source code in kiara/pipeline/controller/__init__.py def step_is_ready ( self , step_id : str ) -> bool : \"\"\"Return whether the step with the provided id is ready to be processed. A ``True`` result means that all input fields are currently set with valid values. Arguments: step_id: the id of the step to check Returns: whether the step is ready (``True``) or not (``False``) \"\"\" return self . get_step_inputs ( step_id ) . items_are_valid ()","title":"step_is_ready()"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.batch","text":"","title":"batch"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.batch.BatchController","text":"A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline Optional[Pipeline] the pipeline to control None auto_process bool whether to automatically start processing the pipeline as soon as the input set is valid True Source code in kiara/pipeline/controller/batch.py class BatchController ( PipelineController ): \"\"\"A [PipelineController][kiara.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively. This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Arguments: pipeline: the pipeline to control auto_process: whether to automatically start processing the pipeline as soon as the input set is valid \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , auto_process : bool = True , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _auto_process : bool = auto_process self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor , kiara = kiara ) @property def auto_process ( self ) -> bool : return self . _auto_process @auto_process . setter def auto_process ( self , auto_process : bool ): self . _auto_process = auto_process def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_exc () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) # for j_id in job_ids: # job = self._processor.get_job_details(j_id) # assert job is not None # if job.error: # print(job.error) finally : self . _is_running = False def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return if self . _auto_process : self . process_pipeline () def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False","title":"BatchController"},{"location":"reference/kiara/pipeline/controller/__init__/#kiara.pipeline.controller.batch.BatchControllerManual","text":"A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline Optional[Pipeline] the pipeline to control None auto_process whether to automatically start processing the pipeline as soon as the input set is valid required Source code in kiara/pipeline/controller/batch.py class BatchControllerManual ( PipelineController ): \"\"\"A [PipelineController][kiara.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively. This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Arguments: pipeline: the pipeline to control auto_process: whether to automatically start processing the pipeline as soon as the input set is valid \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _finished_until : typing . Optional [ int ] = None self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor , kiara = kiara ) @property def last_finished_stage ( self ) -> int : if self . _finished_until is None : return 0 else : return self . _finished_until def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return def pipeline_inputs_changed ( self , event : \"PipelineInputEvent\" ): self . _finished_until = None # print(\"============================\") # min_stage_to_clear = sys.maxsize # for inp in event.updated_pipeline_inputs: # stage = self.pipeline.get_stage_for_pipeline_input(inp) # if stage < min_stage_to_clear: # min_stage_to_clear = stage # # for stage, steps in self.pipeline.get_steps_by_stage().items(): # if stage < min_stage_to_clear: # continue # for step_id, step in steps.items(): # step_inputs = self.get_step_inputs(step_id) # empty = { k: None for k in step_inputs.keys() } # step_inputs.set_values(**empty) # step_outputs = self.get_step_outputs(step_id) # empty = { k: None for k in step_outputs.keys() } # step_outputs.set_values(**empty) def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False def process_stage ( self , stage_nr : int ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Union [ None , str , Exception ]]]: if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True result : typing . Dict [ int , typing . Dict [ str , typing . Union [ None , str , Exception ]] ] = {} try : for idx , stage in enumerate ( self . processing_stages ): current_stage = idx + 1 if current_stage > stage_nr : break if self . _finished_until is not None and idx <= self . _finished_until : continue job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( current_stage , {})[ step_id ] = None continue else : exc = Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) result . setdefault ( current_stage , {})[ step_id ] = exc try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) result . setdefault ( current_stage , {})[ step_id ] = job_id except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) result . setdefault ( current_stage , {})[ step_id ] = e self . _processor . wait_for ( * job_ids ) # for job_id in job_ids: # job = self.get_job_details(job_id) # dbg(job.dict()) self . _finished_until = idx finally : self . _is_running = False print ( \"BATCH FINISHED\" ) return result","title":"BatchControllerManual"},{"location":"reference/kiara/pipeline/controller/batch/","text":"BatchController ( PipelineController ) \u00b6 A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline Optional[Pipeline] the pipeline to control None auto_process bool whether to automatically start processing the pipeline as soon as the input set is valid True Source code in kiara/pipeline/controller/batch.py class BatchController ( PipelineController ): \"\"\"A [PipelineController][kiara.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively. This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Arguments: pipeline: the pipeline to control auto_process: whether to automatically start processing the pipeline as soon as the input set is valid \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , auto_process : bool = True , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _auto_process : bool = auto_process self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor , kiara = kiara ) @property def auto_process ( self ) -> bool : return self . _auto_process @auto_process . setter def auto_process ( self , auto_process : bool ): self . _auto_process = auto_process def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_exc () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) # for j_id in job_ids: # job = self._processor.get_job_details(j_id) # assert job is not None # if job.error: # print(job.error) finally : self . _is_running = False def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return if self . _auto_process : self . process_pipeline () def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_exc () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) # for j_id in job_ids: # job = self._processor.get_job_details(j_id) # assert job is not None # if job.error: # print(job.error) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return if self . _auto_process : self . process_pipeline () BatchControllerManual ( PipelineController ) \u00b6 A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline Optional[Pipeline] the pipeline to control None auto_process whether to automatically start processing the pipeline as soon as the input set is valid required Source code in kiara/pipeline/controller/batch.py class BatchControllerManual ( PipelineController ): \"\"\"A [PipelineController][kiara.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively. This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Arguments: pipeline: the pipeline to control auto_process: whether to automatically start processing the pipeline as soon as the input set is valid \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _finished_until : typing . Optional [ int ] = None self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor , kiara = kiara ) @property def last_finished_stage ( self ) -> int : if self . _finished_until is None : return 0 else : return self . _finished_until def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return def pipeline_inputs_changed ( self , event : \"PipelineInputEvent\" ): self . _finished_until = None # print(\"============================\") # min_stage_to_clear = sys.maxsize # for inp in event.updated_pipeline_inputs: # stage = self.pipeline.get_stage_for_pipeline_input(inp) # if stage < min_stage_to_clear: # min_stage_to_clear = stage # # for stage, steps in self.pipeline.get_steps_by_stage().items(): # if stage < min_stage_to_clear: # continue # for step_id, step in steps.items(): # step_inputs = self.get_step_inputs(step_id) # empty = { k: None for k in step_inputs.keys() } # step_inputs.set_values(**empty) # step_outputs = self.get_step_outputs(step_id) # empty = { k: None for k in step_outputs.keys() } # step_outputs.set_values(**empty) def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False def process_stage ( self , stage_nr : int ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Union [ None , str , Exception ]]]: if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True result : typing . Dict [ int , typing . Dict [ str , typing . Union [ None , str , Exception ]] ] = {} try : for idx , stage in enumerate ( self . processing_stages ): current_stage = idx + 1 if current_stage > stage_nr : break if self . _finished_until is not None and idx <= self . _finished_until : continue job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( current_stage , {})[ step_id ] = None continue else : exc = Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) result . setdefault ( current_stage , {})[ step_id ] = exc try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) result . setdefault ( current_stage , {})[ step_id ] = job_id except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) result . setdefault ( current_stage , {})[ step_id ] = e self . _processor . wait_for ( * job_ids ) # for job_id in job_ids: # job = self.get_job_details(job_id) # dbg(job.dict()) self . _finished_until = idx finally : self . _is_running = False print ( \"BATCH FINISHED\" ) return result pipeline_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/pipeline/controller/batch.py def pipeline_inputs_changed ( self , event : \"PipelineInputEvent\" ): self . _finished_until = None # print(\"============================\") # min_stage_to_clear = sys.maxsize # for inp in event.updated_pipeline_inputs: # stage = self.pipeline.get_stage_for_pipeline_input(inp) # if stage < min_stage_to_clear: # min_stage_to_clear = stage # # for stage, steps in self.pipeline.get_steps_by_stage().items(): # if stage < min_stage_to_clear: # continue # for step_id, step in steps.items(): # step_inputs = self.get_step_inputs(step_id) # empty = { k: None for k in step_inputs.keys() } # step_inputs.set_values(**empty) # step_outputs = self.get_step_outputs(step_id) # empty = { k: None for k in step_outputs.keys() } # step_outputs.set_values(**empty) pipeline_outputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False process_pipeline ( self ) \u00b6 Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False step_inputs_changed ( self , event ) \u00b6 Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return","title":"batch"},{"location":"reference/kiara/pipeline/controller/batch/#kiara.pipeline.controller.batch.BatchController","text":"A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline Optional[Pipeline] the pipeline to control None auto_process bool whether to automatically start processing the pipeline as soon as the input set is valid True Source code in kiara/pipeline/controller/batch.py class BatchController ( PipelineController ): \"\"\"A [PipelineController][kiara.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively. This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Arguments: pipeline: the pipeline to control auto_process: whether to automatically start processing the pipeline as soon as the input set is valid \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , auto_process : bool = True , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _auto_process : bool = auto_process self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor , kiara = kiara ) @property def auto_process ( self ) -> bool : return self . _auto_process @auto_process . setter def auto_process ( self , auto_process : bool ): self . _auto_process = auto_process def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_exc () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) # for j_id in job_ids: # job = self._processor.get_job_details(j_id) # assert job is not None # if job.error: # print(job.error) finally : self . _is_running = False def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return if self . _auto_process : self . process_pipeline () def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False","title":"BatchController"},{"location":"reference/kiara/pipeline/controller/batch/#kiara.pipeline.controller.batch.BatchController.pipeline_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False","title":"pipeline_outputs_changed()"},{"location":"reference/kiara/pipeline/controller/batch/#kiara.pipeline.controller.batch.BatchController.process_pipeline","text":"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_exc () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . id } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) # for j_id in job_ids: # job = self._processor.get_job_details(j_id) # assert job is not None # if job.error: # print(job.error) finally : self . _is_running = False","title":"process_pipeline()"},{"location":"reference/kiara/pipeline/controller/batch/#kiara.pipeline.controller.batch.BatchController.step_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return if self . _auto_process : self . process_pipeline ()","title":"step_inputs_changed()"},{"location":"reference/kiara/pipeline/controller/batch/#kiara.pipeline.controller.batch.BatchControllerManual","text":"A PipelineController that executes all pipeline steps non-interactively. This is the default implementation of a PipelineController , and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Parameters: Name Type Description Default pipeline Optional[Pipeline] the pipeline to control None auto_process whether to automatically start processing the pipeline as soon as the input set is valid required Source code in kiara/pipeline/controller/batch.py class BatchControllerManual ( PipelineController ): \"\"\"A [PipelineController][kiara.pipeline.controller.PipelineController] that executes all pipeline steps non-interactively. This is the default implementation of a ``PipelineController``, and probably the most simple implementation of one. It waits until all inputs are set, after which it executes all pipeline steps in the required order. Arguments: pipeline: the pipeline to control auto_process: whether to automatically start processing the pipeline as soon as the input set is valid \"\"\" def __init__ ( self , pipeline : typing . Optional [ \"Pipeline\" ] = None , processor : typing . Optional [ \"ModuleProcessor\" ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): self . _finished_until : typing . Optional [ int ] = None self . _is_running : bool = False super () . __init__ ( pipeline = pipeline , processor = processor , kiara = kiara ) @property def last_finished_stage ( self ) -> int : if self . _finished_until is None : return 0 else : return self . _finished_until def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return def pipeline_inputs_changed ( self , event : \"PipelineInputEvent\" ): self . _finished_until = None # print(\"============================\") # min_stage_to_clear = sys.maxsize # for inp in event.updated_pipeline_inputs: # stage = self.pipeline.get_stage_for_pipeline_input(inp) # if stage < min_stage_to_clear: # min_stage_to_clear = stage # # for stage, steps in self.pipeline.get_steps_by_stage().items(): # if stage < min_stage_to_clear: # continue # for step_id, step in steps.items(): # step_inputs = self.get_step_inputs(step_id) # empty = { k: None for k in step_inputs.keys() } # step_inputs.set_values(**empty) # step_outputs = self.get_step_outputs(step_id) # empty = { k: None for k in step_outputs.keys() } # step_outputs.set_values(**empty) def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False def process_stage ( self , stage_nr : int ) -> typing . Mapping [ int , typing . Mapping [ str , typing . Union [ None , str , Exception ]]]: if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True result : typing . Dict [ int , typing . Dict [ str , typing . Union [ None , str , Exception ]] ] = {} try : for idx , stage in enumerate ( self . processing_stages ): current_stage = idx + 1 if current_stage > stage_nr : break if self . _finished_until is not None and idx <= self . _finished_until : continue job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): result . setdefault ( current_stage , {})[ step_id ] = None continue else : exc = Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) result . setdefault ( current_stage , {})[ step_id ] = exc try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) result . setdefault ( current_stage , {})[ step_id ] = job_id except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) result . setdefault ( current_stage , {})[ step_id ] = e self . _processor . wait_for ( * job_ids ) # for job_id in job_ids: # job = self.get_job_details(job_id) # dbg(job.dict()) self . _finished_until = idx finally : self . _is_running = False print ( \"BATCH FINISHED\" ) return result","title":"BatchControllerManual"},{"location":"reference/kiara/pipeline/controller/batch/#kiara.pipeline.controller.batch.BatchControllerManual.pipeline_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline inputs have changed. Note Whenever pipeline inputs change, the connected step inputs also change and an (extra) event will be fired for those. Which means you can choose to only implement the step_inputs_changed method if you want to. This behaviour might change in the future. Parameters: Name Type Description Default event PipelineInputEvent the pipeline input event required Source code in kiara/pipeline/controller/batch.py def pipeline_inputs_changed ( self , event : \"PipelineInputEvent\" ): self . _finished_until = None # print(\"============================\") # min_stage_to_clear = sys.maxsize # for inp in event.updated_pipeline_inputs: # stage = self.pipeline.get_stage_for_pipeline_input(inp) # if stage < min_stage_to_clear: # min_stage_to_clear = stage # # for stage, steps in self.pipeline.get_steps_by_stage().items(): # if stage < min_stage_to_clear: # continue # for step_id, step in steps.items(): # step_inputs = self.get_step_inputs(step_id) # empty = { k: None for k in step_inputs.keys() } # step_inputs.set_values(**empty) # step_outputs = self.get_step_outputs(step_id) # empty = { k: None for k in step_outputs.keys() } # step_outputs.set_values(**empty)","title":"pipeline_inputs_changed()"},{"location":"reference/kiara/pipeline/controller/batch/#kiara.pipeline.controller.batch.BatchControllerManual.pipeline_outputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several pipeline outputs have changed. Parameters: Name Type Description Default event PipelineOutputEvent the pipeline output event required Source code in kiara/pipeline/controller/batch.py def pipeline_outputs_changed ( self , event : \"PipelineOutputEvent\" ): if self . pipeline_is_finished (): # TODO: check if something is running self . _is_running = False","title":"pipeline_outputs_changed()"},{"location":"reference/kiara/pipeline/controller/batch/#kiara.pipeline.controller.batch.BatchControllerManual.process_pipeline","text":"Execute the connected pipeline end-to-end. Controllers can elect to overwrite this method, but this is optional. Source code in kiara/pipeline/controller/batch.py def process_pipeline ( self ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) raise Exception ( \"Pipeline already running.\" ) self . _is_running = True try : for stage in self . processing_stages : job_ids = [] for step_id in stage : if not self . can_be_processed ( step_id ): if self . can_be_skipped ( step_id ): continue else : raise Exception ( f \"Required pipeline step ' { step_id } ' can't be processed, inputs not ready yet: { ', ' . join ( self . invalid_inputs ( step_id )) } \" ) try : job_id = self . process_step ( step_id ) job_ids . append ( job_id ) except Exception as e : # TODO: cancel running jobs? if is_debug (): import traceback traceback . print_stack () log . error ( f \"Processing of step ' { step_id } ' from pipeline ' { self . pipeline . title } ' failed: { e } \" ) return False self . _processor . wait_for ( * job_ids ) finally : self . _is_running = False","title":"process_pipeline()"},{"location":"reference/kiara/pipeline/controller/batch/#kiara.pipeline.controller.batch.BatchControllerManual.step_inputs_changed","text":"Method to override if the implementing controller needs to react to events where one or several step inputs have changed. Parameters: Name Type Description Default event StepInputEvent the step input event required Source code in kiara/pipeline/controller/batch.py def step_inputs_changed ( self , event : \"StepInputEvent\" ): if self . _is_running : log . debug ( \"Pipeline running, doing nothing.\" ) return if not self . pipeline_is_ready (): log . debug ( f \"Pipeline not ready after input event: { event } \" ) return","title":"step_inputs_changed()"},{"location":"reference/kiara/processing/__init__/","text":"Job ( ProcessingInfo ) pydantic-model \u00b6 Source code in kiara/processing/__init__.py class Job ( ProcessingInfo ): @classmethod def create_event_msg ( cls , job : \"Job\" ): topic = job . status . value [ 2 : - 2 ] payload = f \" { topic } { job . json () } \" return payload class Config : use_enum_values = True _exception : typing . Optional [ Exception ] = PrivateAttr ( default = None ) pipeline_id : str = Field ( description = \"The id of the pipeline this jobs runs for.\" ) pipeline_name : str = Field ( description = \"The name/type of the pipeline.\" ) step_id : str = Field ( description = \"The id of the step within the pipeline.\" ) inputs : PipelineValuesInfo = Field ( description = \"The input values.\" ) outputs : PipelineValuesInfo = Field ( description = \"The output values.\" ) status : JobStatus = Field ( description = \"The current status of the job.\" , default = JobStatus . CREATED , ) error : typing . Optional [ str ] = Field ( description = \"Potential error message.\" ) @property def exception ( self ) -> typing . Optional [ Exception ]: return self . _exception @property def runtime ( self ) -> typing . Optional [ float ]: if self . started is None or self . finished is None : return None runtime = self . finished - self . started return runtime . total_seconds () @validator ( \"status\" ) def _validate_status ( cls , v ): if isinstance ( v , int ): if v < 0 or v > 100 : raise ValueError ( \"Status must be a status string, or an integer between 0 and 100.\" ) return v error : str pydantic-field \u00b6 Potential error message. inputs : PipelineValuesInfo pydantic-field required \u00b6 The input values. outputs : PipelineValuesInfo pydantic-field required \u00b6 The output values. pipeline_id : str pydantic-field required \u00b6 The id of the pipeline this jobs runs for. pipeline_name : str pydantic-field required \u00b6 The name/type of the pipeline. status : JobStatus pydantic-field \u00b6 The current status of the job. step_id : str pydantic-field required \u00b6 The id of the step within the pipeline. JobLog ( BaseModel ) pydantic-model \u00b6 Source code in kiara/processing/__init__.py class JobLog ( BaseModel ): log : typing . Dict [ int , LogMessage ] = Field ( description = \"The logs for this job.\" , default_factory = dict ) percent_finished : int = Field ( description = \"Describes how much of the job is finished. A negative number means the module does not support progress tracking.\" , default =- 1 , ) def add_log ( self , msg : str , log_level : int = logging . DEBUG ): _msg = LogMessage ( msg = msg , log_level = log_level ) self . log [ len ( self . log )] = _msg log : Dict [ int , kiara . processing . LogMessage ] pydantic-field \u00b6 The logs for this job. percent_finished : int pydantic-field \u00b6 Describes how much of the job is finished. A negative number means the module does not support progress tracking. JobStatus ( Enum ) \u00b6 An enumeration. Source code in kiara/processing/__init__.py class JobStatus ( Enum ): CREATED = \"__job_created__\" STARTED = \"__job_started__\" SUCCESS = \"__job_success__\" FAILED = \"__job_failed__\" LogMessage ( BaseModel ) pydantic-model \u00b6 Source code in kiara/processing/__init__.py class LogMessage ( BaseModel ): timestamp : datetime = Field ( description = \"The time the message was logged.\" , default_factory = datetime . now ) log_level : int = Field ( description = \"The log level.\" ) msg : str = Field ( description = \"The log message\" ) log_level : int pydantic-field required \u00b6 The log level. msg : str pydantic-field required \u00b6 The log message timestamp : datetime pydantic-field \u00b6 The time the message was logged. ProcessingInfo ( MetadataModel ) pydantic-model \u00b6 Source code in kiara/processing/__init__.py class ProcessingInfo ( MetadataModel ): id : str = Field ( description = \"The id of the job.\" ) module_type : str = Field ( description = \"The module type name.\" ) module_config : typing . Dict [ str , typing . Any ] = Field ( description = \"The module configuration.\" ) module_doc : DocumentationMetadataModel = Field ( description = \"Documentation for the module that runs the job.\" ) job_log : JobLog = Field ( description = \"Details about the job progress.\" , default_factory = JobLog ) submitted : datetime = Field ( description = \"When the job was submitted.\" , default_factory = datetime . now ) started : typing . Optional [ datetime ] = Field ( description = \"When the job was started.\" , default = None ) finished : typing . Optional [ datetime ] = Field ( description = \"When the job was finished.\" , default = None ) finished : datetime pydantic-field \u00b6 When the job was finished. id : str pydantic-field required \u00b6 The id of the job. job_log : JobLog pydantic-field \u00b6 Details about the job progress. module_config : Dict [ str , Any ] pydantic-field required \u00b6 The module configuration. module_doc : DocumentationMetadataModel pydantic-field required \u00b6 Documentation for the module that runs the job. module_type : str pydantic-field required \u00b6 The module type name. started : datetime pydantic-field \u00b6 When the job was started. submitted : datetime pydantic-field \u00b6 When the job was submitted. parallel \u00b6 ThreadPoolProcessorConfig ( ProcessorConfig ) pydantic-model \u00b6 Source code in kiara/processing/parallel.py class ThreadPoolProcessorConfig ( ProcessorConfig ): max_workers : typing . Optional [ int ] = Field ( description = \"The max mount of workers for the thread pool.\" , default = None ) max_workers : int pydantic-field \u00b6 The max mount of workers for the thread pool. processor \u00b6 ModuleProcessor ( ABC ) \u00b6 Source code in kiara/processing/processor.py class ModuleProcessor ( abc . ABC ): @classmethod def from_config ( cls , config : typing . Union [ None , typing . Mapping [ str , typing . Any ], ProcessorConfig ], kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleProcessor\" : from kiara.processing.parallel import ThreadPoolProcessorConfig from kiara.processing.synchronous import SynchronousProcessorConfig if not config : config = SynchronousProcessorConfig ( module_processor_type = \"synchronous\" ) if isinstance ( config , typing . Mapping ): processor_type = config . get ( \"module_processor_type\" , None ) if not processor_type : raise ValueError ( \"No 'module_processor_type' provided: {config} \" ) if processor_type == \"synchronous\" : config = SynchronousProcessorConfig ( ** config ) elif processor_type == \"multi-threaded\" : config = ThreadPoolProcessorConfig () else : raise ValueError ( f \"Invalid processor type: { processor_type } \" ) if isinstance ( config , SynchronousProcessorConfig ): from kiara.processing.synchronous import SynchronousProcessor proc : ModuleProcessor = SynchronousProcessor ( kiara = kiara , ** config . dict ( exclude = { \"module_processor_type\" }) ) elif isinstance ( config , ThreadPoolProcessorConfig ): from kiara.processing.parallel import ThreadPoolProcessor proc = ThreadPoolProcessor ( kiara = kiara , ** config . dict ( exclude = { \"module_processor_type\" }) ) else : raise TypeError ( f \"Invalid processor config class: { type ( config ) } \" ) return proc def __init__ ( self , zmq_context : typing . Optional [ Context ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : Kiara = kiara if zmq_context is None : zmq_context = Context . instance () self . _zmq_context : Context = zmq_context self . _socket : Socket = self . _zmq_context . socket ( zmq . PUB ) self . _socket . connect ( \"inproc://kiara_in\" ) self . _active_jobs : typing . Dict [ str , Job ] = {} self . _finished_jobs : typing . Dict [ str , Job ] = {} # TODO: clean up those? self . _inputs : typing . Dict [ str , StepInputs ] = {} self . _outputs : typing . Dict [ str , StepOutputs ] = {} def get_job_details ( self , job_id : str ) -> typing . Optional [ Job ]: if job_id in self . _active_jobs . keys (): return self . _active_jobs [ job_id ] elif job_id in self . _finished_jobs . keys (): return self . _finished_jobs [ job_id ] else : return None def start ( self , pipeline_id : str , pipeline_name : str , step_id : str , module : \"KiaraModule\" , inputs : ValueSet , outputs : ValueSet , ) -> str : job_id = str ( uuid . uuid4 ()) # TODO: make snapshot of current state of data? full_inputs = module . create_full_inputs ( ** inputs ) wrapped_inputs = StepInputs ( inputs = full_inputs , kiara = self . _kiara ) wrapped_outputs = StepOutputs ( outputs = outputs , kiara = self . _kiara ) self . _inputs [ job_id ] = wrapped_inputs self . _outputs [ job_id ] = wrapped_outputs job = Job ( id = job_id , pipeline_id = pipeline_id , pipeline_name = pipeline_name , step_id = step_id , module_type = module . _module_type_id , # type: ignore module_config = module . config . dict (), module_doc = module . get_type_metadata () . documentation , inputs = PipelineValuesInfo . from_value_set ( inputs ), outputs = PipelineValuesInfo . from_value_set ( outputs ), ) job . job_log . add_log ( \"job created\" ) self . _active_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) try : self . process ( job_id = job_id , module = module , inputs = wrapped_inputs , outputs = wrapped_outputs , job_log = job . job_log , ) return job_id except Exception as e : job . error = str ( e ) if is_debug (): try : import traceback traceback . print_exc () except Exception : pass if isinstance ( e , KiaraProcessingException ): e . _module = module e . _inputs = inputs job . _exception = e raise e else : kpe = KiaraProcessingException ( e , module = module , inputs = inputs ) job . _exception = kpe raise kpe def job_status_updated ( self , job_id : str , status : typing . Union [ JobStatus , str , Exception ] ): job = self . _active_jobs . get ( job_id , None ) if job is None : raise Exception ( f \"Can't retrieve active job with id ' { job_id } ', no such job registered.\" ) if status == JobStatus . SUCCESS : job . job_log . add_log ( \"job finished successfully\" ) job . status = JobStatus . SUCCESS job = self . _active_jobs . pop ( job_id ) job . finished = datetime . now () self . _finished_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) elif status == JobStatus . FAILED or isinstance ( status , ( str , Exception )): job . job_log . add_log ( \"job failed\" ) job . status = JobStatus . FAILED job . finished = datetime . now () if isinstance ( status , str ): job . error = status elif isinstance ( status , Exception ): job . error = str ( status ) job . _exception = status job = self . _active_jobs . pop ( job_id ) self . _finished_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) elif status == JobStatus . STARTED : job . job_log . add_log ( \"job started\" ) job . status = JobStatus . STARTED job . started = datetime . now () self . _socket . send_string ( Job . create_event_msg ( job )) else : raise ValueError ( f \"Invalid value for status: { status } \" ) def sync_outputs ( self , * job_ids : str ): for j_id in job_ids : job_details = self . get_job_details ( j_id ) if not job_details : raise Exception ( f \"Can't sync outputs, no job with id: { j_id } \" ) job_inputs = self . _inputs [ j_id ] proc_info = ProcessingInfo ( ** job_details . dict ()) input_infos : typing . Dict [ str , ValueInfo ] = { k : v . get_info () for k , v in job_inputs . items () } # for field_name, value in d.items(): value_lineage = ValueLineage . create ( module_type = job_details . module_type , module_config = job_details . module_config , module_doc = job_details . module_doc , # output_name=field_name, inputs = input_infos , ) d = self . _outputs [ j_id ] d . sync ( proccessing_info = proc_info , lineage = value_lineage ) def wait_for ( self , * job_ids : str , sync_outputs : bool = True ): \"\"\"Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state.\"\"\" self . _wait_for ( * job_ids ) for job_id in job_ids : job = self . get_job_details ( job_id ) if job is None : raise Exception ( f \"Can't find job with id: { job_id } \" ) if job . status == JobStatus . SUCCESS : job . job_log . percent_finished = 100 if sync_outputs : self . sync_outputs ( * job_ids ) @abc . abstractmethod def process ( self , job_id : str , module : \"KiaraModule\" , inputs : ValueSet , outputs : ValueSet , job_log : JobLog , ) -> str : pass @abc . abstractmethod def _wait_for ( self , * job_ids : str ): pass wait_for ( self , * job_ids , * , sync_outputs = True ) \u00b6 Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state. Source code in kiara/processing/processor.py def wait_for ( self , * job_ids : str , sync_outputs : bool = True ): \"\"\"Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state.\"\"\" self . _wait_for ( * job_ids ) for job_id in job_ids : job = self . get_job_details ( job_id ) if job is None : raise Exception ( f \"Can't find job with id: { job_id } \" ) if job . status == JobStatus . SUCCESS : job . job_log . percent_finished = 100 if sync_outputs : self . sync_outputs ( * job_ids )","title":"processing"},{"location":"reference/kiara/processing/__init__/#kiara.processing.Job","text":"Source code in kiara/processing/__init__.py class Job ( ProcessingInfo ): @classmethod def create_event_msg ( cls , job : \"Job\" ): topic = job . status . value [ 2 : - 2 ] payload = f \" { topic } { job . json () } \" return payload class Config : use_enum_values = True _exception : typing . Optional [ Exception ] = PrivateAttr ( default = None ) pipeline_id : str = Field ( description = \"The id of the pipeline this jobs runs for.\" ) pipeline_name : str = Field ( description = \"The name/type of the pipeline.\" ) step_id : str = Field ( description = \"The id of the step within the pipeline.\" ) inputs : PipelineValuesInfo = Field ( description = \"The input values.\" ) outputs : PipelineValuesInfo = Field ( description = \"The output values.\" ) status : JobStatus = Field ( description = \"The current status of the job.\" , default = JobStatus . CREATED , ) error : typing . Optional [ str ] = Field ( description = \"Potential error message.\" ) @property def exception ( self ) -> typing . Optional [ Exception ]: return self . _exception @property def runtime ( self ) -> typing . Optional [ float ]: if self . started is None or self . finished is None : return None runtime = self . finished - self . started return runtime . total_seconds () @validator ( \"status\" ) def _validate_status ( cls , v ): if isinstance ( v , int ): if v < 0 or v > 100 : raise ValueError ( \"Status must be a status string, or an integer between 0 and 100.\" ) return v","title":"Job"},{"location":"reference/kiara/processing/__init__/#kiara.processing.Job.error","text":"Potential error message.","title":"error"},{"location":"reference/kiara/processing/__init__/#kiara.processing.Job.inputs","text":"The input values.","title":"inputs"},{"location":"reference/kiara/processing/__init__/#kiara.processing.Job.outputs","text":"The output values.","title":"outputs"},{"location":"reference/kiara/processing/__init__/#kiara.processing.Job.pipeline_id","text":"The id of the pipeline this jobs runs for.","title":"pipeline_id"},{"location":"reference/kiara/processing/__init__/#kiara.processing.Job.pipeline_name","text":"The name/type of the pipeline.","title":"pipeline_name"},{"location":"reference/kiara/processing/__init__/#kiara.processing.Job.status","text":"The current status of the job.","title":"status"},{"location":"reference/kiara/processing/__init__/#kiara.processing.Job.step_id","text":"The id of the step within the pipeline.","title":"step_id"},{"location":"reference/kiara/processing/__init__/#kiara.processing.JobLog","text":"Source code in kiara/processing/__init__.py class JobLog ( BaseModel ): log : typing . Dict [ int , LogMessage ] = Field ( description = \"The logs for this job.\" , default_factory = dict ) percent_finished : int = Field ( description = \"Describes how much of the job is finished. A negative number means the module does not support progress tracking.\" , default =- 1 , ) def add_log ( self , msg : str , log_level : int = logging . DEBUG ): _msg = LogMessage ( msg = msg , log_level = log_level ) self . log [ len ( self . log )] = _msg","title":"JobLog"},{"location":"reference/kiara/processing/__init__/#kiara.processing.JobLog.log","text":"The logs for this job.","title":"log"},{"location":"reference/kiara/processing/__init__/#kiara.processing.JobLog.percent_finished","text":"Describes how much of the job is finished. A negative number means the module does not support progress tracking.","title":"percent_finished"},{"location":"reference/kiara/processing/__init__/#kiara.processing.JobStatus","text":"An enumeration. Source code in kiara/processing/__init__.py class JobStatus ( Enum ): CREATED = \"__job_created__\" STARTED = \"__job_started__\" SUCCESS = \"__job_success__\" FAILED = \"__job_failed__\"","title":"JobStatus"},{"location":"reference/kiara/processing/__init__/#kiara.processing.LogMessage","text":"Source code in kiara/processing/__init__.py class LogMessage ( BaseModel ): timestamp : datetime = Field ( description = \"The time the message was logged.\" , default_factory = datetime . now ) log_level : int = Field ( description = \"The log level.\" ) msg : str = Field ( description = \"The log message\" )","title":"LogMessage"},{"location":"reference/kiara/processing/__init__/#kiara.processing.LogMessage.log_level","text":"The log level.","title":"log_level"},{"location":"reference/kiara/processing/__init__/#kiara.processing.LogMessage.msg","text":"The log message","title":"msg"},{"location":"reference/kiara/processing/__init__/#kiara.processing.LogMessage.timestamp","text":"The time the message was logged.","title":"timestamp"},{"location":"reference/kiara/processing/__init__/#kiara.processing.ProcessingInfo","text":"Source code in kiara/processing/__init__.py class ProcessingInfo ( MetadataModel ): id : str = Field ( description = \"The id of the job.\" ) module_type : str = Field ( description = \"The module type name.\" ) module_config : typing . Dict [ str , typing . Any ] = Field ( description = \"The module configuration.\" ) module_doc : DocumentationMetadataModel = Field ( description = \"Documentation for the module that runs the job.\" ) job_log : JobLog = Field ( description = \"Details about the job progress.\" , default_factory = JobLog ) submitted : datetime = Field ( description = \"When the job was submitted.\" , default_factory = datetime . now ) started : typing . Optional [ datetime ] = Field ( description = \"When the job was started.\" , default = None ) finished : typing . Optional [ datetime ] = Field ( description = \"When the job was finished.\" , default = None )","title":"ProcessingInfo"},{"location":"reference/kiara/processing/__init__/#kiara.processing.ProcessingInfo.finished","text":"When the job was finished.","title":"finished"},{"location":"reference/kiara/processing/__init__/#kiara.processing.ProcessingInfo.id","text":"The id of the job.","title":"id"},{"location":"reference/kiara/processing/__init__/#kiara.processing.ProcessingInfo.job_log","text":"Details about the job progress.","title":"job_log"},{"location":"reference/kiara/processing/__init__/#kiara.processing.ProcessingInfo.module_config","text":"The module configuration.","title":"module_config"},{"location":"reference/kiara/processing/__init__/#kiara.processing.ProcessingInfo.module_doc","text":"Documentation for the module that runs the job.","title":"module_doc"},{"location":"reference/kiara/processing/__init__/#kiara.processing.ProcessingInfo.module_type","text":"The module type name.","title":"module_type"},{"location":"reference/kiara/processing/__init__/#kiara.processing.ProcessingInfo.started","text":"When the job was started.","title":"started"},{"location":"reference/kiara/processing/__init__/#kiara.processing.ProcessingInfo.submitted","text":"When the job was submitted.","title":"submitted"},{"location":"reference/kiara/processing/__init__/#kiara.processing.parallel","text":"","title":"parallel"},{"location":"reference/kiara/processing/__init__/#kiara.processing.parallel.ThreadPoolProcessorConfig","text":"Source code in kiara/processing/parallel.py class ThreadPoolProcessorConfig ( ProcessorConfig ): max_workers : typing . Optional [ int ] = Field ( description = \"The max mount of workers for the thread pool.\" , default = None )","title":"ThreadPoolProcessorConfig"},{"location":"reference/kiara/processing/__init__/#kiara.processing.processor","text":"","title":"processor"},{"location":"reference/kiara/processing/__init__/#kiara.processing.processor.ModuleProcessor","text":"Source code in kiara/processing/processor.py class ModuleProcessor ( abc . ABC ): @classmethod def from_config ( cls , config : typing . Union [ None , typing . Mapping [ str , typing . Any ], ProcessorConfig ], kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleProcessor\" : from kiara.processing.parallel import ThreadPoolProcessorConfig from kiara.processing.synchronous import SynchronousProcessorConfig if not config : config = SynchronousProcessorConfig ( module_processor_type = \"synchronous\" ) if isinstance ( config , typing . Mapping ): processor_type = config . get ( \"module_processor_type\" , None ) if not processor_type : raise ValueError ( \"No 'module_processor_type' provided: {config} \" ) if processor_type == \"synchronous\" : config = SynchronousProcessorConfig ( ** config ) elif processor_type == \"multi-threaded\" : config = ThreadPoolProcessorConfig () else : raise ValueError ( f \"Invalid processor type: { processor_type } \" ) if isinstance ( config , SynchronousProcessorConfig ): from kiara.processing.synchronous import SynchronousProcessor proc : ModuleProcessor = SynchronousProcessor ( kiara = kiara , ** config . dict ( exclude = { \"module_processor_type\" }) ) elif isinstance ( config , ThreadPoolProcessorConfig ): from kiara.processing.parallel import ThreadPoolProcessor proc = ThreadPoolProcessor ( kiara = kiara , ** config . dict ( exclude = { \"module_processor_type\" }) ) else : raise TypeError ( f \"Invalid processor config class: { type ( config ) } \" ) return proc def __init__ ( self , zmq_context : typing . Optional [ Context ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : Kiara = kiara if zmq_context is None : zmq_context = Context . instance () self . _zmq_context : Context = zmq_context self . _socket : Socket = self . _zmq_context . socket ( zmq . PUB ) self . _socket . connect ( \"inproc://kiara_in\" ) self . _active_jobs : typing . Dict [ str , Job ] = {} self . _finished_jobs : typing . Dict [ str , Job ] = {} # TODO: clean up those? self . _inputs : typing . Dict [ str , StepInputs ] = {} self . _outputs : typing . Dict [ str , StepOutputs ] = {} def get_job_details ( self , job_id : str ) -> typing . Optional [ Job ]: if job_id in self . _active_jobs . keys (): return self . _active_jobs [ job_id ] elif job_id in self . _finished_jobs . keys (): return self . _finished_jobs [ job_id ] else : return None def start ( self , pipeline_id : str , pipeline_name : str , step_id : str , module : \"KiaraModule\" , inputs : ValueSet , outputs : ValueSet , ) -> str : job_id = str ( uuid . uuid4 ()) # TODO: make snapshot of current state of data? full_inputs = module . create_full_inputs ( ** inputs ) wrapped_inputs = StepInputs ( inputs = full_inputs , kiara = self . _kiara ) wrapped_outputs = StepOutputs ( outputs = outputs , kiara = self . _kiara ) self . _inputs [ job_id ] = wrapped_inputs self . _outputs [ job_id ] = wrapped_outputs job = Job ( id = job_id , pipeline_id = pipeline_id , pipeline_name = pipeline_name , step_id = step_id , module_type = module . _module_type_id , # type: ignore module_config = module . config . dict (), module_doc = module . get_type_metadata () . documentation , inputs = PipelineValuesInfo . from_value_set ( inputs ), outputs = PipelineValuesInfo . from_value_set ( outputs ), ) job . job_log . add_log ( \"job created\" ) self . _active_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) try : self . process ( job_id = job_id , module = module , inputs = wrapped_inputs , outputs = wrapped_outputs , job_log = job . job_log , ) return job_id except Exception as e : job . error = str ( e ) if is_debug (): try : import traceback traceback . print_exc () except Exception : pass if isinstance ( e , KiaraProcessingException ): e . _module = module e . _inputs = inputs job . _exception = e raise e else : kpe = KiaraProcessingException ( e , module = module , inputs = inputs ) job . _exception = kpe raise kpe def job_status_updated ( self , job_id : str , status : typing . Union [ JobStatus , str , Exception ] ): job = self . _active_jobs . get ( job_id , None ) if job is None : raise Exception ( f \"Can't retrieve active job with id ' { job_id } ', no such job registered.\" ) if status == JobStatus . SUCCESS : job . job_log . add_log ( \"job finished successfully\" ) job . status = JobStatus . SUCCESS job = self . _active_jobs . pop ( job_id ) job . finished = datetime . now () self . _finished_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) elif status == JobStatus . FAILED or isinstance ( status , ( str , Exception )): job . job_log . add_log ( \"job failed\" ) job . status = JobStatus . FAILED job . finished = datetime . now () if isinstance ( status , str ): job . error = status elif isinstance ( status , Exception ): job . error = str ( status ) job . _exception = status job = self . _active_jobs . pop ( job_id ) self . _finished_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) elif status == JobStatus . STARTED : job . job_log . add_log ( \"job started\" ) job . status = JobStatus . STARTED job . started = datetime . now () self . _socket . send_string ( Job . create_event_msg ( job )) else : raise ValueError ( f \"Invalid value for status: { status } \" ) def sync_outputs ( self , * job_ids : str ): for j_id in job_ids : job_details = self . get_job_details ( j_id ) if not job_details : raise Exception ( f \"Can't sync outputs, no job with id: { j_id } \" ) job_inputs = self . _inputs [ j_id ] proc_info = ProcessingInfo ( ** job_details . dict ()) input_infos : typing . Dict [ str , ValueInfo ] = { k : v . get_info () for k , v in job_inputs . items () } # for field_name, value in d.items(): value_lineage = ValueLineage . create ( module_type = job_details . module_type , module_config = job_details . module_config , module_doc = job_details . module_doc , # output_name=field_name, inputs = input_infos , ) d = self . _outputs [ j_id ] d . sync ( proccessing_info = proc_info , lineage = value_lineage ) def wait_for ( self , * job_ids : str , sync_outputs : bool = True ): \"\"\"Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state.\"\"\" self . _wait_for ( * job_ids ) for job_id in job_ids : job = self . get_job_details ( job_id ) if job is None : raise Exception ( f \"Can't find job with id: { job_id } \" ) if job . status == JobStatus . SUCCESS : job . job_log . percent_finished = 100 if sync_outputs : self . sync_outputs ( * job_ids ) @abc . abstractmethod def process ( self , job_id : str , module : \"KiaraModule\" , inputs : ValueSet , outputs : ValueSet , job_log : JobLog , ) -> str : pass @abc . abstractmethod def _wait_for ( self , * job_ids : str ): pass","title":"ModuleProcessor"},{"location":"reference/kiara/processing/parallel/","text":"ThreadPoolProcessorConfig ( ProcessorConfig ) pydantic-model \u00b6 Source code in kiara/processing/parallel.py class ThreadPoolProcessorConfig ( ProcessorConfig ): max_workers : typing . Optional [ int ] = Field ( description = \"The max mount of workers for the thread pool.\" , default = None ) max_workers : int pydantic-field \u00b6 The max mount of workers for the thread pool.","title":"parallel"},{"location":"reference/kiara/processing/parallel/#kiara.processing.parallel.ThreadPoolProcessorConfig","text":"Source code in kiara/processing/parallel.py class ThreadPoolProcessorConfig ( ProcessorConfig ): max_workers : typing . Optional [ int ] = Field ( description = \"The max mount of workers for the thread pool.\" , default = None )","title":"ThreadPoolProcessorConfig"},{"location":"reference/kiara/processing/parallel/#kiara.processing.parallel.ThreadPoolProcessorConfig.max_workers","text":"The max mount of workers for the thread pool.","title":"max_workers"},{"location":"reference/kiara/processing/processor/","text":"ModuleProcessor ( ABC ) \u00b6 Source code in kiara/processing/processor.py class ModuleProcessor ( abc . ABC ): @classmethod def from_config ( cls , config : typing . Union [ None , typing . Mapping [ str , typing . Any ], ProcessorConfig ], kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleProcessor\" : from kiara.processing.parallel import ThreadPoolProcessorConfig from kiara.processing.synchronous import SynchronousProcessorConfig if not config : config = SynchronousProcessorConfig ( module_processor_type = \"synchronous\" ) if isinstance ( config , typing . Mapping ): processor_type = config . get ( \"module_processor_type\" , None ) if not processor_type : raise ValueError ( \"No 'module_processor_type' provided: {config} \" ) if processor_type == \"synchronous\" : config = SynchronousProcessorConfig ( ** config ) elif processor_type == \"multi-threaded\" : config = ThreadPoolProcessorConfig () else : raise ValueError ( f \"Invalid processor type: { processor_type } \" ) if isinstance ( config , SynchronousProcessorConfig ): from kiara.processing.synchronous import SynchronousProcessor proc : ModuleProcessor = SynchronousProcessor ( kiara = kiara , ** config . dict ( exclude = { \"module_processor_type\" }) ) elif isinstance ( config , ThreadPoolProcessorConfig ): from kiara.processing.parallel import ThreadPoolProcessor proc = ThreadPoolProcessor ( kiara = kiara , ** config . dict ( exclude = { \"module_processor_type\" }) ) else : raise TypeError ( f \"Invalid processor config class: { type ( config ) } \" ) return proc def __init__ ( self , zmq_context : typing . Optional [ Context ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : Kiara = kiara if zmq_context is None : zmq_context = Context . instance () self . _zmq_context : Context = zmq_context self . _socket : Socket = self . _zmq_context . socket ( zmq . PUB ) self . _socket . connect ( \"inproc://kiara_in\" ) self . _active_jobs : typing . Dict [ str , Job ] = {} self . _finished_jobs : typing . Dict [ str , Job ] = {} # TODO: clean up those? self . _inputs : typing . Dict [ str , StepInputs ] = {} self . _outputs : typing . Dict [ str , StepOutputs ] = {} def get_job_details ( self , job_id : str ) -> typing . Optional [ Job ]: if job_id in self . _active_jobs . keys (): return self . _active_jobs [ job_id ] elif job_id in self . _finished_jobs . keys (): return self . _finished_jobs [ job_id ] else : return None def start ( self , pipeline_id : str , pipeline_name : str , step_id : str , module : \"KiaraModule\" , inputs : ValueSet , outputs : ValueSet , ) -> str : job_id = str ( uuid . uuid4 ()) # TODO: make snapshot of current state of data? full_inputs = module . create_full_inputs ( ** inputs ) wrapped_inputs = StepInputs ( inputs = full_inputs , kiara = self . _kiara ) wrapped_outputs = StepOutputs ( outputs = outputs , kiara = self . _kiara ) self . _inputs [ job_id ] = wrapped_inputs self . _outputs [ job_id ] = wrapped_outputs job = Job ( id = job_id , pipeline_id = pipeline_id , pipeline_name = pipeline_name , step_id = step_id , module_type = module . _module_type_id , # type: ignore module_config = module . config . dict (), module_doc = module . get_type_metadata () . documentation , inputs = PipelineValuesInfo . from_value_set ( inputs ), outputs = PipelineValuesInfo . from_value_set ( outputs ), ) job . job_log . add_log ( \"job created\" ) self . _active_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) try : self . process ( job_id = job_id , module = module , inputs = wrapped_inputs , outputs = wrapped_outputs , job_log = job . job_log , ) return job_id except Exception as e : job . error = str ( e ) if is_debug (): try : import traceback traceback . print_exc () except Exception : pass if isinstance ( e , KiaraProcessingException ): e . _module = module e . _inputs = inputs job . _exception = e raise e else : kpe = KiaraProcessingException ( e , module = module , inputs = inputs ) job . _exception = kpe raise kpe def job_status_updated ( self , job_id : str , status : typing . Union [ JobStatus , str , Exception ] ): job = self . _active_jobs . get ( job_id , None ) if job is None : raise Exception ( f \"Can't retrieve active job with id ' { job_id } ', no such job registered.\" ) if status == JobStatus . SUCCESS : job . job_log . add_log ( \"job finished successfully\" ) job . status = JobStatus . SUCCESS job = self . _active_jobs . pop ( job_id ) job . finished = datetime . now () self . _finished_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) elif status == JobStatus . FAILED or isinstance ( status , ( str , Exception )): job . job_log . add_log ( \"job failed\" ) job . status = JobStatus . FAILED job . finished = datetime . now () if isinstance ( status , str ): job . error = status elif isinstance ( status , Exception ): job . error = str ( status ) job . _exception = status job = self . _active_jobs . pop ( job_id ) self . _finished_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) elif status == JobStatus . STARTED : job . job_log . add_log ( \"job started\" ) job . status = JobStatus . STARTED job . started = datetime . now () self . _socket . send_string ( Job . create_event_msg ( job )) else : raise ValueError ( f \"Invalid value for status: { status } \" ) def sync_outputs ( self , * job_ids : str ): for j_id in job_ids : job_details = self . get_job_details ( j_id ) if not job_details : raise Exception ( f \"Can't sync outputs, no job with id: { j_id } \" ) job_inputs = self . _inputs [ j_id ] proc_info = ProcessingInfo ( ** job_details . dict ()) input_infos : typing . Dict [ str , ValueInfo ] = { k : v . get_info () for k , v in job_inputs . items () } # for field_name, value in d.items(): value_lineage = ValueLineage . create ( module_type = job_details . module_type , module_config = job_details . module_config , module_doc = job_details . module_doc , # output_name=field_name, inputs = input_infos , ) d = self . _outputs [ j_id ] d . sync ( proccessing_info = proc_info , lineage = value_lineage ) def wait_for ( self , * job_ids : str , sync_outputs : bool = True ): \"\"\"Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state.\"\"\" self . _wait_for ( * job_ids ) for job_id in job_ids : job = self . get_job_details ( job_id ) if job is None : raise Exception ( f \"Can't find job with id: { job_id } \" ) if job . status == JobStatus . SUCCESS : job . job_log . percent_finished = 100 if sync_outputs : self . sync_outputs ( * job_ids ) @abc . abstractmethod def process ( self , job_id : str , module : \"KiaraModule\" , inputs : ValueSet , outputs : ValueSet , job_log : JobLog , ) -> str : pass @abc . abstractmethod def _wait_for ( self , * job_ids : str ): pass wait_for ( self , * job_ids , * , sync_outputs = True ) \u00b6 Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state. Source code in kiara/processing/processor.py def wait_for ( self , * job_ids : str , sync_outputs : bool = True ): \"\"\"Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state.\"\"\" self . _wait_for ( * job_ids ) for job_id in job_ids : job = self . get_job_details ( job_id ) if job is None : raise Exception ( f \"Can't find job with id: { job_id } \" ) if job . status == JobStatus . SUCCESS : job . job_log . percent_finished = 100 if sync_outputs : self . sync_outputs ( * job_ids )","title":"processor"},{"location":"reference/kiara/processing/processor/#kiara.processing.processor.ModuleProcessor","text":"Source code in kiara/processing/processor.py class ModuleProcessor ( abc . ABC ): @classmethod def from_config ( cls , config : typing . Union [ None , typing . Mapping [ str , typing . Any ], ProcessorConfig ], kiara : typing . Optional [ \"Kiara\" ] = None , ) -> \"ModuleProcessor\" : from kiara.processing.parallel import ThreadPoolProcessorConfig from kiara.processing.synchronous import SynchronousProcessorConfig if not config : config = SynchronousProcessorConfig ( module_processor_type = \"synchronous\" ) if isinstance ( config , typing . Mapping ): processor_type = config . get ( \"module_processor_type\" , None ) if not processor_type : raise ValueError ( \"No 'module_processor_type' provided: {config} \" ) if processor_type == \"synchronous\" : config = SynchronousProcessorConfig ( ** config ) elif processor_type == \"multi-threaded\" : config = ThreadPoolProcessorConfig () else : raise ValueError ( f \"Invalid processor type: { processor_type } \" ) if isinstance ( config , SynchronousProcessorConfig ): from kiara.processing.synchronous import SynchronousProcessor proc : ModuleProcessor = SynchronousProcessor ( kiara = kiara , ** config . dict ( exclude = { \"module_processor_type\" }) ) elif isinstance ( config , ThreadPoolProcessorConfig ): from kiara.processing.parallel import ThreadPoolProcessor proc = ThreadPoolProcessor ( kiara = kiara , ** config . dict ( exclude = { \"module_processor_type\" }) ) else : raise TypeError ( f \"Invalid processor config class: { type ( config ) } \" ) return proc def __init__ ( self , zmq_context : typing . Optional [ Context ] = None , kiara : typing . Optional [ \"Kiara\" ] = None , ): if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () self . _kiara : Kiara = kiara if zmq_context is None : zmq_context = Context . instance () self . _zmq_context : Context = zmq_context self . _socket : Socket = self . _zmq_context . socket ( zmq . PUB ) self . _socket . connect ( \"inproc://kiara_in\" ) self . _active_jobs : typing . Dict [ str , Job ] = {} self . _finished_jobs : typing . Dict [ str , Job ] = {} # TODO: clean up those? self . _inputs : typing . Dict [ str , StepInputs ] = {} self . _outputs : typing . Dict [ str , StepOutputs ] = {} def get_job_details ( self , job_id : str ) -> typing . Optional [ Job ]: if job_id in self . _active_jobs . keys (): return self . _active_jobs [ job_id ] elif job_id in self . _finished_jobs . keys (): return self . _finished_jobs [ job_id ] else : return None def start ( self , pipeline_id : str , pipeline_name : str , step_id : str , module : \"KiaraModule\" , inputs : ValueSet , outputs : ValueSet , ) -> str : job_id = str ( uuid . uuid4 ()) # TODO: make snapshot of current state of data? full_inputs = module . create_full_inputs ( ** inputs ) wrapped_inputs = StepInputs ( inputs = full_inputs , kiara = self . _kiara ) wrapped_outputs = StepOutputs ( outputs = outputs , kiara = self . _kiara ) self . _inputs [ job_id ] = wrapped_inputs self . _outputs [ job_id ] = wrapped_outputs job = Job ( id = job_id , pipeline_id = pipeline_id , pipeline_name = pipeline_name , step_id = step_id , module_type = module . _module_type_id , # type: ignore module_config = module . config . dict (), module_doc = module . get_type_metadata () . documentation , inputs = PipelineValuesInfo . from_value_set ( inputs ), outputs = PipelineValuesInfo . from_value_set ( outputs ), ) job . job_log . add_log ( \"job created\" ) self . _active_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) try : self . process ( job_id = job_id , module = module , inputs = wrapped_inputs , outputs = wrapped_outputs , job_log = job . job_log , ) return job_id except Exception as e : job . error = str ( e ) if is_debug (): try : import traceback traceback . print_exc () except Exception : pass if isinstance ( e , KiaraProcessingException ): e . _module = module e . _inputs = inputs job . _exception = e raise e else : kpe = KiaraProcessingException ( e , module = module , inputs = inputs ) job . _exception = kpe raise kpe def job_status_updated ( self , job_id : str , status : typing . Union [ JobStatus , str , Exception ] ): job = self . _active_jobs . get ( job_id , None ) if job is None : raise Exception ( f \"Can't retrieve active job with id ' { job_id } ', no such job registered.\" ) if status == JobStatus . SUCCESS : job . job_log . add_log ( \"job finished successfully\" ) job . status = JobStatus . SUCCESS job = self . _active_jobs . pop ( job_id ) job . finished = datetime . now () self . _finished_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) elif status == JobStatus . FAILED or isinstance ( status , ( str , Exception )): job . job_log . add_log ( \"job failed\" ) job . status = JobStatus . FAILED job . finished = datetime . now () if isinstance ( status , str ): job . error = status elif isinstance ( status , Exception ): job . error = str ( status ) job . _exception = status job = self . _active_jobs . pop ( job_id ) self . _finished_jobs [ job_id ] = job self . _socket . send_string ( Job . create_event_msg ( job )) elif status == JobStatus . STARTED : job . job_log . add_log ( \"job started\" ) job . status = JobStatus . STARTED job . started = datetime . now () self . _socket . send_string ( Job . create_event_msg ( job )) else : raise ValueError ( f \"Invalid value for status: { status } \" ) def sync_outputs ( self , * job_ids : str ): for j_id in job_ids : job_details = self . get_job_details ( j_id ) if not job_details : raise Exception ( f \"Can't sync outputs, no job with id: { j_id } \" ) job_inputs = self . _inputs [ j_id ] proc_info = ProcessingInfo ( ** job_details . dict ()) input_infos : typing . Dict [ str , ValueInfo ] = { k : v . get_info () for k , v in job_inputs . items () } # for field_name, value in d.items(): value_lineage = ValueLineage . create ( module_type = job_details . module_type , module_config = job_details . module_config , module_doc = job_details . module_doc , # output_name=field_name, inputs = input_infos , ) d = self . _outputs [ j_id ] d . sync ( proccessing_info = proc_info , lineage = value_lineage ) def wait_for ( self , * job_ids : str , sync_outputs : bool = True ): \"\"\"Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state.\"\"\" self . _wait_for ( * job_ids ) for job_id in job_ids : job = self . get_job_details ( job_id ) if job is None : raise Exception ( f \"Can't find job with id: { job_id } \" ) if job . status == JobStatus . SUCCESS : job . job_log . percent_finished = 100 if sync_outputs : self . sync_outputs ( * job_ids ) @abc . abstractmethod def process ( self , job_id : str , module : \"KiaraModule\" , inputs : ValueSet , outputs : ValueSet , job_log : JobLog , ) -> str : pass @abc . abstractmethod def _wait_for ( self , * job_ids : str ): pass","title":"ModuleProcessor"},{"location":"reference/kiara/processing/processor/#kiara.processing.processor.ModuleProcessor.wait_for","text":"Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state. Source code in kiara/processing/processor.py def wait_for ( self , * job_ids : str , sync_outputs : bool = True ): \"\"\"Wait for the jobs with the specified ids, also optionally sync their outputs with the pipeline value state.\"\"\" self . _wait_for ( * job_ids ) for job_id in job_ids : job = self . get_job_details ( job_id ) if job is None : raise Exception ( f \"Can't find job with id: { job_id } \" ) if job . status == JobStatus . SUCCESS : job . job_log . percent_finished = 100 if sync_outputs : self . sync_outputs ( * job_ids )","title":"wait_for()"},{"location":"reference/kiara/processing/synchronous/","text":"","title":"synchronous"},{"location":"reference/kiara/rendering/__init__/","text":"","title":"rendering"},{"location":"reference/kiara/rendering/jinja/","text":"","title":"jinja"},{"location":"reference/kiara/rendering/mgmt/","text":"","title":"mgmt"},{"location":"reference/kiara/sessions/__init__/","text":"","title":"sessions"},{"location":"reference/kiara/utils/__init__/","text":"check_valid_field_names ( * field_names ) \u00b6 Check whether the provided field names are all valid. Returns: Type Description List[str] an iterable of strings with invalid field names Source code in kiara/utils/__init__.py def check_valid_field_names ( * field_names ) -> typing . List [ str ]: \"\"\"Check whether the provided field names are all valid. Returns: an iterable of strings with invalid field names \"\"\" return [ x for x in field_names if x in INVALID_VALUE_NAMES or x . startswith ( \"_\" )] find_free_id ( stem , current_ids , sep = '_' ) \u00b6 Find a free var (or other name) based on a stem string, based on a list of provided existing names. Parameters: Name Type Description Default stem str the base string to use required current_ids Iterable[str] currently existing names required method str the method to create new names (allowed: 'count' -- for now) required method_args dict prototing_config for the creation method required Returns: Type Description str a free name Source code in kiara/utils/__init__.py def find_free_id ( stem : str , current_ids : typing . Iterable [ str ], sep = \"_\" , ) -> str : \"\"\"Find a free var (or other name) based on a stem string, based on a list of provided existing names. Args: stem (str): the base string to use current_ids (Iterable[str]): currently existing names method (str): the method to create new names (allowed: 'count' -- for now) method_args (dict): prototing_config for the creation method Returns: str: a free name \"\"\" start_count = 1 if stem not in current_ids : return stem i = start_count # new_name = None while True : new_name = f \" { stem }{ sep }{ i } \" if new_name in current_ids : i = i + 1 continue break return new_name get_auto_workflow_alias ( module_type , use_incremental_ids = False ) \u00b6 Return an id for a workflow obj of a provided module class. If 'use_incremental_ids' is set to True, a unique id is returned. Parameters: Name Type Description Default module_type str the name of the module type required use_incremental_ids bool whether to return a unique (incremental) id False Returns: Type Description str a module id Source code in kiara/utils/__init__.py def get_auto_workflow_alias ( module_type : str , use_incremental_ids : bool = False ) -> str : \"\"\"Return an id for a workflow obj of a provided module class. If 'use_incremental_ids' is set to True, a unique id is returned. Args: module_type (str): the name of the module type use_incremental_ids (bool): whether to return a unique (incremental) id Returns: str: a module id \"\"\" if not use_incremental_ids : return module_type nr = _AUTO_MODULE_ID . setdefault ( module_type , 0 ) _AUTO_MODULE_ID [ module_type ] = nr + 1 return f \" { module_type } _ { nr } \" class_loading \u00b6 find_all_kiara_modules () \u00b6 Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_kiara_modules () -> typing . Dict [ str , typing . Type [ \"KiaraModule\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" from kiara.module import KiaraModule modules = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.modules\" , base_class = KiaraModule , # type: ignore set_id_attribute = \"_module_type_name\" , remove_namespace_tokens = [ \"core.\" ], ) result = {} # need to test this, since I couldn't add an abstract method to the KiaraModule class itself (mypy complained because it is potentially overloaded) for k , cls in modules . items (): if not hasattr ( cls , \"process\" ): msg = f \"Ignoring module class ' { cls } ': no 'process' method.\" if is_debug (): log . warning ( msg ) else : log . debug ( msg ) continue # TODO: check signature of process method if k . startswith ( \"_\" ): tokens = k . split ( \".\" ) if len ( tokens ) == 1 : k = k [ 1 :] else : k = \".\" . join ( tokens [ 1 :]) result [ k ] = cls return result find_all_metadata_models () \u00b6 Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_metadata_models () -> typing . Dict [ str , typing . Type [ \"MetadataModel\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" return load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.metadata_models\" , base_class = MetadataModel , set_id_attribute = \"_metadata_key\" , remove_namespace_tokens = [ \"core.\" ], ) find_all_value_types () \u00b6 Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_value_types () -> typing . Dict [ str , typing . Type [ \"ValueType\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" all_value_types = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.value_types\" , base_class = ValueType , # type: ignore set_id_attribute = \"_value_type_name\" , remove_namespace_tokens = True , ) invalid = [ x for x in all_value_types . keys () if \".\" in x ] if invalid : raise Exception ( f \"Invalid value type name(s), type names can't contain '.': { ', ' . join ( invalid ) } \" ) return all_value_types find_subclasses_under ( base_class , module , prefix = '' , remove_namespace_tokens = None , module_name_func = None ) \u00b6 Find all (non-abstract) subclasses of a base class that live under a module (recursively). Parameters: Name Type Description Default base_class Type[~SUBCLASS_TYPE] the parent class required module Union[str, module] the module to search required prefix Optional[str] a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used '' remove_namespace_tokens Optional[Iterable[str]] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None None Returns: Type Description Mapping[str, Type[~SUBCLASS_TYPE]] a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value Source code in kiara/utils/class_loading.py def find_subclasses_under ( base_class : typing . Type [ SUBCLASS_TYPE ], module : typing . Union [ str , ModuleType ], prefix : typing . Optional [ str ] = \"\" , remove_namespace_tokens : typing . Optional [ typing . Iterable [ str ]] = None , module_name_func : typing . Callable = None , ) -> typing . Mapping [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all (non-abstract) subclasses of a base class that live under a module (recursively). Arguments: base_class: the parent class module: the module to search prefix: a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None Returns: a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value \"\"\" if hasattr ( sys , \"frozen\" ): raise NotImplementedError ( \"Pyinstaller bundling not supported yet.\" ) if isinstance ( module , str ): module = importlib . import_module ( module ) _import_modules_recursively ( module ) subclasses : typing . Iterable [ typing . Type [ SUBCLASS_TYPE ]] = _get_all_subclasses ( base_class ) result = {} for sc in subclasses : if not sc . __module__ . startswith ( module . __name__ ): continue if inspect . isabstract ( sc ): if is_debug (): # import traceback # traceback.print_stack() log . warning ( f \"Ignoring abstract subclass: { sc } \" ) else : log . debug ( f \"Ignoring abstract subclass: { sc } \" ) continue if module_name_func is None : module_name_func = _get_subclass_name name = module_name_func ( sc ) path = sc . __module__ [ len ( module . __name__ ) + 1 :] # noqa if path : full_name = f \" { path } . { name } \" else : full_name = name if prefix is None : prefix = module . __name__ + \".\" if remove_namespace_tokens : for rnt in remove_namespace_tokens : if prefix . startswith ( rnt ): prefix = prefix [ 0 : - len ( rnt )] # noqa if prefix : full_name = f \" { prefix } . { full_name } \" result [ full_name ] = sc return result load_all_subclasses_for_entry_point ( entry_point_name , base_class , set_id_attribute = None , remove_namespace_tokens = None ) \u00b6 Find all subclasses of a base class via package entry points. Parameters: Name Type Description Default entry_point_name str the entry point name to query entries for required base_class Type[~SUBCLASS_TYPE] the base class to look for required set_id_attribute Optional[str] whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set None remove_namespace_tokens Union[Iterable[str], bool] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed None TODO Source code in kiara/utils/class_loading.py def load_all_subclasses_for_entry_point ( entry_point_name : str , base_class : typing . Type [ SUBCLASS_TYPE ], set_id_attribute : typing . Union [ None , str ] = None , remove_namespace_tokens : typing . Union [ typing . Iterable [ str ], bool , None ] = None , ) -> typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all subclasses of a base class via package entry points. Arguments: entry_point_name: the entry point name to query entries for base_class: the base class to look for set_id_attribute: whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed TODO \"\"\" log2 = logging . getLogger ( \"stevedore\" ) out_hdlr = logging . StreamHandler ( sys . stdout ) out_hdlr . setFormatter ( logging . Formatter ( f \" { entry_point_name } plugin search error -> %(message)s\" ) ) out_hdlr . setLevel ( logging . INFO ) log2 . addHandler ( out_hdlr ) log2 . setLevel ( logging . INFO ) log . debug ( f \"Finding { entry_point_name } items from search paths...\" ) mgr = ExtensionManager ( namespace = entry_point_name , invoke_on_load = False , propagate_map_exceptions = True , ) result_entrypoints : typing . Dict [ str , typing . Type ] = {} result_dynamic : typing . Dict [ str , typing . Type ] = {} for plugin in mgr : name = plugin . name if isinstance ( plugin . plugin , type ) and issubclass ( plugin . plugin , base_class ): ep = plugin . entry_point module_cls = ep . load () if set_id_attribute : if hasattr ( module_cls , set_id_attribute ): if not getattr ( module_cls , set_id_attribute ) == name : log . warning ( f \"Item id mismatch for type { entry_point_name } : { getattr ( module_cls , set_id_attribute ) } != { name } , entry point key takes precedence: { name } )\" ) setattr ( module_cls , set_id_attribute , name ) else : setattr ( module_cls , set_id_attribute , name ) result_entrypoints [ name ] = module_cls elif ( isinstance ( plugin . plugin , tuple ) and len ( plugin . plugin ) >= 1 and callable ( plugin . plugin [ 0 ]) ) or callable ( plugin . plugin ): modules = _callable_wrapper ( plugin . plugin ) for k , v in modules . items (): _name = f \" { name } . { k } \" if _name in result_dynamic . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { _name } \" ) result_dynamic [ _name ] = v else : raise Exception ( f \"Can't load subclasses for entry point { entry_point_name } and base class { base_class } : invalid plugin type { type ( plugin . plugin ) } \" ) for k , v in result_dynamic . items (): if k in result_entrypoints . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { k } \" ) result_entrypoints [ k ] = v result : typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]] = {} for k , v in result_entrypoints . items (): if remove_namespace_tokens : if remove_namespace_tokens is True : k = k . split ( \".\" )[ - 1 ] elif isinstance ( remove_namespace_tokens , typing . Iterable ): for rnt in remove_namespace_tokens : if k . startswith ( rnt ): k = k [ len ( rnt ) :] # noqa if k in result . keys (): msg = \"\" if set_id_attribute : msg = f \" Check whether ' { v . __name__ } ' is missing the ' { set_id_attribute } ' class attribute (in case this is a sub-class), or it's ' { k } ' value is also set in another class?\" raise Exception ( f \"Duplicate item name for base class { base_class } : { k } . { msg } \" ) result [ k ] = v return result concurrency \u00b6 ThreadSaveCounter \u00b6 A thread-safe counter, can be used in kiara modules to update completion percentage. Source code in kiara/utils/concurrency.py class ThreadSaveCounter ( object ): \"\"\"A thread-safe counter, can be used in kiara modules to update completion percentage.\"\"\" def __init__ ( self ): self . _current = 0 self . _lock = threading . Lock () @property def current ( self ): return self . _current def current_percent ( self , total : int ) -> int : return int (( self . current / total ) * 100 ) def increment ( self ): with self . _lock : self . _current += 1 return self . _current def decrement ( self ): with self . _lock : self . _current -= 1 return self . _current modules \u00b6 find_file_for_module ( module_name , kiara = None ) \u00b6 Find the python file a module belongs to. Source code in kiara/utils/modules.py def find_file_for_module ( module_name : str , kiara : typing . Optional [ \"Kiara\" ] = None ) -> str : \"\"\"Find the python file a module belongs to.\"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () m_cls = kiara . get_module_class ( module_type = module_name ) python_module = m_cls . get_type_metadata () . python_class . get_module () # TODO: some sanity checks module_file = python_module . __file__ assert module_file is not None if module_file . endswith ( \"__init__.py\" ): extra_bit = ( python_module . __name__ . replace ( \".\" , os . path . sep ) + os . path . sep + \"__init__.py\" ) else : extra_bit = python_module . __name__ . replace ( \".\" , os . path . sep ) + \".py\" python_file_path = module_file [ 0 : - len ( extra_bit )] # noqa return python_file_path output \u00b6 OutputDetails ( BaseModel ) pydantic-model \u00b6 Source code in kiara/utils/output.py class OutputDetails ( BaseModel ): @classmethod def from_data ( cls , data : typing . Any ): if isinstance ( data , str ): if \"=\" in data : data = [ data ] else : data = [ f \"format= { data } \" ] if isinstance ( data , typing . Iterable ): data = list ( data ) if len ( data ) == 1 and isinstance ( data [ 0 ], str ) and \"=\" not in data [ 0 ]: data = [ f \"format= { data [ 0 ] } \" ] output_details_dict = dict_from_cli_args ( * data ) else : raise TypeError ( f \"Can't parse output detail config: invalid input type ' { type ( data ) } '.\" ) output_details = OutputDetails ( ** output_details_dict ) return output_details format : str = Field ( description = \"The output format.\" ) target : str = Field ( description = \"The output target.\" ) config : typing . Dict [ str , typing . Any ] = Field ( description = \"Output configuration.\" , default_factory = dict ) @root_validator ( pre = True ) def _set_defaults ( cls , values ): target : str = values . pop ( \"target\" , \"terminal\" ) format : str = values . pop ( \"format\" , None ) if format is None : if target == \"terminal\" : format = \"terminal\" else : if target == \"file\" : format = \"json\" else : ext = target . split ( \".\" )[ - 1 ] if ext in [ \"yaml\" , \"json\" ]: format = ext else : format = \"json\" result = { \"format\" : format , \"target\" : target , \"config\" : dict ( values )} return result config : Dict [ str , Any ] pydantic-field \u00b6 Output configuration. format : str pydantic-field required \u00b6 The output format. target : str pydantic-field required \u00b6 The output target.","title":"utils"},{"location":"reference/kiara/utils/__init__/#kiara.utils.check_valid_field_names","text":"Check whether the provided field names are all valid. Returns: Type Description List[str] an iterable of strings with invalid field names Source code in kiara/utils/__init__.py def check_valid_field_names ( * field_names ) -> typing . List [ str ]: \"\"\"Check whether the provided field names are all valid. Returns: an iterable of strings with invalid field names \"\"\" return [ x for x in field_names if x in INVALID_VALUE_NAMES or x . startswith ( \"_\" )]","title":"check_valid_field_names()"},{"location":"reference/kiara/utils/__init__/#kiara.utils.find_free_id","text":"Find a free var (or other name) based on a stem string, based on a list of provided existing names. Parameters: Name Type Description Default stem str the base string to use required current_ids Iterable[str] currently existing names required method str the method to create new names (allowed: 'count' -- for now) required method_args dict prototing_config for the creation method required Returns: Type Description str a free name Source code in kiara/utils/__init__.py def find_free_id ( stem : str , current_ids : typing . Iterable [ str ], sep = \"_\" , ) -> str : \"\"\"Find a free var (or other name) based on a stem string, based on a list of provided existing names. Args: stem (str): the base string to use current_ids (Iterable[str]): currently existing names method (str): the method to create new names (allowed: 'count' -- for now) method_args (dict): prototing_config for the creation method Returns: str: a free name \"\"\" start_count = 1 if stem not in current_ids : return stem i = start_count # new_name = None while True : new_name = f \" { stem }{ sep }{ i } \" if new_name in current_ids : i = i + 1 continue break return new_name","title":"find_free_id()"},{"location":"reference/kiara/utils/__init__/#kiara.utils.get_auto_workflow_alias","text":"Return an id for a workflow obj of a provided module class. If 'use_incremental_ids' is set to True, a unique id is returned. Parameters: Name Type Description Default module_type str the name of the module type required use_incremental_ids bool whether to return a unique (incremental) id False Returns: Type Description str a module id Source code in kiara/utils/__init__.py def get_auto_workflow_alias ( module_type : str , use_incremental_ids : bool = False ) -> str : \"\"\"Return an id for a workflow obj of a provided module class. If 'use_incremental_ids' is set to True, a unique id is returned. Args: module_type (str): the name of the module type use_incremental_ids (bool): whether to return a unique (incremental) id Returns: str: a module id \"\"\" if not use_incremental_ids : return module_type nr = _AUTO_MODULE_ID . setdefault ( module_type , 0 ) _AUTO_MODULE_ID [ module_type ] = nr + 1 return f \" { module_type } _ { nr } \"","title":"get_auto_workflow_alias()"},{"location":"reference/kiara/utils/__init__/#kiara.utils.class_loading","text":"","title":"class_loading"},{"location":"reference/kiara/utils/__init__/#kiara.utils.class_loading.find_all_kiara_modules","text":"Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_kiara_modules () -> typing . Dict [ str , typing . Type [ \"KiaraModule\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" from kiara.module import KiaraModule modules = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.modules\" , base_class = KiaraModule , # type: ignore set_id_attribute = \"_module_type_name\" , remove_namespace_tokens = [ \"core.\" ], ) result = {} # need to test this, since I couldn't add an abstract method to the KiaraModule class itself (mypy complained because it is potentially overloaded) for k , cls in modules . items (): if not hasattr ( cls , \"process\" ): msg = f \"Ignoring module class ' { cls } ': no 'process' method.\" if is_debug (): log . warning ( msg ) else : log . debug ( msg ) continue # TODO: check signature of process method if k . startswith ( \"_\" ): tokens = k . split ( \".\" ) if len ( tokens ) == 1 : k = k [ 1 :] else : k = \".\" . join ( tokens [ 1 :]) result [ k ] = cls return result","title":"find_all_kiara_modules()"},{"location":"reference/kiara/utils/__init__/#kiara.utils.class_loading.find_all_metadata_models","text":"Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_metadata_models () -> typing . Dict [ str , typing . Type [ \"MetadataModel\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" return load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.metadata_models\" , base_class = MetadataModel , set_id_attribute = \"_metadata_key\" , remove_namespace_tokens = [ \"core.\" ], )","title":"find_all_metadata_models()"},{"location":"reference/kiara/utils/__init__/#kiara.utils.class_loading.find_all_value_types","text":"Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_value_types () -> typing . Dict [ str , typing . Type [ \"ValueType\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" all_value_types = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.value_types\" , base_class = ValueType , # type: ignore set_id_attribute = \"_value_type_name\" , remove_namespace_tokens = True , ) invalid = [ x for x in all_value_types . keys () if \".\" in x ] if invalid : raise Exception ( f \"Invalid value type name(s), type names can't contain '.': { ', ' . join ( invalid ) } \" ) return all_value_types","title":"find_all_value_types()"},{"location":"reference/kiara/utils/__init__/#kiara.utils.class_loading.find_subclasses_under","text":"Find all (non-abstract) subclasses of a base class that live under a module (recursively). Parameters: Name Type Description Default base_class Type[~SUBCLASS_TYPE] the parent class required module Union[str, module] the module to search required prefix Optional[str] a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used '' remove_namespace_tokens Optional[Iterable[str]] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None None Returns: Type Description Mapping[str, Type[~SUBCLASS_TYPE]] a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value Source code in kiara/utils/class_loading.py def find_subclasses_under ( base_class : typing . Type [ SUBCLASS_TYPE ], module : typing . Union [ str , ModuleType ], prefix : typing . Optional [ str ] = \"\" , remove_namespace_tokens : typing . Optional [ typing . Iterable [ str ]] = None , module_name_func : typing . Callable = None , ) -> typing . Mapping [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all (non-abstract) subclasses of a base class that live under a module (recursively). Arguments: base_class: the parent class module: the module to search prefix: a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None Returns: a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value \"\"\" if hasattr ( sys , \"frozen\" ): raise NotImplementedError ( \"Pyinstaller bundling not supported yet.\" ) if isinstance ( module , str ): module = importlib . import_module ( module ) _import_modules_recursively ( module ) subclasses : typing . Iterable [ typing . Type [ SUBCLASS_TYPE ]] = _get_all_subclasses ( base_class ) result = {} for sc in subclasses : if not sc . __module__ . startswith ( module . __name__ ): continue if inspect . isabstract ( sc ): if is_debug (): # import traceback # traceback.print_stack() log . warning ( f \"Ignoring abstract subclass: { sc } \" ) else : log . debug ( f \"Ignoring abstract subclass: { sc } \" ) continue if module_name_func is None : module_name_func = _get_subclass_name name = module_name_func ( sc ) path = sc . __module__ [ len ( module . __name__ ) + 1 :] # noqa if path : full_name = f \" { path } . { name } \" else : full_name = name if prefix is None : prefix = module . __name__ + \".\" if remove_namespace_tokens : for rnt in remove_namespace_tokens : if prefix . startswith ( rnt ): prefix = prefix [ 0 : - len ( rnt )] # noqa if prefix : full_name = f \" { prefix } . { full_name } \" result [ full_name ] = sc return result","title":"find_subclasses_under()"},{"location":"reference/kiara/utils/__init__/#kiara.utils.class_loading.load_all_subclasses_for_entry_point","text":"Find all subclasses of a base class via package entry points. Parameters: Name Type Description Default entry_point_name str the entry point name to query entries for required base_class Type[~SUBCLASS_TYPE] the base class to look for required set_id_attribute Optional[str] whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set None remove_namespace_tokens Union[Iterable[str], bool] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed None TODO Source code in kiara/utils/class_loading.py def load_all_subclasses_for_entry_point ( entry_point_name : str , base_class : typing . Type [ SUBCLASS_TYPE ], set_id_attribute : typing . Union [ None , str ] = None , remove_namespace_tokens : typing . Union [ typing . Iterable [ str ], bool , None ] = None , ) -> typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all subclasses of a base class via package entry points. Arguments: entry_point_name: the entry point name to query entries for base_class: the base class to look for set_id_attribute: whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed TODO \"\"\" log2 = logging . getLogger ( \"stevedore\" ) out_hdlr = logging . StreamHandler ( sys . stdout ) out_hdlr . setFormatter ( logging . Formatter ( f \" { entry_point_name } plugin search error -> %(message)s\" ) ) out_hdlr . setLevel ( logging . INFO ) log2 . addHandler ( out_hdlr ) log2 . setLevel ( logging . INFO ) log . debug ( f \"Finding { entry_point_name } items from search paths...\" ) mgr = ExtensionManager ( namespace = entry_point_name , invoke_on_load = False , propagate_map_exceptions = True , ) result_entrypoints : typing . Dict [ str , typing . Type ] = {} result_dynamic : typing . Dict [ str , typing . Type ] = {} for plugin in mgr : name = plugin . name if isinstance ( plugin . plugin , type ) and issubclass ( plugin . plugin , base_class ): ep = plugin . entry_point module_cls = ep . load () if set_id_attribute : if hasattr ( module_cls , set_id_attribute ): if not getattr ( module_cls , set_id_attribute ) == name : log . warning ( f \"Item id mismatch for type { entry_point_name } : { getattr ( module_cls , set_id_attribute ) } != { name } , entry point key takes precedence: { name } )\" ) setattr ( module_cls , set_id_attribute , name ) else : setattr ( module_cls , set_id_attribute , name ) result_entrypoints [ name ] = module_cls elif ( isinstance ( plugin . plugin , tuple ) and len ( plugin . plugin ) >= 1 and callable ( plugin . plugin [ 0 ]) ) or callable ( plugin . plugin ): modules = _callable_wrapper ( plugin . plugin ) for k , v in modules . items (): _name = f \" { name } . { k } \" if _name in result_dynamic . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { _name } \" ) result_dynamic [ _name ] = v else : raise Exception ( f \"Can't load subclasses for entry point { entry_point_name } and base class { base_class } : invalid plugin type { type ( plugin . plugin ) } \" ) for k , v in result_dynamic . items (): if k in result_entrypoints . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { k } \" ) result_entrypoints [ k ] = v result : typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]] = {} for k , v in result_entrypoints . items (): if remove_namespace_tokens : if remove_namespace_tokens is True : k = k . split ( \".\" )[ - 1 ] elif isinstance ( remove_namespace_tokens , typing . Iterable ): for rnt in remove_namespace_tokens : if k . startswith ( rnt ): k = k [ len ( rnt ) :] # noqa if k in result . keys (): msg = \"\" if set_id_attribute : msg = f \" Check whether ' { v . __name__ } ' is missing the ' { set_id_attribute } ' class attribute (in case this is a sub-class), or it's ' { k } ' value is also set in another class?\" raise Exception ( f \"Duplicate item name for base class { base_class } : { k } . { msg } \" ) result [ k ] = v return result","title":"load_all_subclasses_for_entry_point()"},{"location":"reference/kiara/utils/__init__/#kiara.utils.concurrency","text":"","title":"concurrency"},{"location":"reference/kiara/utils/__init__/#kiara.utils.concurrency.ThreadSaveCounter","text":"A thread-safe counter, can be used in kiara modules to update completion percentage. Source code in kiara/utils/concurrency.py class ThreadSaveCounter ( object ): \"\"\"A thread-safe counter, can be used in kiara modules to update completion percentage.\"\"\" def __init__ ( self ): self . _current = 0 self . _lock = threading . Lock () @property def current ( self ): return self . _current def current_percent ( self , total : int ) -> int : return int (( self . current / total ) * 100 ) def increment ( self ): with self . _lock : self . _current += 1 return self . _current def decrement ( self ): with self . _lock : self . _current -= 1 return self . _current","title":"ThreadSaveCounter"},{"location":"reference/kiara/utils/__init__/#kiara.utils.modules","text":"","title":"modules"},{"location":"reference/kiara/utils/__init__/#kiara.utils.modules.find_file_for_module","text":"Find the python file a module belongs to. Source code in kiara/utils/modules.py def find_file_for_module ( module_name : str , kiara : typing . Optional [ \"Kiara\" ] = None ) -> str : \"\"\"Find the python file a module belongs to.\"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () m_cls = kiara . get_module_class ( module_type = module_name ) python_module = m_cls . get_type_metadata () . python_class . get_module () # TODO: some sanity checks module_file = python_module . __file__ assert module_file is not None if module_file . endswith ( \"__init__.py\" ): extra_bit = ( python_module . __name__ . replace ( \".\" , os . path . sep ) + os . path . sep + \"__init__.py\" ) else : extra_bit = python_module . __name__ . replace ( \".\" , os . path . sep ) + \".py\" python_file_path = module_file [ 0 : - len ( extra_bit )] # noqa return python_file_path","title":"find_file_for_module()"},{"location":"reference/kiara/utils/__init__/#kiara.utils.output","text":"","title":"output"},{"location":"reference/kiara/utils/__init__/#kiara.utils.output.OutputDetails","text":"Source code in kiara/utils/output.py class OutputDetails ( BaseModel ): @classmethod def from_data ( cls , data : typing . Any ): if isinstance ( data , str ): if \"=\" in data : data = [ data ] else : data = [ f \"format= { data } \" ] if isinstance ( data , typing . Iterable ): data = list ( data ) if len ( data ) == 1 and isinstance ( data [ 0 ], str ) and \"=\" not in data [ 0 ]: data = [ f \"format= { data [ 0 ] } \" ] output_details_dict = dict_from_cli_args ( * data ) else : raise TypeError ( f \"Can't parse output detail config: invalid input type ' { type ( data ) } '.\" ) output_details = OutputDetails ( ** output_details_dict ) return output_details format : str = Field ( description = \"The output format.\" ) target : str = Field ( description = \"The output target.\" ) config : typing . Dict [ str , typing . Any ] = Field ( description = \"Output configuration.\" , default_factory = dict ) @root_validator ( pre = True ) def _set_defaults ( cls , values ): target : str = values . pop ( \"target\" , \"terminal\" ) format : str = values . pop ( \"format\" , None ) if format is None : if target == \"terminal\" : format = \"terminal\" else : if target == \"file\" : format = \"json\" else : ext = target . split ( \".\" )[ - 1 ] if ext in [ \"yaml\" , \"json\" ]: format = ext else : format = \"json\" result = { \"format\" : format , \"target\" : target , \"config\" : dict ( values )} return result","title":"OutputDetails"},{"location":"reference/kiara/utils/class_loading/","text":"find_all_kiara_modules () \u00b6 Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_kiara_modules () -> typing . Dict [ str , typing . Type [ \"KiaraModule\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" from kiara.module import KiaraModule modules = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.modules\" , base_class = KiaraModule , # type: ignore set_id_attribute = \"_module_type_name\" , remove_namespace_tokens = [ \"core.\" ], ) result = {} # need to test this, since I couldn't add an abstract method to the KiaraModule class itself (mypy complained because it is potentially overloaded) for k , cls in modules . items (): if not hasattr ( cls , \"process\" ): msg = f \"Ignoring module class ' { cls } ': no 'process' method.\" if is_debug (): log . warning ( msg ) else : log . debug ( msg ) continue # TODO: check signature of process method if k . startswith ( \"_\" ): tokens = k . split ( \".\" ) if len ( tokens ) == 1 : k = k [ 1 :] else : k = \".\" . join ( tokens [ 1 :]) result [ k ] = cls return result find_all_metadata_models () \u00b6 Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_metadata_models () -> typing . Dict [ str , typing . Type [ \"MetadataModel\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" return load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.metadata_models\" , base_class = MetadataModel , set_id_attribute = \"_metadata_key\" , remove_namespace_tokens = [ \"core.\" ], ) find_all_value_types () \u00b6 Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_value_types () -> typing . Dict [ str , typing . Type [ \"ValueType\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" all_value_types = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.value_types\" , base_class = ValueType , # type: ignore set_id_attribute = \"_value_type_name\" , remove_namespace_tokens = True , ) invalid = [ x for x in all_value_types . keys () if \".\" in x ] if invalid : raise Exception ( f \"Invalid value type name(s), type names can't contain '.': { ', ' . join ( invalid ) } \" ) return all_value_types find_subclasses_under ( base_class , module , prefix = '' , remove_namespace_tokens = None , module_name_func = None ) \u00b6 Find all (non-abstract) subclasses of a base class that live under a module (recursively). Parameters: Name Type Description Default base_class Type[~SUBCLASS_TYPE] the parent class required module Union[str, module] the module to search required prefix Optional[str] a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used '' remove_namespace_tokens Optional[Iterable[str]] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None None Returns: Type Description Mapping[str, Type[~SUBCLASS_TYPE]] a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value Source code in kiara/utils/class_loading.py def find_subclasses_under ( base_class : typing . Type [ SUBCLASS_TYPE ], module : typing . Union [ str , ModuleType ], prefix : typing . Optional [ str ] = \"\" , remove_namespace_tokens : typing . Optional [ typing . Iterable [ str ]] = None , module_name_func : typing . Callable = None , ) -> typing . Mapping [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all (non-abstract) subclasses of a base class that live under a module (recursively). Arguments: base_class: the parent class module: the module to search prefix: a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None Returns: a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value \"\"\" if hasattr ( sys , \"frozen\" ): raise NotImplementedError ( \"Pyinstaller bundling not supported yet.\" ) if isinstance ( module , str ): module = importlib . import_module ( module ) _import_modules_recursively ( module ) subclasses : typing . Iterable [ typing . Type [ SUBCLASS_TYPE ]] = _get_all_subclasses ( base_class ) result = {} for sc in subclasses : if not sc . __module__ . startswith ( module . __name__ ): continue if inspect . isabstract ( sc ): if is_debug (): # import traceback # traceback.print_stack() log . warning ( f \"Ignoring abstract subclass: { sc } \" ) else : log . debug ( f \"Ignoring abstract subclass: { sc } \" ) continue if module_name_func is None : module_name_func = _get_subclass_name name = module_name_func ( sc ) path = sc . __module__ [ len ( module . __name__ ) + 1 :] # noqa if path : full_name = f \" { path } . { name } \" else : full_name = name if prefix is None : prefix = module . __name__ + \".\" if remove_namespace_tokens : for rnt in remove_namespace_tokens : if prefix . startswith ( rnt ): prefix = prefix [ 0 : - len ( rnt )] # noqa if prefix : full_name = f \" { prefix } . { full_name } \" result [ full_name ] = sc return result load_all_subclasses_for_entry_point ( entry_point_name , base_class , set_id_attribute = None , remove_namespace_tokens = None ) \u00b6 Find all subclasses of a base class via package entry points. Parameters: Name Type Description Default entry_point_name str the entry point name to query entries for required base_class Type[~SUBCLASS_TYPE] the base class to look for required set_id_attribute Optional[str] whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set None remove_namespace_tokens Union[Iterable[str], bool] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed None TODO Source code in kiara/utils/class_loading.py def load_all_subclasses_for_entry_point ( entry_point_name : str , base_class : typing . Type [ SUBCLASS_TYPE ], set_id_attribute : typing . Union [ None , str ] = None , remove_namespace_tokens : typing . Union [ typing . Iterable [ str ], bool , None ] = None , ) -> typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all subclasses of a base class via package entry points. Arguments: entry_point_name: the entry point name to query entries for base_class: the base class to look for set_id_attribute: whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed TODO \"\"\" log2 = logging . getLogger ( \"stevedore\" ) out_hdlr = logging . StreamHandler ( sys . stdout ) out_hdlr . setFormatter ( logging . Formatter ( f \" { entry_point_name } plugin search error -> %(message)s\" ) ) out_hdlr . setLevel ( logging . INFO ) log2 . addHandler ( out_hdlr ) log2 . setLevel ( logging . INFO ) log . debug ( f \"Finding { entry_point_name } items from search paths...\" ) mgr = ExtensionManager ( namespace = entry_point_name , invoke_on_load = False , propagate_map_exceptions = True , ) result_entrypoints : typing . Dict [ str , typing . Type ] = {} result_dynamic : typing . Dict [ str , typing . Type ] = {} for plugin in mgr : name = plugin . name if isinstance ( plugin . plugin , type ) and issubclass ( plugin . plugin , base_class ): ep = plugin . entry_point module_cls = ep . load () if set_id_attribute : if hasattr ( module_cls , set_id_attribute ): if not getattr ( module_cls , set_id_attribute ) == name : log . warning ( f \"Item id mismatch for type { entry_point_name } : { getattr ( module_cls , set_id_attribute ) } != { name } , entry point key takes precedence: { name } )\" ) setattr ( module_cls , set_id_attribute , name ) else : setattr ( module_cls , set_id_attribute , name ) result_entrypoints [ name ] = module_cls elif ( isinstance ( plugin . plugin , tuple ) and len ( plugin . plugin ) >= 1 and callable ( plugin . plugin [ 0 ]) ) or callable ( plugin . plugin ): modules = _callable_wrapper ( plugin . plugin ) for k , v in modules . items (): _name = f \" { name } . { k } \" if _name in result_dynamic . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { _name } \" ) result_dynamic [ _name ] = v else : raise Exception ( f \"Can't load subclasses for entry point { entry_point_name } and base class { base_class } : invalid plugin type { type ( plugin . plugin ) } \" ) for k , v in result_dynamic . items (): if k in result_entrypoints . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { k } \" ) result_entrypoints [ k ] = v result : typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]] = {} for k , v in result_entrypoints . items (): if remove_namespace_tokens : if remove_namespace_tokens is True : k = k . split ( \".\" )[ - 1 ] elif isinstance ( remove_namespace_tokens , typing . Iterable ): for rnt in remove_namespace_tokens : if k . startswith ( rnt ): k = k [ len ( rnt ) :] # noqa if k in result . keys (): msg = \"\" if set_id_attribute : msg = f \" Check whether ' { v . __name__ } ' is missing the ' { set_id_attribute } ' class attribute (in case this is a sub-class), or it's ' { k } ' value is also set in another class?\" raise Exception ( f \"Duplicate item name for base class { base_class } : { k } . { msg } \" ) result [ k ] = v return result","title":"class_loading"},{"location":"reference/kiara/utils/class_loading/#kiara.utils.class_loading.find_all_kiara_modules","text":"Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_kiara_modules () -> typing . Dict [ str , typing . Type [ \"KiaraModule\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" from kiara.module import KiaraModule modules = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.modules\" , base_class = KiaraModule , # type: ignore set_id_attribute = \"_module_type_name\" , remove_namespace_tokens = [ \"core.\" ], ) result = {} # need to test this, since I couldn't add an abstract method to the KiaraModule class itself (mypy complained because it is potentially overloaded) for k , cls in modules . items (): if not hasattr ( cls , \"process\" ): msg = f \"Ignoring module class ' { cls } ': no 'process' method.\" if is_debug (): log . warning ( msg ) else : log . debug ( msg ) continue # TODO: check signature of process method if k . startswith ( \"_\" ): tokens = k . split ( \".\" ) if len ( tokens ) == 1 : k = k [ 1 :] else : k = \".\" . join ( tokens [ 1 :]) result [ k ] = cls return result","title":"find_all_kiara_modules()"},{"location":"reference/kiara/utils/class_loading/#kiara.utils.class_loading.find_all_metadata_models","text":"Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_metadata_models () -> typing . Dict [ str , typing . Type [ \"MetadataModel\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" return load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.metadata_models\" , base_class = MetadataModel , set_id_attribute = \"_metadata_key\" , remove_namespace_tokens = [ \"core.\" ], )","title":"find_all_metadata_models()"},{"location":"reference/kiara/utils/class_loading/#kiara.utils.class_loading.find_all_value_types","text":"Find all KiaraModule subclasses via package entry points. TODO Source code in kiara/utils/class_loading.py def find_all_value_types () -> typing . Dict [ str , typing . Type [ \"ValueType\" ]]: \"\"\"Find all [KiaraModule][kiara.module.KiaraModule] subclasses via package entry points. TODO \"\"\" all_value_types = load_all_subclasses_for_entry_point ( entry_point_name = \"kiara.value_types\" , base_class = ValueType , # type: ignore set_id_attribute = \"_value_type_name\" , remove_namespace_tokens = True , ) invalid = [ x for x in all_value_types . keys () if \".\" in x ] if invalid : raise Exception ( f \"Invalid value type name(s), type names can't contain '.': { ', ' . join ( invalid ) } \" ) return all_value_types","title":"find_all_value_types()"},{"location":"reference/kiara/utils/class_loading/#kiara.utils.class_loading.find_subclasses_under","text":"Find all (non-abstract) subclasses of a base class that live under a module (recursively). Parameters: Name Type Description Default base_class Type[~SUBCLASS_TYPE] the parent class required module Union[str, module] the module to search required prefix Optional[str] a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used '' remove_namespace_tokens Optional[Iterable[str]] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None None Returns: Type Description Mapping[str, Type[~SUBCLASS_TYPE]] a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value Source code in kiara/utils/class_loading.py def find_subclasses_under ( base_class : typing . Type [ SUBCLASS_TYPE ], module : typing . Union [ str , ModuleType ], prefix : typing . Optional [ str ] = \"\" , remove_namespace_tokens : typing . Optional [ typing . Iterable [ str ]] = None , module_name_func : typing . Callable = None , ) -> typing . Mapping [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all (non-abstract) subclasses of a base class that live under a module (recursively). Arguments: base_class: the parent class module: the module to search prefix: a string to use as a result items namespace prefix, defaults to an empty string, use 'None' to indicate the module path should be used remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None Returns: a map containing the (fully namespaced) id of the subclass as key, and the actual class object as value \"\"\" if hasattr ( sys , \"frozen\" ): raise NotImplementedError ( \"Pyinstaller bundling not supported yet.\" ) if isinstance ( module , str ): module = importlib . import_module ( module ) _import_modules_recursively ( module ) subclasses : typing . Iterable [ typing . Type [ SUBCLASS_TYPE ]] = _get_all_subclasses ( base_class ) result = {} for sc in subclasses : if not sc . __module__ . startswith ( module . __name__ ): continue if inspect . isabstract ( sc ): if is_debug (): # import traceback # traceback.print_stack() log . warning ( f \"Ignoring abstract subclass: { sc } \" ) else : log . debug ( f \"Ignoring abstract subclass: { sc } \" ) continue if module_name_func is None : module_name_func = _get_subclass_name name = module_name_func ( sc ) path = sc . __module__ [ len ( module . __name__ ) + 1 :] # noqa if path : full_name = f \" { path } . { name } \" else : full_name = name if prefix is None : prefix = module . __name__ + \".\" if remove_namespace_tokens : for rnt in remove_namespace_tokens : if prefix . startswith ( rnt ): prefix = prefix [ 0 : - len ( rnt )] # noqa if prefix : full_name = f \" { prefix } . { full_name } \" result [ full_name ] = sc return result","title":"find_subclasses_under()"},{"location":"reference/kiara/utils/class_loading/#kiara.utils.class_loading.load_all_subclasses_for_entry_point","text":"Find all subclasses of a base class via package entry points. Parameters: Name Type Description Default entry_point_name str the entry point name to query entries for required base_class Type[~SUBCLASS_TYPE] the base class to look for required set_id_attribute Optional[str] whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set None remove_namespace_tokens Union[Iterable[str], bool] a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed None TODO Source code in kiara/utils/class_loading.py def load_all_subclasses_for_entry_point ( entry_point_name : str , base_class : typing . Type [ SUBCLASS_TYPE ], set_id_attribute : typing . Union [ None , str ] = None , remove_namespace_tokens : typing . Union [ typing . Iterable [ str ], bool , None ] = None , ) -> typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]]: \"\"\"Find all subclasses of a base class via package entry points. Arguments: entry_point_name: the entry point name to query entries for base_class: the base class to look for set_id_attribute: whether to set the entry point id as attribute to the class, if None, no id attribute will be set, if a string, the attribute with that name will be set remove_namespace_tokens: a list of strings to remove from module names when autogenerating subclass ids, and prefix is None, or a boolean in which case all or none namespaces will be removed TODO \"\"\" log2 = logging . getLogger ( \"stevedore\" ) out_hdlr = logging . StreamHandler ( sys . stdout ) out_hdlr . setFormatter ( logging . Formatter ( f \" { entry_point_name } plugin search error -> %(message)s\" ) ) out_hdlr . setLevel ( logging . INFO ) log2 . addHandler ( out_hdlr ) log2 . setLevel ( logging . INFO ) log . debug ( f \"Finding { entry_point_name } items from search paths...\" ) mgr = ExtensionManager ( namespace = entry_point_name , invoke_on_load = False , propagate_map_exceptions = True , ) result_entrypoints : typing . Dict [ str , typing . Type ] = {} result_dynamic : typing . Dict [ str , typing . Type ] = {} for plugin in mgr : name = plugin . name if isinstance ( plugin . plugin , type ) and issubclass ( plugin . plugin , base_class ): ep = plugin . entry_point module_cls = ep . load () if set_id_attribute : if hasattr ( module_cls , set_id_attribute ): if not getattr ( module_cls , set_id_attribute ) == name : log . warning ( f \"Item id mismatch for type { entry_point_name } : { getattr ( module_cls , set_id_attribute ) } != { name } , entry point key takes precedence: { name } )\" ) setattr ( module_cls , set_id_attribute , name ) else : setattr ( module_cls , set_id_attribute , name ) result_entrypoints [ name ] = module_cls elif ( isinstance ( plugin . plugin , tuple ) and len ( plugin . plugin ) >= 1 and callable ( plugin . plugin [ 0 ]) ) or callable ( plugin . plugin ): modules = _callable_wrapper ( plugin . plugin ) for k , v in modules . items (): _name = f \" { name } . { k } \" if _name in result_dynamic . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { _name } \" ) result_dynamic [ _name ] = v else : raise Exception ( f \"Can't load subclasses for entry point { entry_point_name } and base class { base_class } : invalid plugin type { type ( plugin . plugin ) } \" ) for k , v in result_dynamic . items (): if k in result_entrypoints . keys (): raise Exception ( f \"Duplicate item name for type { entry_point_name } : { k } \" ) result_entrypoints [ k ] = v result : typing . Dict [ str , typing . Type [ SUBCLASS_TYPE ]] = {} for k , v in result_entrypoints . items (): if remove_namespace_tokens : if remove_namespace_tokens is True : k = k . split ( \".\" )[ - 1 ] elif isinstance ( remove_namespace_tokens , typing . Iterable ): for rnt in remove_namespace_tokens : if k . startswith ( rnt ): k = k [ len ( rnt ) :] # noqa if k in result . keys (): msg = \"\" if set_id_attribute : msg = f \" Check whether ' { v . __name__ } ' is missing the ' { set_id_attribute } ' class attribute (in case this is a sub-class), or it's ' { k } ' value is also set in another class?\" raise Exception ( f \"Duplicate item name for base class { base_class } : { k } . { msg } \" ) result [ k ] = v return result","title":"load_all_subclasses_for_entry_point()"},{"location":"reference/kiara/utils/concurrency/","text":"ThreadSaveCounter \u00b6 A thread-safe counter, can be used in kiara modules to update completion percentage. Source code in kiara/utils/concurrency.py class ThreadSaveCounter ( object ): \"\"\"A thread-safe counter, can be used in kiara modules to update completion percentage.\"\"\" def __init__ ( self ): self . _current = 0 self . _lock = threading . Lock () @property def current ( self ): return self . _current def current_percent ( self , total : int ) -> int : return int (( self . current / total ) * 100 ) def increment ( self ): with self . _lock : self . _current += 1 return self . _current def decrement ( self ): with self . _lock : self . _current -= 1 return self . _current","title":"concurrency"},{"location":"reference/kiara/utils/concurrency/#kiara.utils.concurrency.ThreadSaveCounter","text":"A thread-safe counter, can be used in kiara modules to update completion percentage. Source code in kiara/utils/concurrency.py class ThreadSaveCounter ( object ): \"\"\"A thread-safe counter, can be used in kiara modules to update completion percentage.\"\"\" def __init__ ( self ): self . _current = 0 self . _lock = threading . Lock () @property def current ( self ): return self . _current def current_percent ( self , total : int ) -> int : return int (( self . current / total ) * 100 ) def increment ( self ): with self . _lock : self . _current += 1 return self . _current def decrement ( self ): with self . _lock : self . _current -= 1 return self . _current","title":"ThreadSaveCounter"},{"location":"reference/kiara/utils/doc/","text":"","title":"doc"},{"location":"reference/kiara/utils/global_metadata/","text":"","title":"global_metadata"},{"location":"reference/kiara/utils/jupyter/","text":"","title":"jupyter"},{"location":"reference/kiara/utils/modules/","text":"find_file_for_module ( module_name , kiara = None ) \u00b6 Find the python file a module belongs to. Source code in kiara/utils/modules.py def find_file_for_module ( module_name : str , kiara : typing . Optional [ \"Kiara\" ] = None ) -> str : \"\"\"Find the python file a module belongs to.\"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () m_cls = kiara . get_module_class ( module_type = module_name ) python_module = m_cls . get_type_metadata () . python_class . get_module () # TODO: some sanity checks module_file = python_module . __file__ assert module_file is not None if module_file . endswith ( \"__init__.py\" ): extra_bit = ( python_module . __name__ . replace ( \".\" , os . path . sep ) + os . path . sep + \"__init__.py\" ) else : extra_bit = python_module . __name__ . replace ( \".\" , os . path . sep ) + \".py\" python_file_path = module_file [ 0 : - len ( extra_bit )] # noqa return python_file_path","title":"modules"},{"location":"reference/kiara/utils/modules/#kiara.utils.modules.find_file_for_module","text":"Find the python file a module belongs to. Source code in kiara/utils/modules.py def find_file_for_module ( module_name : str , kiara : typing . Optional [ \"Kiara\" ] = None ) -> str : \"\"\"Find the python file a module belongs to.\"\"\" if kiara is None : from kiara.kiara import Kiara kiara = Kiara . instance () m_cls = kiara . get_module_class ( module_type = module_name ) python_module = m_cls . get_type_metadata () . python_class . get_module () # TODO: some sanity checks module_file = python_module . __file__ assert module_file is not None if module_file . endswith ( \"__init__.py\" ): extra_bit = ( python_module . __name__ . replace ( \".\" , os . path . sep ) + os . path . sep + \"__init__.py\" ) else : extra_bit = python_module . __name__ . replace ( \".\" , os . path . sep ) + \".py\" python_file_path = module_file [ 0 : - len ( extra_bit )] # noqa return python_file_path","title":"find_file_for_module()"},{"location":"reference/kiara/utils/output/","text":"OutputDetails ( BaseModel ) pydantic-model \u00b6 Source code in kiara/utils/output.py class OutputDetails ( BaseModel ): @classmethod def from_data ( cls , data : typing . Any ): if isinstance ( data , str ): if \"=\" in data : data = [ data ] else : data = [ f \"format= { data } \" ] if isinstance ( data , typing . Iterable ): data = list ( data ) if len ( data ) == 1 and isinstance ( data [ 0 ], str ) and \"=\" not in data [ 0 ]: data = [ f \"format= { data [ 0 ] } \" ] output_details_dict = dict_from_cli_args ( * data ) else : raise TypeError ( f \"Can't parse output detail config: invalid input type ' { type ( data ) } '.\" ) output_details = OutputDetails ( ** output_details_dict ) return output_details format : str = Field ( description = \"The output format.\" ) target : str = Field ( description = \"The output target.\" ) config : typing . Dict [ str , typing . Any ] = Field ( description = \"Output configuration.\" , default_factory = dict ) @root_validator ( pre = True ) def _set_defaults ( cls , values ): target : str = values . pop ( \"target\" , \"terminal\" ) format : str = values . pop ( \"format\" , None ) if format is None : if target == \"terminal\" : format = \"terminal\" else : if target == \"file\" : format = \"json\" else : ext = target . split ( \".\" )[ - 1 ] if ext in [ \"yaml\" , \"json\" ]: format = ext else : format = \"json\" result = { \"format\" : format , \"target\" : target , \"config\" : dict ( values )} return result config : Dict [ str , Any ] pydantic-field \u00b6 Output configuration. format : str pydantic-field required \u00b6 The output format. target : str pydantic-field required \u00b6 The output target.","title":"output"},{"location":"reference/kiara/utils/output/#kiara.utils.output.OutputDetails","text":"Source code in kiara/utils/output.py class OutputDetails ( BaseModel ): @classmethod def from_data ( cls , data : typing . Any ): if isinstance ( data , str ): if \"=\" in data : data = [ data ] else : data = [ f \"format= { data } \" ] if isinstance ( data , typing . Iterable ): data = list ( data ) if len ( data ) == 1 and isinstance ( data [ 0 ], str ) and \"=\" not in data [ 0 ]: data = [ f \"format= { data [ 0 ] } \" ] output_details_dict = dict_from_cli_args ( * data ) else : raise TypeError ( f \"Can't parse output detail config: invalid input type ' { type ( data ) } '.\" ) output_details = OutputDetails ( ** output_details_dict ) return output_details format : str = Field ( description = \"The output format.\" ) target : str = Field ( description = \"The output target.\" ) config : typing . Dict [ str , typing . Any ] = Field ( description = \"Output configuration.\" , default_factory = dict ) @root_validator ( pre = True ) def _set_defaults ( cls , values ): target : str = values . pop ( \"target\" , \"terminal\" ) format : str = values . pop ( \"format\" , None ) if format is None : if target == \"terminal\" : format = \"terminal\" else : if target == \"file\" : format = \"json\" else : ext = target . split ( \".\" )[ - 1 ] if ext in [ \"yaml\" , \"json\" ]: format = ext else : format = \"json\" result = { \"format\" : format , \"target\" : target , \"config\" : dict ( values )} return result","title":"OutputDetails"},{"location":"reference/kiara/utils/output/#kiara.utils.output.OutputDetails.config","text":"Output configuration.","title":"config"},{"location":"reference/kiara/utils/output/#kiara.utils.output.OutputDetails.format","text":"The output format.","title":"format"},{"location":"reference/kiara/utils/output/#kiara.utils.output.OutputDetails.target","text":"The output target.","title":"target"},{"location":"reference/kiara/workflow/__init__/","text":"kiara_workflow \u00b6 KiaraWorkflow \u00b6 A thin wrapper class around a PipelineModule , mostly handling initialization from simplified configuration data. Source code in kiara/workflow/kiara_workflow.py class KiaraWorkflow ( object ): \"\"\"A thin wrapper class around a [PipelineModule][kiara.pipeline.PipelineModule], mostly handling initialization from simplified configuration data.\"\"\" def __init__ ( self , workflow_id : str , config : ModuleConfig , kiara : \"Kiara\" , controller : typing . Optional [ PipelineController ] = None , ): self . _controller : typing . Optional [ PipelineController ] = controller self . _workflow_id : str = workflow_id self . _workflow_config : ModuleConfig = config self . _kiara : Kiara = kiara root_module_args : typing . Dict [ str , typing . Any ] = { \"id\" : self . _workflow_id } if self . _workflow_config . module_type == \"pipeline\" : root_module_args [ \"module_type\" ] = \"pipeline\" root_module_args [ \"module_config\" ] = self . _workflow_config . module_config elif self . _kiara . is_pipeline_module ( self . _workflow_config . module_type ): root_module_args [ \"module_type\" ] = self . _workflow_config . module_type root_module_args [ \"module_config\" ] = self . _workflow_config . module_config else : # means it's a python module, and we wrap it into a single-module pipeline root_module_args [ \"module_type\" ] = \"pipeline\" steps_conf = { \"steps\" : [ { \"module_type\" : self . _workflow_config . module_type , \"step_id\" : slugify ( self . _workflow_config . module_type , separator = \"_\" ), \"module_config\" : self . _workflow_config . module_config , } ], \"input_aliases\" : \"auto\" , \"output_aliases\" : \"auto\" , } root_module_args [ \"module_config\" ] = steps_conf self . _root_module : PipelineModule = self . _kiara . create_module ( ** root_module_args ) # type: ignore assert isinstance ( self . _root_module , PipelineModule ) self . _pipeline : typing . Optional [ Pipeline ] = None @property def structure ( self ) -> PipelineStructure : return self . _root_module . structure @property def pipeline ( self ) -> Pipeline : if self . _pipeline is None : self . _pipeline = Pipeline ( self . structure , controller = self . _controller ) return self . _pipeline @property def controller ( self ) -> PipelineController : return self . pipeline . controller @property def status ( self ) -> StepStatus : return self . pipeline . status @property def inputs ( self ) -> ValueSet : return self . pipeline . inputs @property def outputs ( self ) -> ValueSet : return self . pipeline . outputs def get_current_state ( self ) -> PipelineState : return self . pipeline . get_current_state () @property def current_state ( self ) -> PipelineState : return self . get_current_state () # @inputs.setter # def inputs(self, inputs: typing.Mapping[str, typing.Any]): # self.pipeline.set_pipeline_inputs(**inputs) @property def input_names ( self ) -> typing . List [ str ]: return list ( self . inputs . get_all_field_names ()) @property def output_names ( self ) -> typing . List [ str ]: return list ( self . outputs . get_all_field_names ()) @property def workflow_id ( self ) -> str : return self . _workflow_id @property def steps ( self ) -> StepsInfo : return self . pipeline . structure . to_details () . steps_info def __repr__ ( self ): return f \" { self . __class__ . __name__ } (workflow_id= { self . workflow_id } , root_module= { self . _root_module } )\" def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield f \"[b]Workflow: { self . workflow_id } [/b]\" doc = self . _root_module . get_type_metadata () . documentation . description if doc and doc != DEFAULT_NO_DESC_VALUE : yield f \" \\n { doc } \\n \" table = Table ( box = box . SIMPLE , show_header = False ) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) doc_link = self . _root_module . get_type_metadata () . context . references . get ( \"documentation\" , None ) if doc_link : # TODO: use direct link url = doc_link . url module_str = f \"[link= { url } ] { self . _root_module . _module_type_id } [/link]\" # type: ignore else : module_str = self . _root_module . _module_type_id # type: ignore table . add_row ( \"root module\" , module_str ) table . add_row ( \"current status\" , self . status . name ) inputs_table = self . inputs . _create_rich_table ( show_headers = True ) table . add_row ( \"inputs\" , inputs_table ) outputs_table = self . outputs . _create_rich_table ( show_headers = True ) table . add_row ( \"outputs\" , outputs_table ) yield table","title":"workflow"},{"location":"reference/kiara/workflow/__init__/#kiara.workflow.kiara_workflow","text":"","title":"kiara_workflow"},{"location":"reference/kiara/workflow/__init__/#kiara.workflow.kiara_workflow.KiaraWorkflow","text":"A thin wrapper class around a PipelineModule , mostly handling initialization from simplified configuration data. Source code in kiara/workflow/kiara_workflow.py class KiaraWorkflow ( object ): \"\"\"A thin wrapper class around a [PipelineModule][kiara.pipeline.PipelineModule], mostly handling initialization from simplified configuration data.\"\"\" def __init__ ( self , workflow_id : str , config : ModuleConfig , kiara : \"Kiara\" , controller : typing . Optional [ PipelineController ] = None , ): self . _controller : typing . Optional [ PipelineController ] = controller self . _workflow_id : str = workflow_id self . _workflow_config : ModuleConfig = config self . _kiara : Kiara = kiara root_module_args : typing . Dict [ str , typing . Any ] = { \"id\" : self . _workflow_id } if self . _workflow_config . module_type == \"pipeline\" : root_module_args [ \"module_type\" ] = \"pipeline\" root_module_args [ \"module_config\" ] = self . _workflow_config . module_config elif self . _kiara . is_pipeline_module ( self . _workflow_config . module_type ): root_module_args [ \"module_type\" ] = self . _workflow_config . module_type root_module_args [ \"module_config\" ] = self . _workflow_config . module_config else : # means it's a python module, and we wrap it into a single-module pipeline root_module_args [ \"module_type\" ] = \"pipeline\" steps_conf = { \"steps\" : [ { \"module_type\" : self . _workflow_config . module_type , \"step_id\" : slugify ( self . _workflow_config . module_type , separator = \"_\" ), \"module_config\" : self . _workflow_config . module_config , } ], \"input_aliases\" : \"auto\" , \"output_aliases\" : \"auto\" , } root_module_args [ \"module_config\" ] = steps_conf self . _root_module : PipelineModule = self . _kiara . create_module ( ** root_module_args ) # type: ignore assert isinstance ( self . _root_module , PipelineModule ) self . _pipeline : typing . Optional [ Pipeline ] = None @property def structure ( self ) -> PipelineStructure : return self . _root_module . structure @property def pipeline ( self ) -> Pipeline : if self . _pipeline is None : self . _pipeline = Pipeline ( self . structure , controller = self . _controller ) return self . _pipeline @property def controller ( self ) -> PipelineController : return self . pipeline . controller @property def status ( self ) -> StepStatus : return self . pipeline . status @property def inputs ( self ) -> ValueSet : return self . pipeline . inputs @property def outputs ( self ) -> ValueSet : return self . pipeline . outputs def get_current_state ( self ) -> PipelineState : return self . pipeline . get_current_state () @property def current_state ( self ) -> PipelineState : return self . get_current_state () # @inputs.setter # def inputs(self, inputs: typing.Mapping[str, typing.Any]): # self.pipeline.set_pipeline_inputs(**inputs) @property def input_names ( self ) -> typing . List [ str ]: return list ( self . inputs . get_all_field_names ()) @property def output_names ( self ) -> typing . List [ str ]: return list ( self . outputs . get_all_field_names ()) @property def workflow_id ( self ) -> str : return self . _workflow_id @property def steps ( self ) -> StepsInfo : return self . pipeline . structure . to_details () . steps_info def __repr__ ( self ): return f \" { self . __class__ . __name__ } (workflow_id= { self . workflow_id } , root_module= { self . _root_module } )\" def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield f \"[b]Workflow: { self . workflow_id } [/b]\" doc = self . _root_module . get_type_metadata () . documentation . description if doc and doc != DEFAULT_NO_DESC_VALUE : yield f \" \\n { doc } \\n \" table = Table ( box = box . SIMPLE , show_header = False ) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) doc_link = self . _root_module . get_type_metadata () . context . references . get ( \"documentation\" , None ) if doc_link : # TODO: use direct link url = doc_link . url module_str = f \"[link= { url } ] { self . _root_module . _module_type_id } [/link]\" # type: ignore else : module_str = self . _root_module . _module_type_id # type: ignore table . add_row ( \"root module\" , module_str ) table . add_row ( \"current status\" , self . status . name ) inputs_table = self . inputs . _create_rich_table ( show_headers = True ) table . add_row ( \"inputs\" , inputs_table ) outputs_table = self . outputs . _create_rich_table ( show_headers = True ) table . add_row ( \"outputs\" , outputs_table ) yield table","title":"KiaraWorkflow"},{"location":"reference/kiara/workflow/kiara_workflow/","text":"KiaraWorkflow \u00b6 A thin wrapper class around a PipelineModule , mostly handling initialization from simplified configuration data. Source code in kiara/workflow/kiara_workflow.py class KiaraWorkflow ( object ): \"\"\"A thin wrapper class around a [PipelineModule][kiara.pipeline.PipelineModule], mostly handling initialization from simplified configuration data.\"\"\" def __init__ ( self , workflow_id : str , config : ModuleConfig , kiara : \"Kiara\" , controller : typing . Optional [ PipelineController ] = None , ): self . _controller : typing . Optional [ PipelineController ] = controller self . _workflow_id : str = workflow_id self . _workflow_config : ModuleConfig = config self . _kiara : Kiara = kiara root_module_args : typing . Dict [ str , typing . Any ] = { \"id\" : self . _workflow_id } if self . _workflow_config . module_type == \"pipeline\" : root_module_args [ \"module_type\" ] = \"pipeline\" root_module_args [ \"module_config\" ] = self . _workflow_config . module_config elif self . _kiara . is_pipeline_module ( self . _workflow_config . module_type ): root_module_args [ \"module_type\" ] = self . _workflow_config . module_type root_module_args [ \"module_config\" ] = self . _workflow_config . module_config else : # means it's a python module, and we wrap it into a single-module pipeline root_module_args [ \"module_type\" ] = \"pipeline\" steps_conf = { \"steps\" : [ { \"module_type\" : self . _workflow_config . module_type , \"step_id\" : slugify ( self . _workflow_config . module_type , separator = \"_\" ), \"module_config\" : self . _workflow_config . module_config , } ], \"input_aliases\" : \"auto\" , \"output_aliases\" : \"auto\" , } root_module_args [ \"module_config\" ] = steps_conf self . _root_module : PipelineModule = self . _kiara . create_module ( ** root_module_args ) # type: ignore assert isinstance ( self . _root_module , PipelineModule ) self . _pipeline : typing . Optional [ Pipeline ] = None @property def structure ( self ) -> PipelineStructure : return self . _root_module . structure @property def pipeline ( self ) -> Pipeline : if self . _pipeline is None : self . _pipeline = Pipeline ( self . structure , controller = self . _controller ) return self . _pipeline @property def controller ( self ) -> PipelineController : return self . pipeline . controller @property def status ( self ) -> StepStatus : return self . pipeline . status @property def inputs ( self ) -> ValueSet : return self . pipeline . inputs @property def outputs ( self ) -> ValueSet : return self . pipeline . outputs def get_current_state ( self ) -> PipelineState : return self . pipeline . get_current_state () @property def current_state ( self ) -> PipelineState : return self . get_current_state () # @inputs.setter # def inputs(self, inputs: typing.Mapping[str, typing.Any]): # self.pipeline.set_pipeline_inputs(**inputs) @property def input_names ( self ) -> typing . List [ str ]: return list ( self . inputs . get_all_field_names ()) @property def output_names ( self ) -> typing . List [ str ]: return list ( self . outputs . get_all_field_names ()) @property def workflow_id ( self ) -> str : return self . _workflow_id @property def steps ( self ) -> StepsInfo : return self . pipeline . structure . to_details () . steps_info def __repr__ ( self ): return f \" { self . __class__ . __name__ } (workflow_id= { self . workflow_id } , root_module= { self . _root_module } )\" def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield f \"[b]Workflow: { self . workflow_id } [/b]\" doc = self . _root_module . get_type_metadata () . documentation . description if doc and doc != DEFAULT_NO_DESC_VALUE : yield f \" \\n { doc } \\n \" table = Table ( box = box . SIMPLE , show_header = False ) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) doc_link = self . _root_module . get_type_metadata () . context . references . get ( \"documentation\" , None ) if doc_link : # TODO: use direct link url = doc_link . url module_str = f \"[link= { url } ] { self . _root_module . _module_type_id } [/link]\" # type: ignore else : module_str = self . _root_module . _module_type_id # type: ignore table . add_row ( \"root module\" , module_str ) table . add_row ( \"current status\" , self . status . name ) inputs_table = self . inputs . _create_rich_table ( show_headers = True ) table . add_row ( \"inputs\" , inputs_table ) outputs_table = self . outputs . _create_rich_table ( show_headers = True ) table . add_row ( \"outputs\" , outputs_table ) yield table","title":"kiara_workflow"},{"location":"reference/kiara/workflow/kiara_workflow/#kiara.workflow.kiara_workflow.KiaraWorkflow","text":"A thin wrapper class around a PipelineModule , mostly handling initialization from simplified configuration data. Source code in kiara/workflow/kiara_workflow.py class KiaraWorkflow ( object ): \"\"\"A thin wrapper class around a [PipelineModule][kiara.pipeline.PipelineModule], mostly handling initialization from simplified configuration data.\"\"\" def __init__ ( self , workflow_id : str , config : ModuleConfig , kiara : \"Kiara\" , controller : typing . Optional [ PipelineController ] = None , ): self . _controller : typing . Optional [ PipelineController ] = controller self . _workflow_id : str = workflow_id self . _workflow_config : ModuleConfig = config self . _kiara : Kiara = kiara root_module_args : typing . Dict [ str , typing . Any ] = { \"id\" : self . _workflow_id } if self . _workflow_config . module_type == \"pipeline\" : root_module_args [ \"module_type\" ] = \"pipeline\" root_module_args [ \"module_config\" ] = self . _workflow_config . module_config elif self . _kiara . is_pipeline_module ( self . _workflow_config . module_type ): root_module_args [ \"module_type\" ] = self . _workflow_config . module_type root_module_args [ \"module_config\" ] = self . _workflow_config . module_config else : # means it's a python module, and we wrap it into a single-module pipeline root_module_args [ \"module_type\" ] = \"pipeline\" steps_conf = { \"steps\" : [ { \"module_type\" : self . _workflow_config . module_type , \"step_id\" : slugify ( self . _workflow_config . module_type , separator = \"_\" ), \"module_config\" : self . _workflow_config . module_config , } ], \"input_aliases\" : \"auto\" , \"output_aliases\" : \"auto\" , } root_module_args [ \"module_config\" ] = steps_conf self . _root_module : PipelineModule = self . _kiara . create_module ( ** root_module_args ) # type: ignore assert isinstance ( self . _root_module , PipelineModule ) self . _pipeline : typing . Optional [ Pipeline ] = None @property def structure ( self ) -> PipelineStructure : return self . _root_module . structure @property def pipeline ( self ) -> Pipeline : if self . _pipeline is None : self . _pipeline = Pipeline ( self . structure , controller = self . _controller ) return self . _pipeline @property def controller ( self ) -> PipelineController : return self . pipeline . controller @property def status ( self ) -> StepStatus : return self . pipeline . status @property def inputs ( self ) -> ValueSet : return self . pipeline . inputs @property def outputs ( self ) -> ValueSet : return self . pipeline . outputs def get_current_state ( self ) -> PipelineState : return self . pipeline . get_current_state () @property def current_state ( self ) -> PipelineState : return self . get_current_state () # @inputs.setter # def inputs(self, inputs: typing.Mapping[str, typing.Any]): # self.pipeline.set_pipeline_inputs(**inputs) @property def input_names ( self ) -> typing . List [ str ]: return list ( self . inputs . get_all_field_names ()) @property def output_names ( self ) -> typing . List [ str ]: return list ( self . outputs . get_all_field_names ()) @property def workflow_id ( self ) -> str : return self . _workflow_id @property def steps ( self ) -> StepsInfo : return self . pipeline . structure . to_details () . steps_info def __repr__ ( self ): return f \" { self . __class__ . __name__ } (workflow_id= { self . workflow_id } , root_module= { self . _root_module } )\" def __rich_console__ ( self , console : Console , options : ConsoleOptions ) -> RenderResult : yield f \"[b]Workflow: { self . workflow_id } [/b]\" doc = self . _root_module . get_type_metadata () . documentation . description if doc and doc != DEFAULT_NO_DESC_VALUE : yield f \" \\n { doc } \\n \" table = Table ( box = box . SIMPLE , show_header = False ) table . add_column ( \"property\" , style = \"i\" ) table . add_column ( \"value\" ) doc_link = self . _root_module . get_type_metadata () . context . references . get ( \"documentation\" , None ) if doc_link : # TODO: use direct link url = doc_link . url module_str = f \"[link= { url } ] { self . _root_module . _module_type_id } [/link]\" # type: ignore else : module_str = self . _root_module . _module_type_id # type: ignore table . add_row ( \"root module\" , module_str ) table . add_row ( \"current status\" , self . status . name ) inputs_table = self . inputs . _create_rich_table ( show_headers = True ) table . add_row ( \"inputs\" , inputs_table ) outputs_table = self . outputs . _create_rich_table ( show_headers = True ) table . add_row ( \"outputs\" , outputs_table ) yield table","title":"KiaraWorkflow"},{"location":"reference/kiara/workflow/wrapped/","text":"","title":"wrapped"},{"location":"reference/mkdocstrings_handlers/kiara/__init__/","text":"","title":"kiara"},{"location":"value_types/","text":"xxxxxxx","title":"Index"},{"location":"value_types/SUMMARY/","text":"any deserialize_config load_config value_info value_lineage","title":"SUMMARY"},{"location":"value_types/any/","text":"any \u00b6 type_name any documentation Any type / No type information. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name AnyType module_name kiara.data.types.core full_name kiara.data.types.core.AnyType","title":"any"},{"location":"value_types/any/#any","text":"type_name any documentation Any type / No type information. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name AnyType module_name kiara.data.types.core full_name kiara.data.types.core.AnyType","title":"any"},{"location":"value_types/deserialize_config/","text":"deserialize_config \u00b6 type_name deserialize_config documentation A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name DeserializeConfigData module_name kiara.data.types.core full_name kiara.data.types.core.DeserializeConfigDa\u2026","title":"deserialize_config"},{"location":"value_types/deserialize_config/#deserialize_config","text":"type_name deserialize_config documentation A dictionary representing a configuration to deserialize a value. This contains all inputs necessary to re-create the value. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name DeserializeConfigData module_name kiara.data.types.core full_name kiara.data.types.core.DeserializeConfigDa\u2026","title":"deserialize_config"},{"location":"value_types/load_config/","text":"load_config \u00b6 type_name load_config documentation A dictionary representing load config for a kiara value. The load config schema itself can be looked up via the wrapper class '[LoadConfig][kiara.data.persistence.LoadConfig]'. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name LoadConfigData module_name kiara.data.types.core full_name kiara.data.types.core.LoadConfigData","title":"load_config"},{"location":"value_types/load_config/#load_config","text":"type_name load_config documentation A dictionary representing load config for a kiara value. The load config schema itself can be looked up via the wrapper class '[LoadConfig][kiara.data.persistence.LoadConfig]'. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name LoadConfigData module_name kiara.data.types.core full_name kiara.data.types.core.LoadConfigData","title":"load_config"},{"location":"value_types/value_info/","text":"value_info \u00b6 type_name value_info documentation A dictionary representing a kiara ValueInfo object. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name ValueInfoData module_name kiara.data.types.core full_name kiara.data.types.core.ValueInfoData","title":"value_info"},{"location":"value_types/value_info/#value_info","text":"type_name value_info documentation A dictionary representing a kiara ValueInfo object. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name ValueInfoData module_name kiara.data.types.core full_name kiara.data.types.core.ValueInfoData","title":"value_info"},{"location":"value_types/value_lineage/","text":"value_lineage \u00b6 type_name value_lineage documentation A dictionary representing a kiara ValueLineage. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name ValueLineageData module_name kiara.data.types.core full_name kiara.data.types.core.ValueLineageData","title":"value_lineage"},{"location":"value_types/value_lineage/#value_lineage","text":"type_name value_lineage documentation A dictionary representing a kiara ValueLineage. origin Authors Markus Binsteiner (markus@frkl.io) context Labels package : kiara References source_repo : https://github.com/DHARPA-Project/kiara documentation : https://dharpa.org/kiara_documentation/ python_class class_name ValueLineageData module_name kiara.data.types.core full_name kiara.data.types.core.ValueLineageData","title":"value_lineage"}]}